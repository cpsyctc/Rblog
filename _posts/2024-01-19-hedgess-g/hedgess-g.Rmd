---
title: "Hedges's g"
description: |
  A bit about Hedges's g!
author:
  - name: Chris Evans
    url: https://www.psyctc.org/R_blog/
    affiliation: PSYCTC.org
    affiliation_url: https://www.psyctc.org/psyctc/
    orcid_id: 0000-0002-4197-4202
date: 2024-01-19
categories:
  - Effect sizes
output:
  distill::distill_article:
    toc: true
    toc_depth: 4
    highlight_downlit: true
    self_contained: false
    code_folding: true
    includes: 
      in_header: ../../Gurgle.Rhtml
creative_commons: CC BY-SA
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 16, fig.height = 12, cache = TRUE)
library(tidyverse)
library(flextable)
library(janitor)
library(holodeck)  # for tidyverse sympathetic simulation
library(CECPfuns)
library(esc)

### set ggplot defaults
theme_set(theme_bw())
theme_update(plot.title = element_text(hjust = .5),
             plot.subtitle = element_text(hjust = .5),
             text = element_text(size = 24))
```
```{r, xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clone fa-2x\" style=\"color: #301e64\"></i>",
    success_text = "<i class=\"fa fa-check fa-2x\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times fa-2x\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```
This all started when I told a co-author, completely wrongly, that Hedges's *g* only applied for between groups effect estimation: oops.  In fact *g* corrects for bias in d: the tendency for *d* to overestimate population effect size particularly at smaller sample sizes.  That's true whether the *d* is a between groups *d* or a within subjects *d* (though there are several ways of getting *d* for within subjects effects: that's for another post).  

I have found the literature on this rather confusing and I think sometimes contradictory.  Here's where I am with this currently.  (I do hate it when I feel I don't understand something statistical correctly and might, as above, be giving misleading information.)

Clearly *g* can be seen as a correction to d, or as an alternative effect size measure.  In the psychological literature that I see it is usually described as a different effect size measure to Cohen's *d.*  That's perfectly fine and I don't think it really matters how you describe it.  I do think we should use *g* not *d* but I think people often say that *d* is wrong and that *g* should be used when the dataset size is such that the difference between two values is meaningless given all the other things likely to make generalisation suspect.

This breaks into three parts from here:

1. What is Cohen's *d*?  Are there two versions?

2. The relationship between Hedges's *g* and Cohen's *d* (including approximations and, what I thought was a mistake in Wikipedia).

3. Showing that Cohen's *d* does overestimate a population effect size.

4. This means that if you are reading articles and some use *d* and some use *g* you may want to convert from *d* to *g* to make all the values comparable.  For single values, assuming you have the *n* you can do that using one of [my shiny apps](https://shiny.psyctc.org/index.html). The specific one is [here](https://shiny.psyctc.org/apps/g_from_d_and_n/).

5. Footnotes: 

  + the differences between *d* and *g* are small at reasonable sample sizes.
  
  + the conversion from *d* to *g* is based on the Gaussian population model and there is a parametric confidence interval calculation for *g* ... but if you have raw data then bootstrapping *g* is surely better.  I may add something on this later.
   
# Starting point: what is Cohen's *d*?

There are lots of good explanations of this around the internet but for a two group, between group comparison it is simply the difference in the means between the two groups divided by the "common standard deviation". This is a very general idea about effect sizes: a effect, here the mean difference, is scaled in terms of a standard deviation.

As an equation that this:

$$ d = \frac{mean_{1} - mean_{2}}{SD_{pooled}} $$

and the common SD is the weighted average of the SDs. Actually, it's slightly more complicated than that, and this is where we come to there being two versions.

One defines the pooled SD as:

$$ SD_{pooled} = \sqrt{\frac{(n_{1}-1)SD_{1}^{2} + (n_{2}-1)SD_{2}^{2}}{n_{1} + n_{2} - 2}} $$

If the two groups are of the same size that collapses to this:

$$ SD_{pooled} = \sqrt{\frac{SD_{1}^{2} + SD_{2}^{2}}{2}} $$

i.e. the common standard deviation is the square root of the mean of the two variances.

The other definition has the pooled SD as 

$$ SD_{pooled} = \sqrt{\frac{(n_{1}-1)SD_{1}^{2} + (n_{2}-1)SD_{2}^{2}}{n_{1} + n_{2}}} $$

i.e. without subtracting 1 from each of the sample sizes.  This seems to be a rather messy area!

## A simple, single simulation

So what does this mean?  Here's an example of scores from two datasets, one *n* = 350, the other *n* = 900.

```{r illustration1, layout="l-body-outset"}
effectSize <- .5
sampSizes <- c(350, 900)
set.seed(1234)
tibble(group = 1:2) %>% # grouping variable
  mutate(sampSize = if_else(group == 1, sampSizes[1], sampSizes[2])) %>%
  rowwise() %>%
  mutate(obsID = list(1:sampSize)) %>%
  unnest(obsID) %>%  
  group_by(group) %>%
  sim_discr(n_vars = 1, var = 1, cov = 0, group_means = c(0, effectSize), name = "score") %>%
  ungroup() %>%
  mutate(facGroup = factor(group)) %>%
  rename(score = score_1) -> tibIllustration1

tibIllustration1 %>%
  group_by(facGroup) %>% 
  reframe(n = n(),
          CI = list(getBootCImean(score)),
          sd = sd(score)) %>%
  unnest_wider(CI) -> tibStats


vecColours <- c("1" = "blue", "2" = "red")
ggplot(data = tibIllustration1,
       aes(x = score, group = facGroup, fill = facGroup)) +
  geom_histogram(alpha = .6) +
  geom_vline(data = tibStats,
             aes(xintercept = obsmean,
                 colour = facGroup)) +
  scale_color_manual("Group",
                     values = vecColours) +
  scale_fill_manual("Group",
                     values = vecColours) +
  ggtitle("Histogram of two group illustration data",
          subtitle = "Vertical lines mark group mean scores")
```

Here are the sample statistics including the 95% confidence limits around the observed mean value in each group (LCLmean and UCLmean).

```{r groupStats}
tibStats %>%
  flextable() %>%
  autofit() %>%
  colformat_double(digits = 3)
```

Here are the same data as a violin plot with the means marked again.

```{r illustration2, layout="l-body-outset"}
tibIllustration1 %>%
  summarise(mean = mean(score)) %>%
  pull() -> tmpMean

ggplot(data = tibIllustration1,
       aes(x = facGroup, y = score, group = facGroup, fill = facGroup)) +
  geom_violin(scale = "count",
              alpha = .6) +
  geom_hline(yintercept = tmpMean,
             linetype = 3) +
  geom_linerange(data = tibStats,
                 inherit.aes = FALSE,
                 aes(x = facGroup, ymin = LCLmean, ymax = UCLmean)) +
  geom_point(data = tibStats,
                 inherit.aes = FALSE,
                 aes(x = facGroup, y = obsmean)) +
  xlab("Group") +
  scale_fill_manual("Group",
                     values = vecColours) +
  ggtitle("Violin plot of two group illustration data",
          subtitle = paste0("Horizontal reference line marks overall mean score",
                            "\nPoints and error bars mark means and 95% bootstrap CI(means)"))
```

The very tight 95% confidence intervals for the means there are clearly very different: this is a hugely statistically significant difference.  Here are the sample statistics.

```{r illustration3, layout="l-body-outset"}
tibStats %>%
  select(-c(LCLmean, UCLmean)) %>%
  rename(mean = obsmean) %>%
  flextable() %>%
  colformat_double(digits = 3)

getCommonSD <- function(SD1, SD2, n1, n2){
  numerator <- (n1 - 1)*SD1^2 + (n2 - 1)* SD2^2
  sqrt(numerator / (n1 + n2 - 2))
}
  
tibStats %>% 
  select(-c(LCLmean, UCLmean)) %>%
  rename(group = facGroup) %>%
  rename(mean = obsmean) %>%
  pivot_wider(names_from = group, values_from = c(mean, sd, n)) %>%
  rowwise() %>%
  mutate(commonSD = getCommonSD(sd_1, sd_2, n_1, n_2),
         CohenD = (mean_2 - mean_1) / commonSD,
         HedgesG = hedges_g(CohenD, n_1 + n_2)) %>%
  ungroup() -> tibResults

tibResults %>% 
  select(CohenD) %>%
  pull() -> tmpCohenD

tibResults %>% 
  select(commonSD) %>%
  pull() -> tmpCommonSD

tibIllustration1 %>%
  summarise(SDall = sd(score)) %>%
  pull() -> tmpSDall

```

Of course we're not interested in the statistical significance of the difference in means, what we want to know is the effect size.  We can see that the SD for each group is pretty close to 1.0 (the population SDs as it happens).  The common SD is 
`r round(tmpCommonSD, 3)`.  This is *not* the SD if we pool across all the data from the two groups, that's quite a bit higher at
`r round(tmpSDall, 3)` because of the mean difference between the groups contributing to the SD across all the data.

The difference between the means is
`r round(tibResults$mean_2 - tibResults$mean_1, 3)` so Cohen's *d* is this divided by the common SD, it's 
`r round(tmpCohenD, 3)`.

I hope that gives a sense of Cohen's *d* and why it's a not unreasonable measure of effect size.  Now we come to Hedges's *g*.

# Showing that *d* overestimates the population effect size

```{r simulation1}
startTime <- proc.time()
effectSize <- .5
maxReps <- 10000
sampSizes <- 5:50 * 10
nSampSizes <- length(sampSizes)
set.seed(12345)
tibble(group = 1:2) %>% # grouping variable
  mutate(sampSize = list(sampSizes), # generate sample sizes
         repN = list(1:maxReps)) %>% # replications for each sample size
  unnest_longer(sampSize) %>%
  unnest_longer(repN) %>%
  rowwise() %>%
  mutate(obsID = list(1:sampSize)) %>%
  unnest(obsID) %>%  
  group_by(group) %>%
  sim_discr(n_vars = 1, var = 1, cov = 0, group_means = c(0, effectSize), name = "score") %>%
  rename(score = score_1) %>%
  pivot_wider(names_from = group, values_from = score, names_prefix = "scoreInGroup") %>% 
  # mutate(diff = scoreInGroup2 - scoreInGroup1) %>%
  group_by(sampSize, repN) %>% 
  reframe(n = n(),
          mean1 = mean(scoreInGroup1),
          sd1 = sd(scoreInGroup1),
          mean2 = mean(scoreInGroup2),
          sd2 = sd(scoreInGroup2)) %>%
  rowwise() %>%
  mutate(commonSD = sqrt((sd1^2 + sd2^2) / 2),
         CohenD = (mean2 - mean1) / commonSD,
         HedgesG = hedges_g(CohenD, 2 * sampSize)) %>%
  ungroup() -> tibResults
endTime <- proc.time()
elapsedTime <- endTime - startTime
# elapsedTime
```

What I have done is to generate 
`r maxReps` two equal sized samples of size from from 50 to 500 (in steps of 10).  I sampled from Gaussian populations where the SD is 1 for both groups and the mean score of the first group is zero and for the other is 
`r round(effectSize, 1)`, so the population effect size whose formula is 

$$ \delta = \frac{\mu_{1} - \mu_{2}}{\sigma} $$

with the lower case Greek letters marking that this is the population not the sample and \(\delta\) meaning the population effect size, \(\mu\) indicating a population mean and \(\sigma\) the population standard deviation which is the same for both samples here.  So the population effect size here is 
`r round(effectSize, 1)` for all samples.  Using Gaussian distributions makes the simulation easy but is not vital for the effect size.

Across all 
`r prettyNum(maxReps, big.mark = ",")`
replications at each sample size, i.e. a across a grand total of 
`r prettyNum(maxReps * nSampSizes, big.mark = ",")` simulated samples we see these summary statistics.

```{r simulation2, layout="l-body-outset"}
tibResults %>% 
  summarise(mean1 = mean(mean1), 
            mean2 = mean(mean2), 
            meanSD1 = mean(sd1), 
            meanSD2 = mean(sd2),
            meanCohenD = mean(CohenD),
            meanHedgesG = mean(HedgesG)) -> tibMeanStats

tibMeanStats %>% 
  pivot_longer(cols = everything()) %>%
  flextable() %>%
  colformat_double(digits = 4)
```

That shows that the simulation is working well with the means and SDs very close to what we want (thanks to the `sim_discr()` function from the R package holodeck). Summarising across so many sample sizes, including so many quite large ones makes the difference between *d* and *g* look meaningless there but when we show it against sample size the issue is clear.  Here the dotted horizontal line is the true population effect size.

```{r simulation3, layout="l-body-outset"}
tibResults %>%
  pivot_longer(cols = CohenD : HedgesG, names_to = "Measure") %>%
  mutate(Measure = case_when(
    Measure == "CohenD" ~ "Cohen's d",
    Measure == "HedgesG" ~ "Hedges's g")) -> tibResultsLong
```

```{r simulation4, layout="l-body-outset"}
suppressMessages(tibResultsLong %>%
                   group_by(sampSize, Measure) %>%
                   summarise(CI = list(getBootCImean(value))) %>%
                   unnest_wider(CI) %>%
                   ungroup() -> tibResCIs)
```

```{r simulation5, layout="l-body-outset"}
valXdodge <- 5
ggplot(data = tibResCIs,
       aes(x = sampSize, y = obsmean, colour = Measure, group = Measure)) +
  geom_point(position = position_dodge2(width = valXdodge)) +
  geom_line(position = position_dodge2(width = valXdodge)) +
  geom_hline(yintercept = .5,
             linetype = 3) +
  geom_linerange(aes(ymin = LCLmean, ymax = UCLmean),
                 position = position_dodge2(width = valXdodge)) +
  scale_x_continuous("Sample size",
                     breaks = seq(0, 500, 50)) +
  scale_y_continuous("Value of measure",
                     breaks = seq(.49, .51, .001)) +
  ggtitle("Cohen's d and Hedges's g against sample size",
          subtitle = paste0("Vertical error bars are 95% bootstrap CIs",
                            "\nPopulation effect size is  ",
                            effectSize,
                            ", ",
                            prettyNum(maxReps, big.mark = ","),
                            " replications for each sample size")) +
  labs(caption = "The different measures are nudged slightly to prevent overprinting")
```

That shows pretty clearly that *d* is overestimating the population effect size and much more so at the smaller sample sizes (per group sample sizes).  The values for the simulations for each sample size go up and down with a very similar profile because *g* is a near linear transform of *d.*   One important thing to notice about this is that as the sample sizes increase the differences between the two measures become very small which can be seen even more clearly in this table of the mean values for each sample size.

```{r simulation6}
tibResCIs %>%
  select(-c(LCLmean, UCLmean)) %>%
  pivot_wider(id_cols = sampSize, names_from = Measure, values_from = obsmean) %>%
  mutate(sampSize = as.integer(sampSize)) %>%
  flextable() %>%
  colformat_double(digits = 3)
```

# So what is the transform that converts a Cohen's *d* value to a Hedges's *g* value?

Hedges's g, which was first presented in:

Hedges, L. V. (1981). Distribution Theory for Glass’s Estimator of Effect size and Related Estimators. Journal of Educational Statistics, 6(2), 107–128. https://doi.org/10.3102/10769986006002107 

*is* a correction to Cohen's *d.*   As shown above, the correction is necessary because Cohen's *d* is a biased estimate of population effect size.  This is true whether the data are between group or within subject contrasts.  The bias decreases with increasing sample size and with quite large *n* the difference is completely trivial.  But what is the transform?  This took me down a bit of a rabbit hole!

Managing to get that paper off the internet (thanks JSTOR) started me on this adventure (rabbit hole, it ended up feeling very much the latter).

I read that paper, to the extent that I could follow it. I wasn't helped by the fact the paper looks as if it was typed on a good old monospaced typewriter.  That took me back.  I think I taught myself to touch type on one of these in 1981:
![Olympia E35](sg3.jpg)
Selling for £150 now apparently.  I think I paid £15 for mine and gave it away long ago!  Actually, the paper must have been typed on something more sophisticated than that but there are hand annotations.  

The accurate equation that corrects for the bias was shown by Hedges to be this:

![](gamma.png)

with the explanation:

![](m.png)

(I will come back to that but for now m is the combined sample size.)

By the miracles of modern LaTeX I can write that more elegantly as:

$$c(m) = \frac{\Gamma(\frac{m}{2})}{\sqrt{\frac{m}{2}}\Gamma(\frac{m-1}{2})}$$

and, still without understanding it let alone being able to follow Hedges's proof, I can convert into an R function:

```
getJ <- function(x){
  gamma(x / 2) / (sqrt(x / 2 ) * gamma((x - 1) / 2))
}
```

I have used "J" there as a lot of more recent papers and web pages seem to use where Hedges used "c(m")".

That function, with its gamma functions (no, I don't know what they are) is accurate but very computationally expensive so Hedges's original work in FORTRAN would have been slow on the then University of Chicago mainframe (IBM 360 and 370 machines would have been common university machines then with perhaps 32kb of memory).  

OK Christopher, enough technological history!

That led him to find an approximation that didn't need the gamma function and could be computed easily even on the hardware around then.  In the paper it is

![](approximation.png)

What puzzled me was that many things I saw had that as

$$ c(m) = 1 - \frac{3}{4m - 9} $$

with "- 9" instead of "- 1". Another discovery was that other things talked about a different approximation, down to Durlak or to Hunter & Schmidt:

$$ c(m) = \frac{m-3}{m-2.25}\sqrt{\frac{m-2}{m}} $$

so in R code:

```
getDurlakG <- function(G, totN) {
  G * ((totN - 3) / (totN - 2.25)) * sqrt((totN - 2) / totN)
}
```

So I decided that I had to do some simulating to see which approximation was best.

This code creates the simulations, each of two samples from Gaussian populations where the population effect sizes change across simulations as do the sample sizes and their asymmetry, i.e. whether the two samples are equal in size or different.

```{r simulate1}
set.seed(12345) # to get replicable outputs
nSims <- 5000 # to get good stability across replications of the simulations
procTime1 <- proc.time() # just checking timing
### this is my way of doing simulations now using pretty basic tidyverse
tibble(simulnN = list(1:nSims), # to create nSims separate simulations for each set of population parameters below
       popMean2 = list(c(0, seq(.1, .9, .2))), # create these mean differences (will be the same as the population effect sizes)
       n1 = list(seq(10, 60, 10)), # will simulate sizes of the first sample of 10, 20, 30, 40, 50 and 60
       asymm = list(seq(1:3))) %>% # I wanted to look both at equal sample sizes and "asymmetrical" sizes so this will make n2 = 2 or 3x n1 in what follows
  ### these lines just "unnest" those parameter values to create a very long tibble (dataset of 5000 * 5 * 6 * 3 = 450,000 rows one for each simulation)
  unnest_longer(simulnN) %>%
  unnest_longer(popMean2) %>%
  unnest_longer(n1) %>%
  unnest_longer(asymm) %>%
  ### now create n2 and totN (m in the text above)
  mutate(n2 = n1 * asymm,
         totN = n1 + n2) %>%
  ### go to rowwise do do things, well, ... rowwise (otherwise dplyr would try to handle each variable as a vector)
  rowwise() %>%
  mutate(samp1 = list(rnorm(n1, 0, 1)), # generate the first sample data
         samp2 = list(rnorm(n2, popMean2, 1)), # and the second
         ### get some sample statistics
         sampMean1 = mean(samp1),
         sampMean2 = mean(samp2),
         sampSD1 = sd(samp1),
         sampSD2 = sd(samp2)) %>%
  # ungroup() takes us out of rowwise processing and I export the tibble to tibSims
  ungroup() -> tibSims
### finding out how long that all took
procTime2 <- proc.time()
elapsed1 <- procTime2 - procTime1
```

Then this block of code uses those samples to get various statistics for each, including various ways of getting *g*.

```{r getStats1}
### define some functions to process all those simulations
getPooledSD <- function(n1, sd1, n2, sd2) {
  sqrt((((n1 - 1) * sd1 * sd1 ) + ((n2 - 1) * sd2 * sd2)) / (n1 + n2))
}
# getPooledSD(10, 1, 20, 5)

getPooledSD2 <- function(n1, sd1, n2, sd2) {
  sqrt((((n1 - 1) * sd1^2 ) + ((n2 - 1) * sd2^2)) / (n1 + n2 - 2))
}
# getPooledSD2(10, 1, 20, 5)

unadjG <- function(mean1, n1, sd1, mean2, n2, sd2) {
  (mean2 - mean1) / getPooledSD2(n1, sd1, n2, sd2)
}
# unadjG(0, 10, 1, 1, 20, 5)

getJ <- function(x){
  gamma(x / 2) / (sqrt(x / 2 ) * gamma((x - 1) / 2))
}
# getJ(2)

getDurlakG <- function(G, totN) {
  G * ((totN - 3) / (totN - 2.25)) * sqrt((totN - 2) / totN)
}

getHedgesG1 <- function(G, totN) {
  G * (1 - (3 / (4 * totN - 1)))
}

getHedgesG9 <- function(G, totN) {
  G * (1 - (3 / (4 * totN - 9)))
}

procTime3 <- proc.time()
tibSims %>% 
  rowwise() %>% # rowwise processing again
  mutate(pooledSD = getPooledSD(n1, sampSD1, n2, sampSD2),
         pooledSD2 = getPooledSD2(n1, sampSD1, n2, sampSD2),
         unadjG = unadjG(sampMean1, n1, sampSD1, sampMean2, n2, sampSD2), # get uncorrected g (not really g)
         compJG = getJ(totN - 2) * unadjG, # use the acccurate but computationally costly correction to that
         durlakG = getDurlakG(unadjG, totN), # get Durlak's correction
         hedgesG1 = getHedgesG1(unadjG, totN), # get Hedges's correction as per original paper
         hedgesG9 = getHedgesG9(unadjG, totN), # get the correction with 9 instead of 1
         DescToolsD = DescTools::CohenD(samp2, samp1)[1], # d from the DescTools package
         DescToolsG = DescTools::CohenD(samp2, samp1, correct = TRUE)[1], # g from ditto
         effsizeD = effsize::cohen.d(samp2, samp1)$estimate, # d from the effsize package
         effsizeG = effsize::cohen.d(samp2, samp1, hedges.correction = TRUE)$estimate, # g from ditto
         ### and finally g from the esc package
         escG = esc::hedges_g(DescToolsD, totN)) %>% 
  ### ungroup and save
  ungroup() -> tibStats
### timing again for my amusement
procTime4 <- proc.time()
elapsed2 <- procTime4 - procTime3
```

Here are the top 10 rows of tibStats (after sorting them into more sensible row order).  I am just using the raw way that R prints a tibble as it shows the list variables samp1 and samp2.

```{r sortTibStats}
tibStats %>%
  arrange(popMean2, n1, n2, simulnN) -> tibStats

tibStats
```

That's very messy but shows that `samp1` is a vector of, here, length 10 as n1 is 10.  I like this way that you can pack varying length vectors into a tibble.  (They are actually stored as lists so can actually be much more complex structures than simple vectors.)  This creates a very regular ("tidy") dataset but with great flexibility for things like simulation. 

This is a cleaner (thanks `flextable()`) and more interesting look showing the sample statistics from the first ten rows.

```{r tibStats1}
tibStats %>%
  filter(row_number() <= 10) %>% # just first ten rows
  select(simulnN : totN, sampMean1 : pooledSD2) %>%
  flextable() %>%
  autofit() %>%
  colformat_double(digits = 2)
```

That shows that the first ten simulations of the
`r nSims`
for n1 = n2 = 10 (hence asymm = 1) with those sample parameters and then the values of the sample statistics for each.  You can see that the values of those means and SDs are flayling around a lot as you would expect them to with such small samples.

Why `pooledSD` and `pooledSD2`?  This is about the issue noted much earlier about whether you compute the pooled SD as:

$$ SD_{pooled} = \sqrt{\frac{(n_{1}-1)SD_{1}^{2} + (n_{2}-1)SD_{2}^{2}}{n_{1} + n_{2} - 2}} $$

or as:

$$ SD_{pooled} = \sqrt{\frac{(n_{1}-1)SD_{1}^{2} + (n_{2}-1)SD_{2}^{2}}{n_{1} + n_{2}}} $$

You can see that `pooledSD2` is the first of those as it's always larger than `pooledSD`.  I didn't actually pursue this as I think most work on *g* uses the first equation.

Here is a first look at what that gives us.

```{r plotting1, fig.height=16, layout="l-body-outset"}
ggplot(data = filter(tibStats,
                     asymm == 1),
       aes(x = popMean2, y = escG, group = popMean2)) +
  facet_grid(rows = vars(n1)) +
  geom_violin() +
  geom_hline(yintercept = 0, 
             linetype = 3) +
  geom_point(aes(y = popMean2, group = popMean2),
             colour = "green") +
  ylab("Hedges's g") +
  scale_x_continuous("Population group mean difference",
                     breaks = c(0, seq(.1, .9, .2))) +
  ggtitle(str_c("Facetted violin plot of Hedges's g (from esc package) against population effect size",
                "\nfacetted by first sample size, symmetric case so n1 = n2)"),
          subtitle = "Green points mark population effect size, dotted horizontal line marks zero")
```

That looks as it should.  Reading the top facet first and left to right, the first violin (of the *g* values from
`r nSims`)
simulations looks from the green point to have mean *g* of zero as it should, its shape looks fairly Gaussian as I would have expected (not sure I can 100% justify that but plausible). Then going across that facet the mean observed *g* values go up and look to go up pretty much as they should from .1 to .9.  Reading down the facets it looks pretty clear that the scatter of the observed *g* values gets tighter as the sample sizes go up from 10 to 60, as of course they should. 

All reassuring so let's see we have answers to my questions from this.  So what are my questions at this point?


* Do the various ways of getting *d* and *g* from R packages give matching values?  (Should be a "yes"!)
* Which formula for the pooled SD are they using?
* Which correction formula are they using to get  *g*?
* Is the "- 1" in the original Hedges paper a typo which should have been "- 9"?
* Is the computationally expensive gamma formula for the population *g* getting very close to the true population value?
* How do the other correction formulae compare with each other and with the true population value?

## Do the various ways of getting *d* and *g* from R packages give matching values?  (Should be a "yes"!)

### *d*

Let's start with *d* (this post is really about *g* after all!)  The variable `unadjG` that I created is actually my computation of *d* (`unadjG` as it is the starting point to get *g* by transforming *d*).  This shows the mean squared differences between the variables broken down by the simulation parameters.

```{r q1a}
tibStats %>%
  mutate(DescToolsD_v_unadjG = (DescToolsD - unadjG)^2,
         DescToolsD_v_effsizeD = (DescToolsD - effsizeD)^2,
         effsizeD_v_unadjG = (effsizeD - unadjG)^2) %>%
  select(simulnN : totN, unadjG, DescToolsD, effsizeG, starts_with("DescToolsD_"), effsizeD_v_unadjG) -> tmpTib

tmpTib %>%
  group_by(popMean2, n2) %>%
  summarise(meanDescToolsD_v_unadjG = mean(DescToolsD_v_unadjG),
            meanDescToolsD_v_effsizeD = mean(DescToolsD_v_effsizeD),
            meanEffsizeD_v_unadjG = mean(effsizeD_v_unadjG)) %>%
  ungroup() -> tmpTib2

tmpTib2 %>% 
  mutate(meanDescToolsD_v_unadjG = formatC(meanDescToolsD_v_unadjG, format = "e", digits = 2),
         meanDescToolsD_v_effsizeD = formatC(meanDescToolsD_v_effsizeD, format = "e", digits = 2),
         meanEffsizeD_v_unadjG = formatC(meanEffsizeD_v_unadjG, format = "e", digits = 2)) %>%
  as_grouped_data(groups = "popMean2") %>%
  flextable()
```

Clearly `effsize::cohen.d()` is using the same method to get *d* as I have and `DescTools::CohenD()` is doing things slightly differently but we can see that the differences here are tiny.  R is using scientific notation there to squeeze things in. So the first row there for `meanDescToolsD_v_unadjG`, i.e. the mean squared difference between the result from `DescTools::CohenD()` and that from `effsize::cohen.d()` is the mean  across all the 
`r nSims` 
simulations for a zero population difference and both samples of size 10: "1.57e-33". In ordinary representation that is 0.000,000,000,000,000,000,000,000,000,000,001,572.  That is totally unimportant and differences like that can occur if different methods are used to do the same thing simply because computers don't work to infinite precision.

### *g*

```{r q1b}
tibStats %>%
  mutate(DescToolsG_v_effsizeG = (DescToolsG - effsizeG)^2,
         DescToolsG_v_escG = (DescToolsG - escG)^2,
         effsizeG_v_escG = (effsizeG - escG)^2) %>%
  select(simulnN : totN, unadjG, DescToolsG, effsizeG, escG, starts_with("DescToolsG_"), effsizeG_v_escG) -> tmpTib

tmpTib %>%
  group_by(popMean2, n2) %>%
  summarise(meanDescToolsG_v_effsizeG = mean(DescToolsG_v_effsizeG),
            meanDescToolsG_v_escG = mean(DescToolsG_v_escG),
            meanEffsizeG_v_escG = mean(effsizeG_v_escG)) %>%
  ungroup() -> tmpTib2

tmpTib2 %>% 
  mutate(meanDescToolsG_v_effsizeG = formatC(meanDescToolsG_v_effsizeG, format = "e", digits = 2),
         meanDescToolsG_v_escG = formatC(meanDescToolsG_v_escG, format = "e", digits = 2),
         meanEffsizeG_v_escG = formatC(meanEffsizeG_v_escG, format = "e", digits = 2)) %>%
  as_grouped_data(groups = "popMean2") %>%
  flextable()
```

OK, so they all agree for all simulations so `esc::hedges_g()`, gets the same values as `effsize::cohen.d()` (with "hedges.correction = TRUE") and the same as `DescTools::CohenD()` with "correct = TRUE") so they are all using the same formula if perhaps slightly different computation.

Looking at the code where I most easily could (`esc::hedges_g()` is the simplest) I could see that the formula they are using is:

$$ c(m) = 1 - \frac{3}{4m - 9} $$



## Which formula for the pooled SD are they using?

This is easy to answer now.  My code for `unadjG` was:
```
getPooledSD2 <- function(n1, sd1, n2, sd2) {
  sqrt((((n1 - 1) * sd1^2 ) + ((n2 - 1) * sd2^2)) / (n1 + n2 - 2))
}

unadjG <- function(mean1, n1, sd1, mean2, n2, sd2) {
  (mean2 - mean1) / getPooledSD2(n1, sd1, n2, sd2)
}
```

I was using 

$$ SD_{pooled} = \sqrt{\frac{(n_{1}-1)SD_{1}^{2} + (n_{2}-1)SD_{2}^{2}}{n_{1} + n_{2} - 2}} $$ 

and I got the same *d* as from the code in the DescTools and the effsize packages so that's what they are using.


## Which correction formula are they using to get  *g*?

I think we can start to answer this just looking at raw computed values from the different methods for the first ten simulations.

```{r q4a}
tibStats %>%
  filter(row_number() < 11) %>%
  select(simulnN : totN, compJG, durlakG, hedgesG1, hedgesG9, escG) %>%
  flextable()
```

Well we can see that `esc::hedges_g()` which created `escG` is getting the same as my 'hedgesG9".  As we know that `esc::hedges_g()` gets essentially the same *g* as from `effsize::cohen.d()` and from `DescTools::CohenD()` we can tell they are all using the 

$$ c(m) = 1 - \frac{3}{4m - 9} $$

equation.

We can see clearly that the `compJG` which is getting *g* using the computationally expensive gamma formula is giving slightly different results from those, but only slightly different but sufficiently different that we can see that's not the method they are using.  We can also see that they are not using the Durlak formula.

We can see that the results are flayling around the population value of zero, as you'd expect with a very small sample size of 10.


## Is the "- 1" in the original Hedges paper a typo which should have been "- 9"?

The findings in that last section are very clear: **Yes!**


## Is the computationally expensive gamma formula for the population *g* getting very close to the true population value?

This shows the mean squared deviation of those *g* values from the true population values, and also the "root mean squared" (RMS) deviation.

```{r q6a}
tibStats %>%
  select(simulnN : totN, compJG, durlakG, hedgesG9) %>%
  group_by(popMean2, n2) %>%
  summarise(meansq_dev_gamma = mean(popMean2 - compJG)^2,
            RMSgamma = sqrt(meansq_dev_gamma),
            meansq_dev_durlak = mean(popMean2 - durlakG)^2,
            RMSdurlak = sqrt(meansq_dev_durlak),
            meansq_dev_hedges9 = mean(popMean2 - hedgesG9)^2,
            RMShedges9 = sqrt(meansq_dev_hedges9)) %>%
  ungroup() -> tmpTib

tmpTib %>%
  select(popMean2, meansq_dev_gamma, RMSgamma) %>%
  as_grouped_data(groups = "popMean2") %>%
  flextable()
```

I am slightly surprised that these are not even tighter to the population values but it is clear that they are good enough estimation, we never need *g* to more than three decimal places.  I suspect also that these are sampling vagaries even across 
`r nSims`
simulations and that if I pushed the number of simulations up much higher the differences would be smaller.

## How do the other correction formulae compare with each other and with the true population value?

```{r q7a}
tmpTib %>%
  select(popMean2, starts_with("RMS")) %>%
  as_grouped_data(groups = "popMean2") %>%
  flextable()
```

Just the RMS values here to squash things in across the page.

Hm, neither the Durlak nor the Hedges approximations are markedly worse than the gamma estimates.

## Are the findings affected by unequal sample sizes?

```{r q8a, fig.height=16, layout="l-body-outset"}
getCImeanGauss <- function(x) {
  ### little function to get parametric 95% CI of the mean
  ### assumes complete data
  tmp <- t.test(x)
  return(list(obsMean = tmp$estimate,
              LCL = unlist(tmp$conf.int)[1],
              UCL = unlist(tmp$conf.int)[2]))
}

tibStats %>%
  select(simulnN : totN, compJG, durlakG, hedgesG9) %>%
  mutate(diffGamma = sqrt((popMean2 - compJG)^2),
         diffDurlak = sqrt((popMean2 - durlakG)^2),
         diffHedges9 = sqrt((popMean2 - hedgesG9)^2)) %>%
  group_by(asymm, n1, popMean2) %>%
  summarise(RMSGamma = list(getCImeanGauss(diffGamma)),
            RMSDurlak = list(getCImeanGauss(diffDurlak)),
            RMSHedges = list(getCImeanGauss(diffHedges9))) %>%
  ungroup() %>%
  unnest_wider(RMSGamma) %>%
  rename(meanGamma = obsMean,
         LCLgamma = LCL,
         UCLgamma = UCL) %>%
  unnest_wider(RMSDurlak) %>%
  rename(meanDurlak = obsMean,
         LCLDurlak = LCL,
         UCLDurlak = UCL) %>%
  unnest_wider(RMSHedges) %>%
  rename(meanHedges = obsMean,
         LCLHedges = LCL,
         UCLHedges = UCL) -> tmpTib

tmpTib %>%
  pivot_longer(cols = ends_with("Gamma")) %>%
  select(asymm, n1, popMean2, name, value) -> tmpTibGamma

tmpTib %>%
  pivot_longer(cols = ends_with("Durlak")) %>%
  select(asymm, n1, popMean2, name, value) -> tmpTibDurlak

tmpTib %>%
  pivot_longer(cols = ends_with("Hedges")) %>%
  select(asymm, n1, popMean2, name, value) -> tmpTibHedges

bind_rows(tmpTibGamma,
          tmpTibDurlak,
          tmpTibHedges) %>%
  mutate(transform = if_else(str_detect(name, fixed("amma")),
                             "Gamma",
                             "Hedges"),
         transform = if_else(str_detect(name, fixed("Durlak")),
                             "Durlak",
                             transform),
         statistic = if_else(str_detect(name, fixed("mean")),
                             "mean",
                             "UCL"),
         statistic = if_else(str_detect(name, fixed("LCL")),
                             "LCL",
                             statistic)) -> tmpTib2

tmpTib2 %>%
  select(-name) %>%
  pivot_wider(id_cols = c(asymm, n1, popMean2, transform),
              values_from = value,
              names_from = statistic) %>%
  rename(`transform:` = transform) -> tmpTib3


ggplot(data = tmpTib3,
       aes(x = popMean2, y = mean, colour = `transform:`)) +
  facet_grid(cols = vars(asymm),
             rows = vars(n1),
             labeller = label_both) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymin = LCL, ymax = UCL)) +
  ylab("RMS") +
  scale_x_continuous("Population effect size",
                     breaks = c(0, seq(.1, .9, .2))) +
  
  theme(legend.position = "bottom")
```

Hm, that surprised me.  Yes, at least for small samples asymmetry of sample sizes at least up to 3:1 *reduces* the RMS of the estimates's deviations from the population values.  That's clearly most marked for n1 = 10 (and hence n2 = 10, 20 or 30).  The other clear finding here is that the Hedges's estimator is such a good fit to the value from the gamma transformation that the two overprint one another.

Let's focus in on n1 = 10.  Here the parametric 95% confidence intervals for the RMS means get to be visible.

```{r q8b, fig.height=16, layout="l-body-outset"}
ggplot(data = filter(tmpTib3,
                     n1 == 10),
       aes(x = popMean2, y = mean, colour = `transform:`)) +
  facet_grid(cols = vars(asymm),
             labeller = label_both) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymin = LCL, ymax = UCL)) +
  ggtitle("RMS deviations from true population effect size, against population effect sizes",
          subtitle = "Facetted by asymmetry of sample sizes, first sample size = 10") +
  scale_x_continuous("Population effect size",
                     breaks = c(0, seq(.1, .9, .2))) +
  theme(legend.position = "bottom")
```

```{r saveImage}
### this was just to store all the data to avoid recomputing it
save.image(file = "simulateEffectSize.Rda")
```

# Conclusions

## (1) The differences between *d* and *g* are small at reasonable sample sizes

If we see ourselves as estimating population effects (and why not?!) then *d* is undoubtedly an upwardly biased estimator of the population effect size and *g* is much less so (or not).  However, the differences between *g* and *d* are tiny for the sorts of dataset sizes we tend to be using

In case you need to convert an observed value of *d* to *g* for any given *n*.  That is [here](https://shiny.psyctc.org/apps/g_from_d_and_n/) in my [shiny apps](https://shiny.psyctc.org/).

Given the sizes of the differences, even ignoring the issue about distribution shape (next note), I think that insisting that *d* is "wrong" tends to be "virtue claiming" rather than actually giving us more robust, accurate and replicable values when most MH/therapy datasets aren't random samples from defined infinite populations and their means, SDs and even distribution shapes are likely to be considerably affected by realities of the construction of the datasets than true sampling vagaries.  However, the fact remains that *d* overestimates population effect sizes and *g* doesn't so I think we should be using *g* but with humility!  (See the position of humility in the excellent Wagenmakers, E.-J., Sarafoglou, A., Aarts, S., Albers, C., Algermissen, J., Bahník, Š., van Dongen, N., Hoekstra, R., Moreau, D., van Ravenzwaaij, D., Sluga, A., Stanke, F., Tendeiro, J., & Aczel, B. (2021). Seven steps toward more transparency in statistical practice. Nature Human Behaviour, 5(11), 1473–1480. https://doi.org/10.1038/s41562-021-01211-8.)
  
## (2) Gaussian models

All this is based on Gaussian population models and those also give us a parametric confidence interval calculation for *g* that only needs the value of *g* and the *n*.  I may put a shiny app to give a CI around an observed *g* given the *n(.  That which may occasionally be useful if you find you can only get access to those numbers, not the raw data, e.g. when wanting to compare your *g* with a value given in a paper.  However, as our data in the MH/therapy realm may be far from Gaussian you are probably better off using bootstrapping to get a CI around your observed *g* if you have raw data.  I may add something on this later.

## (3) Similarly: heteroscedasticity

All this is based on simulations where the population standard deviations are the same in each group.  I think it is known that the adjustment formulae for *g* are not completely robust to heteroscedasticity: another reason to be wary and humble!

<center>Visit count<center>
<center>
<div id="sfc51z6k8zksc9fffg9btn14p556fue87zt"></div><script type="text/javascript" src="https://counter11.optistats.ovh/private/counter.js?c=51z6k8zksc9fffg9btn14p556fue87zt&down=async" async></script><br><a href="https://www.freecounterstat.com">counter widget</a><noscript><a href="https://www.freecounterstat.com" title="counter widget"><img src="https://counter11.optistats.ovh/private/freecounterstat.php?c=51z6k8zksc9fffg9btn14p556fue87zt" border="0" title="counter widget" alt="counter widget"></a></noscript>
</div></center>

# History

* *Major rewrite over the last week or so 12.viii.25.  Sorted out that the "- 9" formula is the correct one not as I had thought from the original paper's type "- 1".  Added much more extensive simulation and exploration of different issues.*
* *First created 28.v.25 (I think)*

## Last updated{.appendix}
```{r,echo=FALSE}
cat(paste(format(Sys.time(), "%d/%m/%Y"), "at", format(Sys.time(), "%H:%M")))
```