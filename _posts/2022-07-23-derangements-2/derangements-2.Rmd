---
title: "Derangements #2"
description: |
  Follows on from 'Scores from matching things'
author:
  - name: Chris Evans
    url: https://www.psyctc.org/R_blog/
    affiliation: PSYCTC.org
    affiliation_url: https://www.psyctc.org/psyctc/
    orcid_id: 0000-0002-4197-4202    
date: 2022-07-23
preview: pipeline_exported.png
output:
  distill::distill_article:
    toc: true
    toc_depth: 4
    hightlight_downlit: true
    self_contained: false
    code_folding: true
creative_commons: CC BY-SA
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(huxtable)
options(huxtable.knitr_output_format = "html") 
```
```{r createGraphic, echo=FALSE}
as_tibble(list(x = 1,
               y = 1)) -> tibDat

ggplot(data = tibDat,
       aes(x = x, y = y)) +
  geom_text(label = "Derangements #2",
            size = 20,
            colour = "red",
            angle = 30,
            lineheight = 1) +
  xlab("") +
  ylab("") +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()) 
```
*Updated to add [contact me](https://www.psyctc.org/psyctc/contact-me/) 11.ix.22*

In my last post here, [Scores from matching things](https://www.psyctc.org/Rblog/posts/2022-07-15-matching-scores/) I gave the background to the probabilities of achieving certain scores on matching tasks by chance alone to help explain the perhaps counter-intuitive finding that matching four or more things correctly is unlikely by chance alone at *p* < .05 *regardless of the number of objects to be matched*.

This just adds a bit more to that, mostly as plots and complements both that Rblog post and an "ordinary" blog post, [Sometimes n=4 is enough](https://www.psyctc.org/psyctc/2022/07/23/sometimes-n4-is-enough/).

What I wanted was to show how rapidly the probabilities of achieving any particular score stabilise to an asymptotic value as *n* increases.  Here we are for *n* from 4 to 15 and scores from 4 to 10.

```{r}
### create some functions (as in previous post)
all.derangements <- function(n){
	cumprob <- prob <- number <- term <- score <- rev(0:n)
	for (m in 1:n) {
		i <- m+1
		s <- n-m
		term[i] <- ((-1)^(m))/(factorial(m))
	}	
	term[1] <- 1
	for (i in 0:n) {
		s <- i+1
		prob[s] <- (sum(term[1:s]))/factorial(n-i)
	}
	number <- factorial(n)*prob
	for (s in 0:n) {
		m <- n-s
		i <- m+1
		cumprob[i] <- sum(prob[1:i])
	}
	tmp <- cbind(n, score,number,prob,cumprob)
	tmp
}

p.derange.score <- function(score,n){
	if (score > n) stop("Score cannot be greater than n")
	if (score == (n-1)) stop ("Score cannot be n-1")
	cumprob <- prob <- term <- rev(0:n)
	for (m in 1:n) {
		i <- m+1
		s <- n-m
		term[i] <- ((-1)^(m))/(factorial(m))
	}	
	term[1] <- 1
	for (i in 0:n) {
		s <- i+1
		prob[s] <- (sum(term[1:s]))/factorial(n-i)
	}
	for (s in 0:n) {
		m <- n-s
		i <- m+1		
		cumprob[i] <- sum(prob[1:i])
	}
	cumprob[n+1-score]
}

### now let's go a bit further
### get all the possible scores for n from 4 to 30
lapply(4:30, FUN = all.derangements) -> tmpList
### I always forget this nice little bit of base R and I'm a bit surprised that there doesn't seem to be a nice tidyverse alternative
do.call(rbind.data.frame, tmpList) %>%
  as_tibble() -> tmpTib
### this was just to produce some tables for my blog post at https://www.psyctc.org/psyctc/2022/07/23/sometimes-n4-is-enough/
# tmpTib %>% 
#   write_csv(file = "derangements.csv")

### ditto
# 1:14 %>% 
#   as_tibble() %>%
#   rename(n = value) %>%
#   mutate(PossibleWays = factorial(n),
#          PossibleWays = prettyNum(PossibleWays, big.mark = ",")) %>%
#   write_csv(file = "numbers.csv")

### but Vectorizing the function seemed cleaner so ...
Vectorize(FUN = all.derangements) -> All.derangements
All.derangements(4:14) -> tmpList
### back to the do.call() just for the tables 
# do.call(rbind, tmpList) %>%
#   as_tibble() %>%
#   filter(score == 4) %>%
#   select(n, number) %>%
#   mutate(number = prettyNum(number, big.mark = ",")) %>%
#   write_csv("correct.csv")

# do.call(rbind, tmpList) %>%
#   as_tibble() %>%
#   filter(score == 4) %>%
#   mutate(totalPerms = factorial(n)) %>%
#   select(-prob) %>%
#   select(n, totalPerms, everything()) %>%
#   write_csv("final.csv")
  

### OK, now for this Rblog post!
All.derangements(4:15) -> tmpList
do.call(rbind, tmpList) %>%
  as_tibble() %>%
  filter(score > 3 & score < 11) %>%
  mutate(score = ordered(score,
                         levels = 4:10)) -> tmpTib

ggplot(data = tmpTib,
       aes(x = n, y = cumprob, colour = score, group = score)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 3:15,
                     minor_breaks = 3:15,
                     limits = c(3, 15)) +
  ylab("p") +
  theme_bw() 
```

Here's the same on a log10 y axis to separate the p values for the higher scores.

```{r logplot}
ggplot(data = tmpTib,
       aes(x = n, y = cumprob, colour = score, group = score)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 3:15,
                     minor_breaks = 3:15,
                     limits = c(3, 15)) +
  scale_y_continuous(trans = "log10") +
  ylab("p") +
  theme_bw() 
```

This next table shows how rapidly p values of real interest stabilise.  The table is ordered by number of objects (n) within score.  The column p is the probability of getting that score or better by chance alone, diffProb is the absolute change in that p value from the one for the previous n, diffPerc is the difference as a percentage of the previous p value.  diffProbLT001 flags when the change in absolute p vaue is below .001 at which point I think in my realm any further precision is spurious.  However, diffLT1pct flags when the change in p value is below 1% of the previous p value just in case someone wants that sort precise convergence.

```{r huxtable}
All.derangements(4:20) -> tmpList
do.call(rbind, tmpList) %>%
  as_tibble() %>%
  filter(score > 3 & score < 11)  -> tmpTib


### just working out how stable the p values get how soon
tmpTib %>%
  arrange(score) %>%
  group_by(score) %>%
  mutate(diffProb = abs(cumprob - lag(cumprob)),
         diffProbLT001 = if_else(diffProb < .001, "Y", "N"),
         diffPerc = 100 * diffProb /lag(cumprob),
         diffLT1pct = if_else(diffPerc < 1, "Y", "N")) %>% 
  ungroup() %>%
  select(-c(number, prob)) %>%
  rename(p = cumprob) %>%
  select(score, everything()) %>%
  as_hux() %>%
  set_position("left") %>% # left align the whole table
  set_bold(row = everywhere, col = everywhere) %>% # everything into bold
  set_align(everywhere, everywhere, "center") %>% # everything centred
  set_align(everywhere, 1:2, "right") %>% # but now right justify the first two columns
  map_text_color(by_values("Y" = "green")) %>% # colour matches by text recognition
  map_text_color(by_values("N" = "red"))
```

Do [contact me](https://www.psyctc.org/psyctc/contact-me/) if this interests you and if you might want to use the method with real data.