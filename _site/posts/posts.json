[
  {
    "path": "posts/2023-11-27-cispearman/",
    "title": "Confidence interval around Spearman correlation coefficient",
    "description": "Explores four methods of estimating a CI around an observed Spearman correlation.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-11-27",
    "categories": [
      "CECPfuns package",
      "Correlation",
      "Non-parametric statistics",
      "Rank methods"
    ],
    "contents": "\n\nContents\nShiny app\nThis post\nSummary\nGeeky stuff\nAcknowledgements\n\nShiny app\nThere is now a shiny app using this function here.\nThis post\nUntil last week (November 2023!) if I wanted a confidence interval (CI) for a Spearman correlation given the n that created it I used an analytic, parametric method which is actually for the Pearson correlation and I think I was not alone in doing that. However, I have just discovered that there are better approaches and that work on them goes back to the mid 20th Century. Ooops.\nThey are all approximations that drew on parametric models and if you have raw data, you are far better off using a bootstrap method to get your CI. However, if you are looking at a Spearman correlation in the literature and only have that and the n then you can get a CI but you seem to have a choice of four ways of doing this.\nSo here are the 95% CIs for observed correlations of zero, .5 and .8 for n from 8 to 100.\n\n\nShow code\n\ntibble(n = list(8:100), rs = list(.8, .5, 0)) %>%\n  unnest_longer(rs) %>%\n  unnest_longer(n) %>%\n  rowwise() %>%\n  mutate(CIGBW = list(getCISpearman(rs, n, Gaussian = TRUE, FHP = FALSE)),\n         CItBW = list(getCISpearman(rs, n, Gaussian = FALSE, FHP = FALSE)),\n         CIGFieller = list(getCISpearman(rs, n, Gaussian = TRUE, FHP = TRUE)),\n         CItFieller = list(getCISpearman(rs, n, Gaussian = FALSE, FHP = TRUE))) %>%\n  ungroup() %>%\n  unnest_longer(col = starts_with(\"CI\"), simplify = FALSE) %>%\n  rename(limit = CIGBW_id) %>%\n  select(-ends_with(\"_id\")) %>%\n  select(n, rs, limit, everything()) -> tibCLs\n\n# tibCLs %>%\n#   mutate(diffGauss = CIGBW - CIGFieller,\n#          sqDiffGauss = diffGauss^2,\n#          difft = CItBW - CItFieller,\n#          sqDifft = difft^2) %>%\n#   summarise(across(diffGauss : sqDifft, mean))\n\ntibCLs %>%\n  pivot_longer(cols = starts_with(\"CI\")) %>%\n  rename(method = name) %>%\n  mutate(Method = case_when(\n    method == \"CIGBW\" ~ \"B&W, Gaussian\",\n    method == \"CItBW\" ~ \"B&W, t dist.\",\n    method == \"CIGFieller\" ~ \"FH&P, Gaussian\",\n    method == \"CItFieller\" ~ \"FH&P, t dist.\"),\n    approx = if_else(str_detect(Method, fixed(\"B&W\")), \"B&W\", \"FH&P\"),\n    distribn = if_else(str_detect(Method, fixed(\"Gaussian\")), \"Gaussian\", \"t dist.\")) -> tibCLsLong\n\nggplot(data = tibCLsLong,\n       aes(x = n, y = value, colour = method, shape = method))  +\n  facet_grid(rows = vars(rs)) +\n  geom_point(size = .4) +\n  geom_hline(aes(yintercept = rs)) +\n  ylim(c(-1, 1))\n\n\n\nThat illustrates several things:\nOf course the CI is symmetrical around zero for the null case of no correlation but it is asymmetrical around the observed correlation for non-zero observed correlations, more so the stronger the correlation.\nThe methods differ in the widths of their CIs more for smaller n and for stronger correlations.\nBy n = 100 the differences between the methods are negligible.\nIt looks as if the differences between the methods are consistent across the values for n within each observed correlation and confidence limit (i.e. LCL or UCL) but it’s hard to eyeball that from the plot.\nSo here is the same plot but just for n from 8 to 15.\n\n\nShow code\n\ntibCLsLong %>%\n  filter(n <= 15) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = n, y = value, colour = method, shape = method))  +\n  facet_grid(rows = vars(rs)) +\n  geom_point(size = .4) +\n  geom_hline(aes(yintercept = rs)) +\n  ylim(c(-.85, 1))\n\n\n\nThe four methods involve two different changes: the approximation to the standard error of the correlation and whether to look that up against the Gaussian distribution or against the t distribution with df = n - 2. The next two plots facet the issues in the two possible different orders.\nThis first plot pulls the issue of looking up against the Gaussian or t distributions to the columns in the facetted plot allowing a clearer comparison of the two approximations to the SE of the correlation.\n\n\nShow code\n\nggplot(data = tibCLsLong,\n       aes(x = n, y = value, colour = approx))  +\n  facet_grid(rows = vars(rs),\n             cols = vars(distribn)) +\n  geom_point(size = .4) +\n  geom_hline(aes(yintercept = rs)) +\n  ylim(c(-1, 1))\n\n\n\nThat seems to show fairly clearly that for the zero correlation case the B&W, i.e. the Bonett & White (2000) approximation gives slightly tighter CIs than the FH&P, i.e. the Fieller, Hartley & Pearson (1957) one but that the approximations have the reverse relationship with CI widths for the non-zero correlations.\nThis next plot makes it easy to look at how the choice of lookup, against the Gaussian or the t distributions affects the CIs.\n\n\nShow code\n\nggplot(data = tibCLsLong,\n       aes(x = n, y = value, colour = distribn))  +\n  facet_grid(rows = vars(rs),\n             cols = vars(approx)) +\n  geom_point(size = .4) +\n  geom_hline(aes(yintercept = rs)) +\n  ylim(c(-1, 1))\n\n\n\nSo the Gaussian method gives tighter CIs. However, simulation work I think suggests that these are slightly too tight and that the t distribution method gives coverage closer to 95% in simulations.\nSummary\nThe getCISpearman() function from the CECPfuns package gives you the four methods of getting a CI around an observed Spearman correlation coefficient.\nI have set the default to getCISpearman(rs, n, ci = .95, Gaussian = FALSE, FHP = FALSE) which gets you the Bonett & White (2000) approximation to the SE of the Spearman correlation rather than the Fieller, Hartley & Pearson (1957) method and gets the coverage by looking up the desired coverage (default .95, i.e. 95%) against the t distribution rather than the Gaussian.\nHowever, the differences between any of the four possible methods depend on the observed correlation and are trivial for any sensible therapy/MH data and questions for n >= 25 and the methods are known to be very approximate for n < 25.\nI haven’t explored this here but the methods known to have poor coverage accuracy as the absolute population correlation goes above .95 so it’s wise to interpret CIs very cautiously for observed correlations above .9.\nHowever, there is evidence from simulations that Pearson correlation CIs are highly sensitive to deviations from bivariate Gaussian population distributions so when data are either clearly not Gaussian or where linear measurement is probably not present, use of these CI estimates for rank correlation methods like the Spearman correlation may be more robust than simple parametric CIs for the Pearson correlation.\nHowever (again!) if you have raw data then bca (“bias correction and acceleration”) bootstrap estimation of a CI around either the observed Pearson or Spearman correlation (depending on your interests and belief about linearity of measurement) will be more robust than these CI estimates that only use the observed correlation and n.\nGeeky stuff\nThis compares the ordering of the CLs from the four methods. It’s pretty geeky as realistically the differences don’t matter for typical therapy/MH work but I have kept this because it interests me and I like the use of order() in the code, but this is angels on the head of a pin stuff!\n\n\nShow code\n\ntibCLs %>% \n  filter(limit == \"LCL\") %>%\n  filter(n <= 10) %>%\n  rowwise() %>%\n  arrange(rs, n, limit) %>%\n  mutate(rs = sprintf(\"%3.1f\", rs)) %>%\n  select(rs, n, limit, CItFieller, CItBW, CIGFieller, CIGBW) %>%\n  as_grouped_data(groups = \"rs\") %>%\n  flextable() %>%\n  colformat_double(digits = 4)\n\nrsnlimitCItFiellerCItBWCIGFiellerCIGBW0.08LCL-0.8099-0.7984-0.7175-0.70479LCL-0.7590-0.7467-0.6771-0.664110LCL-0.7150-0.7022-0.6427-0.62960.58LCL-0.5207-0.5451-0.3391-0.36309LCL-0.4174-0.4419-0.2678-0.290710LCL-0.3346-0.3585-0.2102-0.23210.88LCL-0.0280-0.15730.19370.09139LCL0.1043-0.01050.26810.177410LCL0.19860.09690.32380.2426\n\nThat shows that the ordering of the lower CLs is not consistent across observed correlations even for that limited range of n. Here’s the same for the UCL.\n\n\nShow code\n\ntibCLs %>% \n  filter(limit == \"UCL\") %>%\n  filter(n <= 10) %>%\n  rowwise() %>%\n  arrange(rs, n, limit) %>%\n  mutate(rs = sprintf(\"%3.1f\", rs)) %>%\n  select(rs, n, limit, CItFieller, CItBW, CIGFieller, CIGBW) %>%\n  as_grouped_data(groups = \"rs\") %>%\n  flextable() %>%\n  colformat_double(digits = 4)\n\nrsnlimitCItFiellerCItBWCIGFiellerCIGBW0.08UCL0.80990.79840.71750.70479UCL0.75900.74670.67710.664110UCL0.71500.70220.64270.62960.58UCL0.93230.93660.89600.90139UCL0.91270.91750.87940.884910UCL0.89500.90030.86480.87050.88UCL0.97690.98220.96410.97089UCL0.97000.97610.95810.965310UCL0.96370.97050.95280.9603\n\nJust to confirm that I turned those limits into rank order across the four methods as shown here for the LCLs and n from 8 to 15.\n\n\nShow code\n\ntibCLsLong %>%\n  filter(limit == \"LCL\") %>%\n  filter(n < 16) %>%\n  group_by(rs, n) %>%\n  mutate(order = order(value)) %>%\n  ungroup() %>%\n  select(-c(value, Method, approx, distribn)) %>%\n  # mutate(rowN = row_number()) %>%\n  pivot_wider(names_from = \"method\", values_from = \"order\") %>%\n  mutate(rs = sprintf(\"%3.1f\", rs)) %>%\n  as_grouped_data(groups = \"rs\") %>%\n  flextable()\n\nrsnlimitCIGBWCItBWCIGFiellerCItFieller0.88LCL24139LCL241310LCL241311LCL241312LCL241313LCL241314LCL214315LCL21430.58LCL24139LCL241310LCL241311LCL241312LCL241313LCL241314LCL241315LCL24130.08LCL42319LCL423110LCL423111LCL423112LCL423113LCL423114LCL423115LCL4231\n\nSome of the differences in the LCL values that led to those orderings may have been tiny but it’s clear that the methods aren’t consistently ordering the LCLs within a given observed correlation nor are the orders the same for the different observed correlations.\nAcknowledgements\nThis started from finding the excellent answers from onestop and retodomax\nto the question How to calculate a confidence interval for Spearman’s rank correlation? on CrossValidated. As referenced there …\nBishara, A. J., & Hittner, J. B. (2017). Confidence intervals for correlations when data are not normal. Behavior Research Methods, 49(1), 294–309. https://doi.org/10.3758/s13428-016-0702-8 gives extensive simulation work covering much more than these CIs. I checked my code against the results from the R code given in Supplement A to that paper. Then …\nBonett, D. G., & Wright, T. A. (2000). Sample size requirements for estimating pearson, kendall and spearman correlations. Psychometrika, 65(1), 23–28. https://doi.org/10.1007/BF02294183 is a classic (interesting to see how typesetting of equations has improved since 2000!) and …\nRuscio, J. (2008). Constructing Confidence Intervals for Spearman’s Rank Correlation with Ordinal Data: A Simulation Study Comparing Analytic and Bootstrap Methods. Journal of Modern Applied Statistical Methods, 7(2), 416–434. https://doi.org/10.22237/jmasm/1225512360 was another excellent paper on the topic.\nThanks to all those authors.\n\n\n\n",
    "preview": "posts/2023-11-27-cispearman/cispearman_files/figure-html5/plot1-1.png",
    "last_modified": "2023-11-27T22:14:26+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1344
  },
  {
    "path": "posts/2023-11-18-explore-distributions-with-plots/",
    "title": "Explore distributions with plots",
    "description": "Graphical exploration of fit between distributions and against theoretical distributions.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-11-18",
    "categories": [
      "R tricks",
      "R graphics",
      "distributions",
      "Null hypothesis tests (NHSTs)",
      "Gaussian distribution"
    ],
    "contents": "\n\nContents\nUsing the qq plot to diagnose types of misfit\nOrientation of the qq plot\n\nHeader graphic for the fun of it!\n\n\nShow code\n\n### just creating a graphic for the post\n### get a tibble with the density of the standard Gaussian distribution from -3 to 3\ntibble(x = seq(-3, 3, .05),\n       y = dnorm(x)) -> tibDnorm\n\n### now generate a tibble of n = 350 sampling from the standard Gaussian distribution\nset.seed(12345) # make it replicable\ntibble(x = rnorm(350)) -> tibRnorm\n\n# png(file = \"KStest.png\", type = \"cairo\", width = 6000, height = 4800, res = 300)\nggplot(data = tibRnorm,\n       aes(x = x)) +\n  geom_histogram(aes(x = x,\n                     after_stat(density)),\n                 bins = 20,\n                 alpha = .6) +\n  geom_line(data = tibDnorm,\n            inherit.aes = FALSE,\n            aes(x = x, y = y))  +\n  geom_vline(xintercept = 0) +\n  ### paramters work for knitted output\n  annotate(\"text\",\n           label = \"Distribution plots\",\n           x = -2.8,\n           y = .07,\n           colour = \"red\",\n           size = 19,\n           angle = 29,\n           hjust = 0)\n\n\nShow code\n\n# dev.off()\n\n### different sizing to get nice png\n# png(file = \"distPlots.png\", type = \"cairo\", width = 6000, height = 4800, res = 300)\n# ggplot(data = tibRnorm,\n#        aes(x = x)) +\n#   geom_histogram(aes(x = x,\n#                      after_stat(density)),\n#                  bins = 20,\n#                  alpha = .6) +\n#   geom_line(data = tibDnorm,\n#             inherit.aes = FALSE,\n#             aes(x = x, y = y))  +\n#   geom_vline(xintercept = 0) +\n#   ### changed parameters for the png\n#   annotate(\"text\",\n#            label = \"Dist. plots\",\n#            x = -2.5,\n#            y = .1,\n#            colour = \"red\",\n#            size = 95,\n#            angle = 32,\n#            hjust = 0)\n# dev.off()\n\n\nThis follows on from my recent post about the Kolmogorov-Smirnov test and then Tests of fit to Gaussian distribution. In many ways it would have been more logical to have started with this post as generally visual inspection should precede testing but I got down this rabbit hole from testing many distributions in simulations where visual inspection wasn’t realistically possible. All three of these posts link with and expand upon entries in my online glossary for the OMbook.\nOK, that header graphic with the theoretical density curve for the Gaussian distribution superimposed on a histogram is probably the classic graphical exploration of distribution and I grew up on this in SPSS and I still like it as it’s easy to understand. However, it’s a very poor real test of fit betwen distributions as our visual system simply doesn’t do similarity of arbitrary curves well. (There may be cultural aspects to this as my “we” is a global north one growing up in very rectilinear spaces, sorry if that’s underselling the abilities of people growing up in spaces with few straight lines to compare curves. Some old but I suspect still valid transcultural studies of perception of perspective and distance coming back to me from student days!)\nThe next sensible graphic to use when looking at distributions is the ecdf (empirical cumulative distribution function).\n\n\nShow code\n\nggplot(data = tibRnorm,\n       aes(x = x)) +\n  stat_ecdf() +\n  ylab(\"ecdf = cumulative proportion of scores below x value\") +\n  xlab(\"x = scores\") +\n  ggtitle(\"An ecdf of the same simulation data as shown in the header image\",\n          subtitle = \"350 points sampled from a true Gaussian distribution\")\n\n\n\nThis next ecdf shows the empirical plot for our data in black and the expected curve from a truly Gaussian distribution of the same size. It’s getting easier to assess fit.\n\n\nShow code\n\ntibble(x = seq(-3, 3, .05),\n       y = pnorm(x)) -> tibPnorm\n\n\nggplot(data = tibRnorm,\n       aes(x = x)) +\n  geom_point(data = tibPnorm,\n             aes(x = x, y = y),\n            colour = \"green\") +\n  stat_ecdf() +\n  ylab(\"ecdf = cumulative proportion of scores below x value\") +\n  xlab(\"x = scores\") +\n  ggtitle(\"An ecdf of the same simulation data as shown in the header image\",\n          subtitle = \"350 points sampled from a true Gaussian distribution\")\n\n\n\nIn fact, this next plot shows how the maximum discrepancy between those two curves is the misfit criterion in the Kolmogorov-Smirnov test. Back to this post for all about that.\nK-S test exampleHowever, there’s an even better way to compare distributions: the qq plot.\n\n\nShow code\n\nggplot(data = tibRnorm,\n       aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line() +\n  ylab(paste0(\"Observed values for same quantile\",\n              \"\\nas for perfect Gaussian score (x-axis)\")) +\n  xlab(paste0(\"Values for the true Gaussian distribution\",\n              \"\\nfor same quantile as for observed point on y-axis\")) +\n  ggtitle(\"A qq plot of the same simulation data as shown in the header image\",\n          subtitle = paste0(\"350 points sampled from a true Gaussian distribution\",\n                            \"\\nReference line based on true Gaussian distribution\"))\n\n\n\nThat very close fit to the straight line is what our visual systems are very good at assessing but what is going on here. I’ve replaced the usual rather uninformative “y” and “x” labels on the axes but what is really creating this? It’s a “parametric plot”: it sweeps through some parameter of the data to create the x and y values. The parameter here is the quantile, or the index number of the sorted data. We have 350 observations so what’s happening is that they are being put in order of increasing value so the first value is at the 1/350th quantile. Here are the first ten observed values after sorting to perhaps make this clearer.\n\n\nShow code\n\ntibRnorm %>%\n  rename(y = x) %>%\n  arrange(y) %>%\n  mutate(index = row_number(),\n         quantile = paste0(sprintf(\"%3.0f\", index), \"/350\"),\n         quantileN = index / 350) -> tmpTib\n\ntmpTib %>%\n  filter(index <= 10) %>%\n  flextable() %>%\n  align(j = 3, align = \"right\") %>%\n  colformat_double(digits = 3)\n\nyindexquantilequantileN-2.5821  1/3500.003-2.3802  2/3500.006-2.3473  3/3500.009-2.2904  4/3500.011-2.1865  5/3500.014-2.1506  6/3500.017-2.1357  7/3500.020-2.1248  8/3500.023-2.1139  9/3500.026-1.84510 10/3500.029\n\nSo that shows the smallest ten observed values (y) and I’ve given them an index (just their order!), and their quantile as a fraction and as a decimal value. Now we can get from that to the x values from the Gaussian distribution.\nSmall print alert\nHowever, a slight tweak is needed to find the correct matching quantile to look up in the standard Gaussian distribution. We have seen the lowest score has quantile 1/350 ≃ 0.00286, that’s the upper limit of quantile. Had we had a slightly different sample size, say 351, we would have had the upper limit as 1/351, one fewer and it would have been 1/349. If we are matching to the perfect Gaussian the thing to match to is the point halfway between zero and that quantile, i.e. so .5(0 + 1/350), the point for the next observed quantile is halfway between 1/350 and 2/350, generally we use the centre point of range of quantiles up to our observed one, so not the index number divided by the sample size, i.e. i/350 here, i/n generically, but (i - .5)/350 here, (i - .5)/n generically. These values are shown as centreQuantileN in the table below.\nSorry about that, I only got understand that when I found what I was doing wasn’t matching the points in the qqplot and had to search around to get to understand why. It’s really very small print so just skip on if it’s puzzling.\nEnd of small print alert!\nSo we look up the value of the standard Gaussian (the x values in the histogram at top of this post) to get the x value to plot for each observed value in our dataset. Like this for the first ten observations.\n\n\nShow code\n\ntibRnorm %>%\n  summarise(mean = mean(x), sd = sd(x)) -> tibRnormStats\n\ntmpTib %>%\n  mutate(centreQuantileN = (index - .5) / 350,\n         # x = qnorm(centreQuantileN, mean = tibRnormStats$mean, sd = tibRnormStats$sd)) %>%\n         x = qnorm(centreQuantileN)) %>%\n  filter(index <= 10) -> tmpTib\n\ntmpTib %>%\n  filter(index <= 10) %>%\n  flextable() %>%\n  align(j = 3, align = \"right\") %>%\n  colformat_double(digits = 3)\n\nyindexquantilequantileNcentreQuantileNx-2.5821  1/3500.0030.001-2.983-2.3802  2/3500.0060.004-2.629-2.3473  3/3500.0090.007-2.450-2.2904  4/3500.0110.010-2.326-2.1865  5/3500.0140.013-2.230-2.1506  6/3500.0170.016-2.152-2.1357  7/3500.0200.019-2.084-2.1248  8/3500.0230.021-2.025-2.1139  9/3500.0260.024-1.972-1.84510 10/3500.0290.027-1.925\n\nWe can see from the table that those ten x and y values are not radically dissimilar. These ten points are shown in red in the qqplot below.\n\n\nShow code\n\nggplot(data = tibRnorm,\n       aes(sample = x)) +\n  geom_point(data = tmpTib,\n             aes(x = x, y = y),\n             # shape = 4,\n             colour = \"red\",\n             size = 4) +\n  stat_qq() +\n  stat_qq_line() +\n  ylab(paste0(\"Observed values for same quantile\",\n              \"\\nas for perfect Gaussian score (x-axis)\")) +\n  xlab(paste0(\"Values for the true Gaussian distribution\",\n              \"\\nfor same quantile as for observed point on y-axis\")) +\n  ggtitle(\"A qq plot of the same simulation data as shown in the header image\",\n          subtitle = paste0(\"350 points sampled from a true Gaussian distribution\",\n                            \"\\nReference line based on true Gaussian distribution\"))\n\n\n\nSo we can see that the qqplot is created by sweeping up through the all the ordered observed values creating the corresponding, expected, quantiles from the true Gaussian distribution. Now we have a visual comparison that people who grew up in a rectilinear world around them find very easy: eyeball fit to a straight line.\nOf course, even though we’re good at eyeballing misfit to a straight line, we’re not good at assessing the statistical probability of seeing deviations as big as we are by chance alone. ggqqplot{ggpubr}, i.e. ggqqplot() from the ggpubr R package helps with this by adding a 95% confidence envelope to the plot.\n\n\nShow code\n\nggpubr::ggqqplot(tibRnorm, x = \"x\") +\n  ggtitle(\"A qq plot of the same simulation data as shown in the header image\",\n          subtitle = paste0(\"350 points sampled from a true Gaussian distribution\",\n                            \"\\nReference line based on true Gaussian distribution\")) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) \n\n\n\nWe can see that all the points seem to lie inside the 95% confidence interval envelope here so we would be behaving sensibly, statistically, to accept that they may well have come from random sampling from a true Gaussian distribution. This was very easy, there is still a bit of a questionmark when a few points lie outside the envelope, then we turn to (good) tests of misfit to the Gaussian distribution as in the last post here: Tests of fit to Gaussian distribution!\nUsing the qq plot to diagnose types of misfit\nI won’t go into this is detail here but the ways in which the points in a qq plot fall away from the line of perfect fit tells you about the nature of the misfit, but that’s getting way beyond what we need here!\nOrientation of the qq plot\nI’m used to the orientation I’ve used here, with the observed quantiles on the y axis and the expected ones on the x axis. Apparently this can vary and the switched orientation can be used and a tantalising comment I saw somewhere suggested that countries differ in the orientation they use … but it didn’t say which countries use which orientation!\n\n\n\n",
    "preview": "posts/2023-11-18-explore-distributions-with-plots/explore-distributions-with-plots_files/figure-html5/makegraphic-1.png",
    "last_modified": "2023-11-25T16:14:51+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-11-15-test-fit-to-gaussian/",
    "title": "Tests of fit to Gaussian distribution",
    "description": "Introduction to four tests of fit to Gaussian distribution available in R",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-11-15",
    "categories": [
      "R tricks",
      "distributions",
      "distribution tests",
      "Null hypothesis tests (NHSTs)",
      "Gaussian distribution"
    ],
    "contents": "\n\nContents\nWhy the fuss about translating, rescaling and standardising?\nBetter tests than the K-S test\nUnpacking the Shapiro-Francia test\n\nSo how do these tests compare on Gaussian samples\nThe bias issue\n\nWhat about samples from a non-Gaussian distribution\nSummary\n\nHeader graphic for the fun of it!\n\n\nShow code\n\n### just creating a graphic for the post\n### get a tibble with the density of the standard Gaussian distribution from -3 to 3\ntibble(x = seq(-3, 3, .05),\n       y = dnorm(x)) -> tibDnorm\n\n### now generate a tibble of n = 350 sampling from the standard Gaussian distribution\nset.seed(12345) # make it replicable\ntibble(x = rnorm(350)) -> tibRnorm\n\n# png(file = \"KStest.png\", type = \"cairo\", width = 6000, height = 4800, res = 300)\nggplot(data = tibRnorm,\n       aes(x = x)) +\n  geom_histogram(aes(x = x,\n                     after_stat(density)),\n                 bins = 20,\n                 alpha = .6) +\n  geom_line(data = tibDnorm,\n            inherit.aes = FALSE,\n            aes(x = x, y = y))  +\n  geom_vline(xintercept = 0) +\n  ### paramters work for knitted output\n  annotate(\"text\",\n           label = \"Tests of fit\",\n           x = -2.3,\n           y = .1,\n           colour = \"red\",\n           size = 26,\n           angle = 29,\n           hjust = 0)\n\n\nShow code\n\n# dev.off()\n\n### different sizing to get nice png\n# png(file = \"fitTests.png\", type = \"cairo\", width = 6000, height = 4800, res = 300)\n# ggplot(data = tibRnorm,\n#        aes(x = x)) +\n#   geom_histogram(aes(x = x,\n#                      after_stat(density)),\n#                  bins = 20,\n#                  alpha = .6) +\n#   geom_line(data = tibDnorm,\n#             inherit.aes = FALSE,\n#             aes(x = x, y = y))  +\n#   geom_vline(xintercept = 0) +\n#   ### changed parameters for the png\n#   annotate(\"text\",\n#            label = \"Tests of fit\",\n#            x = -2.5,\n#            y = .1,\n#            colour = \"red\",\n#            size = 100,\n#            angle = 32,\n#            hjust = 0)\n# dev.off()\n\n\nThis follows on from my recent post about the Kolmogorov-Smirnov test. The K-S test is actually not a good test of fit to general Gaussian distribution shape, and that post confirmed that satisfyingly for me as well as explaining the test. The tests I’m putting in here are better. This post leads into one on “Explore distributions with plots](https://www.psyctc.org/Rblog/posts/2023-11-18-explore-distributions-with-plots/). When I can I will also add to a shiny online app to implement these tests in my developing collection of shiny apps. All three posts also expand on entries in my online glossary for the OMbook.\nI’m using the same simulated sample as I used in the K-S test post of n = 350 from a standard Gaussian model with population mean zero and SD of 1.0. Those parameters, the mean and the SD, are the two parameters which translate and rescale the standard Gaussian distribution to any of a huge, potentially infinite set of Gaussian distributions. This reality that we want to test for fit to the general Gaussian shape not to the standard Gaussian is the issue that sinks the otherwise rather nice K-S test. So let’s start by showing what is meant by translating Gaussians: changeing their mean, i.e. their central location.\n\n\nShow code\n\nset.seed(12345)\ntibRnorm %>%\n  mutate(xMean1 = rnorm(350, mean = 1),\n         xMeanNeg1 = rnorm(350, mean = -1),\n         xMean3 = rnorm(350, 3)) -> tibRnorms\n\nset.seed(12345)\ntibble(x = seq(-5, 7, .05),\n       Mean0 = dnorm(x, mean = 0),\n       Mean1 = dnorm(x, mean = 1),\n       MeanNeg1 = dnorm(x, mean = -1),\n       Mean3 = dnorm(x, 3)) -> tibDnorms\n\ntibDnorms %>%\n  pivot_longer(cols = -x, names_to = \"popMean\") -> tibLongDnorms\n\ntibble(popMean = c(\"Mean0\", \"Mean1\", \"MeanNeg1\", \"Mean3\"),\n       x = c(0, 1, -1, 3)) -> tmpTibMeans\n\nvecColours <- c(\"Mean0\" = \"red\",\n                \"Mean1\" = \"green\",\n                \"Mean3\" = \"blue\",\n                \"MeanNeg1\" = \"purple\")\n\nggplot(data = tibLongDnorms,\n       aes(x = x, y = value)) +\n  # geom_point() +\n  geom_ribbon(data = filter(tibLongDnorms,\n                            popMean == \"Mean0\"),\n              aes(ymax = value, ymin = 0),\n              fill = \"red\",\n              alpha = .5) +\n  geom_line(linewidth = 1.2,\n            aes(colour = popMean)) +  \n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = x, colour = popMean)) +\n  xlab(\"Value\") +\n  ylab(\"Density\") +\n  scale_colour_manual(\"Population mean\",\n                      values = vecColours) +\n  ggtitle(paste(\"Illustrating 'translation' of the Gaussian distribution,\",\n                \"\\ni.e. changing the population mean\"),\n          subtitle = paste(\"Standard Gaussian shaded with mean = 0, translated Gaussian distributions\",\n                           \"\\nVertical lines mark population means, 0 for the standard Gaussian\",\n                           \"\\nand -1, 1 and 3 for translated Gaussian distributions\"))\n\n\n\nThe other issue is rescaling and this shows rescaling, i.e. altering the population SD.\n\n\nShow code\n\nset.seed(12345)\ntibDnorms %>% \n  mutate(`SD0.5` = dnorm(x, mean = 0, sd = .5),\n         SD2 = dnorm(x, mean = 0, sd = 2)) -> tibDnorms\n\ntibDnorms %>%\n  pivot_longer(cols = c(Mean0, `SD0.5`, SD2), \n               names_to = \"popParms\") %>%\n  mutate(popParms = if_else(popParms == \"Mean0\", \"SD1\", popParms)) -> tibLongDnorms\n\nvecColours <- c(\"SD1\" = \"red\",\n                \"SD0.5\" = \"green\",\n                \"SD2\" = \"blue\")\n\nggplot(data = tibLongDnorms,\n       aes(x = x, y = value)) +\n  geom_ribbon(data = filter(tibLongDnorms,\n                            popParms == \"SD1\"),\n              aes(ymax = value, ymin = 0),\n              fill = \"red\",\n              alpha = .5) +\n  geom_line(linewidth = 1.2,\n            aes(colour = popParms)) +  \n  geom_vline(xintercept = 0) +\n  xlab(\"Value\") +\n  ylab(\"Density\") +\n  guides(fill = \"none\") +\n  scale_colour_manual(\"Population SD\",\n                      values = vecColours) +\n  ggtitle(paste(\"Illustrating 'rescaling' of the Gaussian distribution\",\n                \"\\ni.e. changing the population SD\"),\n          subtitle = paste(\"Standard Gaussian shaded with mean = 0, SD = 1\",\n                           \"\\nDistributions for SD = 0.5 and 2 superimposed on the standard Gaussian\"))\n\n\n\nOf course, any Gaussian distribution can have mean != 0 and SD != 1 (“!=” is R for “not equal to”), these are “arbitary Gaussian distributions” to my mind. The key point is that they can all transformed by substracting their smaple from each value so their new mean is zero. Then they can be rescaled by dividing the translated scores by the SD of the observed scores so the new SD is 1. For what it’s worth, this is called “standardising” and if you started with a distribution that is Gaussian in shape, you will now have yourself a standard Gaussian distribution. Here’s a silly little example which I hope makes this clear.\n\n\nShow code\n\nset.seed(12345)\ntibble(rawValues = rnorm(10, mean = 3, sd = 5)) %>%\n  ### sort them into ascending order \n  ### (just to make it easier see the standardisation)\n  arrange(rawValues) -> tmpTib\n\ntmpTib %>%\n  flextable() %>%\n  colformat_double(digits = 4)\n\nrawValues-6.0898-1.59660.73251.57921.61912.45355.92766.02946.15056.5473\n\nSo there we have ten random numbers that happen to come from a Gaussian distribution. Start by getting the sample statistics.\n\n\nShow code\n\n### get the summary statistics\ntmpTib %>%\n  summarise(sampleMean = mean(rawValues),\n            sampleSD = sd(rawValues)) -> tmpStats\n\ntmpStats %>%\n  flextable() %>%\n  colformat_double(digits = 4)\n\nsampleMeansampleSD2.33534.0683\n\nOK we can see these don’t come from a standard Gaussian distribution: the mean is well away from zero and the SD is much higher than 4. Let’s start to standardise these data. Start by using the sample mean to get the centred values by subtracting the mean from the observed values.\n\n\nShow code\n\ntmpTib %>%\n  mutate(centredVals = rawValues - tmpStats$sampleMean) -> tmpTib\n\ntmpTib %>%\n  flextable() %>%\n  colformat_double(digits = 4)\n\nrawValuescentredVals-6.0898-8.4251-1.5966-3.93190.7325-1.60281.5792-0.75611.6191-0.71622.45350.11825.92763.59246.02943.69426.15053.81526.54734.2121\n\nCheck that mean centring has worked.\n\n\nShow code\n\ntmpTib %>%\n  summarise(across(everything(), ~ mean(.x))) %>%\n  flextable() %>%\n  colformat_double(digits = 4) %>%\n  add_header_lines(values = \"Mean values\") %>%\n  align(i = 1, align = \"center\", part = \"header\")\n\nMean valuesrawValuescentredVals2.3353-0.0000\n\nIt has: the mean of centred values is now zero. However, the SD is not affected by any addition or subtrating of any fixed value so it’s still as it was and not 1.0. So rescale by dividing the centred values by the observed sample SD. (This is the SD based on dividing the deviations from the mean by n - 1, i.e. the unbiased estimate of the population SD.)\n\n\nShow code\n\ntmpTib %>%\n  mutate(rescaledVals = centredVals / tmpStats$sampleSD) -> tmpTib\n\ntmpTib %>%\n  flextable() %>%\n  colformat_double(digits = 4)\n\nrawValuescentredValsrescaledVals-6.0898-8.4251-2.0709-1.5966-3.9319-0.96650.7325-1.6028-0.39401.5792-0.7561-0.18581.6191-0.7162-0.17602.45350.11820.02915.92763.59240.88306.02943.69420.90806.15053.81520.93786.54734.21211.0353\n\nCheck rescaling has fixed the SD to 1.0.\n\n\nShow code\n\ntmpTib %>%\n  summarise(across(everything(), ~ sd(.x))) %>%\n  flextable() %>%\n  colformat_double(digits = 4) %>%\n  add_header_lines(values = \"SD values\") %>%\n  align(i = 1, align = \"center\", part = \"header\")\n\nSD valuesrawValuescentredValsrescaledVals4.06834.06831.0000\n\nSo that has given us rescaled values with mean of zero and SD of 1 as we wanted. Any dataset of numbers can be standardised in this way.\nWhy the fuss about translating, rescaling and standardising?\nIf you read the K-S test post you see that the problem with the K-S test is a good test of fit to the standard Gaussian but because it needs an arbitrary distribution to be standardised using the observed mean and SD it is a biased test of fit to the general Gaussian shape because it has to standardise the observed data in order to test what it can: fit to the standard Gaussian distribution. Perhaps surprisingly, using just two “degrees of freedom” in the data by doing this standardisation biases the test as it is bound to improve fit to the standard Gaussian. Hence the simple one sample K-S tends to report more than 95% of actual Gaussian distributions as having a good fit even though the p value criterion (alpha level, false positive rate) was set at .05. Are there better tests? Yes, that’s why I’m ploughing through this: so I understand what they are and how they work. What criteria of fit/misfit they use.\nBetter tests than the K-S test\nI’m not a professional statistician but, as so often, I think the Wikipedia page “Normality test” is good on the general issues but at a fairly geeky level. It leads onto more detail on some of the tests. The ones that R offers that I’m considering here are:\nShapiro-Wilk test: shapiro.test{stats}, i.e. this is the shapiro.test() function and it’s in the R stats package, one of the default packages in R. Pretty succinct R help is here and more detail is in the Wikipedia page. The test dates from 1965 and involves some maths I don’t really understand!\nCramer von Mises test: cvm.test{nortest}. See the package help here and the seriously deep Wikipedia page. This dates from 1928-30 according to the Wikipedia article. As I understand that and the R help, the criterion of misfit used by the test is a squared difference between the observed, standardised score/value and the expected value for the quantile of the standard Gaussian distribution.\nAnderson-Darling test: ad.test{nortest} so provided by the supplementary package “nortest”. See the package help here and the Wikipedia page. The Anderson-Darling test apparently dates from 1952 and is similar to the Cramer von Mises test but, as the Wikipedia page says, the criterion will weight deviations at the tails (towards the minimum and maximum) more heavily than the CvM test will.\nShapiro-Francia test: sftest{nortest}. See the package help here and Wikipedia page. This dates from 1972 and is a bit different from the previous tests and from the K-S test. The Wikipedia page, clearly rightly, describes it as a simplification of the Shapiro-Wilk test and as such I can see that it has some similarities with the CvM and A-D tests too. All of them, unlike the K-S test which looks at the maximum discrepancy between the ecdf and the theoretically predicted cdf of the Gaussian distribution. However, as I understand it, the S-W, CvM and A-D tests all use difference criteria about all deviations between observed and expected cumulative distributions. By contrast, the S-F test looks at the correlation of the ordered scores seen in the empirical data and the typical ordered values you would see if taken a random sample size for from a standard Gaussian distribution: the lower the correlation, the worse the fit and that’s the criterion. As the page says, this is in some ways taking the q-q plot (or qqplot) method of visual inspection of fit to the Gaussian and turns it into a correlation criterion. Neat! (I’ll come to the qqplot in the next post here.)\nUnpacking the Shapiro-Francia test\nI can implement that in (my crude) R. The Wikipedia page gives the example of having scores of 5.6, -1.2, 7.8 and 3.4, so reordered, of -1.2, 3.4, 5.6 and 7.8. It says that the expected 2nd score if taking four values from the standard Gaussian is -.297.\n\n\nShow code\n\nvecObs <- sort(c(5.6, -1.2, 7.8, 3.4))\nset.seed(12345)\nnDraws <- 10000\ntibble(drawN = 1:nDraws) %>% # do the following nDraws times\n  rowwise() %>% # makes sure each draw is independent\n  ### now get the (sorted) samples of size four from the standard Gaussian distribution\n  mutate(drawVals = list(sort(rnorm(4)))) %>% \n  ### have to manipulate those back to four separate variables\n  unnest_wider(drawVals, names_sep = \"_\") %>%\n  ### get their mean values across the nDraws samples\n  summarise(across(starts_with(\"draw\"), ~ mean(.x))) %>%\n  ### get just them\n  select(-drawN) %>% \n  ### covert back to a single vector of the means\n  unlist() -> vecSim\n\n### now get the correlation with the observed values (from Wikipedia's example)\ncor(vecObs, vecSim) -> s.fcorr\n\n\nSo I don’t get exactly the same expected score as Wikipedia gives which is not surprising as this is a process involving random samples. I get\n-0.286 where wikipedia gives -.297 but that’s not too worrying I think. The four such simulated values I got from my\n10,000 samples (of size four) from the standard Gaussian distribution were:\n-1.021, -0.286, 0.293 and 1.025\nand the Pearson correlation of that with the observed values of\n-1.2, 3.4, 5.6 and 7.8 was\n0.983. That final correlation is the criterion of (actually fit, not misfit) that is used by the S-F test.\nSo how do these tests compare on Gaussian samples\nThe bias issue\nLet’s create 10,000 samples from distributions that are Gaussian in shape but have mean values greater than zero and SD bigger than 1.0.\n\n\nShow code\n\nset.seed(12345)\nsampN <- 350\nnReplicns <- 10000\ntibble(replicn = 1:nReplicns,\n       ### now get different mean and SD for each run, none from a standard Gaussian\n       ### chose a mean from a uniform distribution between 1 and 3, i.e. well above zero\n       popMean = runif(nReplicns, min = 1, max = 3),\n       ### and an SD from a uniform distribution between 2 and 3, well above 1.0\n       popSD = runif(nReplicns, min = 2, max = 3)) %>%\n  rowwise() %>% # to get a new sample for each replication\n  mutate(x = list(rnorm(sampN, mean = popMean, sd = popSD))) %>%\n  ### twist those samples to long format to make these next stages easy to do\n  ### (i.e. this is the tidyverse way of doing things)\n  unnest_longer(x) %>%\n  group_by(replicn) %>% # to test each replication\n  summarise(sampMean = mean(x), # get the sample mean\n            sampSD = sd(x), # get the sample SD\n            ### now get the univariate K-S test against the standard Gaussian\n            ### with the \"$p.value\" I am just pulling the p value from the results\n            ksP = ks.test(x, \"pnorm\", sampMean, sampSD, alternative = \"two.sided\")$p.value,\n            ### slightly different approach to get the p values for the new tests\n            ### use tidy{broom} to sweep up all the values from the Shapiro-Wilk test\n            ShapiroWilk = broom::tidy(stats::shapiro.test(x)),\n            ### same for the Anderson-Darling test and then the CvM and S-F tests\n            adtest = broom::tidy(nortest::ad.test(x)),\n            cvmtest = broom::tidy(nortest::cvm.test(x)),\n            sftest = broom::tidy(nortest::sf.test(x))) %>% \n  ### now unnest the results that tidy{broom} swept into lists:\n  ### first unnest to separate statistics from the htest list that the tests created\n  unnest(ShapiroWilk) %>%\n  ### I'm only interested in the p value so can dump these variables\n  select(-c(method, statistic)) %>%\n  ### rename the p value to keep it separate from subsequent ones from the other tests\n  rename(ShapiroWp = p.value) %>%\n  ### do the same for the Anderson-Darling test and so on ...\n  unnest(adtest) %>%\n  select(-c(method, statistic)) %>%\n  rename(ADp = p.value) %>%\n  unnest(cvmtest) %>%\n  select(-c(method, statistic)) %>%\n  rename(CvMp = p.value) %>%\n  unnest(sftest) %>%\n  select(-c(method, statistic)) %>%\n  rename(SFp = p.value) -> tibAllGaussTests\n\ntibAllGaussTests %>%\n  ### twist to long format to make it easy to get things by test\n  pivot_longer(cols = ends_with(\"p\"), names_to = \"test\", values_to = \"pValue\") %>%\n  mutate(sig = if_else(pValue < .05, \"p < .05\", \"NS\")) -> tmpTib\n\ntmpTib %>%\n  ### group by test\n  group_by(test) %>%\n  ### and summarise to get rates of statistically significant p values\n  summarise(n = n(),\n            nSig = sum(sig == \"p < .05\")) %>%\n  rowwise() %>%\n  ### get the binomial 95% CI for the observed rate per test\n  ### binconf{Hmisc} returns a matrix so the \"[1, ]\" bit just gets\n  ### the first row to make it easy to tidy \n  mutate(propSig = list(Hmisc::binconf(nSig, n)[1, ])) %>%\n  ### tidy to what we want by unnesting that\n  unnest_wider(propSig) -> tibSummary\n\ntibSummary %>%\n  flextable() %>%\n  ### override flextable's default which is to give many significant digits\n  colformat_double(digits = 3) %>%\n  autofit()\n\ntestnnSigPointEstLowerUpperADp10,0005000.0500.0460.054CvMp10,0005130.0510.0470.056SFp10,0005310.0530.0490.058ShapiroWp10,0005070.0510.0470.055ksP10,00020.0000.0000.001\n\nThat’s pretty nice: across 10,000 simulations the mean rate of false positives for all our new test is close to .05 as it should be and the 95% binomial confidence intervals are all nicely across the .05 rate and fairly tight (as you’d expect from 10,000 simulations). As seen before, the K-S test is terrible with a false positive rate far lower than it should be and way different from that of these new tests. Here’s the same findings as a forest plot. (I do like a plot but here I admit that the table is all that is needed and gives the precise confidence limits and overall rates should anyone want them!)\n\n\nShow code\n\ntibSummary %>%\n  ### this is just to get the tests sorted into the order I want when plotting\n  mutate(Test = ordered(test,\n                        levels = c(\"ksP\",\n                                   \"ShapiroWp\",\n                                   \"CvMp\",\n                                   \"ADp\",\n                                   \"SFp\"),\n                        labels = c(\"K-S\",\n                                   \"S-W\",\n                                   \"CvM\",\n                                   \"A-D\",\n                                   \"S-F\"))) -> tibSummary\n\n### great a vector of colours for the tests\nvecColours <- c(\"K-S\" = \"black\",\n                \"S-W\" = \"blue\",\n                \"CvM\" = \"purple\",\n                \"A-D\" = \"orange\",\n                \"S-F\" = \"green\")\n\n### now plot\nggplot(tibSummary,\n       aes(x = Test, y = PointEst, colour = Test)) +\n  ### points at the observed false positive rates\n  geom_point() +\n  ### add binomial 95% CI error bars around those rates\n  geom_linerange(aes(ymin = Lower, ymax = Upper)) +\n  ### horizontal reference line\n  geom_hline(yintercept = .05,\n             linetype = 3) +\n  ylab(\"Proportion significant\") +\n  scale_colour_manual(\"Test\",\n                      values = vecColours) +\n  ggtitle(\"Forest plot of proportion of tests significant at p < .05\",\n          subtitle = \"From 10,000 simulated samples of n = 350 from true Gaussian population\") +\n  ### I think the labels of the tests on the x axis look better rotated\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\n\nFor what it’s worth, here’s the distribution of those 10,000 simulation p values for all these tests.\n\n\nShow code\n\ntibAllGaussTests %>%\n  select(-starts_with(\"samp\")) %>%\n  pivot_longer(cols = ends_with(\"p\"),\n               names_to = \"test\", values_to = \"p\") %>%\n  mutate(Test = ordered(test,\n                        levels = c(\"ksP\",\n                                   \"ShapiroWp\",\n                                   \"CvMp\",\n                                   \"ADp\",\n                                   \"SFp\"),\n                        labels = c(\"K-S\",\n                                   \"S-W\",\n                                   \"CvM\",\n                                   \"A-D\",\n                                   \"S-F\")),\n         sig = if_else(p < .05, \"p < .05\", \"NS\")) -> tibPvaluesLong\n\nvecSigColours <- c(\"NS\" = \"green\",\n                   \"p < .05\" = \"red\")\nggplot(data = tibPvaluesLong,\n       aes(x = p)) +\n  ### facet the plot by test\n  facet_grid(rows = vars(Test)) +\n  geom_histogram(aes(fill = sig),\n                 breaks = seq(0, 1, .05)) +\n  ylab(\"n\") +\n  xlab(\"p value\") +\n  scale_fill_manual(\"Significance\",\n                    values = vecSigColours) +\n  ggtitle(\"Histograms of p values, tests of fit to Gaussian distribution\",\n          subtitle = \"n = 10,000 simulations, varying mean and SD\")\n\n\n\nPretty clear that the K-S test is hopeless with a false positive rate way below .05, that’s what we saw in the Kolmogorov-Smirnov test, what is new here is how well these four other tests are behaving.\nHow concordant are the p values from these new tests?\n\n\nShow code\n\ntibAllGaussTests %>%\n  ### get just the p values I want to feed into ggpairs{GGally}\n  select(ends_with(\"p\")) %>%\n  # get them in the order I have been using\n  select(ksP, ShapiroWp, CvMp, ADp, SFp) -> tmpTib\n  \n\n### define a function to replace the default that ggpairs{GGally} would use\nlowerFn <- function(data, mapping, method = \"lm\", ...) {\n  ### create a plot passing through the data, mapping and default smoothing\n  ### the \"...\" would pass through any additional arguments\n  p <- ggplot(data = data, mapping = mapping) +\n    ### use a very small alpha (i.e. high transparency)\n    ### as we have 10k points\n    geom_point(alpha = .05) +\n    ### add reference lines at .05\n    geom_vline(xintercept = .05,\n               colour = \"red\") +\n    geom_hline(yintercept = .05,\n               colour = \"red\") +\n    ### get the smoothed regression, defaults to linear and\n    ### to adding the 95% confidence interval around the line\n    geom_smooth(method = method, color = \"blue\", ...) +\n    ### set plot limits\n    xlim(c(0, 1)) +\n    ylim(c(0, 1))\n  ### return that plot\n  p\n}\n\nGGally::ggpairs(tmpTib, columns = 2:5,\n                lower = list(continuous = lowerFn, method = \"lm\"),\n                diag = list(continuous = \"barDiag\"))\n\n\n\nThat SPLOM (ScatterPLOt Matrix) has the histograms of the p values in the leading diagonal cells, the Pearson correlations in the upper triangle (with rather spurious asterixes for the p value for the correlation, all have three asterixes indicating p < .0005 I suspect). The lower triangle is the really interesting one showing the linear regression of the one set of p values on the other. I’ve marked the p = .05 lines in red. Those blue lines are actually the 95% confidence interval for the linear regression but with n = 10,000 and such strong linear relationships they have collapsed onto the regression lines. However it can be see clearly that though the tests give very strongly correlated and apparently linearly related p values they are clear differences in the pairwise similarities with the Shapiro-Wilk test and the Shapiro-Francia tests highly concordant and the Cramer von Mises and Anderson-Darling tests again very concordant but much less concordance for the other four pairs. This makes sense as the logic, but not the specific criterion tested, of the Shapiro-Wilk and Shapiro-Francia tests are very similar. Likewise the Craner von Mises and Anderson-Darling tests are using a similar logic but with somewhat different weightings.\nIt can be seen from the red reference lines marking p = .05 and the clear fact that the linear regression lines don’t all go through the origin, that the agreement on classifying these samples as statistically significantly (p < .05) won’t be anything like as high as the correlations of the p values.\nThis is shown very clearly in this table of the phi correlations values for the p < .05 statistical significance across the tests. Some are really only moderate correlations.\n\n\nShow code\n\n### function to get 1 for p < .05 and 0 for p >= .05\n### makes it easy to use across() in tidyverse pipe below\ngetSig <- function(x){\n  if(x < .05) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n### vectorise it as dplyr manipulations of the data are always of\n### the full vector of values for the variables\ngetSig <- Vectorize(getSig)\n### just checking:\n# getSig(seq(.03, .06, .01))\n\ntibAllGaussTests %>%\n  select(-ksP) %>%\n  select(ends_with(\"p\")) %>%\n  ### using across() and the getSig function\n  mutate(across(everything(), # use all variables\n                list(sig = getSig), # get the recoding done\n                ### and this renames the results\n                .names = \"{.col}.{.fn}\")) %>%\n  select(ends_with(\".sig\")) %>%\n  ### turn it into a matrix so can use good old cor() on it\n  as.matrix() -> tmpMat\n\n### this is a the way in flextable to colour a range of cells by a criterion\n### it uses the function col_bin{scales} to convert ranges of numbers, here\n### just two ranges, < .7 and >= .7, to the colours to use in flextable\nbg_picker <- scales::col_bin(\n    palette = c(\"white\", \"green\"), # colours to use\n    domain = c(0, 1), # range of numbers to remap (here all of them)\n    bins = c(0, .7, 1)) # create the bins with limits and one cutting point\n\ncor(tmpMat) %>%\n  as_tibble(rownames = \"test\") %>%\n  flextable() %>%\n  colformat_double(digits = 2) %>% # set digits to show\n  bg(j = 2:5, bg = bg_picker) %>% # use the colour mapping to create background colours\n  ### and add a footer\n  add_footer_lines(values = \"Phi values > .7 in green\") %>%\n  ### and fix the aligment of the text in the footer\n  align(i = 1, align = \"center\", part = \"footer\")\n\ntestShapiroWp.sigADp.sigCvMp.sigSFp.sigShapiroWp.sig1.000.530.430.79ADp.sig0.531.000.820.47CvMp.sig0.430.821.000.39SFp.sig0.790.470.391.00Phi values > .7 in green\n\nWhat about samples from a non-Gaussian distribution\nThis brings us to: what or which non-Gaussian distribution? There are many: all sorts of discontinuous, discontinuous to the point of being very unbalanced binary and a huge set of continuous distributions. I chose to use the gamma distribution as, if you give it a sensible shape parameter it can look a bit like a distribution of questionnaire scores that you might see in real data. Here’s the gamma density for values from zero to 20. It’s bounded, not going to infinity in both directions, it’s obviously skew to the right but it’s fairly smooth and has a single mode.\n\n\nShow code\n\ntibble(x = seq(0, 20, .1)) %>%\n  mutate(y = dgamma(x, shape = 6)) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = x, y = y)) +\n  # geom_point() +\n  geom_line() +\n  ylab(\"Density\") +\n  xlab(\"Score\") +\n  ggtitle(\"Plot of density of gamma distribution with shape value = 6\")\n\n\n\nWhat I have done next is to create 1,000 samples of size 70 from that distribution and to look at the proportions of the 1,000 that had statistically significant deviation from a Gaussian shape according to our tests.\n\n\nShow code\n\nset.seed(12345)\nsampN <- 70\nnReplicns <- 1000\ntibble(replicn = 1:nReplicns) %>%\n  rowwise() %>% # make sure samples are independent\n  mutate(x = list(rgamma(sampN, shape = 6))) %>%\n  unnest_longer(x) %>%\n  group_by(replicn) %>% # to test each replication\n  summarise(sampMean = mean(x),\n            sampSD = sd(x),\n            ksP = ks.test(x, \"pnorm\", sampMean, sampSD, alternative = \"two.sided\")$p.value, \n            ShapiroWilk = broom::tidy(stats::shapiro.test(x)),\n            adtest = broom::tidy(nortest::ad.test(x)),\n            cvmtest = broom::tidy(nortest::cvm.test(x)),\n            sftest = broom::tidy(nortest::sf.test(x))) %>% \n  unnest(ShapiroWilk) %>%\n  select(-c(method, statistic)) %>%\n  rename(ShapiroWp = p.value) %>%\n  unnest(adtest) %>%\n  select(-c(method, statistic)) %>%\n  rename(ADp = p.value) %>%\n  unnest(cvmtest) %>%\n  select(-c(method, statistic)) %>%\n  rename(CvMp = p.value) %>%\n  unnest(sftest) %>%\n  select(-c(method, statistic)) %>%\n  rename(SFp = p.value) -> tibAllGaussTestsGamma\n\ntibAllGaussTestsGamma %>%\n  pivot_longer(cols = ends_with(\"p\"), names_to = \"test\", values_to = \"pValue\") %>%\n  mutate(sig = if_else(pValue < .05, \"p < .05\", \"NS\")) -> tmpTib\n\ntmpTib %>%\n  group_by(test) %>%\n  summarise(n = n(),\n            nSig = sum(sig == \"p < .05\")) %>%\n            rowwise() %>%\n            mutate(propSig = list(Hmisc::binconf(nSig, n)[1, ])) %>%\n  unnest_wider(propSig) -> tibSummaryGamma\n\ntibSummaryGamma %>%\n  flextable() %>%\n  colformat_double(digits = 3) %>%\n  autofit()\n\ntestnnSigPointEstLowerUpperADp1,0005360.5360.5050.567CvMp1,0004750.4750.4440.506SFp1,0006170.6170.5860.647ShapiroWp1,0006550.6550.6250.684ksP1,000140.0140.0080.023\n\nStarting at the bottom: the K-S test does very badly so not only is it reporting far fewer statistically significant (false) deviations from Gaussian as shown in the other post but with this sample size it only reported statistically significant deviation from Gaussian for 1.4% of the samples so it is failing in its rate of true positives as well as in its low rate for false positives. By contrast all the other tests reporting statistically significant deviation from Gaussian in 47.5% of samples for the CvM test up to 65.5% for the S-W test.\n\n\nShow code\n\ntibSummaryGamma %>%\n   mutate(Test = ordered(test,\n                        levels = c(\"ksP\",\n                                   \"ShapiroWp\",\n                                   \"CvMp\",\n                                   \"ADp\",\n                                   \"SFp\"),\n                        labels = c(\"K-S\",\n                                   \"S-W\",\n                                   \"CvM\",\n                                   \"A-D\",\n                                   \"S-F\"))) -> tibSummaryGamma\nggplot(data = tibSummaryGamma,\n       aes(x = Test, y = PointEst, colour = Test)) +\n  geom_point(size = 3) +\n  geom_linerange(aes(ymin = Lower, ymax = Upper),\n                 linewidth = 2) +\n  geom_hline(aes(yintercept = PointEst,\n                 colour = Test),\n             linetype = 3) +\n  ylab(\"Proportion significant\") +\n  scale_colour_manual(\"Test\",\n                      values = vecColours) +\n  ggtitle(\"Forest plot of proportion of tests significant at p < .05\",\n          subtitle = paste(\"From 1,000 simulated samples of n = 70 from gamma distribution\",\n                           \"\\nError bars are 95% binomial confidence intervals\")) +\n  ### I think the labels of the tests on the x axis look better rotated\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\n\nThat shows that the detection rates are clearly statistically significantly not only, dramatically, between the K-S test and the other tests, but between each pair of the other four tests with the Shapiro-Wilk test showing the best detection rate.\nHow do the p values from the different tests agree across samples for this non-null case?\n\n\nShow code\n\ntibAllGaussTestsGamma %>%\n  select(ends_with(\"p\")) %>%\n  # get them in the order I have been using\n  select(ksP, ShapiroWp, CvMp, ADp, SFp) -> tmpTib\n  \nGGally::ggpairs(tmpTib, columns = 1:5,\n                lower = list(continuous = lowerFn, method = \"lm\"),\n                diag = list(continuous = \"barDiag\"))\n\n\n\nUnsurprisingly the obvious outlier is the K-S. The other four tests show strong correlations in their p values but with some pairs showing linear regression going through the origin, others not. Here’s the phi agreement between methods in showing statistically significant deviation from Gaussian.\n\n\nShow code\n\ntibAllGaussTestsGamma %>%\n  select(ends_with(\"p\")) %>%\n  mutate(across(everything(), list(sig = getSig), .names = \"{.col}.{.fn}\")) %>%\n  select(ends_with(\".sig\")) %>%\n  as.matrix() -> tmpMat\n\n# Define colors and ranges\nbg_picker <- scales::col_bin(\n    palette = c(\"white\", \"green\"),\n    domain = c(0, 1), # min and max range can be also set dynamically \n    bins = c(0, .7, 1)) # as well as the bins\n\ncor(tmpMat) %>%\n  as_tibble(rownames = \"test\") %>%\n  flextable() %>%\n  colformat_double(digits = 2) %>%\n  bg(j = 2:6, bg = bg_picker) %>%\n  add_footer_lines(values = \"Phi values > .7 in green\") %>%\n  align(i = 1, align = \"center\", part = \"footer\")\n\ntestksP.sigShapiroWp.sigADp.sigCvMp.sigSFp.sigksP.sig1.000.090.110.130.09ShapiroWp.sig0.091.000.700.600.89ADp.sig0.110.701.000.860.67CvMp.sig0.130.600.861.000.59SFp.sig0.090.890.670.591.00Phi values > .7 in green\n\nSummary\nI have explored what I think are the four main generally agreed to be good tests of fit to the Gaussian distribution.\nI think I now have some real grasp of the criteria of misfit they use and how they then allocate p values given that criterion.\nNot that it’s new news, this has been explored much more thoroughly by others, but all four tests show appropriate false positive rates testing samples from true population Gaussian distributions of varying means and variances. That is markedly different from the hopeless false positive rate of the K-S test.\nLooking at p values for this exploration of samples from true Gaussian population datas the four tests really fall into two groups of two with very similar p values within the pairs but rather different p values and even rather different allocation of samples (falsely) as statistically significantly deviating from Gaussian. The pairs are:\nthe Shapiro-Wilk and Shapiro-Francia tests and\nthe Cramer von Mises and Anderson-Darling tests\n\nUsing the tests with samples from a non-Gaussian distribution showed that all the four new tests markedly outperformed the K-S test but also that with the precision of estimation from 1,000 replications, there were clear differences between the new tests with the Shapiro-Wilk test showing the best detection rate. This is congruent with summaries of much more extensive simulation and testing of the tests summarised in the R nortest package help files and on Wikipedia. However, as noted in those summaries, this superiority of the S-W test won’t apply for all non-Gaussian distributions as the criteria of misfit and the ways of testing the criteria vary between the tests and will function differently with samples from differently non-Gaussian populations.\n\n\n\n",
    "preview": "posts/2023-11-15-test-fit-to-gaussian/test-fit-to-gaussian_files/figure-html5/makegraphic-1.png",
    "last_modified": "2023-11-25T16:10:38+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-11-11-ks-test/",
    "title": "Kolmogorov-Smirnov test",
    "description": "First post about using R to test fit to univariate Gaussian (a.k.a. 'Normal') distribution: Kolmogorov-Test",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-11-11",
    "categories": [
      "R tricks",
      "distributions",
      "distribution tests",
      "Null hypothesis tests (NHSTs)",
      "Gaussian distribution"
    ],
    "contents": "\n\nContents\nGeeky bit: histogram with Gaussian fit line\nBack to the K-S test!\nBut real data are never Gaussian in distribution!\nBut we can still test for approximate fit\n\n\nThe K-S test\nEmpirical cumulative distribution functions\nK-S test on real data\nK-S test warning\nScaling and location\n\nThe bias issue\n\nSummary\n\n[Updated 17.xi.23 to add link to Tests of fit to Gaussian distribution.]\nGeeky bit: histogram with Gaussian fit line\nThis first bit of code was just going to be to generate a little graphic for this post but actually it reminded me of something I always forget when trying to create the old (cliché?) plot of a Gaussian curve superimposed on a histogram: the easiest way to fit the y scaling is to use density not count for the histogram using after_stat(density) to change the default y scale of the histogram from a count to density (i.e. proportion):\ngeom_histogram(aes(x = x,\n                   after_stat(density)))\n\n\nShow code\n\n### just creating a graphic for the post\n### get a tibble with the density of the standard Gaussian distribution from -3 to 3\ntibble(x = seq(-3, 3, .05),\n       y = dnorm(x)) -> tibDnorm\n\n### now generate a tibble of n = 350 sampling from the standard Gaussian distribution\nset.seed(12345) # make it replicable\ntibble(x = rnorm(350)) -> tibRnorm\n\n# png(file = \"KStest.png\", type = \"cairo\", width = 6000, height = 4800, res = 300)\nggplot(data = tibRnorm,\n       aes(x = x)) +\n  geom_histogram(aes(x = x,\n                     after_stat(density)),\n                 bins = 20,\n                 alpha = .6) +\n  geom_line(data = tibDnorm,\n            inherit.aes = FALSE,\n            aes(x = x, y = y))  +\n  geom_vline(xintercept = 0) +\n  ### paramters work for knitted output\n  annotate(\"text\",\n           label = \"K-S test\",\n           x = -2.3,\n           y = .1,\n           colour = \"red\",\n           size = 33,\n           angle = 29,\n           hjust = 0)\n\n\nShow code\n\n# dev.off()\n\n### different sizing to get nice png\n# png(file = \"KStest.png\", type = \"cairo\", width = 6000, height = 4800, res = 300)\n# ggplot(data = tibRnorm,\n#        aes(x = x)) +\n#   geom_histogram(aes(x = x,\n#                      after_stat(density)),\n#                  bins = 20,\n#                  alpha = .6) +\n#   geom_line(data = tibDnorm,\n#             inherit.aes = FALSE,\n#             aes(x = x, y = y))  +\n#   geom_vline(xintercept = 0) +\n#   ### changed parameters for the png\n#   annotate(\"text\",\n#            label = \"K-S test\",\n#            x = -2.5,\n#            y = .1,\n#            colour = \"red\",\n#            size = 120,\n#            angle = 32,\n#            hjust = 0)\n# dev.off()\n\n\nBack to the K-S test!\nThis post cross links with two others:\nExplore distributions with plots and\nTests of fit to Gaussian distribution.\nWhen I can I will also add to a shiny online app to implement these tests in my developing collection of shiny apps. All three posts also expand on entries in my online glossary for the OMbook.\nBut real data are never Gaussian in distribution!\nSo this is about testing whether or not a dataset of values, scores maybe, fits the Gaussian distribution. The simple answer for any real data I’ve every handled was “no”, and I’m sure that will remain true for any future real datasets I handle. That’s because the Gaussian distribution is a smooth distribution of scores from minus infinity to plus infinity and I think we can safely say that no real dataset, at least in my application areas of mental health and therapy data, is going to have values at minus or plus infinity. Equally, none I handle will be be truly continuous, i.e. smooth; they will always be “discrete” growing in finite steps. For example, on the mean item scoring of the CORE-OM, assuming that all 34 items are completed, the possible scores range from 0 to 4 in steps of 1/34, i.e., steps of\n0.0294118. That means that the possible score range is from\n0, 0.03, 0.06\n… through to …\n3.94, 3.97, 4.\nSo for a full 34 item CORE-OM there are 137 possible scores. That’s a lot of possible scores but it’s still a discrete set and not at all a smooth distribution from minus infinity to plus infinity. For the GAD-7 the score range is from 0 to 21, just 22 possible scores.\nBut we can still test for approximate fit\nSo we know the fit is never a perfect fit to the Gaussian for real data so the question is really:\n“How well does the distribution of values/scores in the data approximate to the Gaussian?”\nTurning that into a frequentist NHST (Null Hypothesis Significance Test) that becomes:\n“Is is it sufficiently unlikely that the distribution of scores we see in our dataset would have arisen by random sampling from an infinitely large population in which the data really do have a Gaussian distribution?”\nSo the null hypothesis about the population is that it does have a Gaussian distribution. What we are doing with tests of fit is applying some test to see if we can reject that model/hypothesis. We usually do that using the conventional criterion that we reject that null model/hypothesis if the probability comes out below .05, one in twenty.\nAs with pretty much any single level test of a single variable we have the usual caveats that the test will only give us an unbiased test of fit, i.e. reject the null model on less than one time in twenty when the population model is Gaussian if:\nthe dataset really is a random sample from an infinite population, and\nall the values/scores in the dataset are independent of one another (“independence of observations”).\nHowever, the general question above still begs the question: “on what criterion of fit?” In fact there are a number of possible tests because a univariate distribution can deviate from Gaussian in many ways and different criteria of fit have differing sensitivities to different deviations from Gaussian. I’m starting some posts about testing for fit to Gaussian distributions with one test, the first one I learned.\nThe K-S test\nThis is the Kolmogorov-Smirnov test and it used to be the default test of fit to Gaussian distribution in SPSS (but it’s a very long time since I used SPSS). The K-S test uses some clever maths, based on some clever statistical theory (see https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test for details) to test the biggest observed difference between the empirical cumulative distribution of the data and the cumulative distribution that would be expected from a Gaussian distribution. What does that mean?!\nEmpirical cumulative distribution functions\n(I think I see that abbreviated both as “ecdf” and as “ECDF” but never as “e.c.d.f.” or “E.C.D.F.”) Here’s the empirical cdf for the data above, that’s simulated data n = 350 that is from a Gaussian distribution.\nHere is the ecdf for a simulated sample n = 350. That sample has been created using R’s rnorm() function which gives an approximation to a truly random, independent observation, sample from a true Gaussian population. It’s as good an approximation as I will ever really need (and this is not the place to dig into why it’s an approximation though a bit of that will come back to us later).\n\n\nShow code\n\nggplot(data = tibRnorm,\n       aes(x = x)) +\n  stat_ecdf() +\n  ylab(\"Cumulative probability across observed values\") +\n  xlab(\"Observed values\") +\n  ggtitle(\"Empirical cumulative density function (ecdf)\",\n          subtitle = \"Sample, n = 350, from Gaussian distribution\")\n\n\n\nSo there’s that typical S shaped (ogive) shape which is the cumulative equivalent of the “bell shaped” histogram shape we saw above.\nWhen used to test the fit of a single sample against a theoretical distribution, here the Gaussian distribution, the criterion of mistfit the K-S test uses is the size of the largest vertical difference between the observed ecdf and the cdf for the Gaussian. (That’s a cdf not an ecdf because it is not empirical, it is defined by the Gaussian population model.) That’s easier to understand from a plot than from narratives like that I think. Here’s the plot.\n\n\nShow code\n\ntibRnorm %>%\n  arrange(x) %>%\n  mutate(x = round(x, 2),\n         indVal = 1 / 350,\n         cumP = cumsum(indVal)) %>%\n  select(-indVal) -> tibECDF\n\ntibble(x = seq(-3, 3, .01)) %>%\n  mutate( x = round(x, 2),\n          y = pnorm(x)) -> tibQnorm\n\ntibECDF %>%\n  left_join(tibQnorm, by = \"x\") %>% \n  mutate(diff = cumP - y,\n         absDiff = abs(diff),\n         minDiff = if_else(diff == min(diff), TRUE, FALSE),\n         maxDiff = if_else(diff == max(diff), TRUE, FALSE),\n         maxAbsDiff = if_else(absDiff == max(absDiff), TRUE, FALSE)) -> tmpTib\n\ntmpTib %>%\n  filter(maxAbsDiff) %>%\n  mutate(x2 = x + .1,\n         txt = paste0(\"For x = \",\n                      x, \n                      \":\\n observed(p) = \",\n                      round(cumP, 3),\n                      \"\\n expected(p) = \",\n                      round(y, 3),\n                      \"\\n difference = \",\n                      round(diff, 3))) -> tmpTib2\n\nggplot(data = tibRnorm,\n       aes(x = x)) +\n  stat_ecdf(colour = \"red\") +\n  geom_point(data = tmpTib,\n            aes(x = x, y = y),\n            size = .8) +\n  geom_linerange(data = tmpTib2,\n                 aes(ymin = cumP, ymax = y)) +\n  geom_label(data = tmpTib2,\n            aes(x = x2, y = cumP, label = txt),\n            hjust = 0,\n            vjust = 1,\n            size = 2.3) +\n  geom_vline(xintercept = tmpTib2$x,\n             linetype = 3) +\n  ylab(\"Cumulative probability across observed values\") +\n  xlab(\"Observed values\") +\n  ggtitle(\"Empirical cumulative density function (ecdf) versus population cdf\",\n          subtitle = \"Sample, n = 350, from Gaussian distribution\")\n\n\n\nThat plot shows the observed ecdf and the cdf expected from the Gaussian population model. Rather than showing the latter as a continuous curve I have plotted it as a series of points, each corresponding to a value that was in the sample data as it’s the vertical distances between those points and the corresponding point on the ecdf that the K-S test inspects.\nI have marked where the largest absolute vertical distance between the ecdf and cdf and annotated the plot to show what the difference in proportions was at that point. This is the criterion of misfit used by the K-S test.\nI think that’s a rather nice example of one way of testing for for fit to the Gaussian distribution and I like that it gives us a clear graphical sense of what’s going on in an NHST (even if I don’t begin to understand the maths). Here’s the raw K-S test ouput from the ks.test() function in R.\n\n\nShow code\n\nks.test(tibRnorm$x, \"pnorm\", alternative = \"two.sided\")\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  tibRnorm$x\nD = 0.078681, p-value = 0.02624\nalternative hypothesis: two-sided\n\nOoops! Statistically significant at the conventional p < .05 despite the fact that it’s a simulation so we know that’s a “false positive” as we know the model is sampling from a true standard Gaussian distribution. So here we have just hit on a sample where that maximum deviation is unlikely to have arisen by random sampling of samples of n = 350 from a population distribution that really is a standard Gaussian distribution.\nOops, lets check that that’s just our luck! I’ve simulated 10,000 K-S tests on samples of n = 350 taken from the standard Gaussian distribution.\nHere is the breakdown of the significance test results.\n\n\nShow code\n\nset.seed(12345)\nsampN <- 350\nnReplicns <- 10000\ntibble(replicn = 1:nReplicns) %>%\n  rowwise() %>% # to get a new sample for each replication\n  mutate(x = list(rnorm(sampN))) %>%\n  unnest_longer(x) %>%\n  group_by(replicn) %>% # to test each replication\n  summarise(ksP = ks.test(x, \"pnorm\", alternative = \"two.sided\")$p.value) %>%\n  ### test those at the conventional alpha of .05\n  mutate(sig = if_else(ksP < .05, \"p < .05\", \"NS\")) -> tibSimKSp\n\ntibSimKSp %>%\n  tabyl(sig) %>%\n  flextable() %>%\n  autofit()\n\nsignpercentNS9,5420.9542p < .054580.0458\n\nFor what it’s worth, here’s the distribution of those 10,000 simulation p values.\n\n\nShow code\n\nvecFillColours <- c(\"NS\" = \"green\", \"p < .05\" = \"red\")\nggplot(data = tibSimKSp,\n       aes(x = ksP, group = sig)) +\n  geom_histogram(aes(fill = sig),\n                 breaks = seq(0, 1, .05)) +\n  scale_fill_manual(\"Significance\",\n                    values = vecFillColours)\n\n\n\nOK, so it looks as if the K-S test is behaving OK at least as far as having a false positive rate about .05 as it should have.\nK-S test on real data\n\n\nShow code\n\ntibDat %>%\n  summarise(nCORE = n(),\n            minCORE = min(firstScore),\n            maxCORE = max(firstScore),\n            rangeCORE = maxCORE - minCORE,\n            meanCORE = mean(firstScore),\n            sdCORE = sd(firstScore)) -> tibCOREparms\n\ntibble(x = seq(tibCOREparms$minCORE, tibCOREparms$maxCORE, tibCOREparms$rangeCORE / tibCOREparms$nCORE)) %>%\n  # rowwise() %>%\n  mutate(y = dnorm(x, mean = tibCOREparms$meanCORE, sd = tibCOREparms$sdCORE))  -> tibDnorm\n\n # png(file = \"testGauss.png\", type = \"cairo\", width = 6000, height = 4800, res = 300)\nggplot(data = tibDat,\n       aes(x = firstScore)) +\n  geom_histogram(aes(x = firstScore,\n                     after_stat(density)),\n                 bins = 30,\n                 alpha = .6) +\n  geom_line(data = tibDnorm,\n            inherit.aes = FALSE,\n            aes(x = x, y = y))  +\n  geom_vline(xintercept = tibCOREparms$meanCORE) +\n  xlim(c(0, 4)) +\n  xlab(\"Initial CORE-OM score\") +\n  ggtitle(\"Histogram of initial CORE-OM scores, n = 182\",\n          subtitle = \"With Gaussian distribution shape fitted to observed mean and SD\")\n\n\n\nHm, that’s perhaps a surprisingly good fit judging by eye.\n\n\nShow code\n\nks.test(tibDat$firstScore, \"pnorm\", alternative = \"two.sided\")\n\nWarning in ks.test.default(tibDat$firstScore, \"pnorm\", alternative =\n\"two.sided\"): ties should not be present for the Kolmogorov-Smirnov\ntest\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  tibDat$firstScore\nD = 0.83585, p-value < 2.2e-16\nalternative hypothesis: two-sided\n\nBut that KS test result says that there is a likelihood of seeing a misfit as big as the KS test found that is less than 1 in 10^15, i.e. less than\n0.000,000,000,000,001. What?! What has happened? Is it something to do with that warning?\nK-S test warning\nThe warning is not the cause of the problem. What has happened is that we were testing a simulated sample from a truly Gaussian distribution that had used the full precision of R’s numbers that meant that creating a sample of n = 350 made it vanishingly unlikely that there would be any ties in that sample.\n(Geek bit: computer numbers are never truly continuous as computer construct numbers using binary bits of information (i.e. zero or one). However, the default “double precision” numbers of R have a precision that they can two numbers differing by only\n1.4901161^{-8}. That’s scientific representation of the minimum distinction between two R double precision numbers, in ordinary decimal format that’s\n0.000,000,014,901,16. Taking a sample of only n = 350 where numbers are distinct if the differ by more than that amount makes it vanishly unlikely that there will be any times in the sample: the data might as well be continuous.\nScaling and location\nSo requirement for the K-S test to be unbiased, i.e. to turn up a false positive rate of .05 if you are testing against that and if the samples are from a Gaussian distribution: of continuous data has been lost but that’s not the problem here. The problem is that the simple KS test tests fit to the standard Gaussian distribution with its range from minus infinity to plus infinity and its mean of zero and SD of 1.0. That is a “single sample” K-S test: one sample is compared to the properties of the assumed population distribution.\nHowever, as noted above, CORE-OM scores cannot fit the standard Gaussian not only because they’re not continuous but much more seriously because they can’t (realistically) have a mean of zero as zero is the minimum possible score on the CORE-OM. In fact, the CORE-OM scores range from zero to four and the observed mean is\n1.92 and SD of\n0.47. I can reset the KS test to test against a Gaussian distribution with those observed parameters (i.e. the population mean and SD). What I doing, in effect though not in the actual maths, is that I am relocating the scores to have a mean of zero and rescaling them to have an SD of 1.0.\n\n\nShow code\n\nks.test(tibDat$firstScore, \n        \"pnorm\",  tibCOREparms$meanCORE, tibCOREparms$sdCORE, # that's added the mean and SD\n        alternative = \"two.sided\",\n        exact = TRUE,\n        simulate.p.value = tRUE,\n        B = 10000)\n\nWarning in ks.test.default(tibDat$firstScore, \"pnorm\",\ntibCOREparms$meanCORE, : ties should not be present for the\nKolmogorov-Smirnov test\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  tibDat$firstScore\nD = 0.058888, p-value = 0.5337\nalternative hypothesis: two-sided\n\nSame warning of course about ties in the data but now the misfit is signalled as non-significant.\nThe catch with this is that we now have a biased test because we used information from the sample to create our comparison distribution.\nThe bias issue\nLet’s creat 10,000 samples from distributions that are Gaussian in shape but have mean values greater than zero and SD bigger than 1.0.\n\n\nShow code\n\nset.seed(12345)\nsampN <- 350\nnReplicns <- 10000\ntibble(replicn = 1:nReplicns,\n       popMean = runif(nReplicns, min = 1, max = 3),\n       popSD = runif(nReplicns, min = 2, max = 3)) %>%\n  rowwise() %>% # to get a new sample for each replication\n  mutate(x = list(rnorm(sampN, mean = popMean, sd = popSD))) %>%\n  unnest_longer(x) %>%\n  group_by(replicn) %>% # to test each replication\n  summarise(sampMean = mean(x),\n            sampSD = sd(x),\n            ksP = ks.test(x, \"pnorm\", sampMean, sampSD, alternative = \"two.sided\")$p.value) %>%\n  ### test those at the conventional alpha of .05\n  mutate(sig = if_else(ksP < .05, \"p < .05\", \"NS\")) -> tibSimKSpBiased\n\ntibSimKSpBiased %>%\n  tabyl(sig) -> tmpTib\n\ntmpTib %>%\n  filter(sig != \"NS\") %>%\n  select(percent) %>%\n  pull() -> falsePosBiased\n\ntmpTib %>%\n  flextable() %>%\n  autofit()\n\nsignpercentNS9,9980.9998p < .0520.0002\n\nOK, so the observed false positive rate of\n0.0002% is way below the rate of .05, or 5%, that we should be seeing if the test were unbiased.\nFor what it’s worth, here’s the distribution of those 10,000 simulation p values.\n\n\nShow code\n\nvecFillColours <- c(\"NS\" = \"green\", \"p < .05\" = \"red\")\nggplot(data = tibSimKSpBiased,\n       aes(x = ksP, group = sig)) +\n  geom_histogram(aes(fill = sig),\n                 breaks = seq(0, 1, .05)) +\n  scale_fill_manual(\"Significance\",\n                    values = vecFillColours)\n\n\n\nI am impressed by the obviousness of the bias! Point made and clearly!\nSummary\nThere are multiple ways of testing for statistically significant misfit to Gaussian distribution because there are multiple ways in which a distribution can differ from Gaussian. More on this in the next post here: Tests of fit to Gaussian distribution\nNo real distributions of raw data I work with will have true fit to the standard Gaussian distribution:\nthey won’t have possible limits at minus and plus infinity\nthey won’t be continuous (i.e. they will only be able to take discrete values)\nthey almost certainly won’t have mean zero nor SD 1.0\n\nHowever,\nsome intermediate variables created in the computation of some modern, sophisticated statistical analyses may be expected to have Gaussian distribution, perhaps even standard Gaussian distribution, so it can be useful to test them for misfit to get an indication of possible problems with the findings of the analyses.\nbootstrap confidence interval estimates may be more stable and accurate if the bootstrap replications show Gaussian (but not necessarily standard Gaussian) distribution so it can be useful to look at their distributions to get an indication of possible problems (and sometimes of the best bootstrap CI method to use to get the CI)\n\nThe Kolmogorov-Smirnov, K-S, test has a nice, easy to understand test and it’s fine if you really can expect a “standard Gaussian” population distribution, i.e. one with mean of zero and SD of 1.0.\nHowever, if there is no reason to think that the population distribution is standard Gaussian, but it is still interesting to test for misfit to Gaussian with mean != zero and SD != 1.0 then the K-S test is not a good test to use partly because it is biased. (There are other issues too but that’s getting seriously geeky.)\nGo to Tests of fit to Gaussian distribution to see alternative tests that keep the correct false positive rate (typically .05 when testing for p < .05) for samples from arbitrary Gaussian distributions and which have much greater sensitivity for most deviations from Gaussian than the K-S test.\n\n\n\n",
    "preview": "posts/2023-11-11-ks-test/ks-test_files/figure-html5/makegraphic-1.png",
    "last_modified": "2023-11-25T16:07:39+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-11-08-trycatch/",
    "title": "Using tryCatch() (within simulations)",
    "description": "I had previously found the structure of tryCatch() difficult: cracked it!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-11-08",
    "categories": [
      "R tricks",
      "simulation"
    ],
    "contents": "\n\nContents\nWhat is tryCatch()\nWhy this post?!\nWhy use tryCatch()?\nMinor option\nMain use for me\n\nVery simple example\n\nUse tryCatch() to prevent that crashing\nWhat if the return value is more complex than a single number?\nThe umbrella issue\n\n\n\nShow code\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nas_tibble(list(x = 1,\n               y = 1)) -> tmpDat\n\n# png(file = \"tryCatch.png\", type = \"cairo\", width = 6000, height = 4800, res = 300)\nggplot(data = tmpDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"tryCatch()\",\n            size = 32,\n            colour = \"red\",\n            angle = 30) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\nShow code\n\n# dev.off()\n\n\n[updated to improve clarity, I hope, 9.xi.23]\nWhat is tryCatch()\nThe tryCatch() function is part of base R and provides a wrapper around another function that, as the name says, tries that function and then catches any errors or warnings when that function is run.\nWhy this post?!\nBecause I don’t find the help for tryCatch() very helpful at all, to me, like quite a bit that I found when I searched the web about tryCatch(), it seemed to be written by people who understood it well and for people who are much more immersed in some of the more subtle and advanced aspects of R than I use. So this is partly to remind me of what I’ve just taught myself, which I will forget as I don’t use tryCatch() all that often. I hope it will also be useful to others.\nWhy use tryCatch()?\nMinor option\nI can see that I might want to use this to put different error or warning messages in some of my CECPfuns package of functions. The package is designed to make it easier for therapy and mental health practitioners and researchers who are not statisticians or R experts to use R’s enormous strengths. Some R functions, however amazing, can return errors and warnings that can be confusing for the my user group (and me!) Perhaps I should look through the other R functions I call in the package to see if some might throw warnings or errors that I could make more user friendly by wrapping them in a tryCatch(). Not an immediate priority.\nMain use for me\nMy main need for tryCatch() comes when I am creating simulations as sometimes these hit data that cannot be analysed and which then returns an error. Unless I trap the error it will crash the simulation: frustrating if it’s been running for hours and is on the 997th iteration of a planned 1,000! What I need, as I suspect do others who run simulations, is for the simulation to absorb the error, return missing values for whatever return value that function was going to return. That way the simulation can keep running. This is exactly what tryCatch() does!\n(This “use case”, as the IT people say, means that in this little post I’m only covering how tryCatch() handles errors, not warnings. A warning coming back during a simulation run doesn’t worry me as it won’t crash the run. There is a lot more that tryCatch() can do that I am also skipping as those things aren’t pertinent for me.)\nVery simple example\nHm, it’s surprisingly difficult to find really simple functions in R that don’t actually already do sensible things with mad inputs, usually already returning NA or NaN (“Not a Number”), and outputting a warning. Those wouldn’t crash a simulation.\nTrust me, complicated analyses such some psychometric and bootstrap ones can, not unreasonably, just return an error and stop on data coming from simulations (and real data)! OK, so here is a silly example, suppose my function wants to get the 5th and 95% quantiles of the first principal component of simulated repertory grids with the simulated grids having the same numbers of elements and constructs and the same scoring range\nHere is a function to do just that. (I have written the function to show the first simulated grid so you see what’s happening.)\n\n\nShow code\n\nsimulateGridPC1v1 <- function(nCons, nElem, minScore, maxScore, nSims) {\n  ### vector to store PCs\n  vecPCs <- rep(NA, nSims)\n  for (i in 1:nSims) {\n    tmpMat <- matrix(sample(minScore:maxScore, nCons * nElem, replace = TRUE), nrow = nCons)\n    if (i == 1){\n      ### print the first simulated grid to give the idea of the simulation\n      print(\"Here is the first simulated grid\")\n      print(tmpMat)\n      print(\" \")\n    }\n    ### get the first principal component of that simulated grid\n    ### yes, I used princomp() with cor = TRUE to make problems likely!\n    vecPCs[i] <- princomp(tmpMat, cor = TRUE)$sdev[1]\n  }\n  print(\"Here are the first PC values:\")\n  print(vecPCs)\n  ### but we just want the 5th and 95th quantiles of the first principal components\n  ### of the nSims simulated grids:\n  print(\"Finally the quantiles:\")\n  quantile(vecPCs, c(.05, .95))\n}\nset.seed(1234) # make simulation replicable\nsimulateGridPC1v1(5, 5, 0, 2, 10) # 5 constructs but only 5 elements and 0, 1, 2 scoring\n\n[1] \"Here is the first simulated grid\"\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    1    1    2\n[2,]    1    1    1    1    0\n[3,]    0    1    2    0    1\n[4,]    2    2    1    2    0\n[5,]    0    1    1    2    1\n[1] \" \"\n[1] \"Here are the first PC values:\"\n [1] 1.652448 1.544131 1.616389 1.409200 1.637459 1.647444 1.564582\n [8] 1.620288 1.644854 1.679319\n[1] \"Finally the quantiles:\"\n      5%      95% \n1.469919 1.667227 \n\nSo that worked for that simulation run with only 10 iterations. However, we clearly need more iterations for sensible estimation of those quantiles, let’s go to 10,000 iterations.\nset.seed(12345) # replicability (in principle)\nsimulateGridPC1v1(5, 5, 0, 2, 10000)\nBut that just gives this:\nQuitting from lines 84-86 [simulation2] (trycatch.Rmd)\nError in `princomp.default()`:\n! cannot use 'cor = TRUE' with a constant variable\nBacktrace:\n 1. global simulateGridPC1v1(5, 5, 0, 2, 10000)\n 3. stats:::princomp.default(tmpMat, cor = TRUE)\nExecution halted\nSo that crashed on some iteration of that function and it crashed because one of the elements came up with no variance, i.e. all the randomly simulated scores on that element were 1 (or were all 0).\nUse tryCatch() to prevent that crashing\nWhat I’ve done is to write a little function tryGetPC1() to replace the call princomp(tmpMat, cor = TRUE)$sdev[1].\n\n\nShow code\n\ntryGetPC1 <- function(tmpMat) {\n  ### function takes as input a simulated grid, tmpMat\n  ### create a tryCatch function\n  ### it is a function but to me the structure/semantics throw me as they feel unusual for R functions\n  retVal <- tryCatch(\n    ### in effect this first bit of code is the predefined first argument to tryCatch\n    ### it codes what will happen if this bit of code runs with no errors or warnings\n    {\n      ### if this call to princomp() runs without error the function will just return the answer from that call\n      return(princomp(tmpMat, cor = TRUE)$sdev[1])\n      ### however ...\n    },\n    ### that comma at the end of that line takes us into the next argument which always starts \"error = \"\n    ### this says what to do if the call in the first argument produced an error, so this is the bit of\n    ### tryCatch() that I need for simulations\n    ### the little function says what to do if that call failed with an error\n    error = function() { \n      ### that's really simple here, just return a single NA\n      ### my real life simulations sometimes need to return something more complex containing NA values\n      ### rather than just a single NA, more on that later in the post\n      return(NA)\n    }\n    ### we could have another comma and a new line saying what to do if the call ran but threw a warning\n    ### but I don't need that\n  )\n}\n\n\nOK, we now put that into the simulation, replacing the simple call to princomp()\n\n\nShow code\n\n### now I embed that in the simulation function\nsimulateGridPC1v2 <- function(nCons, nElem, minScore, maxScore, nSims) {\n  ### vector to store PCs\n  vecPCs <- rep(NA, nSims)\n  for (i in 1:nSims) {\n    ### as before, simulate a grid\n    tmpMat <- matrix(sample(minScore:maxScore, nCons * nElem, replace = TRUE), nrow = nCons)\n    ### but now use the tryCatch() function I created above\n    ### this will return the first PC for the grid if princomp() can compute that\n    ### but instead of throwing an error and stopping, it will just return NA and \n    ### allow everything to continue on if princomp() does trigger an error\n    vecPCs[i] <- tryGetPC1(tmpMat)\n  }\n  ### but we just want the 5th and 95th quantiles of the first principal components\n  ### of the nSims simulated grids:\n  ### need to allow missing values in quantile now\n  return(cat(\"That simulation run gave:\\n   n(unusuable simulations) = \", sum(is.na(vecPCs)),\n             \"\\n   n(usable simulations) = \", sum(!is.na(vecPCs)),\n             \"\\nand quantiles:\",\n             \"\\n   q05 = \", round(quantile(vecPCs, .05, na.rm = TRUE), 4),\n             \"\\n   q95 = \", round(quantile(vecPCs, .95, na.rm = TRUE), 4)))\n}\nset.seed(1234) # make simulation replicable\n### this should work as we know, from using simulateGridPC1v1 that these five simulations \n### don't throw errors\nsimulateGridPC1v2(5, 5, 0, 2, 10) # 5 constructs but only 5 elements and 0, 1, 2 scoring\n\nThat simulation run gave:\n   n(unusuable simulations) =  0 \n   n(usable simulations) =  10 \nand quantiles: \n   q05 =  1.4699 \n   q95 =  1.6672\n\nBut now we can handle simulated grids that can’t be crunched to give a first PC:\n\n\nShow code\n\nset.seed(12345) # replicability (in principle)\nsimulateGridPC1v2(5, 5, 0, 2, 10000)\n\nThat simulation run gave:\n   n(unusuable simulations) =  608 \n   n(usable simulations) =  9392 \nand quantiles: \n   q05 =  1.4265 \n   q95 =  1.8349\n\nSo we can see that 608 of those simulations would have crashed the run but with the tryCatch() the error is caught and the PC is replaced with NA.\nWhat if the return value is more complex than a single number?\nI needed to use this for simulation involving bootstrap CI estimates and which return something more complex than a single value. I realised that there are two sensible ways to handle that:\ncapture the full output from a run of the call that doesn’t throw and error and use dput() to get its structure then use that listing from dput to create a return in which you replace the bits that need to be replaced with NA.\nprobably better:\neither write a wrapper around the call you are using that returns something complex and just get the bits you want\nor just pull out what you want in the first bit of the tryCatch() call\n\nHere’s an example extracting the bits of a percentile bootstrap CI call:\n ### first the percentile method\n  getPercCI <- function(tmpBootRes, conf = conf){\n    tryCatch({\n        tmpCI <- boot::boot.ci(tmpBootRes, type = 'perc', conf = conf)\n        ### here just pick the bits you want to return\n        return(list(percLCLCSC = tmpCI$percent[4],\n                    percUCLCSC = tmpCI$percent[5]))\n      },\n      error = function(tmp) {\n        tmp <- NA\n        ### return the same structured named list but with NA values\n        return(list(percLCLCSC = NA,\n                    percUCLCSC = NA))\n      }\n    )\n  }\nThe umbrella issue\nThat’s it. I suspect I will need to come back to this every time I write a new simulation now. Or perhaps, now I have created the post I will, for the first time, remember how tryCatch() works and how to use it and never need to come back here. That’s what I call the umbrella issue: if I take one out with me, it won’t rain!\n\n\n\n",
    "preview": "posts/2023-11-08-trycatch/tryCatch.png",
    "last_modified": "2023-11-09T10:21:22+01:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4800
  },
  {
    "path": "posts/2023-11-01-plotjacobson/",
    "title": "Background on plotJacobson() function",
    "description": "1.xi.23 This is very early work in progress and will evolve a lot over the next week and then get into CECPfuns R package on github.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-11-01",
    "categories": [
      "RCSC paradigm",
      "Jacobson plot",
      "CECPfuns package",
      "plots and graphs"
    ],
    "contents": "\n\nContents\nIntroduction\nCategory systems\nCueing of the score: goodScorePos\nCategories: direction, RCI, CSC and RCSC\nDirection\nRCI categories “RCI3”\nRCSC categories “RCSC5”\nRCSC categories “RCSC7”\nRCSC categories “RCSC10”\n\nAll the arguments for getJacobsonPlot()\n\nOptional arguments, not for categories\nText options\nLine type options\nPoint labelling\nPoint labelling options\nPoint marker options\nLegend options\n\nSaving the plot\nTechnicalities (geek stuff)\nprintVariables\nThe plot areas\n\nImprovements/todo list\nRejected options\nThings still to do for getJacobsonPlot()\n\n\n[This started as an Rblog post on 1.xi.23. It is work in progress and will evolve a lot over the next weeks. When it’s ready it will be added to the CECPfuns R package on github. Latest updates 13.xi.23]\n\n\nShow code\n\n### this is just getting the data and then  adding a few artificial points\nif (interactive()) {\n  load(file = \"./_posts/2023-11-01-plotjacobson/tmpTib\") # running interactively \n} else {\n  load(file = \"tmpTib\") # in the full Rblog\n}\ntibDat <- tmpTib\ntmpTibDat <- tibble(id = c(997, 998, 999),\n                    firstScore = c(.5, .1, 1.1),\n                    lastScore = c(2, .7, .1))\nbind_rows(tmpTibDat,\n          tibDat) -> tibDat\nrm(tmpTib)\nrm(tmpTibDat)\n\nRCI <- getRCIfromSDandAlpha(sd(tibDat$firstScore), rel = 0.943242)\nCSC <- 1.26\n# minPossScore <- NA\n# maxPossScore <- NA\n# baselineScores <- tibDat$firstScore\n# finalScores <- tibDat$lastScore\n# pointNames <- tibDat$id\n# goodScorePos <- FALSE\n# \n# tibble(baselineScores = baselineScores,\n#        finalScores = finalScores,\n#        pointNames = pointNames) -> tmpTib\n\n### now we can define the function\ngetJacobsonPlot <- function(baselineScores, \n                          finalScores, \n                          pointNames = NA, # can be used to label points\n                          namesNotPoints = FALSE, # only use labels, no points\n                          nameRCIDetOnly = FALSE, # only label deteriorators\n                          namesSize = 2, # sets the font size for labels\n                          CSC = NA, # the CSC value to use\n                          RCI = NA, # the RCI value to use\n                          minPossScore = 0, \n                          maxPossScore = 4, \n                          goodScorePos = FALSE, # set to TRUE/T if a better score is positive\n                          omitNHS = FALSE, # use to omit scores starting in non-help-seeking range\n                          title = NULL, # to override the default title\n                          subtitle = NULL, # ditto for subtitle\n                          titleJustificn = .5, # centre justifies titles, 0 for L, 1 for R\n                          xLab = NULL, # overrides the default x axis label \n                          yLab = NULL, # overrides the default y axis label\n                          noLegend = FALSE, # removes legends for colours\n                          labelCSClines = FALSE, # set to TRUE/T to label CSC lines with CSC value\n                          areaColourScheme = c(\"NONE\", # mapping to use, see notes, schemes are:\n                                               \"RCSC5\",\n                                               \"DIRECTION\",\n                                               \"RCI3\",\n                                               \"CSC4\",\n                                               \"RCSC7\",\n                                               \"RCSC10\",\n                                               \"POLYGONS\"), \n                          legendNameAreas = \"Area mapping\", # set title for area legend\n                          pointColourScheme = c(\"RCSC5\",\n                                                \"RCI3\",\n                                                \"DIRECTION\",\n                                                \"CSC4\",\n                                                \"RCSC7\",\n                                                \"RCSC10\",\n                                                \"NONE\"), # chose point colour mapping, as above\n                          legendNamePoints = \"Point mapping\", # title for point colours\n                          ### now lots of mappings used by the above colour mappings\n                          coloursDirection = c(\"red\", \n                                               \"green\", \n                                               \"black\"),\n                          ### you can override the labels to local choice and language\n                          labelsDirection = c(\"Deterioration\", \n                                              \"Improvement\", \n                                              \"No change\"),\n                          coloursRCI3 = c(\"grey\", \n                                          \"red\", \n                                          \"green\"),\n                          labelsRCI3 = c(\"No reliable change\", \n                                         \"Reliable deterioration\", \n                                         \"Reliable improvement\"),\n                          coloursCSC4 = c(\"green\", \n                                          \"red\", \n                                          \"orange\", \n                                          \"skyblue1\"),\n                          labelsCSC4 = c(\"High to low\", \n                                         \"Low to high\", \n                                         \"Stayed high\", \n                                         \"Stayed low\"),\n                          coloursRCSC5 = c(\"grey\", \n                                           \"red\", \n                                           \"skyblue1\", \n                                           \"green\", \n                                           \"chartreuse4\"),\n                          labelsRCSC5 = c(\"No reliable change\", \n                                          \"Reliable deterioration\", \n                                          \"Reliable improvement, stayed high\", \n                                          \"Reliable improvement, high to low ('Recovered')\", \n                                          \"Reliable improvement, stayed low\"),\n                          ### it's essentially impossible to code labels for the RCSC\n                          ### that would be independent of the cueing of the scores\n                          ### hence different labels (but same colours and order)\n                          ### for positively cued measures\n                          labelsRCSC5pos = c(\"No reliable change\", \n                                             \"Reliable deterioration\", \n                                             \"Reliable improvement, stayed high\", \n                                             \"Reliable improvement, low to high ('Recovered')\", \n                                             \"Reliable improvement, stayed low\"),\n                          coloursRCSC7 = c(\"grey\", \n                                           \"orange\", \n                                           \"red\", \n                                           \"sienna2\", \n                                           \"palegreen\", \n                                           \"green\", \n                                           \"skyblue1\"),\n                          labelsRCSC7 = c(\"No reliable change\", \n                                          \"Reliable deterioration, stayed high\", \n                                          \"Reliable deterioration, low to high\", \n                                          \"Reliable deterioration, stayed low\", \n                                          \"Reliable improvement, stayed high\", \n                                          \"Reliable improvement, high to low ('Recovered')\", \n                                          \"Reliable improvement, stayed low\"),\n                          labelsRCSC7pos = c(\"No reliable change\", \n                                             \"Reliable deterioration, stayed low\", \n                                             \"Reliable deterioration, high to low\", \n                                             \"Reliable deterioration, stayed high\", \n                                             \"Reliable improvement, stayed low\", \n                                             \"Reliable improvement, low to high ('Recovered')\", \n                                             \"Reliable improvement, stayed high\"),\n                          coloursRCSC10 = c(\"yellow\", #1\n                                            \"blue\", #2\n                                            \"purple\", #3\n                                            \"grey\", #4\n                                            \"orange\", #5\n                                            \"red\", #6\n                                            \"violetred4\", #7\n                                            \"palegreen\", #8\n                                            \"green\", #9\n                                            \"chartreuse4\"), #10\n                          labelsRCSC10 = c(\"No reliable change, stayed high\", #1\n                                           \"No reliable change, high to low\", #2\n                                           \"No reliable change, low to high\", #3\n                                           \"No reliable change, stayed low\", #4\n                                           \"Reliable deterioration, stayed high\", #5 \n                                           \"Reliable deterioration, low to high\", #6\n                                           \"Reliable deterioration, stayed low\", #7\n                                           \"Reliable improvement, stayed high\", #8\n                                           \"Reliable improvement, high to low ('Recovered')\", #9\n                                           \"Reliable improvement, stayed low\"), #10\n                          labelsRCSC10pos = c(\"No reliable change, stayed low\", #1p\n                                              \"No reliable change, low to high\", #2p\n                                              \"No reliable change, high to low\", #3p\n                                              \"No reliable change, stayed high\", #4p\n                                              \"Reliable deterioration, stayed low\", #5p\n                                              \"Reliable deterioration, low to high\", #6p\n                                              \"Reliable deterioration, stayed high\", #7p\n                                              \"Reliable improvement, stayed low\", #8p\n                                              \"Reliable improvement, low to high ('Recovered')\", #9p\n                                              \"Reliable improvement, stayed high\"), #10p\n                          ### you can also add mapping to point shapes\n                          pointShapeScheme = c(\"NONE\",\n                                               \"DIRECTION\",\n                                               \"RCI3\"),\n                          ### these allow you to override the default mapping to \n                          ### a square and arrows pointing the appropriate ways\n                          ### NULL leaves the default mapping in place\n                          ### I really don't advise overriding these but you can!\n                          pointShapeNoRelChange = NULL,\n                          pointShapeRelDet = NULL,\n                          pointShapeRelImp = NULL,\n                          ### the following arguments control the aesthetics\n                          pointSize = 1,\n                          ### the default alpha settings allow both points and areas to be coloured\n                          pointAlpha = 1, # 1 is no transparency, 0 is no colour\n                          areaAlpha = .3, # ditto, .3 is probably OK for a pale coloured ground\n                          ### aesthetics for the no-change diagonal line\n                          lineTypeNoChange = 1, # default is solid line\n                          lineWidthNoChange = .5, # default is ggplot default line thickness\n                          lineColourNoChange = 1, # default is black\n                          ### same for the CSC lines\n                          lineTypeCSC = 1, \n                          lineWidthCSC = .5, \n                          lineColourCSC = 1, \n                          ### same for the RCI lines\n                          lineTypeRCI = 1,\n                          lineWidthRCI = .5,\n                          lineColourRCI = 1,\n                          themeToUse = theme_bw(), # set ggplot theme to use, unquoted\n                          ### I can't imagine changing these myself but ...\n                          ### these allow you to override the theme's colours\n                          plotColour = NULL, # background colour of the plot area itself, e.g. \"white\"\n                          borderColour = NULL, # and same for the border around it\n                          ### you might want to use these to suppress messages and warnings\n                          noMessages = FALSE, # suppress messages\n                          noWarnings = FALSE, # suppress my warnings (not other R ones)\n                          ### these are probably only useful to help me debug things\n                          printVariables = FALSE, # will give a print out of all internal variables\n                          warningsToErrors = FALSE, # makes warnings into errors, i.e. \n                          ### this is really only for someone drilling into the code\n                          ### it labels all twelve area polygons with their numbers\n                          labelAllPolygons = FALSE,\n                          ### arguments not implemented yet\n                          greyScale = FALSE # may be implemented to force B&W/greyscale\n){\n  ### \n  ### function to give single group Jacobson plot given vectors of pre and post scores\n  ### and fixed CSC and RCI\n  ### default, with goodScorePos == FALSE, is distress/dysfunction scoring i.e. higher scores worse\n  ### but can override that by using goodScorePos == TRUE\n  ### by default the function expects to be told minimum and maximum scores on the measure\n  ### these are simple numerical min and max, i.e. ignores whether scale is positive or negative \n  ### those default to zero and four as per original CORE scoring but can be overridden (doh!)\n  ### fixing either or both minPossScore and maxPossScore to NULL means the observed minimum\n  ### and maximum will be used, not recommended and may produce odd scale labelling\n  ###\n  ### get the arguments before anything is changed\n  listArgs <- as.list(match.call(expand.dots = FALSE)[-1])\n\n  ### handle warningsToErrors before other arguments\n  ### test it\n  if (!is.logical(warningsToErrors)){\n    stop(\"You have given warningsToErrors as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(warningsToErrors) != 1) {\n    stop(\"The argument warningsToErrors must be a single logical value.  Sorry: please go back and fix this.\")\n  }\n  \n  oldWarn <- getOption(\"warn\")\n  if (warningsToErrors) {\n    options(\"warn\" = 2)\n  }\n  \n  ### handle noMessages before other arguments\n  ### test it\n  if (!is.logical(noMessages)){\n    stop(\"You have given noMessages as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(noMessages) != 1) {\n    stop(\"The argument noMessages must be a single logical value.  Sorry: please go back and fix this.\")\n  }\n  \n  ### handle noWarnings before other arguments\n  ### test it\n  if (!is.logical(noWarnings)){\n    stop(\"You have given noWarnings as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(noWarnings) != 1) {\n    stop(\"The argument noWarnings must be a single logical value.  Sorry: please go back and fix this.\")\n  }\n  \n  ### now match the arguments which are switch choices, these will have the default match.arg error trapping\n  ###\n  areaColourScheme <- str_to_upper(match.arg(areaColourScheme))\n  pointColourScheme <- str_to_upper(match.arg(pointColourScheme))\n  pointShapeScheme <- match.arg(pointShapeScheme) ### all three options start differently so can do this:\n  pointShapeScheme <- str_to_upper(pointShapeScheme)\n  ###\n  ### sanity testing of other arguments\n  ###\n  if (length(baselineScores) != length(finalScores)) {\n    stop(\"Length of baseline and final scores must be the same\")\n  }\n  if (!is.numeric(baselineScores)) {\n    stop(\"Baseline scores must be numeric\")\n  }\n  if (!is.numeric(finalScores)) {\n    stop(\"Final scores must be numeric\")\n  }  \n  if (!is.na(pointNames[1]) & (length(pointNames) != length(baselineScores))) {\n    stop(\"When given, the length of labels (pointNames) must be same as lengths of scores\")\n  }\n  if (!is.na(pointNames[1]) & is.numeric(pointNames)) {\n    if (!noWarnings) {\n      warning(\"You have given a numeric vector for pointNames.  If you have many numbers this will be a mess!\")\n    }\n    pointNames <- as.character(pointNames)\n  }\n  if (!is.na(pointNames[1]) & !(any(is.character(pointNames),\n                                    is.factor(pointNames),\n                                    is.numeric(pointNames)))) {\n    stop(\"You have given pointNames as something other than numeric (discouraged), character or factor. Sorry: please go back and fix this.\")\n  }\n  if (!is.logical(namesNotPoints)){\n    stop(\"You have given namesNotPoints as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(namesNotPoints) != 1) {\n    stop(\"The argument namesNotPoints must be a single logical value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.logical(nameRCIDetOnly)){\n    stop(\"You have given nameRCIDetOnly as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(nameRCIDetOnly) != 1) {\n    stop(\"The argument nameRCIDetOnly must be a single logical value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(namesSize)) {\n    stop(\"The value for namesSize must be numeric. Sorry: please go back and fix this.\")\n  }\n  if (length(namesSize) != 1) {\n    stop(\"The argument namesSize must be a single number. Sorry: please go back and fix this.\")\n  }\n  if (namesSize < 0) {\n    stop(\"The value for namesSize must be a positive number. Sorry: please go back and fix this.\")\n  }\n  if (!is.na(CSC) & !is.numeric(CSC)){\n    stop(\"You have given CSC as something other than a numeric value.  Sorry: please go back and fix this.\")\n  }\n  if (length(CSC) != 1) {\n    stop(\"The argument CSC must be a single numeric value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.na(RCI) & !is.numeric(RCI)){\n    stop(\"You have given RCI as something other than a numeric value.  Sorry: please go back and fix this.\")\n  }\n  if (length(RCI) != 1) {\n    stop(\"The argument RCI must be a single numeric value.  Sorry: please go back and fix this.\")\n  }\n  if (is.na(minPossScore)){\n    stop(\"You have not given a value for minPossScore (NA is the default).  Sorry, but I set that default to force users to choose the appropriate value.  It must be the lowest numeric value possible for the score, not the worst score (which would be the lowest number possible for a positively cued measure).  Sorry: please go back and fix this.\")\n  }\n  if (is.na(minPossScore) & !is.numeric(minPossScore)){\n    stop(\"You have given minPossScore as something other than a numeric value.  Sorry: please go back and fix this.\")\n  }\n  if (length(minPossScore) != 1) {\n    stop(\"The argument minPossScore must be a single numeric value.  Sorry: please go back and fix this.\")\n  }\n  if (is.na(maxPossScore)){\n    stop(\"You have not given a value for maxPossScore (NA is the default).  Sorry, but I set that default to force users to choose the appropriate value.  It must be the highest numeric value possible for the score, not the best score (which would be the lowest number possible for a positively cued measure).  Sorry: please go back and fix this.\")\n  }\n  if (!is.na(maxPossScore) & !is.numeric(maxPossScore)){\n    stop(\"You have given maxPossScore as something other than a numeric value.  Sorry: please go back and fix this.\")\n  }\n  if (length(maxPossScore) != 1) {\n    stop(\"The argument maxPossScore must be a single numeric value.  Sorry: please go back and fix this.\")\n  }\n  if (maxPossScore < minPossScore) {\n    msg <- paste0(\"You have given minPossScore as \",\n                  minPossScore,\n                  \" and maxPossScore as \",\n                  maxPossScore,\n                  \"\\nbut these must be in order of actual scores not cueing of scores.\",\n                  \"\\ni.e. maxPossScore must be higher than minPossScore whether higher scores are better or worse than lower scores.  Sorry: please go back and fix this.\")\n    stop(msg)\n  }\n  if (!is.na(CSC) & (CSC < minPossScore |\n                     CSC > maxPossScore)) {\n    msg <- paste0(\"You have given CSC as \",\n                  CSC,\n                  \" and minPossScore as \",\n                  minPossScore,\n                  \" and maxPossScore as \",\n                  maxPossScore,\n                  \"\\nbut CSC must lie between the possible scores.  Sorry: please go back and fix this.\")\n    stop(msg)\n  }\n  if (!is.logical(goodScorePos)){\n    stop(\"You have given goodScorePos as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(goodScorePos) != 1) {\n    stop(\"The argument goodScorePos must be a single logical value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.logical(omitNHS)){\n    stop(\"You have given omitNHS as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(omitNHS) != 1) {\n    stop(\"The argument omitNHS must be a single logical value.  Sorry: please go back and fix this.\")\n  }\n  \n  if (!is.null(title) & !is.character(title)) {\n    stop(\"If you are giving a title it must be a single character value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(title) & length(title) != 1) {\n    stop(\"If you are giving a title it must be a single value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(subtitle) & !is.character(subtitle)) {\n    stop(\"If you are giving a subtitle it must be a single character value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(subtitle) & length(subtitle) != 1) {\n    stop(\"If you are giving a subtitle it must be a single value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(titleJustificn)) {\n    stop(\"The argument titleJustificn must be a single number. Sorry: please go back and fix this.\")\n  }\n  if (length(titleJustificn) != 1) {\n    stop(\"The argument titleJustificn must be a single number. Sorry: please go back and fix this.\")\n  }\n  if (!(titleJustificn %in% c(0, .5, 1))) {\n    msg <- paste0(\"You gave titleJustificn as \",\n                  titleJustificn,\n                  \"but it must be one of: 0 (left justified), .5 (centred) or 1 (right justified). Sorry: please go back and fix this.\")\n    stop(msg)\n  }\n  if (!is.null(xLab) & !is.character(xLab)) {\n    stop(\"If you are giving a label for the x axis it must be a single character value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(xLab) & length(xLab) != 1) {\n    stop(\"If you are giving a label for the x axis it must be a single value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(yLab) & !is.character(yLab)) {\n    stop(\"If you are giving a label for the y axis it must be a single character value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(yLab) & length(yLab) != 1) {\n    stop(\"If you are giving a label for the y axis it must be a single value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.logical(noLegend)){\n    stop(\"You have given noLegend as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(noLegend) != 1) {\n    stop(\"The argument noLegend must be a single logical value.  Sorry: please go back and fix this.\")\n  }  \n  if (!is.logical(labelCSClines)){\n    stop(\"You have given labelCSClines as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(labelCSClines) != 1) {\n    stop(\"The argument labelCSClines must be a single logical value.  Sorry: please go back and fix this.\")\n  } \n  if (!is.null(legendNameAreas) & !is.character(legendNameAreas)) {\n    stop(\"If you are giving a new label to the area legend it must be a single character value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(legendNameAreas) & length(legendNameAreas) != 1) {\n    stop(\"If you are giving a new label to the area legend it must be a single value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(legendNamePoints) & !is.character(legendNamePoints)) {\n    stop(\"If you are giving a new label to the point legend it must be a single character value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(legendNamePoints) & length(legendNamePoints) != 1) {\n    stop(\"If you are giving a new label to the point it must be a single value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(coloursDirection)) {\n    stop(\"The argument coloursDirection must be a character vector of length three.  Sorry: please go back and fix this.\")\n  }\n  if (length(coloursDirection) != 3) {\n    stop(\"The argument coloursDirection must be a character vector of length three.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(labelsDirection)) {\n    stop(\"The argument labelsDirection must be a character vector of length three.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelsDirection) != 3) {\n    stop(\"The argument labelsDirection must be a character vector of length three.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(coloursRCI3)) {\n    stop(\"The argument coloursRCI3 must be a character vector of length three.  Sorry: please go back and fix this.\")\n  }\n  if (length(coloursRCI3) != 3) {\n    stop(\"The argument coloursRCI3 must be a character vector of length three.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(labelsRCI3)) {\n    stop(\"The argument labelsRCI3 must be a character vector of length three.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelsRCI3) != 3) {\n    stop(\"The argument labelsRCI3 must be a character vector of length three.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(coloursCSC4)) {\n    stop(\"The argument coloursCSC4 must be a character vector of length four.  Sorry: please go back and fix this.\")\n  }\n  if (length(coloursCSC4) != 4) {\n    stop(\"The argument coloursCSC4 must be a character vector of length four.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(labelsCSC4)) {\n    stop(\"The argument labelsCSC4 must be a character vector of length four.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelsCSC4) != 4) {\n    stop(\"The argument labelsCSC4 must be a character vector of length four.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(coloursRCSC5)) {\n    stop(\"The argument labelsCSC4 must be a character vector of length five.  Sorry: please go back and fix this.\")\n  }\n  if (length(coloursRCSC5) != 5) {\n    stop(\"The argument coloursRCSC5 must be a character vector of length five.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(labelsRCSC5)) {\n    stop(\"The argument labelsRCSC5 must be a character vector of length five.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelsRCSC5) != 5) {\n    stop(\"The argument labelsRCSC5 must be a character vector of length five.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(labelsRCSC5pos)) {\n    stop(\"The argument labelsRCSC5pos must be a character vector of length five.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelsRCSC5pos) != 5) {\n    stop(\"The argument labelsRCSC5pos must be a character vector of length five.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(coloursRCSC7)) {\n    stop(\"The argument coloursRCSC7 must be a character vector of length seven.  Sorry: please go back and fix this.\")\n  }\n  if (length(coloursRCSC7) != 7) {\n    stop(\"The argument coloursRCSC7 must be a character vector of length seven.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(labelsRCSC7)) {\n    stop(\"The argument labelsRCSC7 must be a character vector of length seven.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelsRCSC7) != 7) {\n    stop(\"The argument labelsRCSC7 must be a character vector of length seven.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(labelsRCSC7pos)) {\n    stop(\"The argument labelsRCSC7pos must be a character vector of length seven.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelsRCSC7pos) != 7) {\n    stop(\"The argument labelsRCSC7pos must be a character vector of length seven.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(coloursRCSC10)) {\n    stop(\"The argument coloursRCSC10 must be a character vector of length ten.  Sorry: please go back and fix this.\")\n  }\n  if (length(coloursRCSC10) != 10) {\n    stop(\"The argument coloursRCSC10 must be a character vector of length ten.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(labelsRCSC10)) {\n    stop(\"The argument labelsRCSC10 must be a character vector of length ten.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelsRCSC10) != 10) {\n    stop(\"The argument labelsRCSC10 must be a character vector of length ten.  Sorry: please go back and fix this.\")\n  }\n  if (!is.character(labelsRCSC10pos)) {\n    stop(\"The argument labelsRCSC10pos must be a character vector of length ten.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelsRCSC10pos) != 10) {\n    stop(\"The argument labelsRCSC10pos must be a character vector of length ten.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(pointShapeNoRelChange)) {\n    if (length(pointShapeNoRelChange) != 1) {\n      stop(\"The argument pointShapeNoRelChange must be an integer between 0 and 25 (inclusive). Sorry: please go back and fix this.\")\n    }    \n    if (is.numeric(pointShapeNoRelChange)) {\n      if (!(pointShapeNoRelChange %in% 1:25)) {\n        stop(\"The argument pointShapeNoRelChange must be an integer between 0 and 25 (inclusive). Sorry: please go back and fix this.\")\n      }\n    }\n  }\n  if (!is.null(pointShapeRelDet)) {\n    if (length(pointShapeRelDet) != 1) {\n      stop(\"The argument pointShapeRelDet must be an integer between 0 and 25 (inclusive). Sorry: please go back and fix this.\")\n    }    \n    if (is.numeric(pointShapeRelDet)) {\n      if (!(pointShapeRelDet %in% 1:25)) {\n        stop(\"The argument pointShapeRelDet must be an integer between 0 and 25 (inclusive). Sorry: please go back and fix this.\")\n      }\n    }\n  }\n  if (!is.null(pointShapeRelImp)) {\n    if (length(pointShapeRelImp) != 1) {\n      stop(\"The argument pointShapeRelImp must be an integer between 0 and 25 (inclusive). Sorry: please go back and fix this.\")\n    }    \n    if (is.numeric(pointShapeRelImp)) {\n      if (!(pointShapeRelImp %in% 1:25)) {\n        stop(\"The argument pointShapeRelImp must be an integer between 0 and 25 (inclusive). Sorry: please go back and fix this.\")\n      }\n    }\n  }\n  if (!is.numeric(pointSize)) {\n    stop(\"The argument pointSize must be an integer between 0 and 25 (inclusive). Sorry: please go back and fix this.\")\n  }\n  if (length(pointSize) != 1) {\n    stop(\"The argument pointSize must be an integer between 0 and 25 (inclusive). Sorry: please go back and fix this.\")\n  }\n  if (pointSize < 0) {\n    stop(\"The argument pointSize must be a positive number. Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(pointAlpha)) {\n    stop(\"The argument pointAlpha must be a number between 0 and 1 (inclusive). Sorry: please go back and fix this.\")\n  }\n  if (length(pointAlpha) != 1) {\n    stop(\"The argument pointAlpha must be a number between 0 and 1 (inclusive). Sorry: please go back and fix this.\")\n  }\n  if (pointAlpha < 0 | pointAlpha > 1) {\n    stop(\"The argument pointAlpha must be a number between 0 and 1 (inclusive). Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(areaAlpha)) {\n    stop(\"The argument areaAlpha must be a number between 0 and 1 (inclusive). Sorry: please go back and fix this.\")\n  }\n  if (length(areaAlpha) != 1) {\n    stop(\"The argument areaAlpha must be a number between 0 and 1 (inclusive). Sorry: please go back and fix this.\")\n  }\n  if (areaAlpha < 0 | areaAlpha > 1) {\n    stop(\"The argument areaAlpha must be a number between 0 and 1 (inclusive). Sorry: please go back and fix this.\")\n  }\n  \n  if (!is.numeric(lineTypeNoChange) & !is.character(lineTypeNoChange)) {\n    stop(\"The argument lineTypeNoChange must be a character or number. Sorry: please go back and fix this.\")\n  }\n  if (length(lineTypeNoChange) != 1) {\n    stop(\"The argument lineTypeNoChange must be a single value. Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(lineWidthNoChange)) {\n    stop(\"The argument lineWidthNoChange must be a number. Sorry: please go back and fix this.\")\n  }\n  if (length(lineWidthNoChange) != 1) {\n    stop(\"The argument lineWidthNoChange must be a single number. Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(lineColourNoChange) & !is.character(lineColourNoChange)) {\n    stop(\"The argument lineColourNoChange must be a character or number. Sorry: please go back and fix this.\")\n  }\n  if (length(lineColourNoChange) != 1) {\n    stop(\"The argument lineColourNoChange must be a single number or character. Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(lineTypeCSC) & !is.character(lineTypeCSC)) {\n    stop(\"The argument lineTypeCSC must be a character or number. Sorry: please go back and fix this.\")\n  }\n  if (length(lineTypeCSC) != 1) {\n    stop(\"The argument lineTypeCSC must be a single number or character. Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(lineWidthCSC)) {\n    stop(\"The argument lineWidthCSC must be a number. Sorry: please go back and fix this.\")\n  }\n  if (length(lineWidthCSC) != 1) {\n    stop(\"The argument lineWidthCSC must be a single number. Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(lineColourCSC) & !is.character(lineColourCSC)) {\n    stop(\"The argument lineColourCSC must be a character or number. Sorry: please go back and fix this.\")\n  }\n  if (length(lineColourCSC) != 1) {\n    stop(\"The argument lineColourCSC must be a single number or character. Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(lineTypeRCI) & !is.character(lineTypeRCI)) {\n    stop(\"The argument lineTypeRCI must be a character or number. Sorry: please go back and fix this.\")\n  }\n  if (length(lineTypeRCI) != 1) {\n    stop(\"The argument lineTypeRCI must be a single number or character. Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(lineWidthRCI)) {\n    stop(\"The argument lineWidthRCI must be a number. Sorry: please go back and fix this.\")\n  }\n  if (length(lineWidthRCI) != 1) {\n    stop(\"The argument lineWidthRCI must be a single number. Sorry: please go back and fix this.\")\n  }\n  if (!is.numeric(lineColourRCI) & !is.character(lineColourRCI)) {\n    stop(\"The argument lineColourRCI must be a character or number. Sorry: please go back and fix this.\")\n  }\n  if (length(lineColourRCI) != 1) {\n    stop(\"The argument lineColourRCI must be a single number or character. Sorry: please go back and fix this.\")\n  }\n  if (!is.list(themeToUse)) {\n    stop(\"The argument themeToUse must be a list (actually a ggplot theme but I can't test for that!). Sorry: please go back and fix this.\")\n  }\n  if (!is.logical(labelAllPolygons)){\n    stop(\"You have given labelAllPolygons as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(plotColour) & !is.character(plotColour)) {\n    stop(\"If used, the argument plotColour must be a single character value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(plotColour) & length(plotColour) != 1) {\n    stop(\"If used, the argument plotColour must be a single character value not a vector.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(borderColour) & !is.character(borderColour)) {\n    stop(\"If used, the argument borderColour must be a single character value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.null(plotColour) & length(plotColour) != 1) {\n    stop(\"If used, the argument borderColour must be a single character value not a vector.  Sorry: please go back and fix this.\")\n  }\n  if (!is.logical(printVariables)) {\n    stop(\"The argument printVariables must be a logical, i.e. TRUE | FALSE | T | F.  Sorry: please go back and fix this.\")\n  }\n  if (length(printVariables) != 1) {\n    stop(\"The argument printVariables must be a single logical value, not a vector.  Sorry: please go back and fix this.\")\n  }\n  if (length(labelAllPolygons) != 1) {\n    stop(\"The argument labelAllPolygons must be a single logical value.  Sorry: please go back and fix this.\")\n  }\n  if (!is.logical(greyScale)){\n    stop(\"You have given greyScale as something other than a logical value (TRUE, FALSE, T or F).  Sorry: please go back and fix this.\")\n  }\n  if (length(greyScale) != 1) {\n    stop(\"The argument greyScale must be a single logical value.  Sorry: please go back and fix this.\")\n  }\n  ### must have shape and point colour schemes the same if both set\n  if (pointShapeScheme != \"NONE\" & pointColourScheme != \"NONE\") {\n    if (pointShapeScheme != pointColourScheme) {\n      errTxt <- paste0(\"The plot is confusing if pointShapeScheme and pointColourScheme are not the same.\",\n                       \"\\n You input: \\npointShapeScheme = \",\n                       pointShapeScheme,\n                       \" and pointColourScheme = \",\n                       pointColourScheme,\n                       \"\\n either set one of them to 'NONE' or set both to the same value.\")\n      stop(errTxt)\n    }\n  }\n  \n  \n  ### trying to allow theme setting in the arguments\n  oldTheme <- theme_get()\n  theme_set(themeToUse)\n  \n  ### \n  ### start to handle the arguments\n  ###\n\n  ###\n  ### set point shapes\n  ### makes sense to do this before handling the data?\n  ###\n  if(pointShapeScheme == \"NONE\"){\n    ### default point shape scheme is by RCI with filled circles\n    pointShape <- 16\n    vecPointShapes <- rep(pointShape, 3)\n    ### obviously could have done that in one line but makes principle clear\n  } else {\n    ### alternative point shape scheme is by variables shapes in tmpTib (later)\n    ### with these shapes\n    if(is.null(pointShapeNoRelChange)) {\n      pointShapeNoRelChange <- 23\n    }\n    if(is.null(pointShapeRelDet)) {\n      if(!goodScorePos) {\n        pointShapeRelDet <- 24\n      } else {\n        pointShapeRelDet <- 25\n      }\n    }\n    if(is.null(pointShapeRelImp)) {\n      if(!goodScorePos) {\n        pointShapeRelImp <- 25\n      } else {\n        pointShapeRelImp <- 24\n      }\n    }\n    ### set the mapping to be used in scale_shape_manual()\n    vecPointShapes <- c(pointShapeRelDet , pointShapeRelImp, pointShapeNoRelChange)\n    if(pointShapeScheme == \"NONE\") {\n      names(vecPointShapes) <- rep(\"default\", 3)\n      vecShapeLabels <- rep(\"\", 3)\n    }\n    if(pointShapeScheme == \"DIRECTION\") {\n      names(vecPointShapes) <- c(\"Det\", \"Imp\", \"NoChange\")\n      vecShapeLabels = labelsDirection\n    }\n    if(pointShapeScheme == \"RCI3\") {\n      names(vecPointShapes) <- c(\"RelDet\", \"RelImp\", \"NoRelChange\")\n      vecShapeLabels <- labelsRCI3\n    }\n  }\n  \n  ### \n  ### OK now handle the data\n  ###\n   \n  ### start of main function to use the score data\n  ###\n  ### start with names for points if supplied or not\n  if(length(pointNames) > 1) {\n    pointNames <- as.character(pointNames) \n  } else {\n    pointNames <- as.character(1:length(baselineScores))\n  }\n  \n  ### get score range (may come in useful positioning things on plot)\n  scoreRange <- abs(maxPossScore - minPossScore)\n  \n  ### OK now build the data\n  tibble(pointNames = pointNames,\n         firstScore = baselineScores,\n         lastScore = finalScores,\n         goodScorePos = goodScorePos) -> tmpTib\n  \n  ### deal with situations of not having been given a CSC or RCI (or both missing)\n  if(is.na(CSC) & is.na(RCI)){\n    # transfer pertinent arguments which are really only:\n    # tmpTib, minPoss & maxPoss, noChangeline and default point and direction arguments\n    # JacobsonPlotNoRCSC() \n    return()\n  } \n  ### deal with not having been given an RCI\n  if(is.na(RCI)){\n    # JacobsonPlotNoRCI() # transfer pertinent arguments\n    return()\n  }\n  if(is.na(CSC)){\n    # JacobsonPlotNoCSC() # transfer pertinent arguments\n    return()\n  }\n  \n  ###\n  ### now get the missing data summary information\n  ###\n  Original_n <- nrow(tmpTib)\n  tmpTib %>%\n    reframe(FirstMissing = getNNA(firstScore),\n            LastMissing = getNNA(lastScore),\n            ### now we are in the full RCSC situation with a CSC and RCI, so ...\n            nStartTroubled = if_else(goodScorePos, \n                                     sum(firstScore <= CSC, na.rm = TRUE),\n                                     sum(firstScore >= CSC, na.rm = TRUE)),\n            nStartOK = Original_n - nStartTroubled) %>%\n    pivot_longer(cols = everything()) -> tmpTibMissing\n  \n  tmpTib %>%\n    ### get rid of those with missing scores\n    drop_na() -> tmpTib\n  Complete <- nrow(tmpTib)\n  Incomplete <- Original_n - Complete\n  \n  tibble(Original_n = Original_n,\n         Incomplete = Incomplete,\n         Complete = Complete) %>% \n    pivot_longer(cols = everything()) -> tmpTib2\n  \n  bind_rows(tmpTibMissing,\n            tmpTib2) %>%\n    rename(n = value,\n           What = name) %>%\n    mutate(What = ordered(What,\n                          levels = c(\"Original_n\",\n                                     \"FirstMissing\",\n                                     \"LastMissing\",\n                                     \"Incomplete\",\n                                     \"Complete\",\n                                     \"nStartOK\",\n                                     \"nStartTroubled\"),\n                          labels = c(\"Original data\",\n                                     \"Missing first score\",\n                                     \"Missing last score\",\n                                     \"Incomplete (either score missing)\",\n                                     \"Complete data\",\n                                     \"Started in non-help-seeking range\",\n                                     \"Started in help-seeking range\"\n                                     ))) %>%\n    arrange(What) -> tmpTibMissing\n  \n  tmpTibMissing %>%\n    filter(What == \"Started in help-seeking range\") %>%\n    select(n) %>%\n    pull() -> nHelpSeeking\n  \n  rm(tmpTib2)\n  ### finished summarising missing data\n  \n  ### start processing the data\n  tmpTib %>%\n    mutate(RCIcrit = RCI,\n           CSCcrit = CSC, \n           firstLastChange = lastScore - firstScore) -> tmpTib\n  \n  ### I don't like this but ...\n  if (omitNHS) {\n    tmpTib %>%\n      mutate(startHS = if_else(goodScorePos, \n                              firstScore <= CSCcrit,\n                              firstScore >= CSCcrit)) %>%\n      filter(startHS) -> tmpTib\n  }\n  \n### can now build the default subtitle\n  if(is.null(subtitle)) {\n    subtitle = paste0(\"Showing \", \n                      Complete,\n                      \" with first and last scores from \",\n                      Original_n,\n                      \" rows in the dataset;\\n\",\n                      nHelpSeeking,\n                      \" in help-seeking score range, \",\n                      nrow(tmpTib),\n                      \" used.\")\n  }\n  \n  tmpTib %>%\n    mutate(### get direction\n           direction3 = case_when(firstLastChange == 0 ~ \"NoChange\",\n                                  goodScorePos & firstLastChange < 0 ~ \"Det\",\n                                  goodScorePos & firstLastChange > 0 ~ \"Imp\",\n                                  !goodScorePos & firstLastChange < 0 ~ \"Imp\",\n                                  !goodScorePos & firstLastChange > 0 ~ \"Det\"),\n           ### now get RCI categories: \n           RCIchange = if_else(abs(firstLastChange) < RCIcrit, \"NoRelChange\", paste0(\"Rel\", direction3)),\n           firstCSCstatus = case_when(!goodScorePos & firstScore >= CSCcrit ~ \"HS\", \n                                      !goodScorePos & firstScore < CSCcrit ~ \"NHS\",\n                                      goodScorePos & firstScore < CSCcrit ~ \"HS\", \n                                      goodScorePos & firstScore >= CSCcrit ~ \"NHS\"),\n           lastCSCstatus = case_when(!goodScorePos & lastScore >= CSCcrit ~ \"HS\", \n                                     !goodScorePos & lastScore < CSCcrit ~ \"NHS\",\n                                     goodScorePos & lastScore < CSCcrit ~ \"HS\", \n                                     goodScorePos & lastScore >= CSCcrit ~ \"NHS\"),\n           CSC4 = paste0(firstCSCstatus, \"To\", lastCSCstatus),\n           RCSC5 = case_when(\n             RCIchange == \"NoRelChange\" ~ \"NoRelChange\",\n             RCIchange == \"RelDet\"  ~ \"RelDet\",\n             RCIchange == \"RelImp\" & CSC4 == \"HSToHS\" ~ \"RelImpHSToHS\",\n             RCIchange == \"RelImp\" & CSC4 == \"HSToNHS\" ~ \"RelImpHSToNHS\",\n             RCIchange == \"RelImp\" & CSC4 == \"NHSToNHS\" ~ \"RelImpNHSToNHS\"),\n           RCSC7 = case_when(\n             RCIchange == \"NoRelChange\" ~ \"NoRelChange\",\n             RCIchange == \"RelDet\" & CSC4 == \"HSToHS\" ~ \"RelDetHSToHS\",\n             RCIchange == \"RelDet\" & CSC4 == \"NHSToNHS\" ~ \"RelDetNHSToNHS\",\n             RCIchange == \"RelDet\" & CSC4 == \"NHSToHS\" ~ \"RelDetNHSToHS\",\n             RCIchange == \"RelImp\" & CSC4 == \"HSToHS\" ~ \"RelImpHSToHS\",\n             RCIchange == \"RelImp\" & CSC4 == \"HSToNHS\" ~ \"RelImpHSToNHS\",\n             RCIchange == \"RelImp\" & CSC4 == \"NHSToNHS\" ~ \"RelImpNHSToNHS\"),\n           RCSC10 = case_when(\n             RCIchange == \"NoRelChange\" & CSC4 == \"HSToHS\" ~ \"NoRelChangeHSToHS\",\n             RCIchange == \"NoRelChange\" & CSC4 == \"HSToNHS\" ~ \"NoRelChangeHSToNHS\",\n             RCIchange == \"NoRelChange\" & CSC4 == \"NHSToNHS\" ~ \"NoRelChangeNHSToNHS\",\n             RCIchange == \"NoRelChange\" & CSC4 == \"NHSToHS\" ~ \"NoRelChangeNHSToHS\",\n             RCIchange == \"RelDet\" & CSC4 == \"HSToHS\" ~ \"RelDetHSToHS\",\n             RCIchange == \"RelDet\" & CSC4 == \"NHSToHS\" ~ \"RelDetNHSToHS\",\n             RCIchange == \"RelDet\" & CSC4 == \"NHSToNHS\" ~ \"RelDetNHSToNHS\",\n             RCIchange == \"RelImp\" & CSC4 == \"HSToHS\" ~ \"RelImpHSToHS\",\n             RCIchange == \"RelImp\" & CSC4 == \"HSToNHS\" ~ \"RelImpHSToNHS\",\n             RCIchange == \"RelImp\" & CSC4 == \"NHSToNHS\" ~ \"RelImpNHSToNHS\")) -> tmpTib\n  \n  ### handle shapes\n  if(pointShapeScheme == \"NONE\") {\n    tmpTib %>%\n    mutate(Shapes = \"default\") -> tmpTib\n  } else {\n    tmpTib %>%\n      mutate(Shapes = case_when(pointShapeScheme == \"DIRECTION\" ~ direction3,\n                              pointShapeScheme == \"RCI3\" ~ RCIchange)) -> tmpTib\n  }\n  ### now set up the shapes to use\n  ### handle nameRCIDetOnly, i.e. labelling only of reliably deteriorated\n  if(nameRCIDetOnly){\n    tmpTib %>%\n      ### recode pointNames so only reliable deteriorators have an ID/name\n      mutate(pointNames = if_else(RCIchange != \"RelDet\", \"\", pointNames)) -> tmpTib\n  }\n  \n  ### build tibAreas\n  tribble(\n    ~iPolygon, ~namePolygon, ~RCI, ~direction, ~start, ~end, ~CSC4,\n    1, \"RelDetHighToHigh\",         \"RelDet\",      \"Det\", \"High\", \"High\", \"StayedHigh\",\n    2, \"NoRelChangeDetHighToHigh\", \"NoRelChange\", \"Det\", \"High\", \"High\", \"StayedHigh\",\n    3, \"NoRelChangeImpHighToHigh\", \"NoRelChange\", \"Imp\", \"High\", \"High\", \"StayedHigh\",\n    4, \"RelImpHighToHigh\",         \"RelImp\",      \"Imp\", \"High\", \"High\", \"StayedHigh\",\n    5, \"RelImpHighToLow\",          \"RelImp\",      \"Imp\", \"High\", \"Low\",   \"HighToLow\",\n    6, \"RelImpLowToLow\",           \"RelImp\",      \"Imp\", \"Low\",  \"Low\",   \"StayedLow\",\n    7, \"NoRelChangeImpLowToLow\",   \"NoRelChange\", \"Imp\", \"Low\",  \"Low\",   \"StayedLow\",\n    8, \"NoRelChangeDetLowToLow\",   \"NoRelChange\", \"Det\", \"Low\",  \"Low\",   \"StayedLow\",\n    9, \"RelDetLowToLow\",           \"RelDet\",      \"Det\", \"Low\",  \"Low\",   \"StayedLow\",\n    10, \"RelDetLowToHigh\",         \"RelDet\",      \"Det\", \"Low\",  \"High\",  \"LowToHigh\",\n    11, \"NoRelChangeDetLowToHigh\",  \"NoRelChange\", \"Det\", \"Low\",  \"High\",  \"LowToHigh\",\n    12, \"NoRelChangeImpHighToLow\", \"NoRelChange\", \"Imp\", \"High\", \"Low\",   \"HighToLow\") %>%\n    mutate(areaAlpha = rep(areaAlpha, 12)) -> tibAreas1\n\n  ### handle goodScorePos for direction\n  if(goodScorePos) {\n    tibAreas1 %>%\n      mutate(direction = case_when(\n        direction == \"Det\" ~ \"Imp\",\n        direction == \"Imp\" ~ \"Det\")) -> tibAreas1\n  }\n    ### handle goodScorePos for RCI\n  if(goodScorePos) {\n    tibAreas1 %>%\n      mutate(RCI = case_when(\n        RCI == \"RelDet\" ~ \"RelImp\",\n        RCI == \"RelImp\" ~ \"RelDet\",\n        RCI == \"NoRelChange\" ~ \"NoRelChange\")) -> tibAreas1\n  }\n  \n  # RCSC5 for areas:  \n  if(!goodScorePos) {\n    tmpVecRCSC5 <- c(\"RelDet\", #1\n                     \"NoRelChange\", #2\n                     \"NoRelChange\", #3\n                     \"RelImpHighToHigh\", #4\n                     \"RelImpHighToLow\", #5\n                     \"RelImpLowToLow\", #6\n                     \"NoRelChange\", #7\n                     \"NoRelChange\", #8\n                     \"RelDet\", #9\n                     \"RelDet\", #10\n                     \"NoRelChange\", #11\n                     \"NoRelChange\" #12\n    )\n  } else {\n    tmpVecRCSC5 <- c(\"RelImpHighToHigh\", #1\n                     \"NoRelChange\", #2\n                     \"NoRelChange\", #3\n                     \"RelDet\", #4\n                     \"RelDet\", #5\n                     \"RelDet\", #6\n                     \"NoRelChange\", #7\n                     \"NoRelChange\", #8\n                     \"RelImpLowToLow\", #9\n                     \"RelImpLowToHigh\", #10\n                     \"NoRelChange\", #11\n                     \"NoRelChange\" #12\n    ) \n  }\n  tibAreas1 %>%\n    mutate(RCSC5 = tmpVecRCSC5) -> tibAreas1\n  \n  # RCSC7 for areas: \n  if(!goodScorePos) {\n    tmpVecRCSC7 <- c(\"RelDetHSToHS\", #1\n                     \"NoRelChange\", #2\n                     \"NoRelChange\", #3\n                     \"RelImpHSToHS\", #4\n                     \"RelImpHSToNHS\", #5\n                     \"RelImpNHSToNHS\", #6\n                     \"NoRelChange\", #7\n                     \"NoRelChange\", #8\n                     \"RelDetNHSToNHS\", #9\n                     \"RelDetNHSToHS\", #10\n                     \"NoRelChange\", #11\n                     \"NoRelChange\" #12\n    ) \n  } else {\n    tmpVecRCSC7 <- c(\"RelImpNHSToNHS\", #1\n                     \"NoRelChange\", #2\n                     \"NoRelChange\", #3\n                     \"RelDetNHSToNHS\", #4\n                     \"RelDetNHSToHS\", #5\n                     \"RelDetHSToHS\", #6\n                     \"NoRelChange\", #7\n                     \"NoRelChange\", #8\n                     \"RelImpHSToHS\", #9\n                     \"RelImpHSToNHS\", #10\n                     \"NoRelChange\", #11\n                     \"NoRelChange\" #12\n    )\n  }\n  tibAreas1 %>%\n    mutate(RCSC7 = tmpVecRCSC7) -> tibAreas1\n  \n  # RCSC10 for areas: \n  if(!goodScorePos) {\n    tmpVecRCSC10 <- c(\"RelDetHSToHS\",  #1 \n                      \"NoRelChangeHSToHS\", #2 \n                      \"NoRelChangeHSToHS\", #3\n                      \"RelImpHSToHS\", #4\n                      \"RelImpHSToNHS\", #5\n                      \"RelImpNHSToNHS\", #6\n                      \"NoRelChangeNHSToNHS\", #7\n                      \"NoRelChangeNHSToNHS\", #8\n                      \"RelDetNHSToNHS\", #9\n                      \"RelDetNHSToHS\",  #10,\n                      \"NoRelChangeNHSToHS\", #11\n                      # and finally for #12\n                      \"NoRelChangeHSToNHS\")\n  } else {\n    tmpVecRCSC10 <- c(\"RelImpNHSToNHS\",  #1 \n                      \"NoRelChangeNHSToNHS\", #2 \n                      \"NoRelChangeNHSToNHS\", #3\n                      \"RelDetNHSToNHS\", #4\n                      \"RelDetNHSToHS\", #5\n                      \"RelDetHSToHS\", #6\n                      \"NoRelChangeHSToHS\", #7\n                      \"NoRelChangeHSToHS\", #8\n                      \"RelImpHSToHS\", #9\n                      \"RelImpHSToNHS\",  #10,\n                      \"NoRelChangeHSToNHS\", #11\n                      # and finally for #12\n      \"NoRelChangeNHSToHS\")\n  }\n  tibAreas1 %>%\n    mutate(RCSC10 = tmpVecRCSC10) -> tibAreas1\n  \n  ### get names in correct order\n  tibAreas1 %>%\n    pull(namePolygon) -> vecAreas\n  tibAreas1 %>%\n    mutate(namePolygon = ordered(namePolygon,\n                                 levels = vecAreas,\n                                 labels = vecAreas)) -> tibAreas1\n  \n  ### use that to build the 12 tiling polygons\n  ### I've done this rather clumsily, longhand!\n  # 1 relDetHighToHigh\n  dfPolygonRelDetHighToHigh <- data.frame(x = c(CSC,       maxPossScore - RCI, CSC),\n                                          y = c(CSC + RCI, maxPossScore,       maxPossScore))\n  # 2 noRelChangeDetHighToHigh\n  dfPolygonNoRelChangeDetHighToHigh <- data.frame(x = c(CSC, maxPossScore, maxPossScore - RCI, CSC),\n                                                  y = c(CSC,  maxPossScore, maxPossScore, CSC + RCI))\n  # 3 noRelChangeImpHighToHigh\n  dfPolygonNoRelChangeImpHighToHigh <- data.frame(x = c(CSC, CSC + RCI, maxPossScore,       maxPossScore),\n                                                  y = c(CSC, CSC,       maxPossScore - RCI, maxPossScore))\n  # 4 relImpHighToHigh\n  dfPolygonRelImpHighToHigh <- data.frame(x = c(CSC + RCI, maxPossScore, maxPossScore),\n                                          y = c(CSC,       CSC,          maxPossScore - RCI))\n  # 5 relImpHighToLow\n  dfPolygonRelImpHighToLow <- data.frame(x = c(CSC,          CSC,       CSC + RCI, maxPossScore, maxPossScore),\n                                         y = c(minPossScore, CSC - RCI, CSC,       CSC,          minPossScore))\n  # 6 relImpLowToLow\n  dfPolygonRelImpLowToLow <- data.frame(x = c(minPossScore + RCI, CSC,                CSC),\n                                        y = c(minPossScore,       minPossScore, CSC - RCI))\n  # 7 noRelChangeImpLowToLow\n  dfPolygonNoRelChangeImpLowToLow <- data.frame(x = c(minPossScore, minPossScore + RCI, CSC, CSC),\n                                                y = c(minPossScore, minPossScore,       CSC - RCI, CSC))\n  # 8 noRelChangeDetLowToLow\n  dfPolygonNoRelChangeDetLowToLow <- data.frame(x = c(minPossScore, CSC, CSC - RCI, minPossScore),\n                                                y = c(minPossScore, CSC, CSC,       RCI))\n  # 9 relDetLowToLow\n  dfPolygonRelDetLowToLow <- data.frame(x = c(minPossScore, CSC - RCI, minPossScore),\n                                        y = c(RCI,          CSC,       CSC))\n  # 10 relDetLowToHigh\n  dfPolygonRelDetLowToHigh <- data.frame(x = c(minPossScore, CSC - RCI, CSC,       CSC,          minPossScore),\n                                         y = c(CSC,          CSC,       CSC + RCI, maxPossScore, maxPossScore))\n  # 11 noRelChangeDetLowToHigh\n  dfPolygonNoRelChangeDetLowToHigh <- data.frame(x = c(CSC - RCI, CSC, CSC),\n                                                 y = c(CSC,       CSC, CSC + RCI))\n  # 12 noRelChangeImpHighToLow\n  dfPolygonNoRelChangeImpHighToLow <- data.frame(x = c(CSC,       CSC, CSC + RCI),\n                                                 y = c(CSC - RCI, CSC, CSC))\n  \n  tibAreas1 %>%\n    rowwise() %>%\n    mutate(nameFile = paste0(\"dfPolygon\", namePolygon),\n           tmpList = list(get(nameFile))) %>%\n    ungroup() %>%\n    unnest(tmpList) %>%\n    group_by(iPolygon) %>%\n    mutate(xCentre = mean(x),\n           yCentre = mean(y)) %>%\n    ungroup() %>%\n    select(-nameFile) -> tibAreas\n  ### finished creating tibAreas\n  ###\n  ### area schemes\n  ###\n  ### default is to use same alpha for all 12 areas\n  vec12alphas <- rep(areaAlpha, 12)\n  if(areaColourScheme == \"NONE\") {\n    areaColourScheme <- \"ALLWHITE\"\n    vec12colours <- rep(\"white\", 12)\n    vecAreaColours <- vec12colours\n    vecAreaLabels <- rep(\"\", 12)\n    tibAreas %>%\n      mutate(Areas = \"\") -> tibAreas\n  }\n  if(areaColourScheme == \"POLYGONS\") {\n    tibAreas %>%\n      ### set the variable to use for the areas\n      mutate(Areas = namePolygon) -> tibAreas\n    vecAreaColours <- c(\"#8DD3C7\", \"#FFFFB3\", \"#BEBADA\", \n                        \"#FB8072\", \"#80B1D3\", \"#FDB462\",\n                        \"#B3DE69\", \"#FCCDE5\", \"#D9D9D9\", \n                        \"#BC80BD\", \"#CCEBC5\", \"#FFED6F\")\n    ### now the area legend labels\n    vecAreaLabels <- vecAreas\n  }\n  if(areaColourScheme == \"DIRECTION\") {\n    tibAreas %>%\n      ### set the variable to use for the areas\n      mutate(Areas = direction) -> tibAreas\n    vecAreaColours <- coloursDirection\n    names(vecAreaColours) <- c(\"Det\", \"Imp\", \"NoChange\")\n    ### now the legend label\n    vecAreaLabels <- labelsDirection\n    names(vecAreaLabels) <- c(\"Det\", \"Imp\", \"NoChange\")\n  }\n  if(areaColourScheme == \"RCI3\") {\n    tibAreas %>%\n      ### set the variable to use for the Areas\n      mutate(Areas = RCI) -> tibAreas\n    vecAreaColours <- coloursRCI3\n    ### now the legend labels\n    vecAreaLabels <- labelsRCI3\n    ### and name the colours\n    names(vecAreaColours) <- c(\"NoRelChange\", \"RelDet\", \"RelImp\")\n  }\n  if(areaColourScheme == \"CSC4\") {\n    tibAreas %>%\n      ### set the variable to use for the Areas\n      mutate(Areas = CSC4) -> tibAreas\n    vecAreaColours <- coloursCSC4\n    names(vecAreaColours) <- c(\"HighToLow\", \"LowToHigh\", \"StayedHigh\", \"StayedLow\")\n    ### now the legend label\n    vecAreaLabels <- labelsCSC4\n    names(vecAreaLabels) <- c(\"HighToLow\", \"LowToHigh\", \"StayedHigh\", \"StayedLow\")\n    if(goodScorePos) {\n      vecAreaColours <- coloursCSC4\n      names(vecAreaColours) <- c(\"LowToHigh\", \"HighToLow\", \"StayedLow\", \"StayedHigh\")\n    }\n  }  \n  \n  if(areaColourScheme == \"RCSC5\") {\n    tibAreas %>%\n      ### set the variable to use for the areas\n      mutate(Areas = RCSC5) -> tibAreas\n    vecAreaColours <- coloursRCSC5\n    ### now the legend labels\n    vecAreaLabels <- labelsRCSC5\n    if (goodScorePos) {\n      vecAreaLabels <- labelsRCSC5pos\n    }\n  }  \n  if(areaColourScheme == \"RCSC7\") {\n    ### set the order of the colours\n    orderColours <- c(\"NoRelChange\",\n                      \"RelDetHSToHS\",\n                      \"RelDetNHSToHS\",\n                      \"RelDetNHSToNHS\",\n                      \"RelImpHSToHS\",\n                      \"RelImpHSToNHS\",\n                      \"RelImpNHSToNHS\")\n    tibAreas %>%\n      ### set the variable to use for the areas\n      mutate(Areas = RCSC7) -> tibAreas\n    vecAreaColours <- coloursRCSC7\n    names(vecAreaColours) <- orderColours\n    ### now the legend labels\n    vecAreaLabels <- labelsRCSC7\n    names(vecAreaLabels) <- orderColours\n    if (goodScorePos) {\n      vecAreaLabels <- labelsRCSC7pos\n      names(vecAreaLabels) <- orderColours\n    }\n  }  \n  if(areaColourScheme == \"RCSC10\") {\n    ### set order for colours\n    orderColours <- c(\"NoRelChangeHSToHS\",\n                      \"NoRelChangeHSToNHS\",\n                      \"NoRelChangeNHSToHS\",\n                      \"NoRelChangeNHSToNHS\",\n                      \"RelDetHSToHS\",\n                      \"RelDetNHSToHS\",\n                      \"RelDetNHSToNHS\",\n                      \"RelImpHSToHS\",\n                      \"RelImpHSToNHS\",\n                      \"RelImpNHSToNHS\")\n    tibAreas %>%\n      ### set the variable to use for the areas\n      mutate(Areas = RCSC10) -> tibAreas\n    vecAreaColours <- coloursRCSC10\n    names(vecAreaColours) <- orderColours\n    ### now the legend label\n    vecAreaLabels <- labelsRCSC10\n    names(vecAreaLabels) <- orderColours\n    if (goodScorePos) {\n      vecAreaLabels <- labelsRCSC10pos\n      names(vecAreaLabels) <- orderColours\n    }\n  }  \n  \n  ###\n  ### line types\n  ###\n  if (is.null(lineTypeCSC)) {\n    lineTypeCSC = 1\n  }\n  if (is.null(lineTypeNoChange)) {\n    lineTypeNoChange = 1\n  }\n  if (is.null(lineTypeRCI)) {\n    lineTypeRCI = 1\n  }\n  ### line colours\n  if (is.null(lineColourCSC)) {\n    lineColourCSC = 1\n  }\n  if (is.null(lineColourNoChange)) {\n    lineColourNoChange = 1\n  }\n  if (is.null(lineColourRCI)) {\n    lineColourRCI = 1\n  }\n  ### line widths\n  if (is.null(lineWidthCSC)) {\n    lineWidthCSC = .4\n  }\n  if (is.null(lineWidthNoChange)) {\n    lineWidthNoChange = .4\n  }\n  if (is.null(lineWidthRCI)) {\n    lineWidthRCI = .4\n  }\n  \n  ###\n  ### axis labels\n  ###\n  if(is.null(xLab)) {\n    xLab <- \"Baseline score\"\n  }\n  if(is.null(yLab)) {\n    yLab <- \"Last score\"\n  }\n  if(is.null(title)) {\n    title <- \"Jacobson plot\"\n  }\n\n  \n  ###\n  ### set point colours \n  ###\n  if(pointColourScheme == \"NONE\") {\n    tmpTib %>%\n      ### set the variable to use for the points\n      mutate(Points = \"black\") -> tmpTib\n    \n    vecPointColours <- rep(\"black\", 3)\n    names(vecPointColours) <- rep(\"black\", 3)\n    ### now the legend labels\n    if (pointShapeScheme == \"DIRECTION\") {\n      vecPointLabels <- c(\"No change\", \"Deterioration\", \"Improvement\")\n    } else {\n      vecPointLabels <- c(\"No reliable change\", \"Reliable deterioration\", \"Reliable improvement\")\n    }\n  }\n  if(pointColourScheme == \"DIRECTION\") {\n    tmpTib %>%\n      ### set the variable to use for the points\n      mutate(Points = direction3) -> tmpTib\n    vecPointColours <- coloursDirection\n    names(vecPointColours) <- c(\"Det\", \"Imp\", \"NoChange\")\n    ### now the legend labels\n    vecPointLabels <- labelsDirection\n    names(vecPointLabels) <- c(\"Det\", \"Imp\", \"NoChange\")\n  }\n  if(pointColourScheme == \"RCI3\") {\n    tmpTib %>%\n      ### set the variable to use for the points\n      mutate(Points = RCIchange) -> tmpTib\n    vecPointColours <- coloursRCI3\n    names(vecPointColours) <- c(\"NoRelChange\", \"RelDet\", \"RelImp\")\n    ### now the legend label\n    vecPointLabels <- labelsRCI3\n    names(vecPointLabels) <- c(\"NoRelChange\", \"RelDet\", \"RelImp\")\n  }\n  \n  if(pointColourScheme == \"CSC4\") {\n    tmpTib %>%\n      ### set the variable to use for the points\n      mutate(Points = CSC4) -> tmpTib\n    vecPointColours <- coloursCSC4\n    names(vecPointColours) <- c(\"HSToNHS\", \"NHSToHS\", \"HSToHS\", \"NHSToNHS\")\n    ### now the legend label\n    vecPointLabels <- labelsCSC4 \n    names(vecPointLabels) <- c(\"HSToNHS\", \"NHSToHS\", \"HSToHS\", \"NHSToNHS\")\n  }  \n  \n  if(pointColourScheme == \"RCSC5\") {\n    tmpTib %>%\n      ### set the variable to use for the points\n      mutate(Points = RCSC5) -> tmpTib\n    ### create vector of levels for negative cueing\n    vecPointsRCSC5 <- c(\"NoRelChange\",\n                        \"RelDet\",\n                        \"RelImpHSToHS\",\n                        \"RelImpHSToNHS\",\n                        \"RelImpNHSToNHS\")\n    ### create vector for colours\n    vecPointColours <- coloursRCSC5\n    ### name/label that vector\n    names(vecPointColours) <- vecPointsRCSC5\n    ### now the legend label\n    vecPointLabels <- labelsRCSC5\n    names(vecPointLabels) <- vecPointsRCSC5\n    if (goodScorePos) {\n      ### create vector of levels for positive cueing\n      vecPointsRCSC5 <- c(\"NoRelChange\",\n                          \"RelDet\",\n                          \"RelImpNHSToNHS\",\n                          \"RelImpHSToNHS\",\n                          \"RelImpHSToHS\")\n      ### create vector for colours\n      vecPointColours <- coloursRCSC5\n      ### name/label that vector\n      names(vecPointColours) <- vecPointsRCSC5\n      ### now the legend label\n      vecPointLabels <- labelsRCSC5\n      names(vecPointLabels) <- vecPointsRCSC5\n    }\n  }\n  \n  if(pointColourScheme == \"RCSC7\") {\n    tmpTib %>%\n      ### set the variable to use for the points\n      mutate(Points = RCSC7) -> tmpTib\n    ### get vector of levels\n    vecPointsRCSC7 <- c(\"NoRelChange\",\n                        \"RelDetHSToHS\",\n                        \"RelDetNHSToHS\",\n                        \"RelDetNHSToNHS\",                                         \n                        \"RelImpHSToHS\",\n                        \"RelImpHSToNHS\",\n                        \"RelImpNHSToNHS\")\n    vecPointsRCSC7 <- ordered(vecPointsRCSC7,\n                              levels = vecPointsRCSC7,\n                              labels = vecPointsRCSC7)\n    ### create vector for colours\n    vecPointColours <- coloursRCSC7\n    ### name/label that vector as this is what ggplot uses to map colours\n    names(vecPointColours) <- vecPointsRCSC7\n    ### now the legend label\n    if(!goodScorePos) {\n      vecPointLabels <- labelsRCSC7\n      names(vecPointLabels) <- vecPointsRCSC7\n    } else {\n      vecPointLabels <- labelsRCSC7pos\n      names(vecPointLabels) <- vecPointsRCSC7  \n    }\n  }\n  \n  if(pointColourScheme == \"RCSC10\") {\n    tmpTib %>%\n      ### set the variable to use for the points\n      mutate(Points = RCSC10) -> tmpTib\n    ### get vector of levels\n    vecPointsRCSC10 <- c(\"NoRelChangeHSToHS\",\n                         \"NoRelChangeHSToNHS\",\n                         \"NoRelChangeNHSToHS\",\n                         \"NoRelChangeNHSToNHS\",\n                         \"RelDetHSToHS\",\n                         \"RelDetNHSToHS\",\n                         \"RelDetNHSToNHS\",                                         \n                         \"RelImpHSToHS\",\n                         \"RelImpHSToNHS\",\n                         \"RelImpNHSToNHS\")\n    if(!goodScorePos) {\n      ### create vector for colours\n      vecPointColours <- coloursRCSC10\n      ### name/label that vector\n      names(vecPointColours) <- vecPointsRCSC10\n      ### similar for the legend labels\n      vecPointLabels <- labelsRCSC10\n      names(vecPointLabels) <- vecPointsRCSC10\n    } else {\n       ### create vector for colours\n      vecPointColours <- coloursRCSC10\n      ### name/label that vector\n      names(vecPointColours) <- vecPointsRCSC10\n      vecPointLabels <- labelsRCSC10pos\n      names(vecPointLabels) <- vecPointsRCSC10\n    }\n  } \n  ### finally (!) start to build the plot    \n  ggplot(data = tmpTib,\n         aes(x = firstScore, y = lastScore)) +\n    ### we never want a legend for alpha so kill that now \n    guides(alpha = \"none\") -> tmpPlot\n  \n  ### set background and surround colours\n  if(!is.null(plotColour)) {\n    # plot area\n    tmpPlot +\n      theme(panel.background = element_rect(fill = plotColour)) -> tmpPlot\n  }\n  if(!is.null(borderColour)) {\n    # plot area\n    tmpPlot +\n      theme(plot.background = element_rect(fill = borderColour)) -> tmpPlot\n  }\n  \n  ### plot areas \n  if (areaColourScheme != \"ALLWHITE\") {\n    tmpPlot +\n      geom_polygon(data = tibAreas,\n                aes(x = x, y = y, group = namePolygon, fill = Areas),\n                alpha = areaAlpha) +\n      scale_fill_manual(name = legendNameAreas,\n                        values = vecAreaColours,\n                        labels = vecAreaLabels,\n                        guide = guide_legend(order = 1)) +\n      ### allow use of new scales for colour and fill later\n      ggnewscale::new_scale_colour() +\n      ggnewscale::new_scale_fill() -> tmpPlot\n  }\n\n  ### ### lines\n  ### add leading diagonal of no change\n  tmpPlot +\n    geom_abline(slope = 1, intercept = 0,\n                linetype = lineTypeNoChange,\n                colour = lineColourNoChange,\n                linewidth = lineWidthNoChange) -> tmpPlot\n  \n  ### put in CSC lines\n  tmpPlot +\n    geom_vline(xintercept = CSC,\n               linetype = lineTypeCSC,\n               colour = lineColourCSC,\n               linewidth = lineWidthCSC) +\n    geom_hline(yintercept = CSC,\n               linetype = lineTypeCSC,\n               colour = lineColourCSC,\n               linewidth = lineWidthCSC) -> tmpPlot\n  \n  ### add RCI tramlines\n  tmpPlot +\n    geom_abline(slope = 1, intercept = -RCI,\n                linetype = lineTypeRCI,\n                colour = lineColourRCI,\n                linewidth = lineWidthRCI) +\n    geom_abline(slope = 1, intercept = RCI,\n                linetype = lineTypeRCI,\n                colour = lineColourRCI,\n                linewidth = lineWidthRCI) -> tmpPlot\n  ### generate point mapping\n  ### labelled points\n  if(namesNotPoints) {\n    if(!nameRCIDetOnly | is.null(nameRCIDetOnly)) {\n      ### only makes sense to label fewer than 50 points\n      if (Complete < 50) {\n        tmpPlot +\n          ggrepel::geom_text_repel(aes(colour = RCIchange,\n                                       label = pointNames),\n                                   size = namesSize) -> tmpPlot\n      } else {\n        ### got fewer than 50 points so label them\n        tmpPlot +\n          geom_text(aes(colour = RCIchange,\n                        label = pointNames),\n                    size = namesSize) -> tmpPlot\n      }\n    }\n    if (nameRCIDetOnly){\n      tmpPlot +\n        ggrepel::geom_text_repel(aes(colour = RCIchange,\n                                     label = pointNames),\n                                 size = namesSize) -> tmpPlot\n    }\n  }\n  if (!namesNotPoints){\n    ### this needs to be different if no point colour used\n    if (pointColourScheme != \"NONE\") {\n      tmpPlot +\n        geom_point(data = tmpTib,\n                   aes(colour = Points,\n                       fill = Points,\n                       shape = Shapes),\n                   alpha = pointAlpha,\n                   size = pointSize) -> tmpPlot\n    } else {\n      ### no colour scheme so:\n      tmpPlot +\n        geom_point(data = tmpTib,\n                   aes(shape = Shapes),\n                   alpha = pointAlpha,\n                   size = pointSize,\n                   colour = \"black\",\n                   fill = \"black\") -> tmpPlot\n      ### I think fill and colour will default to black OK?\n    }\n  }\n  \n  ### scales\n  tmpPlot +\n    scale_x_continuous(name = xLab,\n                       limits = c(minPossScore, maxPossScore),\n                       expand = c(0, 0) ) +\n    scale_y_continuous(name = yLab,\n                       limits = c(minPossScore, maxPossScore),\n                       expand = c(0, 0) ) +\n    ggtitle(title,\n            subtitle = subtitle) +\n    ### crucial setting to get square plot\n    theme(aspect.ratio = 1) +\n    theme(plot.title = element_text(hjust = titleJustificn),\n          plot.subtitle = element_text(hjust = titleJustificn)) -> tmpPlot\n  \n  if(labelCSClines & !is.na(CSC)) { ### @@@\n    tmpPlot +\n      ### these label the CSC lines.  I'm not sure they're very  informative or aesthetic.\n      geom_text(label = paste0(\"CSC = \",\n                               CSC),\n                # \"\\nfor last scores\"),\n                x = scoreRange / 70,\n                y = CSC + (scoreRange / 70),\n                hjust = 0,\n                size = 3) +\n      geom_text(label = paste0(\"CSC = \",\n                               CSC),\n                # \"\\nfor baseline scores\"),\n                x = CSC + (scoreRange / 70),\n                y = maxPossScore - (scoreRange / 70),\n                hjust = 0,\n                size = 3) -> tmpPlot\n  }\n  \n  ### deal with legends to plot\n  \n  ### noLegend trumps all then ...\n  ### otherwise select by n_distinct\n  ### if n_distinct > 0 only for area use fill and no other\n  ### if n_distinct > 0 for area and point but not shape do fill (area) and colour (points)\n  ### if n_distinct > 0 for all three \n  if (noLegend) {\n    tmpPlot +\n      guides(colour = \"none\",\n             fill = \"none\",\n             shape = \"none\") -> tmpPlot\n  } else {\n    ### do some counting to decide which guides you need and which legends\n    tibAreas %>%\n      summarise(nAreaColours = n_distinct(Areas)) %>%\n      select(nAreaColours) %>%\n      pull() -> nAreaColours\n    tmpTib %>% \n      summarise(nPointColours = n_distinct(Points)) %>%\n      select(nPointColours) %>%\n      pull() -> nPointColours\n    tmpTib %>%\n      summarise(nPointShapes = n_distinct(Shapes)) %>%\n      select(nPointShapes) %>%\n      pull() -> nPointShapes\n\n    if (max(nAreaColours, nPointColours, nPointShapes) == 1){\n      # got no variance so no legends\n      tmpPlot +\n        guides(fill = \"none\",\n               colour = \"none\",\n               shape = \"none\") -> tmpPlot\n    }\n    if (nAreaColours > 1 & max(nPointColours, nPointShapes) == 1){\n      # only got areas so legend only for fill\n      ### rethink the whole setting of colour, fill & shape maps\n      ### they are done by using named values in a vector\n      ### name by the levels of the things you are mapping i.e. direction, RCI etc.\n      ### values are values to use, i.e. colours/fills and shapes\n      ### set up the named vectors when you know the schemes\n      tmpPlot +\n        scale_fill_manual(legendNameAreas,\n                          values = vecAreaColours,\n                          labels = vecAreaLabels) +\n        scale_colour_manual(legendNamePoints,\n                      values = vecPointColours,\n                      labels = vecPointLabels) +\n        guides(colour = \"none\",\n               shape = \"none\") -> tmpPlot\n    }\n    if (nPointShapes > 1 & max(nPointColours, nAreaColours) == 1){\n      # only got shapes\n      tmpPlot +\n        scale_shape_manual(legendNamePoints,\n                           values = vecPointShapes,\n                           labels = vecShapeLabels) +\n        scale_fill_manual(legendNamePoints,\n                    values = vecPointColours,\n                    labels = vecPointLabels) +\n        scale_colour_manual(legendNamePoints,\n                            values = vecPointColours,\n                            labels = vecPointLabels) +\n        guides(colour = \"none\",\n               fill = \"none\") -> tmpPlot  \n    }\n    if (nPointColours > 1 & max(nAreaColours, nPointShapes) == 1){\n      # only got point colours\n      tmpPlot +\n        scale_colour_manual(legendNamePoints,\n                      values = vecPointColours,\n                      labels = vecPointLabels) +\n        guides(fill = \"none\",\n               shape = \"none\") -> tmpPlot\n    }\n    if (nPointShapes > 1 & nPointColours > 1 & nAreaColours == 1){\n      # keep legend for colour and shape\n      tmpPlot +\n        scale_shape_manual(legendNamePoints,\n                           values = vecPointShapes,\n                           labels = vecPointLabels) +\n        scale_colour_manual(legendNamePoints,\n                      values = vecPointColours,\n                      labels = vecPointLabels) +\n        ### have to add fill as using filled points\n        scale_fill_manual(legendNamePoints,\n                      values = vecPointColours,\n                      labels = vecPointLabels) -> tmpPlot\n    }\n    if (nPointShapes == 1 & nPointColours > 1 & nAreaColours > 1){\n      # keep legend for fill and colour\n      tmpPlot +\n        scale_colour_manual(legendNamePoints,\n                      values = vecPointColours,\n                      labels = vecPointLabels) +\n        guides(fill = \"none\",\n               shape = \"none\") -> tmpPlot\n    }\n    if (nPointShapes > 1 & nPointColours == 1 & nAreaColours > 1){\n      # using area colours and point shapes\n      tmpPlot +\n        scale_shape_manual(legendNamePoints,\n                           values = vecPointShapes,\n                           labels = vecPointLabels) +\n        scale_colour_manual(legendNamePoints,\n                      values = vecPointColours,\n                      labels = vecPointLabels) +\n        ### have to add fill as using filled points\n        scale_fill_manual(legendNamePoints,\n                          values = vecPointColours,\n                          labels = vecPointLabels) +\n        guides(fill = \"none\",\n               colour = \"none\") -> tmpPlot\n    }\n    if (nPointShapes > 1 & nPointColours > 1 & nAreaColours > 1){\n      # keep legends for all three, fill colour and shape\n      tmpPlot +\n        scale_shape_manual(legendNamePoints,\n                           values = vecPointShapes,\n                           labels = vecPointLabels) +\n        scale_colour_manual(legendNamePoints,\n                      values = vecPointColours,\n                      labels = vecPointLabels) +\n        ### have to add fill as using filled points\n        scale_fill_manual(legendNamePoints,\n                      values = vecPointColours,\n                      labels = vecPointLabels) +\n        guides(fill = \"none\") -> tmpPlot\n        \n    }\n  }\n  \n  if(labelAllPolygons){\n    tmpPlot +\n      ### if labelling centroids of each polygon ...\n      geom_text(data = tibAreas,\n                aes(x = xCentre, y = yCentre, label = iPolygon),\n                size = 4) -> tmpPlot\n  }\n  \n  if (printVariables) {\n    options(width = 240)\n    ### define functions\n    getClasses <- function(x){\n      ### collapse classes of x\n      vecClass <- class(get(x))\n      if (length(vecClass) == 1) {\n        return( vecClass)\n      }\n      vecTmp <- paste(vecClass[1 : (length(vecClass) - 1)], collapse = \", \")\n      paste0(vecTmp, \", \", vecClass[length(vecClass)])\n    }\n    collapseVec <- function(x){\n      if (is.atomic(get(x))) {\n        paste(get(x), collapse = \", \")\n      } else {\n        \"NA\"\n      }\n    }\n    descVars <- function(vec) {\n      tibble(var = vec) %>%\n        rowwise() %>%\n        mutate(classes = getClasses(var),\n               atomic = is.atomic(get(var)),\n               length = if_else(atomic, as.character(length(get(var))), \"NA\"),\n               values = collapseVec(var)) %>%\n        arrange(atomic, classes) %>%\n        print(n = Inf)\n        # flextable() %>%\n        # autofit() %>%\n        # print()\n    }\n    \n    print(\" \")\n    print(\"Here are all the arguments from the call:\")\n    print(\" \")\n    print(listArgs)\n    print(\" \")\n    print(\"And here are all the internal variables at the end of the function:\")\n    print(\" \")\n    vecLs <- ls()\n    print(vecLs)\n    print(\" \")\n    print(\"And here is the information about them\")\n    print(\" \")\n    descVars(vecLs)\n    print(\" \")\n    # vecLs <- setdiff(vecLs, c(\"getJacobsonPlot\", \"tmpPlot\"))\n    # lapply(vecLs, function(x){print(get(x))})\n    print(\"tmpTib has these columns:\")\n    print(colnames(tmpTib))\n    print(\"...and these values:\")\n    tmpTib %>%\n      print(n = Inf)\n    print(\" \")\n    print(\"tibAreas has these columns:\")\n    print(colnames(tibAreas))\n    print(\"... and these values:\")\n    tibAreas %>%\n      print(n = Inf)\n  }\n  \n  ### reset option to original if reset\n  options(\"warn\" = oldWarn)\n  theme_set(oldTheme)\n  ### finally, return the completed plot\n  tmpPlot\n}\n\n\nIntroduction\ngetJacobsonPlot() is the basic function to create a Jacobson plot (a.k.a. RCSC plot) from, at minimum, starting scores and final scores as two vectors and the minimum and maximum possible scores on the measure.\nHere is an example plot using only the necessary arguments and leaving all the others at their default settings. The data are real CORE-OM data scored on the 0 to 4 scoring. I have added three points falling into categories of the RCSC5 system that didn’t occur in the real data (one reliable deterioration, stayed low, one reliable deterioration, low to high and one reliable improvement, stayed low.)\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4)\n\n\n\nSo far so underwhelming: not a terribly good plot. However, what I hope makes the function really useful for a wide variety of possible users is that it can label points by their shape (for direction of change and reliable change (“RCI3”) and can label the points, and the areas of the plot, by a set of categorisations:\ndirection of change (hardly Jacobson or RCSC but if you don’t have values for the RCI or CSC you can use that’s all you’ve got and the scatter of the plots is still informative)\nreliable change, what I call the RCI3 categorisation, i.e. reliably deteriorated, no reliable change, reliable improvement\n“RCSC5”: a commonly used RCSC categorisation: reliable deterioration, no reliable change, reliable improvement but stayed in a more trouble score range (above the CSC if the measure is has higher scores representing worse states that lower scores), reliably improved and moved from more toubled to less troubled score (often called “recovered” but please let’s stop that!, and reliably improved but less troubled than the CSC score before and remained so\n“RCS7” which separates reliable deterioration into three areas: stayed less severe than CSC, from less severe to more severe and stayed severe\n“RCSC10” (which I can’t see being useful) but which splits the no reliable change into four areas.\nProbably as important as that plethora of mappings the function handles the cueing of the measure, i.e. whether high scores indicate worse or better state. Finally, the function has a large huge number of arguments to the function that enable the user to change many aspects of the plot such as colour mappings, types and thickness of lines and, though I think it’s unlikely this is ever going to be necessary, point shapes. More importantly, all the text values: title, subtitle, mapping legend names, category labels and the two axis labels have sensible defaults but all can be changed to whatever you want, in whatever language and character set, depending on your local R, you would need.\nCategory systems\nAs noted above the category systems I have implemented are:\n1. Direction (i.e. just a scattergram of first and last scores labelling direction of change)\nRCI3: the classic threesome of reliable improvement, no reliable change and reliable deterioration\nCSC4: the simple CSC foursome of HighToHigh, LowToHigh, HighToLow, LowToLow\nThe RCSC categories that use both RCI and CSC:\nRCSC5: NoRelChange, RelDet, RelImpHighToLow (“Recovered”), RelImpHighToHigh, RelImpLowToLow (using the obvious abbreviations and assuming negatively cued measure)\nRCSC7: RelDetHighToHigh, RelDetLowToHigh, RelDetLowToLow, NoRelChange, RelImpHighToHigh, RelImpHighToLow (“Recovered”), RelImpLowToLow\nRCSC10: RelDetHighToHigh, RelDetLowToHigh, RelDetLowToLow, NoRelChangeHighToHigh, NoRelChangeLowToHigh, NoRelChangeHighToLow, NoRelChangeLowToLow, RelImpHighToHigh, RelImpHighToLow (“recovered”), RelImpLowToLow.\n\nThe categories can be marked on the plot by:\nColouring the areas of the plot\nColouring the points\nChanging the shapes of the points (but only for direction or for RCI3)\nCueing of the score: goodScorePos\nThis shows the impact of that goodScorePos argument using the same data which are actually cued negatively\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             subtitle = \"Positively cued scoring: 4 = best, 0 = worst; original data\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             goodScorePos = TRUE,\n             areaColourScheme = \"DIRECTION\",\n             pointColourScheme = \"DIRECTION\",\n             pointSize = 1.8)\n\n\n\nThat also introduces another argument: areaAlpha: on some screens than others the red points are visible on top of the red area but the green points are not very visible. There are two arguments that control the alpha (transparency) of the points and areas: pointAlpha and areaAlpha. The default value of pointAlpha is 1, i.e. no transparency, the default value of areaAlpha is .3, i.e. 30% transparency which for my vision system and screen at least is working for the red “deterioration” but not the green “improvement”. Let’s trying pushing the area transparency down to .1.\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             subtitle = \"Positively cued scoring: 4 = best, 0 = worst; original data\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             goodScorePos = TRUE,\n             areaColourScheme = \"DIRECTION\",\n             pointColourScheme = \"DIRECTION\",\n             pointSize = 1.8,\n             areaAlpha = .1)\n\n\n\nOK (at least to my visual system and monitors!) I hope that illustrates that many things can be tweaked to get better images.\nCategories: direction, RCI, CSC and RCSC\nDirection\nDirection is the simplest. I think people are only going to use this when they have neither a usable RCI nor a usable CSC.\n\n\nShow code\n\n#1\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             title = \"Direction categories, by point colour\",\n             subtitle = \"\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             goodScorePos = FALSE,\n             pointColourScheme = \"DIRECTION\",\n             areaColourScheme = \"NONE\",\n             pointSize = 1.7,\n             lineTypeCSC = 0,\n             lineTypeRCI = 0) -> tmpPlot1\ntmpPlot1\n\n\nShow code\n\n#2\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             title = \"Direction categories, by area colour\",\n             subtitle = \"\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             goodScorePos = FALSE,\n             pointColourScheme = \"NONE\",\n             areaColourScheme = \"DIRECTION\",\n             pointSize = 1.7,\n             lineTypeCSC = 0,\n             lineTypeRCI = 0) -> tmpPlot2\ntmpPlot2\n\n\n\nThat shows one slightly counterintuitive issue: when mapping direction of change by point there are three categories: deteriorated, unchanged and improved but if mapping by the area of the plot there are only two categories as the unchanged points don’t lie in either area but on the boundary between the areas.\n\n\nShow code\n\n#3\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             title = \"Direction categories, by point shapes\",\n             subtitle = \"\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             goodScorePos = FALSE,\n             pointColourScheme = \"NONE\",\n             areaColourScheme = \"NONE\",\n             pointShapeScheme = \"DIRECTION\",\n             pointSize = 1.7,\n             lineTypeCSC = 0,\n             lineTypeRCI = 0) -> tmpPlot3\ntmpPlot3\n\n\nShow code\n\n#4\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             title = \"Direction categories, by point and area colours\",\n             subtitle = \"Default alpha (transparency) for area and point\\nallows points to be seen on same colour areas\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             goodScorePos = FALSE,\n             pointColourScheme = \"DIRECTION\",\n             areaColourScheme = \"DIRECTION\",\n             pointShapeScheme = \"NONE\",\n             pointSize = 1.7,\n             lineTypeCSC = 0,\n             lineTypeRCI = 0) -> tmpPlot4\ntmpPlot4\n\n\nShow code\n\n#5\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             title = \"Direction categories, by area colours and point shapes\",\n             subtitle = \"\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             goodScorePos = FALSE,\n             pointColourScheme = \"NONE\",\n             areaColourScheme = \"DIRECTION\",\n             pointShapeScheme = \"DIRECTION\",\n             pointSize = 1.7,\n             lineTypeCSC = 0,\n             lineTypeRCI = 0) -> tmpPlot5\ntmpPlot5\n\n\nShow code\n\n#6\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             title = \"Direction categories, by point colours and shapes\",\n             subtitle = \"\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             goodScorePos = FALSE,\n             pointColourScheme = \"DIRECTION\",\n             areaColourScheme = \"NONE\",\n             pointShapeScheme = \"DIRECTION\",\n             pointSize = 1.7,\n             lineTypeCSC = 0,\n             lineTypeRCI = 0) -> tmpPlot6\ntmpPlot6\n\n\nShow code\n\n#7\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             title = \"Direction categories, by area and point colours and point shapes\",\n             subtitle = \"\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             goodScorePos = FALSE,\n             pointColourScheme = \"DIRECTION\",\n             areaColourScheme = \"DIRECTION\",\n             pointShapeScheme = \"DIRECTION\",\n             pointSize = 1.5,\n             lineTypeCSC = 0,\n             lineTypeRCI = 0) -> tmpPlot7\ntmpPlot7\n\n\nShow code\n\n# figure <- ggpubr::ggarrange(tmpPlot1, tmpPlot2, tmpPlot3,\n#                             tmpPlot4, tmpPlot5, tmpPlot6, tmpPlot7,\n#                     # labels = c(\"A\", \"B\", \"C\"),\n#                     ncol = 2, nrow = 4)\n# figure\n\n\nRCI categories “RCI3”\nThis is just the conventional three level RCI categorisation.\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCI categories, by point colour\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCI3\",\n                areaColourScheme = \"NONE\",\n                pointSize = 1.7,\n                noLegend = FALSE) -> tmpPlot1\ntmpPlot1\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCI categories, by area colour\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"NONE\",\n                areaColourScheme = \"RCI3\",\n                pointSize = 1.7,\n                noLegend = FALSE) -> tmpPlot2\ntmpPlot2\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCI categories, by point shapes\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"NONE\",\n                areaColourScheme = \"NONE\",\n                pointShapeScheme = \"RCI3\",\n                pointSize = 1.7,\n                noLegend = FALSE) -> tmpPlot3\ntmpPlot3\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCI categories, by point and area colours\",\n                subtitle = \"Default alpha (transparency) for area and point\\nallows points to be seen on same colour areas\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCI3\",\n                areaColourScheme = \"RCI3\",\n                pointShapeScheme = \"NONE\",\n                pointSize = 1.7,\n                noLegend = FALSE) -> tmpPlot4\ntmpPlot4\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCI categories, by area colours and point shapes\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"NONE\",\n                areaColourScheme = \"RCI3\",\n                pointShapeScheme = \"RCI3\",\n                pointSize = 1.7,\n                noLegend = FALSE) -> tmpPlot5\ntmpPlot5\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCI categories, by point colours and shapes\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCI3\",\n                areaColourScheme = \"NONE\",\n                pointShapeScheme = \"RCI3\",\n                pointSize = 1.7,\n                noLegend = FALSE) -> tmpPlot6\ntmpPlot6\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCI categories, by area and point colours and point shapes\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCI3\",\n                areaColourScheme = \"RCI3\",\n                pointShapeScheme = \"RCI3\",\n                pointSize = 1.5,\n                noLegend = FALSE) -> tmpPlot7\ntmpPlot7\n\n\nShow code\n\n# figure <- ggpubr::ggarrange(tmpPlot1, tmpPlot2, tmpPlot3,\n#                             tmpPlot4, tmpPlot5, tmpPlot6, tmpPlot7,\n#                             # labels = c(\"A\", \"B\", \"C\"),\n#                             ncol = 2, nrow = 4)\n# figure\n\n\nRCSC categories “RCSC5”\nRCSC categorisations combine the RCI categories with the CSC categories giving a maximum of 10 categories (as it’s not possible to have clinically significant deterioration with reliable deterioration nor vice versa.)\nI think what I am calling RCSC5 is the most commonly used RCSC categorisation which\nlumps all the “no reliable deterioration” and “reliable deterioration” categories, i.e. letting those RCI categories trump the CSC categories but distinguishing the three CSC categories within those showing reliable improvement and terming those who were in the reliable and clinically improved polygon as “recovered” (a term I dislike as it seems to me both to ignore the “recovery” movement and hugely overstate what we can derive from the measures we are using).\nYou could set pointShapeScheme to “DIRECTION” here but I think the only shape mapping to use logically is “RCI3” as that’s crucial to all the RCSC categorisations. I’ve used “RCI3” here when the title says mapping to point shape as well as area or point colour.\n\n\nShow code\n\n#1\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC5 categories, by point colour\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCSC5\",  # @@@@@@@\n                areaColourScheme = \"NONE\",\n                pointSize = 1.7) -> tmpPlot1\ntmpPlot1\n\n\nShow code\n\n#2\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC5 categories, by area colour\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"NONE\",\n                areaColourScheme = \"RCSC5\",\n                pointSize = 1.7) -> tmpPlot2\ntmpPlot2\n\n\nShow code\n\n#3\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC5 categories, by point and area colours\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCSC5\",\n                areaColourScheme = \"RCSC5\",\n                pointShapeScheme = \"NONE\",\n                pointSize = 1.7) -> tmpPlot3\ntmpPlot3\n\n\nShow code\n\n#4\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC5 categories, by area colours and point shapes\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"NONE\",\n                areaColourScheme = \"RCSC5\",\n                pointShapeScheme = \"RCI3\",\n                pointSize = 1.7) -> tmpPlot4\ntmpPlot4\n\n\nShow code\n\n# figure <- ggpubr::ggarrange(tmpPlot1, tmpPlot2, tmpPlot3, \n#                             tmpPlot4,  \n#                             ncol = 1, nrow = 4)\n# figure\n\n\nRCSC categories “RCSC7”\nLike what I call “RCSC5” this allows the “no reliable change” to trump the CSC categories. However, unlike RCSC5 it splits the reliably deteriorated into three categories. Again, mapping to shape here only uses the reliable change categories.\n\n\nShow code\n\n#1\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC7 categories, by point colour\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCSC7\",\n                areaColourScheme = \"NONE\",\n                pointSize = 1.7) -> tmpPlot1\ntmpPlot1\n\n\nShow code\n\n#2\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC7 categories, by area colour\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"NONE\",\n                areaColourScheme = \"RCSC7\",\n                pointSize = 1.7) -> tmpPlot2\ntmpPlot2\n\n\nShow code\n\n#3\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC7 categories, by point and area colours\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCSC7\",\n                areaColourScheme = \"RCSC7\",\n                pointShapeScheme = \"NONE\",\n                pointSize = 1.7) -> tmpPlot3\ntmpPlot3\n\n\nShow code\n\n# figure <- ggpubr::ggarrange(tmpPlot1, tmpPlot2, tmpPlot3, \n#                             # labels = c(\"A\", \"B\", \"C\"),\n#                             ncol = 1, nrow = 3)\n# figure\n\n\nRCSC categories “RCSC10”\nI think this is a bit mad but I have created it! I haven’t bothered showing it coupled with direction or RCI3 by point shape as I think that really would be getting mad!\n\n\nShow code\n\n#1\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC10 categories, by point colour\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCSC10\",\n                areaColourScheme = \"NONE\",\n                pointSize = 1.7) -> tmpPlot1\ntmpPlot1\n\n\nShow code\n\n#2\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC10 categories, by area colour\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"NONE\",\n                areaColourScheme = \"RCSC10\",\n                pointSize = 1.7) -> tmpPlot2\ntmpPlot2\n\n\nShow code\n\n#4\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                title = \"RCSC10 categories, by point and area colours\",\n                subtitle = \"\",\n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                goodScorePos = FALSE,\n                pointColourScheme = \"RCSC10\",\n                areaColourScheme = \"RCSC10\",\n                pointShapeScheme = \"NONE\",\n                pointSize = 1.7) -> tmpPlot3\ntmpPlot3\n\n\nShow code\n\n# figure <- ggpubr::ggarrange(tmpPlot1, tmpPlot2, tmpPlot3, \n#                             # labels = c(\"A\", \"B\", \"C\"),\n#                             ncol = 1, nrow = 3)\n# figure\n\n\nAll the arguments for getJacobsonPlot()\nHere is the function definition showing the full set of arguments. You’ll see that most have default values. I’ll go through all the arguments below, by their groups, after demonstrating some of the category mappings.\ngetJacobsonPlot <- function(baselineScores, \n                          finalScores, \n                          pointNames = NA, # can be used to label points\n                          namesNotPoints = FALSE, # only use labels, no points\n                          nameRCIDetOnly = FALSE, # only label deteriorators\n                          namesSize = 2, # sets the font size for labels\n                          CSC = NA, # the CSC value to use\n                          RCI = NA, # the RCI value to use\n                          minPossScore = 0, \n                          maxPossScore = 4, \n                          goodScorePos = FALSE, # set to TRUE/T if a better score is positive\n                          omitNHS = FALSE, # use to omit scores starting in non-help-seeking range\n                          title = NULL, # to override the default title\n                          subtitle = NULL, # ditto for subtitle\n                          titleJustificn = .5, # centre justifies titles, 0 for L, 1 for R\n                          xLab = NULL, # overrides the default x axis label \n                          yLab = NULL, # overrides the default y axis label\n                          noLegend = FALSE, # removes legends for colours\n                          labelCSClines = FALSE, # set to TRUE/T to label CSC lines with CSC value\n                          areaColourScheme = c(\"NONE\", # mapping to use, see notes, schemes are:\n                                               \"RCSC5\",\n                                               \"DIRECTION\",\n                                               \"RCI3\",\n                                               \"CSC4\",\n                                               \"RCSC7\",\n                                               \"RCSC10\",\n                                               \"POLYGONS\"), \n                          legendNameAreas = \"Area mapping\", # set title for area legend\n                          pointColourScheme = c(\"RCSC5\",\n                                                \"RCI3\",\n                                                \"DIRECTION\",\n                                                \"CSC4\",\n                                                \"RCSC7\",\n                                                \"RCSC10\",\n                                                \"NONE\"), # chose point colour mapping, as above\n                          legendNamePoints = \"Point mapping\", # title for point colours\n                          ### now lots of mappings used by the above colour mappings\n                          coloursDirection = c(\"red\", \n                                               \"green\", \n                                               \"black\"),\n                          ### you can override the labels to local choice and language\n                          labelsDirection = c(\"Deterioration\", \n                                              \"Improvement\", \n                                              \"No change\"),\n                          coloursRCI3 = c(\"grey\", \n                                          \"red\", \n                                          \"green\"),\n                          labelsRCI3 = c(\"No reliable change\", \n                                         \"Reliable deterioration\", \n                                         \"Reliable improvement\"),\n                          coloursCSC4 = c(\"green\", \n                                          \"red\", \n                                          \"orange\", \n                                          \"skyblue1\"),\n                          labelsCSC4 = c(\"High to low\", \n                                         \"Low to high\", \n                                         \"Stayed high\", \n                                         \"Stayed low\"),\n                          coloursRCSC5 = c(\"grey\", \n                                           \"red\", \n                                           \"skyblue1\", \n                                           \"green\", \n                                           \"chartreuse4\"),\n                          labelsRCSC5 = c(\"No reliable change\", \n                                          \"Reliable deterioration\", \n                                          \"Reliable improvement, stayed high\", \n                                          \"Reliable improvement, high to low ('Recovered')\", \n                                          \"Reliable improvement, stayed low\"),\n                          ### it's essentially impossible to code labels for the RCSC\n                          ### that would be independent of the cueing of the scores\n                          ### hence different labels (but same colours and order)\n                          ### for positively cued measures\n                          labelsRCSC5pos = c(\"No reliable change\", \n                                             \"Reliable deterioration\", \n                                             \"Reliable improvement, stayed high\", \n                                             \"Reliable improvement, low to high ('Recovered')\", \n                                             \"Reliable improvement, stayed low\"),\n                          coloursRCSC7 = c(\"grey\", \n                                           \"orange\", \n                                           \"red\", \n                                           \"sienna2\", \n                                           \"palegreen\", \n                                           \"green\", \n                                           \"skyblue1\"),\n                          labelsRCSC7 = c(\"No reliable change\", \n                                          \"Reliable deterioration, stayed high\", \n                                          \"Reliable deterioration, low to high\", \n                                          \"Reliable deterioration, stayed low\", \n                                          \"Reliable improvement, stayed high\", \n                                          \"Reliable improvement, high to low ('Recovered')\", \n                                          \"Reliable improvement, stayed low\"),\n                          labelsRCSC7pos = c(\"No reliable change\", \n                                             \"Reliable deterioration, stayed low\", \n                                             \"Reliable deterioration, high to low\", \n                                             \"Reliable deterioration, stayed high\", \n                                             \"Reliable improvement, stayed low\", \n                                             \"Reliable improvement, low to high ('Recovered')\", \n                                             \"Reliable improvement, stayed high\"),\n                          coloursRCSC10 = c(\"yellow\", #1\n                                            \"blue\", #2\n                                            \"purple\", #3\n                                            \"grey\", #4\n                                            \"orange\", #5\n                                            \"red\", #6\n                                            \"violetred4\", #7\n                                            \"palegreen\", #8\n                                            \"green\", #9\n                                            \"chartreuse4\"), #10\n                          labelsRCSC10 = c(\"No reliable change, stayed high\", #1\n                                           \"No reliable change, high to low\", #2\n                                           \"No reliable change, low to high\", #3\n                                           \"No reliable change, stayed low\", #4\n                                           \"Reliable deterioration, stayed high\", #5 \n                                           \"Reliable deterioration, low to high\", #6\n                                           \"Reliable deterioration, stayed low\", #7\n                                           \"Reliable improvement, stayed high\", #8\n                                           \"Reliable improvement, high to low ('Recovered')\", #9\n                                           \"Reliable improvement, stayed low\"), #10\n                          labelsRCSC10pos = c(\"No reliable change, stayed low\", #1p\n                                              \"No reliable change, low to high\", #2p\n                                              \"No reliable change, high to low\", #3p\n                                              \"No reliable change, stayed high\", #4p\n                                              \"Reliable deterioration, stayed low\", #5p\n                                              \"Reliable deterioration, low to high\", #6p\n                                              \"Reliable deterioration, stayed high\", #7p\n                                              \"Reliable improvement, stayed low\", #8p\n                                              \"Reliable improvement, low to high ('Recovered')\", #9p\n                                              \"Reliable improvement, stayed high\"), #10p\n                          ### you can also add mapping to point shapes\n                          pointShapeScheme = c(\"NONE\",\n                                               \"DIRECTION\",\n                                               \"RCI3\"),\n                          ### these allow you to override the default mapping to \n                          ### a square and arrows pointing the appropriate ways\n                          ### NULL leaves the default mapping in place\n                          ### I really don't advise overriding these but you can!\n                          pointShapeNoRelChange = NULL,\n                          pointShapeRelDet = NULL,\n                          pointShapeRelImp = NULL,\n                          ### the following arguments control the aesthetics\n                          pointSize = 1,\n                          ### the default alpha settings allow both points and areas to be coloured\n                          pointAlpha = 1, # 1 is no transparency, 0 is no colour\n                          areaAlpha = .3, # ditto, .3 is probably OK for a pale coloured ground\n                          ### aesthetics for the no-change diagonal line\n                          lineTypeNoChange = 1, # default is solid line\n                          lineWidthNoChange = .5, # default is ggplot default line thickness\n                          lineColourNoChange = 1, # default is black\n                          ### same for the CSC lines\n                          lineTypeCSC = 1, \n                          lineWidthCSC = .5, \n                          lineColourCSC = 1, \n                          ### same for the RCI lines\n                          lineTypeRCI = 1,\n                          lineWidthRCI = .5,\n                          lineColourRCI = 1,\n                          themeToUse = theme_bw(), # set ggplot theme to use, unquoted\n                          ### I can't imagine changing these myself but ...\n                          ### these allow you to override the theme's colours\n                          plotColour = NULL, # background colour of the plot area itself, e.g. \"white\"\n                          borderColour = NULL, # and same for the border around it\n                          ### you might want to use these to suppress messages and warnings\n                          noMessages = FALSE, # suppress messages\n                          noWarnings = FALSE, # suppress my warnings (not other R ones)\n                          ### these are probably only useful to help me debug things\n                          printVariables = FALSE, # will give a print out of all internal variables\n                          warningsToErrors = FALSE, # makes warnings into errors, i.e. \n                          ### this is really only for someone drilling into the code\n                          ### it labels all twelve area polygons with their numbers\n                          labelAllPolygons = FALSE,\n                          ### arguments not implemented yet\n                          greyScale = FALSE # may be implemented to force B&W/greyscale\nMost of those can be ignored most of the time. The required arguments are: @@@\nbaselineScores: a vector\nfinalScores: a vector, same length as baselineScores\nArguments that have defaults, hence are optional, but which you will almost certainly wish to add, changing the defaults are as follows:\nminPossScore: defaults to 0 as the data used are CORE-OM data scored on the mean item score, not the “clinical score” method. Must be a single number appropriate for your scale.\nmaxPossScore: defaults to 4, ditto.\nCSC: defaults to NA, but put in whatever you need for your measure, I have used 1.26 as that was correct for the demo data. Using the default NA will drop CSC aspects of plot entirely, probably not what you want.\nRCI: defaults to NA, but put in whatever you need for your measure, I have used 0.3123851, again because that was correct for the demo data. Using the default NA will drop the RCI aspects of plot entirely, again probably not what you want.\ngoodScorePos: defaults to FALSE as I think most therapy change measures are cued so higher scores indicate more distress/dysfunction and the aim of therapy is to reduce scores, change it to TRUE for measures where more positive scores indicate better states or better functioning. That change doesn’t change the numbering on the axes running from lower to higher but it remaps the labelling of points (when used, see below) and of the areas (when used, see below!).\nOptional arguments, not for categories\nOther optional arguments fall into the following categories:\nText options\ntitle\nsubtitle (to add one)\nx axis label\ny axis label\n\nLine type options\nPoint labelling options (generally by ID but could be by categories, e.g gender, therapist ID etc.)\nPoint marker options\nChange categories/area options\nwhat areas?\narea colours\narea labels\n\nLegend options\nThe next sections run through these groups of arguments and show their impacts on the plot.\nText options\nAs we saw, the subtitle argument allows you to add a subtitle using the default formatting of subtitles for the theme.\nOther text control arguments are the title, titleJustificn, xLab and yLab arguments.\nThe title argument just allows you to replace the default title with anything of your own.\nThe titleJustificn argument does what it says: controls the justification of the title (and subtitle actually). Why anyone might want to use right justified title beats me but I think some journals do require left justifcation so titleJustificn = 0 would get that.\nThe xLab and yLab allow you to override the axis labels.\nIn principle these text components can be in any language your local implementation of R (and perhaps your operating system) can handle.\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             title = \"Jacobson (RCSC) plot of data from Quito clinic\",\n             subtitle = paste0(\"n = \", nrow(tibDat), \" clients\"),\n             titleJustificn = 1,\n             xLab = \"CORE-OM score at first session (= assessment)\",\n             yLab = \"Last completed CORE-OM score\")\n\n\n\nLine type options\nThese can be used to change the line types used for the three categories of lines on the usual Jacobson plot:\nthe CSC lines for the horizontal and vertical CSC references lines\nthe diagonal no-change line and\nthe RCI tramlines either side of that no change line.\nFor each of these the arguments are:\nlineType for the line types that R/ggplot offers\nlineColour for the line colour (doh!) and\nlineWidth for … guess!\nSo the full list of these options is: lineTypeCSC, lineColourCSC, lineWidthCSC; lineTypeNoChange, lineColourNoChange, lineWidthNoChange; and lineTypeRCI, lineColourRCI, lineWidthRCI.\nHere is a truly aesthetically horrible plot has removed the RCI tramlines by using lineTypeRCI = 0, turned the CSC lines red with lineColourCSC = “red” and dotted with lineTypeCSC = 3. I suspect that these options are only really useful when making plots to use for teaching.\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4,\n             lineTypeCSC = 3,\n             lineWidthCSC = 0,\n             lineColourCSC = \"red\",\n             lineTypeNoChange = 2,\n             lineWidthNoChange = 1,\n             lineColourNoChange = \"blue\",\n             lineTypeRCI = 0,\n             lineWidthRCI = 0)\n\n\n\nPoint labelling\nThese options are best introduced with ID labelling but the argument could be used to label points by categories, e.g gender, therapist ID etc. though the realities of the plotting area are such that any variable used here should probably be of a maximum of three characters in width. I hope to write another function to handle categories rather differently using a legend or facetting. For the moment, using the one argument, ID and category labelling are mutually exclusively options unless you create a system like “M01”, “M02” etc. for male client IDs and “F01”, “F02” for female client IDs.\nThe pertinent arguments are:\npointNames: defaults to NA, replace with a variable name, in this next example as.character(tibDat$id). That illustrates that it is best to use a character variable. More on this below.\nnamesNotPoints: this defaults to FALSE, set to TRUE to get ID labels instead of points. At the moment (13.xi.23) you have to set this to TRUE to get labels and you can only get labels instead of points. I may fix this so that just giving a variable for pointNames will give you the points AND will label them but that will only work with really very small numbers of points/participants so I haven’t implemented it yet.\nnameRCIDetOnly: defaults to FALSE. If set to TRUE the only points designated reliable deterioration are labelled.\nnamesSize: defaults to 2, change it to another value to make the labels larger (> 2) or smaller (< 2).\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             pointNames = as.character(tibDat$id), \n             pointColourScheme = \"RCI3\",\n             namesNotPoints = TRUE, \n             nameRCIDetOnly = FALSE, \n             # namesSize = 2, \n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4)\n\n\n\nThat illustrates a lot of problems with labels.\nLabels don’t really combine well with using point colours to map points to categories.\nWith many (here 185) points you have to make the text small so the labels are hardly readable or frankly unreadable.\nUnless you really have quite few points, if you make the text big enough to read, the points will probably overlap and still be unreadable.\nIt’s harder to tell the precise location of the points from text labels than point shapes.\nThis next plot maps the RCI categories to plot areas rather than to point colours to at least have all the labels in black and it uses namesSize = 5 to make isolated labels much more readable than with the default namesSize of 2. However, it illustrates the overprinting issue!\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n             finalScores = tibDat$lastScore, \n             pointNames = as.character(tibDat$id), \n             areaColourScheme = \"RCI3\",\n             namesNotPoints = TRUE, \n             nameRCIDetOnly = FALSE, \n             namesSize = 5, \n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4)\n\n\n\nThis, with an artificially reduced dataset illustrates that ID labelling could work with small datasets.\n\n\nShow code\n\ngoodScorePos <- FALSE\ntibDat %>% \n  ### get only real data\n  filter(id < 400) %>%\n  mutate(firstLastChange = lastScore - firstScore,\n         direction3 = case_when(firstLastChange == 0 ~ \"NoChange\",\n                                  goodScorePos & firstLastChange < 0 ~ \"Det\",\n                                  goodScorePos & firstLastChange > 0 ~ \"Imp\",\n                                  !goodScorePos & firstLastChange < 0 ~ \"Imp\",\n                                  !goodScorePos & firstLastChange > 0 ~ \"Det\"),\n         RCIchange = if_else(abs(firstLastChange) < RCI, \"NoRelChange\", paste0(\"Rel\", direction3)),\n         firstCSCstatus = case_when(!goodScorePos & firstScore >= CSC ~ \"HS\", \n                                    !goodScorePos & firstScore < CSC ~ \"NHS\",\n                                    goodScorePos & firstScore < CSC ~ \"HS\", \n                                    goodScorePos & firstScore >= CSC ~ \"NHS\"),\n         lastCSCstatus = case_when(!goodScorePos & lastScore >= CSC ~ \"HS\", \n                                   !goodScorePos & lastScore < CSC ~ \"NHS\",\n                                   goodScorePos & lastScore < CSC ~ \"HS\", \n                                   goodScorePos & lastScore >= CSC ~ \"NHS\"),\n         CSC4 = paste0(firstCSCstatus, \"To\", lastCSCstatus),\n         RCSC5 = case_when(\n           RCIchange == \"NoRelChange\" ~ \"NoRelChange\",\n           RCIchange == \"RelDet\"  ~ \"RelDet\",\n           RCIchange == \"RelImp\" & CSC4 == \"HSToHS\" ~ \"RelImpHSToHS\",\n           RCIchange == \"RelImp\" & CSC4 == \"HSToNHS\" ~ \"RelImpHSToNHS\",\n           RCIchange == \"RelImp\" & CSC4 == \"NHSToNHS\" ~ \"RelImpNHSToNHS\")) %>%\n  ### now reduce numbers\n  group_by(RCSC5) %>%\n  filter(row_number() < 6) -> tmpDat\n\ngetJacobsonPlot(baselineScores = tmpDat$firstScore, \n             finalScores = tmpDat$lastScore, \n             pointNames = as.character(tmpDat$id), \n             namesNotPoints = TRUE, \n             nameRCIDetOnly = FALSE, \n             areaColourScheme = \"RCI3\",\n             pointColourScheme = \"NONE\",\n             namesSize = 5, \n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4)\n\n\n\nHere, with totally artificial age labels, is an example labelling with age. These are totally artificial ages (though the data are real but demographics removed to protect confidentiality). I set them as:\n“E” for Emerging adult, >= 18 and <= 25\n“A” for adult: > 25 and < 60\n“O” for older: > 60\nAnd I allocated in a ratio 3:5:2.\n\n\nShow code\n\ntibDat %>% \n  ### get only real data\n  filter(id < 400) %>%\n  mutate(firstLastChange = lastScore - firstScore,\n         direction3 = case_when(firstLastChange == 0 ~ \"NoChange\",\n                                  goodScorePos & firstLastChange < 0 ~ \"Det\",\n                                  goodScorePos & firstLastChange > 0 ~ \"Imp\",\n                                  !goodScorePos & firstLastChange < 0 ~ \"Imp\",\n                                  !goodScorePos & firstLastChange > 0 ~ \"Det\"),\n         RCIchange = if_else(abs(firstLastChange) < RCI, \"NoRelChange\", paste0(\"Rel\", direction3)),\n         firstCSCstatus = case_when(!goodScorePos & firstScore >= CSC ~ \"HS\", \n                                    !goodScorePos & firstScore < CSC ~ \"NHS\",\n                                    goodScorePos & firstScore < CSC ~ \"HS\", \n                                    goodScorePos & firstScore >= CSC ~ \"NHS\"),\n         lastCSCstatus = case_when(!goodScorePos & lastScore >= CSC ~ \"HS\", \n                                   !goodScorePos & lastScore < CSC ~ \"NHS\",\n                                   goodScorePos & lastScore < CSC ~ \"HS\", \n                                   goodScorePos & lastScore >= CSC ~ \"NHS\"),\n         CSC4 = paste0(firstCSCstatus, \"To\", lastCSCstatus),\n         RCSC5 = case_when(\n           RCIchange == \"NoRelChange\" ~ \"NoRelChange\",\n           RCIchange == \"RelDet\"  ~ \"RelDet\",\n           RCIchange == \"RelImp\" & CSC4 == \"HSToHS\" ~ \"RelImpHSToHS\",\n           RCIchange == \"RelImp\" & CSC4 == \"HSToNHS\" ~ \"RelImpHSToNHS\",\n           RCIchange == \"RelImp\" & CSC4 == \"NHSToNHS\" ~ \"RelImpNHSToNHS\")) %>%\n  ### now reduce numbers\n  group_by(RCSC5) %>%\n  filter(row_number() < 21) %>%\n  ungroup() -> tmpDat\n\ntmpDat %>%\n  summarise(n = n()) %>%\n  pull(n) -> tmpN\n\ntmpDat %>%\n  mutate(age = sample(c(rep(\"E\", 3),\n                        rep(\"A\", 5),\n                        rep(\"O\", 2)), \n                      tmpN,\n                      replace = TRUE)) -> tmpDat\n\ngetJacobsonPlot(baselineScores = tmpDat$firstScore, \n             finalScores = tmpDat$lastScore, \n             pointNames = tmpDat$age, \n             namesNotPoints = TRUE, \n             nameRCIDetOnly = FALSE, \n             namesSize = 3.5, \n             areaColourScheme = \"RCI3\",\n             pointColourScheme = \"NONE\",\n             CSC = 1.26, \n             RCI = 0.3983001,\n             minPossScore = 0, \n             maxPossScore = 4)\n\n\n\nEven here with n =\n65 and more pruning out of points from the usually more occupied areas of the plot, overprinting is becoming a issue.\nA service may be particularly interested in the IDs of reliable deteriorators. (Though I recommend if doing clinical audit, it is best to balance looking at deteriorators by looking at a similar number of clients who showed the best improvements.) This shows the IDs of the reliable deteriorators. (The IDs above 900 are artificial ones I added to the real data to ensure that all areas of the plot had at least one point.)\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore,\n             finalScores = tibDat$lastScore,\n             pointNames = as.character(tibDat$id),\n             namesNotPoints = TRUE,\n             nameRCIDetOnly = TRUE,\n             namesSize = 4,\n             areaColourScheme = \"RCI3\",\n             pointColourScheme = \"NONE\",\n             CSC = 1.26,\n             RCI = 0.3983001,\n             minPossScore = 0,\n             maxPossScore = 4)\n\n\n\nPoint labelling options\nWe have already seen that the label size can be varied with namesSize. (13.xi.23: I think I should add a choice of label colour and alpha/transparency and perhaps font and font characteristics.)\nPoint marker options\nThese are pretty esoteric but these three arguments control the mapping of direction of change to point shapes\npointShapeNoRelChange: defaults to NULL,\npointShapeRelDet: again defaults to NULL,\npointShapeRelImp: defaults to NULL\nIn addition there are:\npointSize: what it says, defaults to 1, you may want to adjust this depending on your output format\npointAlpha: defaults to 1, i.e. no transparency. I can’t think why you might want to increase transparency, particularly if you are using mapping of characters to areas as the more you reduce this, the less visible the points will become.\n13.xi.23: ouch, something is overriding the remapping of the point shapes for no change but improvement and deterioration are working. Damn!\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                areaColourScheme = \"RCI3\",\n                pointColourScheme = \"RCI3\",\n                pointShapeScheme = \"RCI3\",\n                pointShapeNoRelChange = \"*\",\n                pointShapeRelDet = \"D\",\n                pointShapeRelImp = \"B\", \n                pointSize = 2.5)\n\n\n\nLegend options\n13.xi.23 Ouch, more to do. Need to make it possible to override the default legend labels.\nSo for now the only option is noLegend which defaults to FALSE, which, as this shows, simply removes any legends.\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                pointSize = 2,\n                noLegend = TRUE)\n\n\n\nSaving the plot\nBy default the function returns the plot as a ggplot object. That means that if you don’t capture the returned plot, R will usually print it to whatever default printing is current for your R session, usually to the console. However, you can capture the ggplot object and do some post-processing of it, though this is probably for R afficionados. This next example illustrates using this to change some aspects of the plot by altering theme settings. Here I have changed some text colours (why you would want to do that is of course, unclear!). Some theme changes won’t impact on the plot, at least some arguments have been used to set things like area colours.\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n              finalScores = tibDat$lastScore, \n              CSC = 1.26, \n              RCI = 0.3983001,\n              minPossScore = 0, \n              maxPossScore = 4) -> savedPlot\n\nsavedPlot +\n  theme(text = element_text(colour = \"blue\"),\n        axis.text = element_text(colour = \"red\"),\n        plot.title = element_text(hjust = 0), \n        plot.subtitle = element_text(hjust = 0)) \n\n\n\nProbably more importantly this can be used to capture the plot and send it to another print option, e.g. to print it to a PDF, jpeg, png or tiff file. (Use R help for those formats if you aren’t familiar with doing that.)\nTechnicalities (geek stuff)\nprintVariables\n[This needs a lot of work.]\nI have written in an argument, printVariables, a logical value which defaults to FALSE. If TRUE it will print a lot (a lot) of information about the internal state of the function which might help me debug unexpected problems. Here’s what it tells me for the default run.\n\n\nShow code\n\ngetJacobsonPlot(baselineScores = tibDat$firstScore, \n                finalScores = tibDat$lastScore, \n                CSC = 1.26, \n                RCI = 0.3983001,\n                minPossScore = 0, \n                maxPossScore = 4,\n                printVariables = TRUE) -> savedPlot\n\n[1] \" \"\n[1] \"Here are all the arguments from the call:\"\n[1] \" \"\n$baselineScores\ntibDat$firstScore\n\n$finalScores\ntibDat$lastScore\n\n$CSC\n[1] 1.26\n\n$RCI\n[1] 0.3983001\n\n$minPossScore\n[1] 0\n\n$maxPossScore\n[1] 4\n\n$printVariables\n[1] TRUE\n\n[1] \" \"\n[1] \"And here are all the internal variables at the end of the function:\"\n[1] \" \"\n  [1] \"areaAlpha\"                         \"areaColourScheme\"                  \"baselineScores\"                    \"borderColour\"                      \"collapseVec\"                       \"coloursCSC4\"                      \n  [7] \"coloursDirection\"                  \"coloursRCI3\"                       \"coloursRCSC10\"                     \"coloursRCSC5\"                      \"coloursRCSC7\"                      \"Complete\"                         \n [13] \"CSC\"                               \"descVars\"                          \"dfPolygonNoRelChangeDetHighToHigh\" \"dfPolygonNoRelChangeDetLowToHigh\"  \"dfPolygonNoRelChangeDetLowToLow\"   \"dfPolygonNoRelChangeImpHighToHigh\"\n [19] \"dfPolygonNoRelChangeImpHighToLow\"  \"dfPolygonNoRelChangeImpLowToLow\"   \"dfPolygonRelDetHighToHigh\"         \"dfPolygonRelDetLowToHigh\"          \"dfPolygonRelDetLowToLow\"           \"dfPolygonRelImpHighToHigh\"        \n [25] \"dfPolygonRelImpHighToLow\"          \"dfPolygonRelImpLowToLow\"           \"finalScores\"                       \"getClasses\"                        \"goodScorePos\"                      \"greyScale\"                        \n [31] \"Incomplete\"                        \"labelAllPolygons\"                  \"labelCSClines\"                     \"labelsCSC4\"                        \"labelsDirection\"                   \"labelsRCI3\"                       \n [37] \"labelsRCSC10\"                      \"labelsRCSC10pos\"                   \"labelsRCSC5\"                       \"labelsRCSC5pos\"                    \"labelsRCSC7\"                       \"labelsRCSC7pos\"                   \n [43] \"legendNameAreas\"                   \"legendNamePoints\"                  \"lineColourCSC\"                     \"lineColourNoChange\"                \"lineColourRCI\"                     \"lineTypeCSC\"                      \n [49] \"lineTypeNoChange\"                  \"lineTypeRCI\"                       \"lineWidthCSC\"                      \"lineWidthNoChange\"                 \"lineWidthRCI\"                      \"listArgs\"                         \n [55] \"maxPossScore\"                      \"minPossScore\"                      \"nameRCIDetOnly\"                    \"namesNotPoints\"                    \"namesSize\"                         \"nAreaColours\"                     \n [61] \"nHelpSeeking\"                      \"noLegend\"                          \"noMessages\"                        \"noWarnings\"                        \"nPointColours\"                     \"nPointShapes\"                     \n [67] \"oldTheme\"                          \"oldWarn\"                           \"omitNHS\"                           \"Original_n\"                        \"plotColour\"                        \"pointAlpha\"                       \n [73] \"pointColourScheme\"                 \"pointNames\"                        \"pointShape\"                        \"pointShapeNoRelChange\"             \"pointShapeRelDet\"                  \"pointShapeRelImp\"                 \n [79] \"pointShapeScheme\"                  \"pointSize\"                         \"printVariables\"                    \"RCI\"                               \"scoreRange\"                        \"subtitle\"                         \n [85] \"themeToUse\"                        \"tibAreas\"                          \"tibAreas1\"                         \"title\"                             \"titleJustificn\"                    \"tmpPlot\"                          \n [91] \"tmpTib\"                            \"tmpTibMissing\"                     \"tmpVecRCSC10\"                      \"tmpVecRCSC5\"                       \"tmpVecRCSC7\"                       \"vec12alphas\"                      \n [97] \"vec12colours\"                      \"vecAreaColours\"                    \"vecAreaLabels\"                     \"vecAreas\"                          \"vecPointColours\"                   \"vecPointLabels\"                   \n[103] \"vecPointShapes\"                    \"vecPointsRCSC5\"                    \"warningsToErrors\"                  \"xLab\"                              \"yLab\"                             \n[1] \" \"\n[1] \"And here is the information about them\"\n[1] \" \"\n# A tibble: 107 × 5\n# Rowwise: \n    var                               classes                 atomic length values                                                                                                                                                              \n    <chr>                             <chr>                   <lgl>  <chr>  <chr>                                                                                                                                                               \n  1 dfPolygonNoRelChangeDetHighToHigh data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n  2 dfPolygonNoRelChangeDetLowToHigh  data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n  3 dfPolygonNoRelChangeDetLowToLow   data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n  4 dfPolygonNoRelChangeImpHighToHigh data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n  5 dfPolygonNoRelChangeImpHighToLow  data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n  6 dfPolygonNoRelChangeImpLowToLow   data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n  7 dfPolygonRelDetHighToHigh         data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n  8 dfPolygonRelDetLowToHigh          data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n  9 dfPolygonRelDetLowToLow           data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n 10 dfPolygonRelImpHighToHigh         data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n 11 dfPolygonRelImpHighToLow          data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n 12 dfPolygonRelImpLowToLow           data.frame              FALSE  NA     \"NA\"                                                                                                                                                                \n 13 collapseVec                       function                FALSE  NA     \"NA\"                                                                                                                                                                \n 14 descVars                          function                FALSE  NA     \"NA\"                                                                                                                                                                \n 15 getClasses                        function                FALSE  NA     \"NA\"                                                                                                                                                                \n 16 tmpPlot                           gg, ggplot              FALSE  NA     \"NA\"                                                                                                                                                                \n 17 listArgs                          list                    FALSE  NA     \"NA\"                                                                                                                                                                \n 18 tibAreas                          tbl_df, tbl, data.frame FALSE  NA     \"NA\"                                                                                                                                                                \n 19 tibAreas1                         tbl_df, tbl, data.frame FALSE  NA     \"NA\"                                                                                                                                                                \n 20 tmpTib                            tbl_df, tbl, data.frame FALSE  NA     \"NA\"                                                                                                                                                                \n 21 tmpTibMissing                     tbl_df, tbl, data.frame FALSE  NA     \"NA\"                                                                                                                                                                \n 22 oldTheme                          theme, gg               FALSE  NA     \"NA\"                                                                                                                                                                \n 23 themeToUse                        theme, gg               FALSE  NA     \"NA\"                                                                                                                                                                \n 24 borderColour                      NULL                    TRUE   0      \"\"                                                                                                                                                                  \n 25 plotColour                        NULL                    TRUE   0      \"\"                                                                                                                                                                  \n 26 pointShapeNoRelChange             NULL                    TRUE   0      \"\"                                                                                                                                                                  \n 27 pointShapeRelDet                  NULL                    TRUE   0      \"\"                                                                                                                                                                  \n 28 pointShapeRelImp                  NULL                    TRUE   0      \"\"                                                                                                                                                                  \n 29 areaColourScheme                  character               TRUE   1      \"ALLWHITE\"                                                                                                                                                          \n 30 coloursCSC4                       character               TRUE   4      \"green, red, orange, skyblue1\"                                                                                                                                      \n 31 coloursDirection                  character               TRUE   3      \"red, green, black\"                                                                                                                                                 \n 32 coloursRCI3                       character               TRUE   3      \"grey, red, green\"                                                                                                                                                  \n 33 coloursRCSC10                     character               TRUE   10     \"yellow, blue, purple, grey, orange, red, violetred4, palegreen, green, chartreuse4\"                                                                                \n 34 coloursRCSC5                      character               TRUE   5      \"grey, red, skyblue1, green, chartreuse4\"                                                                                                                           \n 35 coloursRCSC7                      character               TRUE   7      \"grey, orange, red, sienna2, palegreen, green, skyblue1\"                                                                                                            \n 36 labelsCSC4                        character               TRUE   4      \"High to low, Low to high, Stayed high, Stayed low\"                                                                                                                 \n 37 labelsDirection                   character               TRUE   3      \"Deterioration, Improvement, No change\"                                                                                                                             \n 38 labelsRCI3                        character               TRUE   3      \"No reliable change, Reliable deterioration, Reliable improvement\"                                                                                                  \n 39 labelsRCSC10                      character               TRUE   10     \"No reliable change, stayed high, No reliable change, high to low, No reliable change, low to high, No reliable change, stayed low, Reliable deterioration, stayed …\n 40 labelsRCSC10pos                   character               TRUE   10     \"No reliable change, stayed low, No reliable change, low to high, No reliable change, high to low, No reliable change, stayed high, Reliable deterioration, stayed …\n 41 labelsRCSC5                       character               TRUE   5      \"No reliable change, Reliable deterioration, Reliable improvement, stayed high, Reliable improvement, high to low ('Recovered'), Reliable improvement, stayed low\"  \n 42 labelsRCSC5pos                    character               TRUE   5      \"No reliable change, Reliable deterioration, Reliable improvement, stayed high, Reliable improvement, low to high ('Recovered'), Reliable improvement, stayed low\"  \n 43 labelsRCSC7                       character               TRUE   7      \"No reliable change, Reliable deterioration, stayed high, Reliable deterioration, low to high, Reliable deterioration, stayed low, Reliable improvement, stayed hig…\n 44 labelsRCSC7pos                    character               TRUE   7      \"No reliable change, Reliable deterioration, stayed low, Reliable deterioration, high to low, Reliable deterioration, stayed high, Reliable improvement, stayed low…\n 45 legendNameAreas                   character               TRUE   1      \"Area mapping\"                                                                                                                                                      \n 46 legendNamePoints                  character               TRUE   1      \"Point mapping\"                                                                                                                                                     \n 47 pointColourScheme                 character               TRUE   1      \"RCSC5\"                                                                                                                                                             \n 48 pointNames                        character               TRUE   185    \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,…\n 49 pointShapeScheme                  character               TRUE   1      \"NONE\"                                                                                                                                                              \n 50 subtitle                          character               TRUE   185    \"Showing 185 with first and last scores from 185 rows in the dataset;\\n169 in help-seeking score range, 185 used., Showing 185 with first and last scores from 185 …\n 51 title                             character               TRUE   1      \"Jacobson plot\"                                                                                                                                                     \n 52 tmpVecRCSC10                      character               TRUE   12     \"RelDetHSToHS, NoRelChangeHSToHS, NoRelChangeHSToHS, RelImpHSToHS, RelImpHSToNHS, RelImpNHSToNHS, NoRelChangeNHSToNHS, NoRelChangeNHSToNHS, RelDetNHSToNHS, RelDetN…\n 53 tmpVecRCSC5                       character               TRUE   12     \"RelDet, NoRelChange, NoRelChange, RelImpHighToHigh, RelImpHighToLow, RelImpLowToLow, NoRelChange, NoRelChange, RelDet, RelDet, NoRelChange, NoRelChange\"           \n 54 tmpVecRCSC7                       character               TRUE   12     \"RelDetHSToHS, NoRelChange, NoRelChange, RelImpHSToHS, RelImpHSToNHS, RelImpNHSToNHS, NoRelChange, NoRelChange, RelDetNHSToNHS, RelDetNHSToHS, NoRelChange, NoRelCh…\n 55 vec12colours                      character               TRUE   12     \"white, white, white, white, white, white, white, white, white, white, white, white\"                                                                                \n 56 vecAreaColours                    character               TRUE   12     \"white, white, white, white, white, white, white, white, white, white, white, white\"                                                                                \n 57 vecAreaLabels                     character               TRUE   12     \", , , , , , , , , , , \"                                                                                                                                            \n 58 vecAreas                          character               TRUE   12     \"RelDetHighToHigh, NoRelChangeDetHighToHigh, NoRelChangeImpHighToHigh, RelImpHighToHigh, RelImpHighToLow, RelImpLowToLow, NoRelChangeImpLowToLow, NoRelChangeDetLow…\n 59 vecPointColours                   character               TRUE   5      \"grey, red, skyblue1, green, chartreuse4\"                                                                                                                           \n 60 vecPointLabels                    character               TRUE   5      \"No reliable change, Reliable deterioration, Reliable improvement, stayed high, Reliable improvement, high to low ('Recovered'), Reliable improvement, stayed low\"  \n 61 vecPointsRCSC5                    character               TRUE   5      \"NoRelChange, RelDet, RelImpHSToHS, RelImpHSToNHS, RelImpNHSToNHS\"                                                                                                  \n 62 xLab                              character               TRUE   1      \"Baseline score\"                                                                                                                                                    \n 63 yLab                              character               TRUE   1      \"Last score\"                                                                                                                                                        \n 64 Complete                          integer                 TRUE   1      \"185\"                                                                                                                                                               \n 65 Incomplete                        integer                 TRUE   1      \"0\"                                                                                                                                                                 \n 66 nAreaColours                      integer                 TRUE   1      \"1\"                                                                                                                                                                 \n 67 nHelpSeeking                      integer                 TRUE   185    \"169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 169, 16…\n 68 nPointColours                     integer                 TRUE   1      \"5\"                                                                                                                                                                 \n 69 nPointShapes                      integer                 TRUE   1      \"1\"                                                                                                                                                                 \n 70 oldWarn                           integer                 TRUE   1      \"0\"                                                                                                                                                                 \n 71 Original_n                        integer                 TRUE   1      \"185\"                                                                                                                                                               \n 72 goodScorePos                      logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 73 greyScale                         logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 74 labelAllPolygons                  logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 75 labelCSClines                     logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 76 nameRCIDetOnly                    logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 77 namesNotPoints                    logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 78 noLegend                          logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 79 noMessages                        logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 80 noWarnings                        logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 81 omitNHS                           logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 82 printVariables                    logical                 TRUE   1      \"TRUE\"                                                                                                                                                              \n 83 warningsToErrors                  logical                 TRUE   1      \"FALSE\"                                                                                                                                                             \n 84 areaAlpha                         numeric                 TRUE   1      \"0.3\"                                                                                                                                                               \n 85 baselineScores                    numeric                 TRUE   185    \"0.5, 0.1, 1.1, 1.44117647058824, 2.17647058823529, 1.79411764705882, 1.70588235294118, 1, 1.91176470588235, 1.38235294117647, 1.73529411764706, 2.17647058823529, …\n 86 CSC                               numeric                 TRUE   1      \"1.26\"                                                                                                                                                              \n 87 finalScores                       numeric                 TRUE   185    \"2, 0.7, 0.1, 1.38235294117647, 1.52941176470588, 1.02941176470588, 1.02941176470588, 0.882352941176471, 0.878787878787879, 0.852941176470588, 1.67647058823529, 1.…\n 88 lineColourCSC                     numeric                 TRUE   1      \"1\"                                                                                                                                                                 \n 89 lineColourNoChange                numeric                 TRUE   1      \"1\"                                                                                                                                                                 \n 90 lineColourRCI                     numeric                 TRUE   1      \"1\"                                                                                                                                                                 \n 91 lineTypeCSC                       numeric                 TRUE   1      \"1\"                                                                                                                                                                 \n 92 lineTypeNoChange                  numeric                 TRUE   1      \"1\"                                                                                                                                                                 \n 93 lineTypeRCI                       numeric                 TRUE   1      \"1\"                                                                                                                                                                 \n 94 lineWidthCSC                      numeric                 TRUE   1      \"0.5\"                                                                                                                                                               \n 95 lineWidthNoChange                 numeric                 TRUE   1      \"0.5\"                                                                                                                                                               \n 96 lineWidthRCI                      numeric                 TRUE   1      \"0.5\"                                                                                                                                                               \n 97 maxPossScore                      numeric                 TRUE   1      \"4\"                                                                                                                                                                 \n 98 minPossScore                      numeric                 TRUE   1      \"0\"                                                                                                                                                                 \n 99 namesSize                         numeric                 TRUE   1      \"2\"                                                                                                                                                                 \n100 pointAlpha                        numeric                 TRUE   1      \"1\"                                                                                                                                                                 \n101 pointShape                        numeric                 TRUE   1      \"16\"                                                                                                                                                                \n102 pointSize                         numeric                 TRUE   1      \"1\"                                                                                                                                                                 \n103 RCI                               numeric                 TRUE   1      \"0.3983001\"                                                                                                                                                         \n104 scoreRange                        numeric                 TRUE   1      \"4\"                                                                                                                                                                 \n105 titleJustificn                    numeric                 TRUE   1      \"0.5\"                                                                                                                                                               \n106 vec12alphas                       numeric                 TRUE   12     \"0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3\"                                                                                                        \n107 vecPointShapes                    numeric                 TRUE   3      \"16, 16, 16\"                                                                                                                                                        \n[1] \" \"\n[1] \"tmpTib has these columns:\"\n [1] \"pointNames\"      \"firstScore\"      \"lastScore\"       \"goodScorePos\"    \"RCIcrit\"         \"CSCcrit\"         \"firstLastChange\" \"direction3\"      \"RCIchange\"       \"firstCSCstatus\"  \"lastCSCstatus\"   \"CSC4\"            \"RCSC5\"          \n[14] \"RCSC7\"           \"RCSC10\"          \"Shapes\"          \"Points\"         \n[1] \"...and these values:\"\n# A tibble: 185 × 17\n    pointNames firstScore lastScore goodScorePos RCIcrit CSCcrit firstLastChange direction3 RCIchange   firstCSCstatus lastCSCstatus CSC4     RCSC5          RCSC7          RCSC10              Shapes  Points        \n    <chr>           <dbl>     <dbl> <lgl>          <dbl>   <dbl>           <dbl> <chr>      <chr>       <chr>          <chr>         <chr>    <chr>          <chr>          <chr>               <chr>   <chr>         \n  1 1               0.5       2     FALSE          0.398    1.26          1.5    Det        RelDet      NHS            HS            NHSToHS  RelDet         RelDetNHSToHS  RelDetNHSToHS       default RelDet        \n  2 2               0.1       0.7   FALSE          0.398    1.26          0.6    Det        RelDet      NHS            NHS           NHSToNHS RelDet         RelDetNHSToNHS RelDetNHSToNHS      default RelDet        \n  3 3               1.1       0.1   FALSE          0.398    1.26         -1      Imp        RelImp      NHS            NHS           NHSToNHS RelImpNHSToNHS RelImpNHSToNHS RelImpNHSToNHS      default RelImpNHSToNHS\n  4 4               1.44      1.38  FALSE          0.398    1.26         -0.0588 Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n  5 5               2.18      1.53  FALSE          0.398    1.26         -0.647  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n  6 6               1.79      1.03  FALSE          0.398    1.26         -0.765  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n  7 7               1.71      1.03  FALSE          0.398    1.26         -0.676  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n  8 8               1         0.882 FALSE          0.398    1.26         -0.118  Imp        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n  9 9               1.91      0.879 FALSE          0.398    1.26         -1.03   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 10 10              1.38      0.853 FALSE          0.398    1.26         -0.529  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 11 11              1.74      1.68  FALSE          0.398    1.26         -0.0588 Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 12 12              2.18      1.58  FALSE          0.398    1.26         -0.601  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 13 13              1.5       1.24  FALSE          0.398    1.26         -0.265  Imp        NoRelChange HS             NHS           HSToNHS  NoRelChange    NoRelChange    NoRelChangeHSToNHS  default NoRelChange   \n 14 14              2.18      2.21  FALSE          0.398    1.26          0.0241 Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 15 15              2.26      1.97  FALSE          0.398    1.26         -0.294  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 16 16              1.68      1.06  FALSE          0.398    1.26         -0.618  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 17 17              2.06      1.94  FALSE          0.398    1.26         -0.119  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 18 18              2.56      1.03  FALSE          0.398    1.26         -1.53   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 19 19              1.15      1.15  FALSE          0.398    1.26          0      NoChange   NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n 20 20              1.62      1.38  FALSE          0.398    1.26         -0.235  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 21 21              1.68      1.56  FALSE          0.398    1.26         -0.118  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 22 22              3.18      2.62  FALSE          0.398    1.26         -0.559  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 23 23              1.59      1.03  FALSE          0.398    1.26         -0.558  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 24 24              1.59      1.03  FALSE          0.398    1.26         -0.559  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 25 25              2.71      2.59  FALSE          0.398    1.26         -0.118  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 26 26              1.53      1.24  FALSE          0.398    1.26         -0.294  Imp        NoRelChange HS             NHS           HSToNHS  NoRelChange    NoRelChange    NoRelChangeHSToNHS  default NoRelChange   \n 27 27              1.85      1.62  FALSE          0.398    1.26         -0.235  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 28 28              2.18      1.68  FALSE          0.398    1.26         -0.5    Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 29 29              2         0.909 FALSE          0.398    1.26         -1.09   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 30 30              2.15      1.47  FALSE          0.398    1.26         -0.681  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 31 31              1.68      1.26  FALSE          0.398    1.26         -0.412  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 32 32              1.91      1.41  FALSE          0.398    1.26         -0.5    Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 33 33              1.47      1.26  FALSE          0.398    1.26         -0.206  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 34 34              1.82      2.21  FALSE          0.398    1.26          0.382  Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 35 35              1.76      2.15  FALSE          0.398    1.26          0.382  Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 36 36              1.68      1.59  FALSE          0.398    1.26         -0.0882 Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 37 37              2.32      2.24  FALSE          0.398    1.26         -0.0882 Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 38 38              1.41      1.15  FALSE          0.398    1.26         -0.265  Imp        NoRelChange HS             NHS           HSToNHS  NoRelChange    NoRelChange    NoRelChangeHSToNHS  default NoRelChange   \n 39 39              2.06      1.32  FALSE          0.398    1.26         -0.735  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 40 40              1.82      2.27  FALSE          0.398    1.26          0.449  Det        RelDet      HS             HS            HSToHS   RelDet         RelDetHSToHS   RelDetHSToHS        default RelDet        \n 41 41              1.88      1.79  FALSE          0.398    1.26         -0.0882 Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 42 42              1.29      1.47  FALSE          0.398    1.26          0.176  Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 43 43              2.06      1.06  FALSE          0.398    1.26         -1      Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 44 44              1.94      1.44  FALSE          0.398    1.26         -0.5    Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 45 45              2.26      2.12  FALSE          0.398    1.26         -0.143  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 46 46              2.12      1.12  FALSE          0.398    1.26         -1      Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 47 47              1.56      1.15  FALSE          0.398    1.26         -0.412  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 48 48              2.09      1.79  FALSE          0.398    1.26         -0.294  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 49 49              1.44      0.559 FALSE          0.398    1.26         -0.882  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 50 50              2.06      1.56  FALSE          0.398    1.26         -0.502  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 51 51              1.97      1.12  FALSE          0.398    1.26         -0.853  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 52 52              1.91      1.67  FALSE          0.398    1.26         -0.242  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 53 53              2.35      0.853 FALSE          0.398    1.26         -1.5    Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 54 54              2.24      2.24  FALSE          0.398    1.26          0      NoChange   NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 55 55              1.32      1.41  FALSE          0.398    1.26          0.0882 Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 56 56              1.68      2.52  FALSE          0.398    1.26          0.839  Det        RelDet      HS             HS            HSToHS   RelDet         RelDetHSToHS   RelDetHSToHS        default RelDet        \n 57 57              2.41      2.12  FALSE          0.398    1.26         -0.294  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 58 58              1.5       0.941 FALSE          0.398    1.26         -0.559  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 59 59              1.24      0.971 FALSE          0.398    1.26         -0.265  Imp        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n 60 60              1.59      1.09  FALSE          0.398    1.26         -0.5    Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 61 61              2         2.24  FALSE          0.398    1.26          0.235  Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 62 62              1.64      1.65  FALSE          0.398    1.26          0.0107 Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 63 63              2.06      1.55  FALSE          0.398    1.26         -0.513  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 64 64              1.82      0.882 FALSE          0.398    1.26         -0.941  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 65 65              2.18      0.853 FALSE          0.398    1.26         -1.32   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 66 66              2.15      1.03  FALSE          0.398    1.26         -1.12   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 67 67              3         1.79  FALSE          0.398    1.26         -1.21   Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 68 68              1.56      0.971 FALSE          0.398    1.26         -0.588  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 69 69              1.76      1     FALSE          0.398    1.26         -0.758  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 70 70              1.85      1.24  FALSE          0.398    1.26         -0.618  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 71 71              2.09      1.58  FALSE          0.398    1.26         -0.515  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 72 72              2.15      0.882 FALSE          0.398    1.26         -1.26   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 73 73              2         1.18  FALSE          0.398    1.26         -0.824  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 74 74              3.03      1.56  FALSE          0.398    1.26         -1.47   Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 75 75              2.38      1.94  FALSE          0.398    1.26         -0.441  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 76 76              2.47      1.91  FALSE          0.398    1.26         -0.561  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 77 77              1.85      1.48  FALSE          0.398    1.26         -0.368  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 78 78              2.12      1.15  FALSE          0.398    1.26         -0.971  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 79 79              1.82      1.27  FALSE          0.398    1.26         -0.551  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 80 80              2.47      1.71  FALSE          0.398    1.26         -0.765  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 81 81              1.61      1.82  FALSE          0.398    1.26          0.217  Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 82 82              2         1.35  FALSE          0.398    1.26         -0.647  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 83 83              1.85      1.32  FALSE          0.398    1.26         -0.529  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 84 84              1.79      1.79  FALSE          0.398    1.26          0      NoChange   NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 85 85              2.38      1.91  FALSE          0.398    1.26         -0.471  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 86 86              1.62      1.44  FALSE          0.398    1.26         -0.176  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 87 87              1.53      0.909 FALSE          0.398    1.26         -0.620  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n 88 88              2.47      1.74  FALSE          0.398    1.26         -0.735  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 89 89              2.74      2.38  FALSE          0.398    1.26         -0.353  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 90 90              2.26      2.29  FALSE          0.398    1.26          0.0294 Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 91 91              1.97      1.44  FALSE          0.398    1.26         -0.529  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 92 92              1.35      1.03  FALSE          0.398    1.26         -0.324  Imp        NoRelChange HS             NHS           HSToNHS  NoRelChange    NoRelChange    NoRelChangeHSToNHS  default NoRelChange   \n 93 93              1.85      1.82  FALSE          0.398    1.26         -0.0294 Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 94 94              1.18      1.03  FALSE          0.398    1.26         -0.147  Imp        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n 95 95              2.12      1.53  FALSE          0.398    1.26         -0.588  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 96 96              1.52      1.35  FALSE          0.398    1.26         -0.162  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n 97 97              1.79      1.38  FALSE          0.398    1.26         -0.412  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 98 98              2.24      1.53  FALSE          0.398    1.26         -0.706  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n 99 99              1.53      1.06  FALSE          0.398    1.26         -0.469  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n100 100             1.59      1.36  FALSE          0.398    1.26         -0.225  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n101 101             1.70      1.76  FALSE          0.398    1.26          0.0677 Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n102 102             2         1.85  FALSE          0.398    1.26         -0.152  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n103 103             2.09      1.06  FALSE          0.398    1.26         -1.03   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n104 104             2.03      1.76  FALSE          0.398    1.26         -0.272  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n105 105             1.62      1.15  FALSE          0.398    1.26         -0.478  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n106 106             1.47      1.21  FALSE          0.398    1.26         -0.265  Imp        NoRelChange HS             NHS           HSToNHS  NoRelChange    NoRelChange    NoRelChangeHSToNHS  default NoRelChange   \n107 107             1.79      0.971 FALSE          0.398    1.26         -0.824  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n108 108             2.5       1.35  FALSE          0.398    1.26         -1.15   Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n109 109             2.09      1.24  FALSE          0.398    1.26         -0.856  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n110 110             2.35      1.18  FALSE          0.398    1.26         -1.18   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n111 111             2.82      2.30  FALSE          0.398    1.26         -0.520  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n112 112             2.47      2.24  FALSE          0.398    1.26         -0.235  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n113 113             1.79      1.06  FALSE          0.398    1.26         -0.735  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n114 114             1.85      1.24  FALSE          0.398    1.26         -0.618  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n115 115             2.5       2.38  FALSE          0.398    1.26         -0.118  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n116 116             2.09      1.41  FALSE          0.398    1.26         -0.676  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n117 117             1.97      1.79  FALSE          0.398    1.26         -0.176  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n118 118             1.97      1.38  FALSE          0.398    1.26         -0.594  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n119 119             1.29      1.64  FALSE          0.398    1.26          0.342  Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n120 120             1.82      1.53  FALSE          0.398    1.26         -0.289  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n121 121             2.09      1.5   FALSE          0.398    1.26         -0.588  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n122 122             1.12      1.32  FALSE          0.398    1.26          0.199  Det        NoRelChange NHS            HS            NHSToHS  NoRelChange    NoRelChange    NoRelChangeNHSToHS  default NoRelChange   \n123 123             1.65      1.69  FALSE          0.398    1.26          0.0404 Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n124 124             1.21      1.12  FALSE          0.398    1.26         -0.0945 Imp        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n125 125             1.71      0.941 FALSE          0.398    1.26         -0.765  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n126 126             2.5       1.94  FALSE          0.398    1.26         -0.559  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n127 127             1.38      0.971 FALSE          0.398    1.26         -0.412  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n128 128             2.09      1.03  FALSE          0.398    1.26         -1.06   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n129 129             1.59      1.35  FALSE          0.398    1.26         -0.235  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n130 130             1.64      1.24  FALSE          0.398    1.26         -0.401  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n131 131             1.85      1.85  FALSE          0.398    1.26          0      NoChange   NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n132 132             1.79      1.65  FALSE          0.398    1.26         -0.147  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n133 133             1.97      1.85  FALSE          0.398    1.26         -0.118  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n134 134             2         1.59  FALSE          0.398    1.26         -0.412  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n135 135             1.85      1.06  FALSE          0.398    1.26         -0.794  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n136 136             3.03      2.97  FALSE          0.398    1.26         -0.0588 Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n137 137             2.79      1.29  FALSE          0.398    1.26         -1.5    Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n138 138             2.68      1.26  FALSE          0.398    1.26         -1.41   Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n139 139             2.29      1.52  FALSE          0.398    1.26         -0.779  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n140 140             2.21      1.41  FALSE          0.398    1.26         -0.800  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n141 141             2.47      1.56  FALSE          0.398    1.26         -0.912  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n142 142             1.68      1.47  FALSE          0.398    1.26         -0.206  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n143 143             1.79      1.35  FALSE          0.398    1.26         -0.441  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n144 144             2.29      2     FALSE          0.398    1.26         -0.294  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n145 145             2.97      1.18  FALSE          0.398    1.26         -1.79   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n146 146             1.97      1.21  FALSE          0.398    1.26         -0.765  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n147 147             1.85      0.971 FALSE          0.398    1.26         -0.882  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n148 148             2.88      2.09  FALSE          0.398    1.26         -0.794  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n149 149             1.74      1.32  FALSE          0.398    1.26         -0.412  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n150 150             2.47      1.97  FALSE          0.398    1.26         -0.5    Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n151 151             2.24      2.29  FALSE          0.398    1.26          0.0588 Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n152 152             2.41      2.24  FALSE          0.398    1.26         -0.169  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n153 153             2.03      1.26  FALSE          0.398    1.26         -0.765  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n154 154             1.38      0.853 FALSE          0.398    1.26         -0.522  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n155 155             1.35      0.971 FALSE          0.398    1.26         -0.382  Imp        NoRelChange HS             NHS           HSToNHS  NoRelChange    NoRelChange    NoRelChangeHSToNHS  default NoRelChange   \n156 156             1.91      1.64  FALSE          0.398    1.26         -0.275  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n157 157             2.88      2.41  FALSE          0.398    1.26         -0.471  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n158 158             1.44      1.03  FALSE          0.398    1.26         -0.412  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n159 159             2.38      0.971 FALSE          0.398    1.26         -1.40   Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n160 160             2.52      2.81  FALSE          0.398    1.26          0.297  Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n161 161             1.53      1.39  FALSE          0.398    1.26         -0.135  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n162 162             3         2.73  FALSE          0.398    1.26         -0.273  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n163 163             1.82      1.59  FALSE          0.398    1.26         -0.235  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n164 164             1.59      1.06  FALSE          0.398    1.26         -0.529  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n165 165             1.45      1.18  FALSE          0.398    1.26         -0.278  Imp        NoRelChange HS             NHS           HSToNHS  NoRelChange    NoRelChange    NoRelChangeHSToNHS  default NoRelChange   \n166 166             2.66      2.36  FALSE          0.398    1.26         -0.293  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n167 167             2.53      1.53  FALSE          0.398    1.26         -1      Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n168 168             1.09      1.03  FALSE          0.398    1.26         -0.0588 Imp        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n169 169             2.41      2.32  FALSE          0.398    1.26         -0.0882 Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n170 170             1         1     FALSE          0.398    1.26          0      NoChange   NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n171 171             1.29      1.44  FALSE          0.398    1.26          0.147  Det        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n172 172             1.03      1.09  FALSE          0.398    1.26          0.0588 Det        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n173 173             1.91      1.18  FALSE          0.398    1.26         -0.730  Imp        RelImp      HS             NHS           HSToNHS  RelImpHSToNHS  RelImpHSToNHS  RelImpHSToNHS       default RelImpHSToNHS \n174 174             0.879     1.15  FALSE          0.398    1.26          0.273  Det        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n175 175             1.03      0.941 FALSE          0.398    1.26         -0.0882 Imp        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n176 176             1.94      1.33  FALSE          0.398    1.26         -0.608  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n177 177             1.35      1.24  FALSE          0.398    1.26         -0.118  Imp        NoRelChange HS             NHS           HSToNHS  NoRelChange    NoRelChange    NoRelChangeHSToNHS  default NoRelChange   \n178 178             1.65      1.29  FALSE          0.398    1.26         -0.353  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n179 179             3.03      1.68  FALSE          0.398    1.26         -1.35   Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n180 180             1.47      1.26  FALSE          0.398    1.26         -0.206  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n181 181             1.79      1.26  FALSE          0.398    1.26         -0.529  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n182 182             1.88      1.41  FALSE          0.398    1.26         -0.471  Imp        RelImp      HS             HS            HSToHS   RelImpHSToHS   RelImpHSToHS   RelImpHSToHS        default RelImpHSToHS  \n183 183             1.85      1.74  FALSE          0.398    1.26         -0.118  Imp        NoRelChange HS             HS            HSToHS   NoRelChange    NoRelChange    NoRelChangeHSToHS   default NoRelChange   \n184 184             1.06      0.941 FALSE          0.398    1.26         -0.118  Imp        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n185 185             1         0.912 FALSE          0.398    1.26         -0.0882 Imp        NoRelChange NHS            NHS           NHSToNHS NoRelChange    NoRelChange    NoRelChangeNHSToNHS default NoRelChange   \n[1] \" \"\n[1] \"tibAreas has these columns:\"\n [1] \"iPolygon\"    \"namePolygon\" \"RCI\"         \"direction\"   \"start\"       \"end\"         \"CSC4\"        \"areaAlpha\"   \"RCSC5\"       \"RCSC7\"       \"RCSC10\"      \"x\"           \"y\"           \"xCentre\"     \"yCentre\"     \"Areas\"      \n[1] \"... and these values:\"\n# A tibble: 44 × 16\n   iPolygon namePolygon              RCI         direction start end   CSC4       areaAlpha RCSC5            RCSC7          RCSC10                  x     y xCentre yCentre Areas\n      <dbl> <ord>                    <chr>       <chr>     <chr> <chr> <chr>          <dbl> <chr>            <chr>          <chr>               <dbl> <dbl>   <dbl>   <dbl> <chr>\n 1        1 RelDetHighToHigh         RelDet      Det       High  High  StayedHigh       0.3 RelDet           RelDetHSToHS   RelDetHSToHS        1.26  1.66    2.04    3.22  \"\"   \n 2        1 RelDetHighToHigh         RelDet      Det       High  High  StayedHigh       0.3 RelDet           RelDetHSToHS   RelDetHSToHS        3.60  4       2.04    3.22  \"\"   \n 3        1 RelDetHighToHigh         RelDet      Det       High  High  StayedHigh       0.3 RelDet           RelDetHSToHS   RelDetHSToHS        1.26  4       2.04    3.22  \"\"   \n 4        2 NoRelChangeDetHighToHigh NoRelChange Det       High  High  StayedHigh       0.3 NoRelChange      NoRelChange    NoRelChangeHSToHS   1.26  1.26    2.53    2.73  \"\"   \n 5        2 NoRelChangeDetHighToHigh NoRelChange Det       High  High  StayedHigh       0.3 NoRelChange      NoRelChange    NoRelChangeHSToHS   4     4       2.53    2.73  \"\"   \n 6        2 NoRelChangeDetHighToHigh NoRelChange Det       High  High  StayedHigh       0.3 NoRelChange      NoRelChange    NoRelChangeHSToHS   3.60  4       2.53    2.73  \"\"   \n 7        2 NoRelChangeDetHighToHigh NoRelChange Det       High  High  StayedHigh       0.3 NoRelChange      NoRelChange    NoRelChangeHSToHS   1.26  1.66    2.53    2.73  \"\"   \n 8        3 NoRelChangeImpHighToHigh NoRelChange Imp       High  High  StayedHigh       0.3 NoRelChange      NoRelChange    NoRelChangeHSToHS   1.26  1.26    2.73    2.53  \"\"   \n 9        3 NoRelChangeImpHighToHigh NoRelChange Imp       High  High  StayedHigh       0.3 NoRelChange      NoRelChange    NoRelChangeHSToHS   1.66  1.26    2.73    2.53  \"\"   \n10        3 NoRelChangeImpHighToHigh NoRelChange Imp       High  High  StayedHigh       0.3 NoRelChange      NoRelChange    NoRelChangeHSToHS   4     3.60    2.73    2.53  \"\"   \n11        3 NoRelChangeImpHighToHigh NoRelChange Imp       High  High  StayedHigh       0.3 NoRelChange      NoRelChange    NoRelChangeHSToHS   4     4       2.73    2.53  \"\"   \n12        4 RelImpHighToHigh         RelImp      Imp       High  High  StayedHigh       0.3 RelImpHighToHigh RelImpHSToHS   RelImpHSToHS        1.66  1.26    3.22    2.04  \"\"   \n13        4 RelImpHighToHigh         RelImp      Imp       High  High  StayedHigh       0.3 RelImpHighToHigh RelImpHSToHS   RelImpHSToHS        4     1.26    3.22    2.04  \"\"   \n14        4 RelImpHighToHigh         RelImp      Imp       High  High  StayedHigh       0.3 RelImpHighToHigh RelImpHSToHS   RelImpHSToHS        4     3.60    3.22    2.04  \"\"   \n15        5 RelImpHighToLow          RelImp      Imp       High  Low   HighToLow        0.3 RelImpHighToLow  RelImpHSToNHS  RelImpHSToNHS       1.26  0       2.44    0.676 \"\"   \n16        5 RelImpHighToLow          RelImp      Imp       High  Low   HighToLow        0.3 RelImpHighToLow  RelImpHSToNHS  RelImpHSToNHS       1.26  0.862   2.44    0.676 \"\"   \n17        5 RelImpHighToLow          RelImp      Imp       High  Low   HighToLow        0.3 RelImpHighToLow  RelImpHSToNHS  RelImpHSToNHS       1.66  1.26    2.44    0.676 \"\"   \n18        5 RelImpHighToLow          RelImp      Imp       High  Low   HighToLow        0.3 RelImpHighToLow  RelImpHSToNHS  RelImpHSToNHS       4     1.26    2.44    0.676 \"\"   \n19        5 RelImpHighToLow          RelImp      Imp       High  Low   HighToLow        0.3 RelImpHighToLow  RelImpHSToNHS  RelImpHSToNHS       4     0       2.44    0.676 \"\"   \n20        6 RelImpLowToLow           RelImp      Imp       Low   Low   StayedLow        0.3 RelImpLowToLow   RelImpNHSToNHS RelImpNHSToNHS      0.398 0       0.973   0.287 \"\"   \n21        6 RelImpLowToLow           RelImp      Imp       Low   Low   StayedLow        0.3 RelImpLowToLow   RelImpNHSToNHS RelImpNHSToNHS      1.26  0       0.973   0.287 \"\"   \n22        6 RelImpLowToLow           RelImp      Imp       Low   Low   StayedLow        0.3 RelImpLowToLow   RelImpNHSToNHS RelImpNHSToNHS      1.26  0.862   0.973   0.287 \"\"   \n23        7 NoRelChangeImpLowToLow   NoRelChange Imp       Low   Low   StayedLow        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToNHS 0     0       0.730   0.530 \"\"   \n24        7 NoRelChangeImpLowToLow   NoRelChange Imp       Low   Low   StayedLow        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToNHS 0.398 0       0.730   0.530 \"\"   \n25        7 NoRelChangeImpLowToLow   NoRelChange Imp       Low   Low   StayedLow        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToNHS 1.26  0.862   0.730   0.530 \"\"   \n26        7 NoRelChangeImpLowToLow   NoRelChange Imp       Low   Low   StayedLow        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToNHS 1.26  1.26    0.730   0.530 \"\"   \n27        8 NoRelChangeDetLowToLow   NoRelChange Det       Low   Low   StayedLow        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToNHS 0     0       0.530   0.730 \"\"   \n28        8 NoRelChangeDetLowToLow   NoRelChange Det       Low   Low   StayedLow        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToNHS 1.26  1.26    0.530   0.730 \"\"   \n29        8 NoRelChangeDetLowToLow   NoRelChange Det       Low   Low   StayedLow        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToNHS 0.862 1.26    0.530   0.730 \"\"   \n30        8 NoRelChangeDetLowToLow   NoRelChange Det       Low   Low   StayedLow        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToNHS 0     0.398   0.530   0.730 \"\"   \n31        9 RelDetLowToLow           RelDet      Det       Low   Low   StayedLow        0.3 RelDet           RelDetNHSToNHS RelDetNHSToNHS      0     0.398   0.287   0.973 \"\"   \n32        9 RelDetLowToLow           RelDet      Det       Low   Low   StayedLow        0.3 RelDet           RelDetNHSToNHS RelDetNHSToNHS      0.862 1.26    0.287   0.973 \"\"   \n33        9 RelDetLowToLow           RelDet      Det       Low   Low   StayedLow        0.3 RelDet           RelDetNHSToNHS RelDetNHSToNHS      0     1.26    0.287   0.973 \"\"   \n34       10 RelDetLowToHigh          RelDet      Det       Low   High  LowToHigh        0.3 RelDet           RelDetNHSToHS  RelDetNHSToHS       0     1.26    0.676   2.44  \"\"   \n35       10 RelDetLowToHigh          RelDet      Det       Low   High  LowToHigh        0.3 RelDet           RelDetNHSToHS  RelDetNHSToHS       0.862 1.26    0.676   2.44  \"\"   \n36       10 RelDetLowToHigh          RelDet      Det       Low   High  LowToHigh        0.3 RelDet           RelDetNHSToHS  RelDetNHSToHS       1.26  1.66    0.676   2.44  \"\"   \n37       10 RelDetLowToHigh          RelDet      Det       Low   High  LowToHigh        0.3 RelDet           RelDetNHSToHS  RelDetNHSToHS       1.26  4       0.676   2.44  \"\"   \n38       10 RelDetLowToHigh          RelDet      Det       Low   High  LowToHigh        0.3 RelDet           RelDetNHSToHS  RelDetNHSToHS       0     4       0.676   2.44  \"\"   \n39       11 NoRelChangeDetLowToHigh  NoRelChange Det       Low   High  LowToHigh        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToHS  0.862 1.26    1.13    1.39  \"\"   \n40       11 NoRelChangeDetLowToHigh  NoRelChange Det       Low   High  LowToHigh        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToHS  1.26  1.26    1.13    1.39  \"\"   \n41       11 NoRelChangeDetLowToHigh  NoRelChange Det       Low   High  LowToHigh        0.3 NoRelChange      NoRelChange    NoRelChangeNHSToHS  1.26  1.66    1.13    1.39  \"\"   \n42       12 NoRelChangeImpHighToLow  NoRelChange Imp       High  Low   HighToLow        0.3 NoRelChange      NoRelChange    NoRelChangeHSToNHS  1.26  0.862   1.39    1.13  \"\"   \n43       12 NoRelChangeImpHighToLow  NoRelChange Imp       High  Low   HighToLow        0.3 NoRelChange      NoRelChange    NoRelChangeHSToNHS  1.26  1.26    1.39    1.13  \"\"   \n44       12 NoRelChangeImpHighToLow  NoRelChange Imp       High  Low   HighToLow        0.3 NoRelChange      NoRelChange    NoRelChangeHSToNHS  1.66  1.26    1.39    1.13  \"\"   \n\nThe plot areas\nThe plotting by area works by breaking up the plot area into the 12 areas/polygons that cover the entire RCSC plot. These are as follows, spiralling in clockwise from top to the right of the CSC (tdc: to dead centre?!)\nrelDetHighToHigh,\nnoRelChangeDetHighToHigh\nnoRelChangeImpHighToHigh\nrelImpHighToHigh\nrelImpHighToLow (“recovered”)\nrelImpLowToLow\nnoRelChangeImpLowToLow\nnoRelChangeDetLowToLow\nrelDetLowToLow\nrelDetLowToHigh … and now back into the two litle triangles in the middle\nnoRelChangeDetLowToLow\nnoRelChangeImpHighToLow\nYou can see these using the labelAllPolygons = TRUE argument to getJacobsonPlot()\n\n\nShow code\n\n### just to restart the plot area\n# ggplot(data = tibble(x = 1:10, y = 1:10), aes(x = x, y = y)) + geom_point() + geom_line()\ngetJacobsonPlot(baselineScores = tibDat$firstScore, finalScores = tibDat$lastScore, \n             pointNames = as.character(tibDat$id),\n             namesNotPoints = FALSE,\n             namesSize = 2,\n             nameRCIDetOnly = FALSE,\n             CSC = 1.26,\n             RCI = 0.3123851,\n             minPossScore = 0, \n             maxPossScore = 4, \n             goodScorePos = FALSE,\n             omitNHS = FALSE, # use to omit scores starting in non-help-seeking range\n             title = \"Jacobson plot, Quito clinic data\",\n             subtitle = NULL, #\"All completions were of full CORE-OM\",\n             xLab = \"Initial score\",\n             yLab = \"Final score\",\n             noLegend = FALSE,\n             labelCSClines = FALSE,\n             areaColourScheme = \"RCSC5\",\n             legendNameArea = \"Area mapping: RCSC5\",\n             pointColourScheme = \"RCI3\",\n             legendNamePoints = \"Point mapping: RCI3\",\n             pointSize = 1,\n             pointAlpha = 1,\n             areaAlpha = .1,\n             pointShapeScheme = \"NONE\",\n             lineTypeCSC = 1,\n             lineWidthCSC = .5,\n             lineColourCSC = 1,\n             lineTypeNoChange = 1,\n             lineWidthNoChange = .5,\n             lineColourNoChange = 1,\n             lineTypeRCI = 1,\n             lineWidthRCI = .5,\n             lineColourRCI = 1,\n             labelAllPolygons = TRUE,\n             noMessages = FALSE, # suppress messages\n             noWarnings = FALSE, # suppress my warnings (not other R ones)\n             warningsToErrors = FALSE,\n             greyScale = FALSE)\n\n\n\nImprovements/todo list\nRejected options\ndefault to return the plot, hence to print it, add option to print and return nothing ?? Can’t think why so not!\nadd option for greyscale rather than coloured plot but only for simple area/point categories or just one greyscale or best to leave greyscales for post-processing of the plot? Yes, I think that is best but put something about that in the vignette and/or Rblog post.\nThings still to do for getJacobsonPlot()\ncheck that all those mappings are correct, then recheck with goodScorePos DONE\nthen go back to the nightmare of labelling and selective labelling\nif all OK, copy the function three times:\nno CSC\nno RCI\nneither\n\nFor each of them, strip out the unnecessary bits, out pass all the arguments from plotJacobson() to each, inside each, ensure you ignore the args that become meaningless e.g. the CSC or RCI lines, can’t do the default subtitle if no CSC\nthen check they work\nif so, think what messages make sense to add\nadd messages in light of Complete about using different alpha or going to geom_count() and about labels not really working above some n\n\nhandle missing CSC and RCI: need to do this by skipping out to an utterly minimal plotting function\nhandle missing CSC or to omit the CSC analysis: need to do this by skipping out to a minimal RCI only plotting function\nhandle missing RCI or to omit the RCI analysis: do this by skipping out to a minimal CSC only plotting function\nadd to CECPfuns hence create all the tests\nFinish this and …\nfinalise the Rblog page to reflect changes\nturn it into a CECPfuns vignette\n\nadditional functions building on this one:\nperhaps add functions to add tables to the plot: RCI and/or RCSC table (into top left for goodScorePos == FALSE, bottom right if TRUE)\nfunction using geom_count()\nfunction using categories but single CSC & RCI, map by shape or override colour\nditto using facets (how to map the facets?)\ngendered function changing CSC (but not RCI?)\nfunction using named items as variables of dataframe or tibble, allow computation of RCI or fixed RCI\n\ncomplementary functions\nfunction to give missing table\nfunction to give RCI table\nfunction to give CSC table\nfunction to give RCSC5 or RCSC7 table\nadd options to those functions to crosstabulate by another variable (gender …)\n\n",
    "preview": "posts/2023-11-01-plotjacobson/plotjacobson_files/figure-html5/defaultPlot-1.png",
    "last_modified": "2023-11-13T20:59:39+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-08-09-confidence-intervals-for-quantiles/",
    "title": "Confidence intervals for quantiles",
    "description": "A general coverage of quantiles, the ECDF (Empirical Cumulative Distribution Function) and confidence intervals (CIs) around quantiles",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-09",
    "categories": [
      "confidence intervals",
      "ECDFs",
      "quantiles (quartiles, [per]centiles, deciles)"
    ],
    "contents": "\n\nContents\nIntroduction and background\nQuick introduction to quantiles (skim through if you know this)\nHistogram of samples from Gaussian distribution\nBoxplots of samples from Gaussian distribution\nECDF of samples from Gaussian distribution\n\nConfidence intervals for quantiles\nPlotting the ECDF with quantiles and their CIs\n\n\n\nIntroduction and background\nThis is one of my “background for practitioners” blog posts not one of my geeky ones. It will lead into another about two specific functions I’ve added to the CECPfuns R package. However, this one will cross-link to entries in the free online OMbook glossary.\nQuick introduction to quantiles (skim through if you know this)\nQuantiles are important and very useful but seriously underused. They are important both because they are useful to describe distributions of data but also because they can help us map from an individual’s data to and from collective data: one of the crucial themes in the mental health and therapy evidence base.\nI recommend that you read my post about ECDFs before going further if you don’t feel familiar with quantiles: the ECDF (Empirical Cumulative Density Function) is a great introduction to quantiles.\nQuick terminological point: quantiles are pretty much the same as percentiles, centiles, deciles and the median and lower and upper quartiles are specific quantiles. I’ll come back to that.\nQuantiles tell us about the location of observed values within a distribution of data. Let’s start with: the Gaussian distribution (often called, but a bit misleadingly, the “Normal”. Note the capital letter: it’s not normal in any normal sense of that word!) This facetted plot shows three simulated samples from the Gaussian distribution. The sample sizes are 100, 1,000 and 10,000.\nHistogram of samples from Gaussian distribution\n\n\nShow code\n\nset.seed(12345) # set seed to get the same data regardless of platform and occasion\n# rnorm(10000) %>%\n#   as_tibble() %>%\n#   mutate(n = 10000) %>%\n#   rename(score = value) -> tibGauss10k\n# \n# rnorm(1000) %>%\n#   as_tibble() %>%\n#   mutate(n = 1000) %>%\n#   rename(score = value) -> tibGauss1k\n# \n# rnorm(100) %>%\n#   as_tibble() %>%\n#   mutate(n = 100) %>%\n#   rename(score = value) -> tibGauss100\n# \n# bind_rows(tibGauss100,\n#           tibGauss1k,\n#           tibGauss10k) -> tibGaussAll\n\n### much more tidyverse way of doing that\nc(100, 1000, 10000) %>% # set your sample sizes\n  as_tibble() %>%\n  rename(n = value) %>%\n  ### now you are going to generate samples per value of n so rowwise()\n  rowwise() %>%\n  mutate(score = list(rnorm(n))) %>%\n  ungroup() %>% # overrided grouping by rowwise() and unnest to get individual values\n  unnest_longer(score) -> tibGauss\n\n### get sample statistics\ntibGauss %>%\n  group_by(n) %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score),\n            ### and bootstrap mean (could have used parametric derivation as this is true Gaussian but I couldn't remember it!)\n            CI = list(getBootCImean(score, verbose = FALSE))) %>%\n  unnest_wider(CI) -> tibGaussStats\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 1) +\n  geom_histogram(aes(y = after_stat(density))) +\n  ylim(c(0, .6)) +\n  ylab(\"Count\") +\n  ggtitle(\"Faceted histogram for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000\")\n\n\n\nBoxplots of samples from Gaussian distribution\nHere, taking us to quantiles, are the same data as boxplots overlaid with jittered points for the actual data. This brings us to the simplest quantiles.\n\n\nShow code\n\ntibGauss %>%\n  filter(n == 1000) -> tmpTibGauss\n\ntibGaussStats %>%\n  filter(n == 1000) -> tmpTib\n\nggplot(data = tibGauss,\n       aes(x = 1, y = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 1) +\n  geom_boxplot(notch = TRUE,\n               varwidth = TRUE,\n               fill = \"grey80\") +\n  geom_jitter(width = .35, height = 0,\n              alpha = .05,\n             colour = \"grey40\") +\n  ylab(\"Scores\") +\n  ggtitle(\"Faceted boxplot with jittered observations for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\")\n\n\n\nThe boxplot uses three quantiles describe the box: the median locates the belt and waist across the box, and the quartiles fix the lower and upper limits of the box. What are these quantiles? The median is the score (not necessarily present in the data) that would bisect the data into two equal sized halves so it’s the value such that half the observed values lie below it and half lie above it. The lower quartile is the value such that a quarter of the observed values lie below it and three quarters above it, the upper quartile is the value such that three quarters of the sample lie below it and one quarter above it.\nSo we can now start to look at these names.\nQuantile\nQuartile\nPercentile (a.k.a. centile)\n.25\nlower\n25%\n.50\nmedian\n50%\n.75\nupper\n75%\nNow we get to the ECDF which, like the histogram, violin plot and boxplot is another way to describe a distribution of observed data. This takes us into the richness of quantiles.\nECDF of samples from Gaussian distribution\n\n\nShow code\n\ntibGaussStats %>%\n  select(n, min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibGaussStatsLong\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             ncol = 1) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibGaussStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            nudge_x = -.04,\n            hjust = 1) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"Faceted ECDF plot for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nI’ve changed to faceting by rows here instead of columns to give a better spread on the plot. The ECDF plots on the y axis the proportion of the sample scoring below the value on the x axis.\nA few pretty obvious comments on the impact of sample size when in the classical model of random sampling from an infinitely large population. These impacts are visible in all those distribution plots above.\nIf the possible scores are genuinely continuous then the distributions are less “lumpy” the larger the sample. (Actually not possible to see this in the boxplot but in histogram it’s very clearly in the shift from a set of vertical bars to a smooth distribution. It’s less obvious in the violin plot as that is a smoothed distribution plot and in the ECDF it shows in the steps in the line that are almost invisible when the sample size is 10,000.)\nAs the sample sizes get bigger, if, as with the Gaussian distribution, the possible scores actually range from -Infinity to +Infinity then the limits, i.e. the minimum (quantile zero roughly) and the maximum (quantile 1.0) move out as the sample size goes up as the larger sample gets more chance of including the rare but not impossible extreme values.\nAs the sample sizes get bigger the observed quantiles get closer to their population values. That can be seen in this next table. This shows\nname = name of the quantile\nproportion = the proportion of that quantile\nthe value in the infinitely large population (know from the maths)\nn100 = the observed value for that quantile in this sample of n = 100\nn1000 = the observed value for that quantile in this sample of n = 1,000\nn10000 = the observed value for that quantile in this sample of n = 10,000\n\n\n\nShow code\n\ntibGaussStats %>% \n  select(n, lquart, median, uquart) %>%\n  pivot_longer(cols = -n) %>%\n  mutate(value = round(value, 4),\n         proportion = case_when(\n                              name == \"lquart\" ~ .25,\n                              name == \"median\" ~ .5,\n                              name == \"uquart\" ~ .75),\n         popVal = qnorm(proportion),\n         popVal = round(popVal, 4)) %>%\n  pivot_wider(names_from = n, names_prefix = \"n\", values_from = value) %>%\n  flextable() %>%\n  autofit()\n\nnameproportionpopValn100n1000n10000lquart0.25-0.6745-0.5901-0.6359-0.6652median0.500.00000.48370.0080-0.0019uquart0.750.67450.90040.64530.6576\n\nIt can be seen there that the observed values for the quantiles get closer to the population values the larger the sample.\nConfidence intervals for quantiles\nSo, as we can see in the above any observed quantile value, like any sample statistic, will have a different value for the next sample assuming any real sampling process, whether truly random (only in simulations in my view) or not. That means that, like any sample statistic, any observed quantile value can be given a confidence interval around it and this CI will be narrower the larger the sample size.\nThis brings us to the fact that there are various ways of computing this confidence interval. The R package quantileCI gives three methods including a bootstrap method. They’re all non-parametric, i.e. not making assumptions about the shape of the distribution of the scores for which you computed the quantiles. From a bit of reading led by Michael Höhle’s R package quantileCI (https://github.com/hoehleatsu/quantileCI) it seems to me that the Nyblom method is probably best (and the differences between the methods unlikely to cause us any headaches with typical MH/therapy score data). As ever the confidence interval gives a range around the observed value for a sample statistic that should include the population value in a given proportion of samples. The proportion usually used is 95%, i.e. a 95% confidence interval. Here they are for our sample data.\n\n\nShow code\n\ntibGauss %>%\n  group_by(n) %>%\n  summarise(lquartCI = list(quantileCI::quantile_confint_nyblom(score, .25)),\n            medianCI = list(quantileCI::quantile_confint_nyblom(score, .5)),\n            uquartCI = list(quantileCI::quantile_confint_nyblom(score, .75))) %>%\n  unnest_wider(lquartCI, names_sep = \":\") %>%\n  unnest_wider(medianCI, names_sep = \":\") %>%\n  unnest_wider(uquartCI, names_sep = \":\") %>%\n  rename(lquartLCL = `lquartCI:1`,\n         lquartUCL = `lquartCI:2`,\n         medianLCL = `medianCI:1`,\n         medianUCL = `medianCI:2`,\n         uquartLCL = `uquartCI:1`,\n         uquartUCL = `uquartCI:2`) %>%\n  mutate(lquartCI = paste0(round(lquartLCL, 2), \" to \", round(lquartUCL, 2)),\n         medianCI = paste0(round(medianLCL, 2), \" to \", round(medianUCL, 2)),\n         uquartCI = paste0(round(uquartLCL, 2), \" to \", round(uquartUCL, 2))) %>%\n  left_join(tibGaussStats, by = \"n\") %>%\n  mutate(lquart = round(lquart, 2),\n         median = round(median, 2),\n         uquart = round(uquart, 2)) %>%\n  select(-c(min, mean, sd, max:UCLmean)) -> tmpTib\n\ntmpTib %>%\n  select(n, lquart, lquartCI, median, medianCI, uquart, uquartCI) %>%\n  flextable() %>%\n  autofit()\n\nnlquartlquartCImedianmedianCIuquartuquartCI100-0.59-0.88 to -0.310.48-0.01 to 0.610.900.71 to 1.441,000-0.64-0.72 to -0.550.01-0.06 to 0.080.650.58 to 0.7410,000-0.67-0.69 to -0.640.00-0.03 to 0.020.660.63 to 0.69\n\nThat shows the observed values for the quartiles and the median for the three samples. It’s very clear that the widths of the intervals get tighter as the sample size increases.\nPlotting the ECDF with quantiles and their CIs\nI’ve added the function plotQuantileCIsfromDat to the CECPfuns R package package. This creates these plots below which I like. They show any requested quantiles, here the quartiles and median, with the ECDF from the data, and plots the confidence intervals for those quantiles. Here are the plots for those three quantiles and for the the three simulated samples that we’ve been using so far.\n\n\nShow code\n\ntibGauss %>% \n  filter(n == 100) %>%\n  select(score) %>% \n  pull() -> tmpVec\n\nplotQuantileCIsfromDat(tmpVec, vecQuantiles = c(.25, .5, .75), addAnnotation = FALSE, printPlot =  FALSE, returnPlot = TRUE) -> tmpPlot100\n\ntibGauss %>% \n  filter(n == 1000) %>%\n  select(score) %>% \n  pull() -> tmpVec\n\nplotQuantileCIsfromDat(tmpVec, vecQuantiles = c(.25, .5, .75), addAnnotation = FALSE, printPlot =  FALSE, returnPlot = TRUE) -> tmpPlot1000\n\ntibGauss %>% \n  filter(n == 10000) %>%\n  select(score) %>% \n  pull() -> tmpVec\n\nplotQuantileCIsfromDat(tmpVec, vecQuantiles = c(.25, .5, .75), addAnnotation = FALSE, printPlot =  FALSE, returnPlot = TRUE) -> tmpPlot10000\n\nlibrary(patchwork)\n### standardise the x axis ranges\ntmpPlot100 + \n  xlim(c(-4, 4)) -> tmpPlot100\ntmpPlot1000 + \n  xlim(c(-4, 4)) -> tmpPlot1000\ntmpPlot10000 + \n  xlim(c(-4, 4)) -> tmpPlot10000\n\ntmpPlot100 /\n  tmpPlot1000 /\n  tmpPlot10000\n\n\n\nUnsurprisingly those plots show that the three quantiles are well separated with non-overlapping confidence intervals even for n = 100 and they show how the confidence intervals tighten as the n increases.\nI hope this was a fairly clear introduction to putting confidence intervals around observed quantiles.\n\n\n\n",
    "preview": "posts/2023-08-09-confidence-intervals-for-quantiles/confidence-intervals-for-quantiles_files/figure-html5/plotGaussian1-1.png",
    "last_modified": "2023-08-25T14:29:12+02:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2023-08-11-mapping-individual-scores-to-referential-data-using-quantiles/",
    "title": "Mapping individual scores to referential data using quantiles",
    "description": "Exploring the use of quantiles and their confidence intervals, and ECDFs to map individuals' scores to referential data.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-09",
    "categories": [
      "confidence intervals",
      "ECDFs",
      "quantiles (quartiles, [per]centiles, deciles)",
      "mapping to referential data"
    ],
    "contents": "\n\nContents\nMapping from individual scores to population distributions\nMapping to non-help-seeking referential scores\nMapping to help-seeking referential scores\n\nWhat has this given us that the CSC paradigm doesn’t?\nCan we ignore sociodemographic variables?\n\n\nThis builds on two earlier posts here:\n* What is an empirical cumulative distribution function? and\n* Confidence intervals for quantiles\nEven if you are very familiar with ECDFs, quantiles and confidence intervals it may be worth at least scan reading those before reading this post.\nMapping from individual scores to population distributions\nOne huge issue in MH/therapy work is that we are very interested in individuals but we also need to be aware of aggregated data: to be able to take a population health and sometimes a health economic viewpoint on what our interventions offer in aggregate. There are no simple and perfect ways to be able to think about both individual and about aggregated data and no perfect ways to map individual data to large dataset data. 1\nOne way of mapping an individual client’s score to referential data is the famous “Clinical Significant Change” model of the late Neil Jacobson and his colleagues. That creates a cutting point score and hence a binary categorisation: whether any individual score is more likely to be in the help-seeking (“clinical”) score distribution or in the non-help-seeking distribution. There are many issues with that mapping, some of which I will explore in future posts here, however it’s rightly been hugely important as one step away from just reporting on the effectiveness of interventions solely in terms of parametric or non-parametric statistical analysis of whether the before/after changes were “statistically significant” or of reporting their effect sizes.\nHow do quantiles offer a more nuanced way of mapping individual scores to referential dataset distributions?\nIf we have a large (enough) non-help-seeking dataset of scores we can use it to give us quantiles for those scores and we can take as a refential mapping. Here’s a real example, n = 1,666.\n\n\nShow code\n\nvecQuantiles <- c(.05, .1, .15, .2, .3, .4, .5, .6, .7, .8, .9, .95)\ntmpVecScores <- na.omit(tmpScores$score_OM_T1)\nplotQuantileCIsfromDat(tmpVecScores, vecQuantiles = vecQuantiles, ci = .95, method = \"N\", type = 8, \n                       addAnnotation = FALSE, printPlot = FALSE, returnPlot = TRUE,\n                       titleText = \"ECDF of CORE-OM scores with 95% CIs around the quantiles\",\n                       subtitleText = paste0(\"Quantiles at \",\n                                             convertVectorToSentence(vecQuantiles))) -> plotCOREOM\nplotCOREOM\n\n\n\n\n\nShow code\n\nplotCOREOM + \n  theme_bw() + \n  ggtitle(\"\", subtitle = \"\") + \n  xlim(c(0, 2.2)) + \n  theme(aspect.ratio = 1) + \n  ylab(\"Prob.\") + \n  xlab(\"Score\") + \n  annotate(geom = \"text\", \n           x = 0, y = .95, \n           size = 75, \n           label = \"OMbk\", \n           colour = \"blue\", \n           hjust = 0, vjust = 1, \n           family = \"DejaVu Sans Mono\")\n\n\nWe can see that even with n = 1,666 the CIs of the quantiles are not very tight, more so for the higher quantiles.\nHere are those quantiles and their 95% CIs, and the widths of the CIs, as a table.\n\n\nShow code\n\nround3 <- function(x){\n  round(x, 3)\n}\ngetCIforQuantiles(tmpVecScores, vecQuantiles) %>%\n  select(-c(n, nOK, nMiss)) %>%\n  mutate(CIwidth = UCL - LCL) %>%\n  mutate(across(quantile:CIwidth, round3)) -> tmpTibQuantiles\n\ntmpTibQuantiles %>%\n  flextable() %>%\n  autofit()\n\nprobquantileLCLUCLCIwidth0.050.2060.1760.2350.0590.100.2650.2650.2940.0290.150.3530.3240.3820.0590.200.4120.3820.4410.0590.300.5290.5000.5590.0590.400.6470.6330.6760.0440.500.7500.7350.7940.0590.600.8820.8530.9170.0640.701.0591.0291.0880.0590.801.2351.2061.2940.0880.901.5291.4711.6180.1470.951.8531.7651.9120.147\n\nThat confirms that the CIs of the .2 and .3 quantiles (20% and 30% percentiles) touch but don’t overlap so we seem to on reasonable grounds to say that we can map any score for, say, a new client asking for help, to that referential data to a these quantiles/percentiles.\n\n\nShow code\n\n(c(0, vecQuantiles, 1)) %>% \n  as_tibble() %>%\n  rename(prob = value) %>%\n  mutate(lwr = 100 * prob,\n         upr = lead(lwr),\n         lwr = paste0(lwr, \"th\"),\n         upr = paste0(upr, \"th\"),\n         slot = paste0(\"Between the \", lwr, \" and the \", upr, \"percentiles\"),\n         slot = if_else(lwr == \"0th\", \"Under the 5th percentile\", slot),\n         slot = if_else(upr == \"100th\", \"Above the 95th percentile\", slot),\n         slotN = row_number()) %>%\n  filter(prob < 1) %>% # trim off the spurious top row created by the lead()\n  left_join(tmpTibQuantiles, by = \"prob\") %>%\n  select(slotN, slot, quantile) %>%\n  mutate(uprQuantile = lead(quantile),\n         ### fix the end points with the minimum and maximum possible scores for the measure\n         quantile = if_else(is.na(quantile), 0, quantile),\n         uprQuantile = if_else(is.na(uprQuantile), 4, uprQuantile)) %>%\n  rename(lwrQuantile = quantile) -> tibQuantileSlots\n\ntibQuantileSlots %>%\n  flextable() %>%\n  autofit()\n\nslotNslotlwrQuantileuprQuantile1Under the 5th percentile0.0000.2062Between the 5th and the 10thpercentiles0.2060.2653Between the 10th and the 15thpercentiles0.2650.3534Between the 15th and the 20thpercentiles0.3530.4125Between the 20th and the 30thpercentiles0.4120.5296Between the 30th and the 40thpercentiles0.5290.6477Between the 40th and the 50thpercentiles0.6470.7508Between the 50th and the 60thpercentiles0.7500.8829Between the 60th and the 70thpercentiles0.8821.05910Between the 70th and the 80thpercentiles1.0591.23511Between the 80th and the 90thpercentiles1.2351.52912Between the 90th and the 95thpercentiles1.5291.85313Above the 95th percentile1.8534.000\n\nMapping to non-help-seeking referential scores\nSo we can say that the size of our referential dataset has given us those 13 discriminable slots into which we can map any score. For example above 1.86 is scoring above the 95% centile from this non-help-seeking referential dataset, a score between 1.53 and 1.85 is scoring above the 90% percentile but not above the 95% percentile. We can do the same for the last scores for the clients and say that someone whose score fell from 1.86 to .7 has moved from above the 95% percentile to between the 40th and 50th.\nMapping to help-seeking referential scores\nIf we wanted to, and had a large referential dataset of initial scores for help-seeking clients we could do the same to map scores to that dataset to get an idea where a client stands in that distribution of initial scores: are they at the upper end (severely affected in the terms of the measure) or low. That enables us to say where a client’s first score lay in that, so a score of 1.9 is above the 95% percentile from this non-help-seeking dataset but might be below the 50% percentile from the help-seeking dataset. Ideally we need quite large datasets of initial scores from services to build that referential data. UK IAPT initial scores?!\nWhat has this given us that the CSC paradigm doesn’t?\nThis is a rather different approach from the CSC: rather than dichotomising the score distribution it allows us to translate any score into a quantile on non-help-seeking referential data, if we have that, and to a quantile on referential help-seeking data.\nLike the CSC this is a translation that is independent, in principle, of the measure used.\nIt allows us to say much more than “above/below the CSC”.\nAssuming we have both non-help-seeking and help-seeking referential data and that we have repeated measures across an intervention we can map change\nCan we ignore sociodemographic variables?\nOne thing to watch, as with any consideration of MH measure scores, is whether sociodemographic variables have sufficient impact on score distributions that we should consider those variables when creating mappings, the class (at this point in our history of dataset creation), is considering gender as a binary variable.\nLet’s go back to plots to explore this.\n\n\nShow code\n\ntmpScores %>%\n  rename(COREOMscore = score_OM_T1,\n         Gender = gender) %>%\n  filter(!is.na(Gender) & !is.na(COREOMscore)) -> tmpScores2\n\ntmpScores2 %>%\n  summarise(median = median(COREOMscore)) %>%\n  pull() -> tmpMedian\n\nggplot(data = tmpScores2,\n       aes(y = COREOMscore, x = Gender, fill = Gender)) +\n  geom_boxplot(notch = TRUE,\n               varwidth = TRUE) +\n  geom_hline(yintercept = tmpMedian) +\n  ylim(c(0, 4)) +\n  ylab(\"CORE-OM scores\")\n\n\nShow code\n\ntmpScores2 %>%\n  filter(Gender == \"women\") %>%\n  select(COREOMscore) %>%\n  pull() -> tmpVecScoresF\n\ntmpScores2 %>%\n  filter(Gender == \"men\") %>%\n  select(COREOMscore) %>%\n  pull() -> tmpVecScoresM\n\n\nSo let’s apply the highly inappropriate between groups t-test to test that strong graphic evidence of a gender effect on the CORE-OM scores.\n\n\nShow code\n\nt.test(tmpVecScoresF, tmpVecScoresM)\n\n\n    Welch Two Sample t-test\n\ndata:  tmpVecScoresF and tmpVecScoresM\nt = 3.9475, df = 1517.5, p-value = 8.258e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.04939864 0.14697956\nsample estimates:\nmean of x mean of y \n0.8928289 0.7946398 \n\nA more appropriate non-parametric Mann-Whitney (a.k.a. Wilcoxon) test.\n\n\nShow code\n\nwilcox.test(tmpVecScoresF, tmpVecScoresM)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  tmpVecScoresF and tmpVecScoresM\nW = 373569, p-value = 4.675e-05\nalternative hypothesis: true location shift is not equal to 0\n\nAnd now go back to the means but instead of using the t-test with its assumption that the population distributions are Gaussian, let’s use the robust, non-parametric bootstrap CIs around the observed means.\n\n\nShow code\n\ntmpScores2 %>%\n  mutate(Gender = \"all\") -> tmpTib\n\n\nbind_rows(tmpScores2,\n          tmpTib) -> tmpTibGenderAll\n\nset.seed(12345)\ntmpTibGenderAll %>%\n  group_by(Gender) %>%\n  summarise(CI = list(getBootCImean(COREOMscore))) %>%\n  unnest_wider(CI) %>%\n  mutate(Gender = ordered(Gender,\n                          levels = c(\"men\", \"all\", \"women\"))) %>%\n  arrange(Gender) -> tmpTibMeanByGender\n\ntmpTibMeanByGender %>%\n  mutate(across(obsmean:UCLmean, round3)) %>%\n  flextable() %>%\n  autofit()\n\nGenderobsmeanLCLmeanUCLmeanmen0.7950.7620.832all0.8530.8280.877women0.8930.8600.925\n\nOK, that’s clear again. (With this sample size it would be very bizarre if it weren’t just confirming the tests and giving us confidence intervals: indicators of the likely imprecision from the sample size.)\nGoing to plots as I always like to, here’s a forest type plot of the means and 95% CIs. Suggests a very impressive gender effect.\n\n\nShow code\n\ntmpTibMeanByGender %>%\n  filter(Gender == \"all\") %>%\n  select(obsmean) %>%\n  pull() -> tmpMeanAll\n\nggplot(data = tmpTibMeanByGender,\n       aes(x = Gender, y = obsmean)) +\n  geom_point() +\n  geom_linerange(aes(ymin = LCLmean, ymax = UCLmean)) +\n  geom_hline(yintercept = tmpMeanAll)\n\n\nShow code\n\n  # ylim(c(0, 4)) +\n  ylab(\"CORE-OM score\")\n\n$y\n[1] \"CORE-OM score\"\n\nattr(,\"class\")\n[1] \"labels\"\n\nShow code\n\n# getBootCIgrpMeanDiff(COREOMscore ~ Gender, tmpScores2)\n\n\nOf course when we put it in the context of the full range of possible CORE-OM scores and jitter the actual scores underneath the means and their CIs it’s less impressive.\n\n\nShow code\n\nggplot(data = tmpTibGenderAll,\n       aes(x = Gender, y = COREOMscore, colour = Gender)) +\n  geom_jitter(width = .25,\n              alpha = .1) +\n  geom_point(data = tmpTibMeanByGender,\n             aes(x = Gender, y = obsmean),\n             size = 2) +\n  geom_linerange(data = tmpTibMeanByGender,\n                 inherit.aes = FALSE,\n                 aes(x = Gender, ymin = LCLmean, ymax = UCLmean, colour = Gender),\n                 linewidth = 1.5) +\n  geom_hline(yintercept = tmpMeanAll) +\n  \n  ylim(c(0, 4)) +\n  ylab(\"CORE-OM score\")\n\n\n\nOK, so we have a definite systematic effect of gender on the central location of the scores and it is incredibly unlikely that it arose by sampling vagaries (assuming random sample, i.e. no biasing effects of gender in the recruitment). However, that’s all about the central location of the scores by gender whether using the median or the mean. Now we come to how the ECDF helps tell us more than these simple central location analyses.\n\n\nShow code\n\nvecGenderColours <- c(\"men\" = \"blue\", \"women\" = \"red\")\nggplot(data = tmpScores2,\n       aes(x = COREOMscore, colour = Gender)) +\n  stat_ecdf() +\n  ylab(\"Probability\") +\n  xlab(\"CORE-OM score\") +\n  scale_color_manual(values = vecGenderColours) +\n  ggtitle(\"Cumulative distribution function for CORE-OM scores by gender\")\n\n\n\nI always have to remind myself that the fact that ECDF line for the women is under that for the men is because the women are tending to score higher generally than the men so the quantiles for the women tend to be higher (to the right of) those for the men. A non-parametric test with the glorious name of the Kolmogorov-Smirnov test is a formal test of whether the largest absolute vertical distance between the lines is larger than you would expect to have happened had Gender had no relationship with score in the population and this just a chance sampling vagary.\n\n\nShow code\n\nks.test(tmpVecScoresF, tmpVecScoresM)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  tmpVecScoresF and tmpVecScoresM\nD = 0.11024, p-value = 0.0001138\nalternative hypothesis: two-sided\n\nGiven that we’ve seen the effects of gender on central location we’re not surprised to see that this is highly statistically significant.\nNow we can finally come to the question of whether this matters in terms of mapping scores to quantiles now taking gender into account.\n\n\nShow code\n\ngetCIforQuantiles(tmpVecScoresF, vecQuantiles = vecQuantiles) %>%\n  mutate(Gender = \"F\") -> tmpQuantilesF\n\ngetCIforQuantiles(tmpVecScoresM, vecQuantiles = vecQuantiles) %>%\n  mutate(Gender = \"M\") -> tmpQuantilesM\n\nplotQuantileCIsfromDat(tmpVecScoresF, vecQuantiles = vecQuantiles, addAnnotation = FALSE, printPlot = FALSE, returnPlot = TRUE) -> tmpPlotF\nplotQuantileCIsfromDat(tmpVecScoresM, vecQuantiles = vecQuantiles, addAnnotation = FALSE, printPlot = FALSE, returnPlot = TRUE) -> tmpPlotM\n\ntmpPlotF / tmpPlotM\n\n\nShow code\n\ntmpPlotM +\n  geom_text(aes(x = quantile, y = prob, label = prob),\n            nudge_y = .05,\n            angle = 85,\n            vjust = 0) +\n  geom_linerange(data = tmpQuantilesF,\n             aes(x = quantile, ymin = 0, ymax = prob),\n             colour = \"red\") +\n  stat_ecdf(data = filter(tmpScores2, Gender == \"women\"),\n            aes(x = COREOMscore), \n            colour = \"red\")\n\n\n\n\n\nShow code\n\nby <- join_by(between(firstScore, lwrQuantile, uprQuantile))\n\nleft_join(tibChangeScores, tibQuantileSlots, by) %>%\n  select(-ends_with(\"Quantile\")) %>%\n  rename(firstSlotN = slotN,\n         firstSlot = slot) -> tmpTib\n\nby <- join_by(between(lastScore, lwrQuantile, uprQuantile))\n\nleft_join(tmpTib, tibQuantileSlots, by) %>%\n  select(-ends_with(\"Quantile\")) %>%\n  rename(lastSlotN = slotN,\n         lastSlot = slot) -> tibQuantileChanges\n\nggplot(data = tibQuantileChanges,\n       aes(x = firstSlotN, y = lastSlotN)) +\n  geom_count() +\n  geom_abline(intercept = 0, slope = 1) +\n  ylim(c(6, 13)) +\n  scale_x_continuous(name = \"First quantile\",\n                     breaks = tibQuantileSlots$slotN,\n                     labels = tibQuantileSlots$slot,\n                     limits = c(6, 13)) +\n  scale_y_continuous(name = \"Last quantile\",\n                     breaks = tibQuantileSlots$slotN,\n                     labels = tibQuantileSlots$slot,\n                     limits = c(6, 13)) +\n  theme(axis.text.x = element_text(angle = 70,\n                                   hjust = 1),\n        aspect.ratio = 1)\n\n\n\n\nI’m avoiding the word “population” here and using “large dataset” as I think we’re pretty much never in possession of true random samples from defined populations in our work so I’m trying to step away that the whole “random-sample-from-population” model doing. The maths of the “random-sample-from-population” is the best we have when we generalise from small datasets, I’m not trying to replace that, just to stop us overvaluing what we get from the model. More on that below.↩︎\n",
    "preview": "posts/2023-08-11-mapping-individual-scores-to-referential-data-using-quantiles/mapping-individual-scores-to-referential-data-using-quantiles_files/figure-html5/realDat1-1.png",
    "last_modified": "2023-12-08T15:37:39+01:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2023-08-25-making-a-working-shiny-server/",
    "title": "Making a working shiny server",
    "description": "This is very much work in progress and will probably always be that way.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-09",
    "categories": [
      "Geeky stuff",
      "shiny",
      "R packages",
      "R tricks",
      "R programming"
    ],
    "contents": "\n\nContents\nBackground\nSo what is shiny?\nWhy am I doing this?\nCreating a server\nSome configuring the server\nBackup of the apps\nLogging the use of the server\n\nMy workflow for writing shiny apps\nMy learning curve about programming shiny apps\nStructure of the shiny project\nHow I now think about each app\nReactivity\nKey parts of shiny app code that enable reactivity\nInputs\nReactive data/objects\nOutputs\n\n\n\nLogging server use\nUsing the shiny default logging\nLogging by building that into the apps\n\n\n\n\n\nBackground\nThere is a huge amount of information about shiny on the internet. The place to start is definitely https://shiny.posit.co/. I’m not trying to replace any of the official information at posit.co (formerly Rstudio.com, much still in transition from that site to the new one), I’m just trying to document what I am learning as I struggle to get my head around shiny. This is something that I started trying several years ago and I did create a usable shiny app back then but realised that if I was going to provide something useful I needed to run my own shiny server. That I finally managed in early August 2023 and I am proving a slow learner though I suspect I am not alone in finding the shiny learning curve quite challenging. I hope this will help.\nSo what is shiny?\nWell it’s actually “just” an R package with the crucial power that it enables you to embed R inside a server so that you can provide interactive apps. See https://shiny.psyctc.org/ to see my expanding list of such apps. Clearly that means that shiny is a pretty impressive and no doubt huge lump of clever code! There are two versions: an open source one and a commercial one (or it may be various commercial versions, they’re all completely beyond my budget!) However, the open source version is true open source software: anyone can download and use it without paying any licence fees (as long as you comply with the licence, which is easy for me!)\nWhy am I doing this?\nFrom time to time over the last few weeks I’ve asked myself that question as I’ve probe depths of my stupidity failing to get things working! My simple hope is that the apps I create will be useful, in principle to anyone but particularly for practitioners and researchers working in mental health (MH) and psychological therapies. That will probably mean that the apps will fall into three groups:\nexplanatory/teaching ones illustrating methodological, mostly statistical or psychometric, issues, my first example is onemodelling screening\nones probably more useful to researchers than practitioners, perhaps requiring a bit of methodological savvy to know what they even do or mean! An early example is one to give a confidence interval around an observed coefficient alpha. (That’s a descendent of the first shiny app I ever created!)\nones that I hope will help practitioners explore their own and colleagues’ data. It’s not a good example but one computing and plotting quantiles for a distribution of, say, starting questionnaire scores is a step in that direction. These are going to be the most challenging to create so will take time to grow.\nCreating a server\nIf you only want to create shiny apps to use on your own machine you don’t need a server, you can just write your apps and run them on your machine. The easiest way is probably to do it in Rstudio. However, I can’t see any reason anyone would do that: you’d just write the app as an ordinary R program!\nSo you want to offer your apps to others, in theory you could do that by leaving your shiny apps running on one machine, it might be one that was doing other things too, as long as the machine is visible in your local network anyone on the network can use your apps there. However, what I want is for my apps to be usable for anyone on the internet. For that I needed a server sitting on a publicly visible IP address (the numeric address system that identifies machines on the internet) and it would also need a human readable address. So my server is shiny.psyctc.org currently on the IPv4 address 46.235.229.183 and the IPv6 address 2a00:1098:a6::1. This is actually a “virtual machine” running on a shared machine hosted by my excellent ISP, Mythic Beasts. I’ve been with them for 15 years now for my web servers (CORE, my non-CORE work and my personal site). They also run Jo-anne’s site and our Email. For a while in the last few years I had shiny running alongside my web servers on another VM but I worried that it is probably easy for a malevolent person to overload a shiny server and bring the machine hosting it to a halt so I gave that up in favour of the shiny server sitting on its own VM.\nFor now this server is Mythic Beasts’ “VPS 4” VMs: 2 CPU cores, 4Gb RAM and a 1Gb SSD drive. Looking at /proc/cpuinfo tells me that the cores are “Intel Xeon E312xx (Sandy Bridge)” running 2099.99 MHz (where does that number come from?!) and 16384Kb of onboard cache. It’s not super responsive but some of that may be because my broadband up in the Alps is pretty slow. I’ll watch the responsiveness as, I hope, it gets used more.\nSome configuring the server\nLike me Mythic Beasts try to keep to open source software so the server is running Debian “bullseye” currently the “oldstable” release. Mythic Beasts saved me a lot of work setting it up with R and shiny but those were from the Debian release version of R so I had to tweak the /etc/apt/sources.list.d to add a file, chris.list, saying:\ndeb http://cloud.r-project.org/bin/linux/debian bullseye-cran40/\nto get R up to 4.3.1 (now).\nTo save myself updating the packages daily I created a little bash script /home/chris/updatePackages:\n#!/bin/bash\nnowDate=$(date +%F)\nR CMD BATCH /home/chris/updatePackages.R $nowDate.Rout\nmail -s \"R update on shiny server $nowDate\" shiny@psyctc.org < $nowDate.Rout\nand this file, /home/chris/updatePackages.R\nlocal({\n  r <- getOption(\"repos\")\n  r[\"CRAN\"] <- \"https://cloud.r-project.org/\"\n  options(repos = r)\n})\n\ndate()\nupdate.packages(ask=FALSE,checkBuilt=TRUE)\nset permissions on those files to 700 (i.e. only usable in any way at all to the owner, a bit of security). Then I added this line to crontab:\n0 4 * * * /home/chris/updatePackages\nso that cron (i.e. automatic) script gets R running, updates all the R packages that have new versions and Emails me the transcript. That crontab line gets that done every day at 04.00.\nBackup of the apps\nMythic Beasts back up the entire VM daily so if, heaven forbid, the server dies or gets killed, they can always restore it to a state it was at at most 23 hours and 59 minutes (and some seconds!) earlier. My workflow creating my apps (next) ensures that there are always two mirror copies of all the apps actually before the third copy gets onto the server. This means that the only thing I’d ever lose would be the last day’s activity logs. I can’t see I’ll ever get so fascinated by the server usage that occasionally losing up to a day’s activity would bother me.\nLogging the use of the server\nHm, with Mythic Beasts I have shiny using its default capability to keep a simple log of usage. There appear to be a number of other ways to log shiny usage but for now I’m just letting the default log file get built and I’ll write a bit of R to parse it soon. Details here when I do.\nMy workflow for writing shiny apps\nOne beauty of using shiny from within Rstudio is that I have all my shiny apps as a “project” in R and use git to do what git does (if you’re sensible): to track the changes you made to the whole project every time you “commit”, i.e. tell git to take that image. It’s then pretty easy to couple that git repository to github and then, after ever commit, I can just tell git from within Rstudio to “push” the entire project/repository to github. That’s at https://github.com/cpsyctc/shiny-server meaning that anyone who wants to can see and copy any of the code should they want. (I am using the MIT open source licence, details are in the github repository). Then the final link is that I can “pull” that repository from github to the shiny server thus ensuring that by the time changes appear on the server I will already have the copy on my laptop and the copy on github. (And yes, the copy on the laptop will also get duplicated both locally and to the cloud within minutes of changes to that.)\nThe pulling of the apps from github to the server is automated with another little bash script, /home/chris/cron_pull.sh:\n#!/bin/bash\ncd /srv/shiny-server\ngit pull\nand the line:\n*/15 * * * * /home/chris/cron_pull.sh\nin crontab. That crontab line (aren’t they cryptic?!) runs the pull every fifteen minutes which is overkill really.\nMy learning curve about programming shiny apps\nI’ve put all that up there mostly just to remind myself what had to be done just to get a working shiny server and sensible workflow. Perhaps it will be useful to others. However, now we come to the crunch: writing shiny apps.\nStructure of the shiny project\nThere are at least two ways of organising apps, both involve having a directory “apps” off the root of the shiny server. The apps each have their own directory inside the apps directory and, for now at least, I am using the way of creating the apps that puts all the code into one file always called app.R (which can make it easy to lose which you have open so I put the name of the app, which is the name of the directory, as a comment at the top of all my app.R files).\nHow I now think about each app\n[I suspect this will evolve a lot. This is as of 26.viii.23!]\nThe crucial thing to understand that seemed to take me a while to really understand is that though you are using R and lots of the code is exactly as it would be in an R or even Rmarkdown file it’s probably best to keep telling yourself that because you are constructing something quite different from the linearity of a R or Rmd, you have to let go of some of your habits. The key thing for me has been to learn to “think reactivity”.\nReactivity\nKey parts of shiny app code that enable reactivity\nAt the moment it is helping me to think of my apps having three parts that aren’t in my usual R code.\nInputs: at the moment I’m working with very basic inputs, the big challenge for me is going to be how to input and use files of data\nReactive data/objects: this is fundamental. Reactive data will have been computed from inputs but it is “reactive”: it changes as any of the inputs that contributed to its construction change.\nOutputs: what it says but to me it’s still a bit confusing as it’s declared in one place, where the whole responsive page is defined, and then the specific bits of it that are constructed out of the inputs and whatever you’ve done with them.\nI am starting to think of an app as having those shiny specific bits and these “not-shiny” parts:\nThe usual loading of packages.\nDeclaring/defining of variables and other objects that aren’t reactive.\nDeclaring/defining of functions that aren’t being pulled in from packages.\nThe key thing that I’m still not adjusting to well is that I can’t make my usual assumption that I can read from the top of the file downwards to understand the order in which things happen. I am also finding it hard to adjust to the fact that the structure of any app.R is like this:\n### this is my convention of putting the name here\n### CIcorrelation\n### \n### load packages\nlibrary(shiny)\nlibrary(shinyWidgets)\n\n# Define UI for application that does the work\nui <- fluidPage(\n  setBackgroundColor(\"#ffff99\"),\n  ### this is from\n  ### https://stackoverflow.com/questions/51298177/how-to-centre-the-titlepanel-in-shiny\n  ### and centers the first title across the whole page by tweaking the css for head blocks\n  tags$head(\n    tags$style(\n      \".title {margin: auto; align: center}\"\n    )\n  ),\n  ### to me it's a bit ugly that I end up putting the title here\n  tags$div(class=\"title\", titlePanel(\"Confidence interval for a Pearson or Spearman correlation\\n\\n\")),\n  \n  # Get input values\n  sidebarLayout(\n    sidebarPanel(\n      p(\"This shiny app is one of a growing number in ...\"),\n      ### more of the text cut from here for this Rblog post\n      ###\n      ### now we get set up the interface bit of the inputs\n    numericInput(\"n\",\n                 \"Total n, (zero or positive integer)\",\n                 value = 100,\n                 min = 0,\n                 max = 10^9,\n                 width=\"100%\"),\n   ### others cut to make Rblog post shorter\n  ),\n  \n  ###\n  ### again, I am struggling to remember that the outputs, really the output placeholders go here\n  mainPanel(\n    h3(\"Your input and results\",align=\"center\"),\n    verbatimTextOutput(\"res\"),\n    ### more snipped and you often have a number of outputs, text, tables, plots ...\n  )\n)\n)\n\n\n# Define server logic required\n### this is the standard shiny server constructor\n### the input and output arguments are vital, the session argument is optional but I think always wise\nserver <- function(input, output, session) {\n  ### \n  ### start with validation functions\n  ### I dropped the ones I had because I could use numericInput() to set the ranges\n  ### but you might need functions to check relationships between inputs (say)\n  \n  ### \n  ### now the functions adapted from CECPfuns plotCIcorrelation\n  ### I think I would do this differently now\n  getCI <- function(R, n, ci = 0.95, dp = 2) {\n    z <- atanh(R)\n    norm <- qnorm((1 - ci)/2)\n    den <- sqrt(n - 3)\n    zl <- z + norm/den\n    zu <- z - norm/den\n    rl <- tanh(zl)\n    ru <- tanh(zu)\n    ci.perc <- round(100 * ci)\n    retText <- paste0(\"Given:\\n\",\n                      \"   R = \", R,\"\\n\",\n                      \"   n = \", n,\"\\n\",\n                      \"   observed correlation = \", round(R, dp),\n                      \"\\n\",\n                      \"   \", ci.perc, \"% confidence interval from \", round(rl, dp),\n                      \" to \", round(ru, dp),\"\\n\\n\")\n    return(retText)\n  }\n  \n  output$res <- renderText({\n    validate(\n      ### I have just left this in to demonstrate how validate works which seems OK\n      ### need(checkForPosInt(input$n, minInt = 0), \n      ###     \"n must be a positive integer > 10 and < 10^9\"),\n    )\n    ###\n    ### this is just passing the input variables to the function above\n    ### I could just as well have put the arguments directly into that function\n    getCI(input$R,\n          input$n,\n          input$ci,\n          input$dp)\n  })\n}\n\n### this bit is at the end of all shiny apps and puts together the two objects constructed earlier\n# Run the application (ends all shiny apps in the one file, app.R format)\nshinyApp(ui = ui, server = server)\nSo the structure is always this.\n### name\n### setup stuff\n### define the user interface (doesn't have to be fluidPage but mine are so far)\nui <- fluidPage(\n   ### general text and aesthetics\n   ### input stuff\n   ### output framework/placeholders\n)\n\nserver <- function(input, output, session) {\n   ### any validate functions to do validation not done by the input settings\n   ### functions (I think they could go outside the server definition, not sure)\n   ###    the ones that use inputs (e.g. input$n) actually build something active into the server\n   ###\n   ### define any reactive objects (none in example above, it's too simple to need them)\n   ###\n   ### named outputs like, here the \"res\" links to the quoted \"res\" in the ui\n   output$res <- renderText(\n      ### validation if there is some goes here so no output construction is attempted \n      ### if the input isn't OK.  Seems a late place to put it but that's because I'm \n      ### still thinking in a linear, top to bottom way\n      validate(need(test),\n         \"error message for failed validation\")\n   )\n}\n\n### do the business\nshinyApp(ui = ui, server = server)\nI guess I am getting to understand this and I think it’s helped me to do that sort of anatomising of a very simple app.R down sort of in two stages: first a simplified state but as it were with the muscles and main organs exposed, then stripped to just the skeleton.\nInputs\nAs I am learning, these are defined in the construction of the ui object and there are nice functions for simple inputs with very useful direct validation such as min and max for numeric input. They all have the names input$name1, input$name2 (except that you want more sensible names than “name1” and “name2”, here they are “input$n”, “input$alpha” etc.) It helps me to think of “input” essentially as a named list and it’s a reactive list in the sense that if the user changes any input, the content of that slot in “input” changes instantly.\nReactive data/objects\nYou may or may not have reactive objects. These are like any object in R but crucially they change value the moment that any of their components are changed in the input. To that extent it’s sensible to think of them more as functions than as objects and they are constructed like functions:\nreactiveSausage <- reactive({\n   ### a sausage is made by stuffing stuffing into skin\n   sausage <- makeSausage(input$stuffing, input$skin)\n   sausage\n})\nOK, I know it’s a silly example and this assumes that makeSausage() is an ordinary function that has been defined or supplied from a loaded package somewhere in app.R. Technically I don’t have to construct the sausage object there and then return it by having it as that last line I could just have had\nreactiveSausage <- reactive({\n   ### a sausage is made by stuffing stuffing into skin\n   makeSausage(input$stuffing, input$skin)\n})\nReactive objects are used like functions with no arguments so if I want to print that reactiveSausage it’s:\nprint(reactiveSausage())\n(But of course, we don’t just print() things in shiny apps. (Well, sometimes we do just for debugging to get that on the console of the machine running the app.) That brings us to outputs.\nOutputs\nThe key thing about outputs of a shiny app is that they are always the consummation of two things: the placeholder of the correct type put in the ui by something like\nverbatimTextOutput(\"res\"),\nand the output constructer in the server constructed with something like:\noutput$res <- renderText(),\nThe “$res” in output maps to the quoted “res” in verbatimTextOutput(\"res\") in the ui. I am starting to find it helpful to think of output\\$, rather as I now think of input\\$ as a sort of reactive list whose named members are accessed and used in the ui. I now think that creating outputs that work needs three things really:\ndefine the correct sort of space for the object as a named placeholder in the ui\ndefine the correct sort of output object to match that placeholder by type (e.g. text, table, datatable, plot) and by name within server\nsomewhere in app.R have the appropriate R code to construct the output used in the server output$object, might be paste() to create text, some tidverse code to create a table or some ggplot() to create a plot.\nLogging server use\nUsing the shiny default logging\nShiny comes with the option to log accesses. The configuration is stored in the very simple config file /etc/shiny-server/shiny-server.conf (on Linux):\n# Instruct Shiny Server to run applications as the user \"shiny\"\nrun_as shiny;\naccess_log /var/log/shiny-server/access.log combined;\n\n# Define a server that listens on port 3838\nserver {\n  listen 3838;\n\n  # Define a location at the base URL\n  location / {\n\n    # Host the directory of Shiny Apps stored in this directory\n    site_dir /srv/shiny-server;\n\n    # Log all Shiny output to files in this directory\n    log_dir /var/log/shiny-server;\n\n    # When a user visits the base URL rather than a particular application,\n    # an index of the applications available in this directory will be shown.\n    directory_index on;\n  }\n}\nI have pushed my log format to “combined” from “default” to try to get more information and I’ve written an R script to parse the log and that should get a bit more interesting as things go forward in which case I’ll expand this bit here or create a new post for it.\nLogging by building that into the apps\nI’m glad I’ve started parsing the simple access.log. However, it’s clear that to get more information about use you have to build things into your apps to get that and there appear to be a number of different options, probably at least some of them incompatible with each other and of course each will slow the app and server down a little and some look to be pretty thin on documentation usable for someone like me and at least some look not to have been updated for some time. (Of course, in the open source world, sometimes that’s absolutely fine: the code is simple and sound, pretty much independent of other things (e.g. any changes to shiny) and hasn’t needed any changing for ages … but too often it’s a clue that the developer doesn’t have time to look after it or that it has been orphaned.)\n\n\n\n",
    "preview": "posts/2023-08-25-making-a-working-shiny-server/shiny1.png",
    "last_modified": "2023-08-27T13:02:33+02:00",
    "input_file": {},
    "preview_width": 1235,
    "preview_height": 973
  },
  {
    "path": "posts/2023-08-07-ecdfs/",
    "title": "What is an empirical cumulative distribution function?",
    "description": "A general introduction to the ECDF and quantiles and why they're useful.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-07",
    "categories": [
      "distributions",
      "ECDFs"
    ],
    "contents": "\n\nContents\nIntroduction and background\nDistributions and quantiles\nECDF of a sample from the Gaussian distribution\nECDF of samples from Gaussian distribution\n\n\nThe ECDF is only one way plotting distributions …\nHistogram of samples from Gaussian distribution\nViolin plots of samples from Gaussian distribution\nBoxplots of samples from Gaussian distribution\n\nQuantiles, quartiles, centiles and percentiles\nSuperimposing individuals’ data on the ECDF\nNot losing site of individuals’ data when aggregating data\nTerminology: using “dataset” in preference to “sample”\n\nIntroduction and background\nThis is one of my “background for practitioners” blog posts, not one of my geeky ones. It will lead into another about two specific functions I’ve added to the CECPfuns R package. However, this one will cross-link to entries in the free online OMbook glossary. It’s also going to link to more posts here on quantiles and mapping individuals’ scores across measures. But first the ECDF and quantiles.\nThe ECDF is pretty much what it says: usually it’s a plot of the the cumulative proportion of a dataset scoring at or below a score. The proportion is plotted on the y axis and the score on the x axis. This means that the line must always start just below the minimum observed score on the x axis and go across the the maximum observed score and the line must be “monotonic positive”: always rising from left to right with no maximum points until the maximum score is reached.\nOne nice thing about the ecdf is that for any value on the y axis, the value on the x axis that maps to this is the corresponding quantile for the observed (empirical) distribution. So if we map from .5 on the y axis, the value on the x axis is the .5 quantile, also known as the median: the score value such that 50% (.5 in proportions) of the scores are below that, and 50% above it.\nQuantiles are important and very useful but seriously underused. They are important both because they are useful to describe distributions of data but also because they can help us map from an individual’s data to and from collective data: one of the crucial themes in the mental health and therapy evidence base.\nQuick terminological point: quantiles are pretty much the same as percentiles, centiles, deciles and the median and lower and upper quartiles are specific quantiles. I’ll come back to that shortly.\nDistributions and quantiles\nLet’s start with datasets not individual scores and with a hugely important theoretical distribution: the Gaussian (often called, but a bit misleadingly, the “Normal” (note the capital letter: it’s not normal in any normal sense of that word!))\nECDF of a sample from the Gaussian distribution\n\n\nShow code\n\nset.seed(12345) # set seed to get the same data regardless of platform and occasion\n# rnorm(10000) %>%\n#   as_tibble() %>%\n#   mutate(n = 10000) %>%\n#   rename(score = value) -> tibGauss10k\n# \n# rnorm(1000) %>%\n#   as_tibble() %>%\n#   mutate(n = 1000) %>%\n#   rename(score = value) -> tibGauss1k\n# \n# rnorm(100) %>%\n#   as_tibble() %>%\n#   mutate(n = 100) %>%\n#   rename(score = value) -> tibGauss100\n# \n# bind_rows(tibGauss100,\n#           tibGauss1k,\n#           tibGauss10k) -> tibGaussAll\n\n### much more tidyverse way of doing that\nc(100, 1000, 10000) %>% # set your sample sizes\n  as_tibble() %>%\n  rename(n = value) %>%\n  ### now you are going to generate samples per value of n so rowwise()\n  rowwise() %>%\n  mutate(score = list(rnorm(n))) %>%\n  ungroup() %>% # overrided grouping by rowwise() and unnest to get individual values\n  unnest_longer(score) -> tibGauss\n\n### get sample statistics\ntibGauss %>%\n  filter(n == 10000) -> tibGauss10000\n\ntibGauss10000 %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score)) -> tibGaussStats10000\n\ntibGaussStats10000 %>%\n  select(min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibGaussStatsLong10000\n\nggplot(data = tibGauss10000,\n       aes(x = score)) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong10000,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibGaussStatsLong10000,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  xlim(c(-4.4, 4.4)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot for a sample from a Gaussian distribution\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nThat shows the typical S shaped (“sigmoid”) shape of the ECDF for a sample from a Gaussian distribution. As it’s a very large (by therapy standards!) sample of n = 10,000 the curve is pretty smooth and the observed median was 0.00. I have marked that with a vertical line and the observed value as I have for some other quantiles, those for 0 (the minimum), .25 (the lower quartile), .75 (the upper quartile) and the maximum.\nDistributions have very different shapes. Here’s another large sample, this time from a “rectangular” distribution, i.e. one where all possible values are equally probable. Here’s an example for a dataset in which scores between 0 and 4 are equiprobable.\n\n\nShow code\n\nrunif(10000, 0, (34*4)) %>%\n  as_tibble() %>%\n  rename(score = value) %>%\n  mutate(score = round(score),\n         score = score / 34) -> tibEquiProb\n\ntibEquiProb %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score)) -> tibEquiProbStats\n\ntibEquiProbStats %>%\n  select(min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibEquiProbStatsLong\n\nggplot(data = tibEquiProb,\n       aes(x = score)) +\n  stat_ecdf() +\n  geom_vline(data = tibEquiProbStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibEquiProbStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  xlim(c(-0.1, 4.1)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot for a sample from rectangular distribution\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nAgain, because the sample size is large, the line is very smooth but now very straight, reflecting the equiprobable scores. The scores there are actually discrete (as of course all values in digital computers are). These were created using the model of CORE-OM scores assuming no omitted items and actually have values from 0, through 1/34 = 0.02941176, 2/34 = 0.05882353, and so on through to 4 - 2/34 = 3.941176, 4 - 1/34 = 3.970588, 4. As you can see, the discrete values are just visible as small steps on the ECDF. (With a dataset of 10,000 all 137 possible scores appeared in the data, that of course wouldn’t be true for smaller datasets and couldn’t be true for n < 137).\nECDFs can be computed for counts but then the curves, as with the discrete CORE-OM scores above, become steps as the numbers must go up as integers. I couldn’t think of a sensible example from MH interventions but this is assuming a service runs median/large groups of size 48 and has a 22% non-attendance rate at any group and shows what the number of clients absent at any one group would look like.\n\n\nShow code\n\nrbinom(10000, 48, .22) %>%\n  as_tibble() %>%\n  rename(nAbsent = value) -> tibCount\n\ntibCount %>%\n  summarise(min = min(nAbsent),\n            median = median(nAbsent),\n            mean = mean(nAbsent),\n            sd = sd(nAbsent),\n            lquart = quantile(nAbsent, .25),\n            uquart = quantile(nAbsent, .75),\n            max = max(nAbsent)) -> tibCountStats\n\ntibCountStats %>%\n  select(min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibCountStatsLong\n\nggplot(data = tibCount,\n       aes(x = nAbsent)) +\n  stat_ecdf() +\n  geom_vline(data = tibCountStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibCountStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  # xlim(c(-0.1, 4.1)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot of number of people not attending any one group (group size 48, probability absent .22)\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nYou can see how that distribution (a binomial) has the quartiles close either side of the median.\nApart from steps and lost smoothness in the ECDF caused by the scores being counts or scores on a fairly limited range of possible scores, e.g. for short questionnaires with binary or short ordinal responses, the other thing that affects the smoothness of ECDF curves is dataset size. This goes back to the samples from the Gaussian distribution but shows three different dataset sizes.\nECDF of samples from Gaussian distribution\n\n\nShow code\n\ntibGauss %>%\n  group_by(n) %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score)) -> tibGaussStats\ntibGaussStats %>%\n  select(n, min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibGaussStatsLong\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             ncol = 1) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibGaussStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            nudge_x = -.04,\n            hjust = 1) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"Faceted ECDF plot for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nI’ve facetted there (by rows) for three dataset sizes: 100, 1,000 and 10,000.\nA few pretty obvious comments on the impact of sample size when in the classical model of random sampling from an infinitely large population. These impacts are visible in all those distribution plots above.\nIf the possible scores are genuinely continuous or the number of possible scores higher than the datasete size then the distributions are less “lumpy” the larger the sample.\nAs the dataset sizes get bigger, if, as with the Gaussian distribution, the possible scores actually range from -Infinity to +Infinity then the limits, i.e. the minimum (quantile zero) and the maximum (quantile 1.0) move out as the sample size goes up as the larger sample gets more chance of including the rare but not impossible extreme values.\nAs the sample sizes get bigger the observed quantiles get closer to their population values. That can be seen in this next table. This shows\nname = name of the quantile\nproportion = the proportion of that quantile\nthe value in the infinitely large population (know from the maths)\nn100 = the observed value for that quantile in this sample of n = 100\nn1000 = the observed value for that quantile in this sample of n = 1,000\nn10000 = the observed value for that quantile in this sample of n = 10,000\n\n\n\nShow code\n\ntibGaussStats %>% \n  select(n, lquart, median, uquart) %>%\n  pivot_longer(cols = -n) %>%\n  mutate(value = round(value, 4),\n         proportion = case_when(\n                              name == \"lquart\" ~ .25,\n                              name == \"median\" ~ .5,\n                              name == \"uquart\" ~ .75),\n         popVal = qnorm(proportion),\n         popVal = round(popVal, 4)) %>%\n  pivot_wider(names_from = n, names_prefix = \"n\", values_from = value) %>%\n  flextable() %>%\n  autofit()\n\nnameproportionpopValn100n1000n10000lquart0.25-0.6745-0.5901-0.6359-0.6652median0.500.00000.48370.0080-0.0019uquart0.750.67450.90040.64530.6576\n\nIt can be seen there that the observed values for the quantiles get closer to the population values (popVal) the larger the sample.\nThe ECDF is only one way plotting distributions …\n… each has advantages and disadvantages. Let’s look at that.\nHistogram of samples from Gaussian distribution\nThe histogram is probably the plot most commonly used to show the shape of a distribution. Here it is for our Gaussian samples.\n\n\nShow code\n\ntibGauss %>%\n  group_by(n) %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score),\n            ### and bootstrap mean (could have used parametric derivation as this is true Gaussian but I couldn't remember it!)\n            CI = list(getBootCImean(score, verbose = FALSE))) %>%\n  unnest_wider(CI) -> tibGaussStats\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 3) +\n  geom_histogram(aes(y = after_stat(density))) +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  ylim(c(0, .6)) +\n  ylab(\"Count\") +\n  ggtitle(\"Faceted histogram for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000\")\n\n\n\nHence the famous name: “bell-shaped distribution” for the Gaussian distribution. One nice thing about the Gaussian distribution is that it is completely defined by two “parameters” (values in the population): the mean and standard deviation (SD). That is to say that from those two statistics (the mean and SD values observed in the sample) you can fit the distribution you believe the population has given those sample values. Like this!\n\n\nShow code\n\ntibGaussStats %>%\n  mutate(label1 = paste0(\"mean = \", round(mean, 3), \"\\n\",\n                         \"sd = \", round(sd, 3))) -> tmpTib\n\n### I thought I could use geom_function() to map the inplied population density curves to the faceted plots but geom_function() isn't facet aware so\n\ntibGaussStats %>%\n  select(n, mean, sd) %>%\n  rowwise() %>%\n  mutate(x = list(seq(-4, 4, .05))) %>%\n  ungroup() %>%\n  unnest_longer(x) %>%\n  mutate(fitted = dnorm(x, mean, sd)) -> tibFitted\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 3) +\n  geom_histogram(aes(y = after_stat(density))) +\n  geom_vline(data = tmpTib,\n             aes(xintercept = mean),\n             colour = \"red\") +\n  geom_line(data = tibFitted,\n            aes(x = x, y = fitted),\n            colour = \"blue\",\n            linewidth = 1.5) +\n  geom_text(data = tmpTib,\n            aes(label = label1),\n            x = -4,\n            y = .59,\n            hjust = 0) +\n  ylim(c(0, .6)) +\n  ylab(\"Density\") +\n  ggtitle(\"Faceted histogram for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\\nFitted (implied) population distribution in blue\")\n\n\n\nThat shows how the observed distributions get closer to the population distribution as the dataset size increases.\nViolin plots of samples from Gaussian distribution\nViolin plots are increasingly used in place of histograms. Probably some of that is “look how modern we are!” but they have the great advantage over histograms that it is easy to plot a number of distributions side by side and eyeball if they look similar or different in shape. Violin plots use a sort of smoothed histogram rotated through ninety degrees and mirrored to give a nice way of comparing different distributions, here the three different samples.\n\n\nShow code\n\ntibGauss %>%\n  mutate(x = 1) -> tmpTibGauss\n\nggplot(data = tmpTibGauss,\n       aes(x = 1, y = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 3) +\n  geom_violin(fill = \"grey80\") +\n  # geom_jitter(width = .35, height = 0,\n  #             alpha = .1,\n  #            colour = \"grey40\") +\n  geom_hline(data = tmpTib,\n             aes(yintercept = mean),\n             colour = \"red\") +\n  geom_text(data = tmpTib,\n            aes(label = label1),\n            x = .6,\n            y = 3.75,\n            hjust = 0) +\n  ylab(\"Scores\") +\n  ggtitle(\"Faceted violin plot with jittered observations for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\")\n\n\n\nAnother thing that is easier to do with violin plots than with histograms (though perfectly possible with histograms too with some trickery) is to put individual scores onto the plot as I have done here.\n\n\nShow code\n\ntibGauss %>%\n  mutate(x = 1) -> tmpTibGauss\n\nggplot(data = tmpTibGauss,\n       aes(x = 1, y = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 3) +\n  geom_violin(fill = \"grey80\") +\n  geom_jitter(width = .35, height = 0,\n              alpha = .2,\n             colour = \"grey40\") +\n  geom_hline(data = tmpTib,\n             aes(yintercept = mean),\n             colour = \"red\") +\n  geom_text(data = tmpTib,\n            aes(label = label1),\n            x = .6,\n            y = 3.75,\n            hjust = 0) +\n  ylab(\"Scores\") +\n  ggtitle(\"Faceted violin plot with jittered observations for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\")\n\n\n\nThat used vertical “jittering” of the points to spread them out vertically (as a point can’t have a value on the y axis but does have a score value on the x-axis.) More on jittering in the Rblog at handling overprinting.\nBoxplots of samples from Gaussian distribution\nOne more way of plotting a distribution: the boxplot, here again I’ve added jittered points for the individual data points. The boxplot brings us back to the simplest quantiles OK, as the box in the typical boxplot is defined by three quantiles: the median for the belt in the box, and the quartiles for the lower and upper limits of the box. The median is the score (not necessarily present in the data) that would bisect the data into two equal sized halves so it’s the value such that half the observed values lie below it and half lie above it. The lower quartile is the value such that a quarter of the observed values lie below it and three quarters above it, the upper quartile is the value such that three quarters of the sample lie below it and one quarter above it.\n\n\nShow code\n\nggplot(data = tmpTibGauss,\n       aes(x = 1, y = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 1) +\n  geom_boxplot(notch = TRUE,\n               varwidth = TRUE,\n               fill = \"grey80\") +\n  geom_jitter(width = .35, height = 0,\n              alpha = .05,\n             colour = \"grey40\") +\n  geom_hline(data = tmpTib,\n             aes(yintercept = mean),\n             colour = \"red\") +\n  geom_text(data = tmpTib,\n            aes(label = label1),\n            x = .6,\n            y = 3.75,\n            hjust = 0) +\n  ylab(\"Scores\") +\n  ggtitle(\"Faceted boxplot with jittered observations for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\")\n\n\n\nQuantiles, quartiles, centiles and percentiles\nThese are all really names for the same things but quantiles are generally mapped to probabilities (from zero to one) and percentiles to probabilities as percentages (from 0% to 100%). There are also deciles: i.e. 0%, 10%, 20% … 80%, 90% and 100%.\nQuantile\nQuartile\nPercentile (a.k.a. centile)\n.25\nlower\n25%\n.50\nmedian\n50%\n.75\nupper\n75%\nSuperimposing individuals’ data on the ECDF\nI don’t think I’ve ever seen this done but one way to put the individual data onto an ECDF is to add a “rug” to the x (score) axis. A rug adds a mark, like the threads at the edge of a rug on the floor to mark the individual points, like this.\n\n\nShow code\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             ncol = 1) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_rug(alpha = .3) +\n  geom_text(data = tibGaussStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  xlim(c(-4.4, 4.4)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot for a sample from a Gaussian distribution\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nThat’s fine for smaller n but you can see that when the n gets up to 10,000 or even 1,000 the overprinting pretty much removes the mapping to the individual data. (That’s true even adding transparency to the rug marks as I have there. See here for a bit more on using transparency with most R ggplot plots.)\nI think this next plot, which uses a bit of R to create a sort of “histogram rug” (“historug” or “histogrug”?) might be a way to handle the challenge of reminding us of the individual scores when creating ECDF plots though I think purists will say, rightly, that it’s starting to muddle interpretation of the y axis as the labels on the y axis are correct for the ECDF but meaningless for the histogrug marks. Perhaps I have to accept that I go too far trying to reveal multiple aspects of a dataset in one plot!\n\n\nShow code\n\ntibGauss %>%\n  mutate(score = round(score, 1)) %>%\n  group_by(n, score) %>%\n  summarise(count = n()) %>%\n  ungroup() %>%\n  mutate(perc = 2 * count / n) %>%\n  ungroup() -> tmpTib\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             ncol = 1) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_bar(data = tmpTib,\n           aes(x = score, y = perc),\n           stat = \"identity\") +\n  geom_text(data = tibGaussStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  xlim(c(-4.4, 4.4)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot for a sample from a Gaussian distribution\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nHowever, that brings me back to a key issue.\nNot losing site of individuals’ data when aggregating data\nOne great thing about all of these plots describing the shapes of distributions is that they move us away from oversimplifying summary statistics, typically just the mean and all of the ECDF, violin plot and boxplot can alert us that the distribution is not Gaussian and so can’t just be summarised by the mean and SD.\nThe issue of superimposing individual scores on these plots is about not just moving from simplifications to distributions’ shapes but also trying to keep individual data in mind.\nOne huge issue in MH/therapy work is that we have to be interested both individuals but also to be aware of aggregated data: to be able to take a population health and sometimes a health economic viewpoint on what our interventions offer in aggregate. However, I am sure that quantitative methods too often lose sight of individuals’ data. There are no simple and perfect ways to be able to think about both individual and about aggregated data and no perfect ways to map individual data to large dataset data.\nTerminology: using “dataset” in preference to “sample”\nI have tried to be pedantic and use the words “population” and “sample” when I’m talking about simulation in which maths and computer power make it easy to create “genuine” samples from infinitely large populations (actually to simpulate them). Otherwise I’ve used “dataset” instead of “sample” as I think that with MH/therapy data we’re pretty much never in possession of truly random samples from defined populations in our work. The “random-sample-from-population” model is a great way to help us understand aggregated data and often it’s the best we have when trying to generalise from small datasets. By using “dataset” not “sample” and by emphasizing the importance of looking at distributions and of trying to keep individual data in mind I’m not trying to overthrow group aggregate summary statistical methods, just trying to stop us overvaluing what we get from those models.\n\n\n\n",
    "preview": "posts/2023-08-07-ecdfs/ecdfs_files/figure-html5/plotECDF1-1.png",
    "last_modified": "2023-12-08T15:37:27+01:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2023-08-07-wisdom-of-years/",
    "title": "Wisdom of years!",
    "description": "I've learned a lot about data analysis from my errors, here's what I wish I'd known earlier!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-06",
    "categories": [
      "R style",
      "wise practices"
    ],
    "contents": "\n\n\n\nThis is just a little post to point to a new developing page “Wisdom1”(https://www.psyctc.org/Rblog/wisdom.html) in my little Rblog site. It’s a compilation of principles and rules to myself all of which I wish I’d learned earlier and which, I believe, save me weeks of time even though, sometimes, they can add minutes, occasionally hours and, once per project (writing DAPs & and DMPs: Data Analysis Plans and Data Management Plans) they may even take days. Very occasionally, when trying to simulate a project, they may take even longer but those, like long DAPs, may turn into papers in their own rights.\nThis will accumulate and I welcome comments and suggestions contact me, so I’ve made it a page not a post and I’m just using this to flag it up.\n\n\n\n",
    "preview": "posts/2023-08-07-wisdom-of-years/wisdom.png",
    "last_modified": "2023-08-25T14:28:55+02:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4800
  },
  {
    "path": "posts/2023-06-15-r-things-i-and-perhaps-others-forget/",
    "title": "R things I, and perhaps others, forget",
    "description": "This is a developing miscellany of the things that I seem to keep forgetting about R.  What I used to call in my student days \"teflon coated facts\"!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-06-16",
    "categories": [
      "Distill package",
      "R graphics",
      "R tricks"
    ],
    "contents": "\n\nContents\nClipping ggplot plot axes\nPutting citations and references into Rmarkdown/Distill documents\nChanging plot with in Distill\nUpdate history of this post\n\nClipping ggplot plot axes\nI always forget how to do this, I guess it’s not something I need very often, but when I do it seems very hard to find the answer by searching as the obvious words to search on seem to take me to controlling the axis itself, not these little expansions/extensions. So here’s how to do it.\nThe default is that ggplot adds a small extension to the axes. So here is the default for a silly little plot. With rather crude annotation to show what I mean.\n\n\nShow code\n\nseq(1, 4, length = 500) %>%\n  as_tibble() %>%\n  rename(x = value) %>%\n  mutate(y = x) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  annotate(geom = \"label\",\n           x = 1.3, y = 1.8,\n           label = \"Left hand x margin here\",\n           size = 8) +\n  geom_segment(x = 1.1, xend = .92,\n               y = 1.74, yend = 1,\n               arrow = arrow(angle = 30, length = unit(0.02, \"npc\"),\n                     ends = \"last\", type = \"open\")) +\n  annotate(geom = \"label\",\n           x = 1.85, y = 1.1,\n           label = \"Left hand y margin here\",\n           size = 8) +\n  geom_segment(x = 1.4, xend = 1,\n               y = 1.1, yend = .92,\n               arrow = arrow(angle = 30, length = unit(0.02, \"npc\"),\n                     ends = \"last\", type = \"open\")) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nHere I use xlim(c(1.5, 3.5)) and ylim(c(1.5, 3.5)) to shorten the axes…\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  xlim(c(1.5, 3.5)) +\n  ylim(c(1.5, 3.5)) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nOf course a lot of points have been dropped to pull the plot back to these limits. But there is still this margin on the axes. The trick is to add\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0))\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nThis is not the same as expand_limits()\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  expand_limits(x = c(1, 4)) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nBut you can use expand_limits() to create space on the plot. Here I’ve used scale_x_continuous(breaks = 1:4) to stop the x axis getting labelled up to 6.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  expand_limits(x = c(1, 6)) +\n  annotate(geom = \"label\",\n           x = 4.3, y = 3,\n           label = \"This allowed me to put\\na label in here\\nwhere I can put in\\nlots of drivel\\nand other nonsense\",\n           hjust = 0,\n           size = 10) +\n  scale_x_continuous(breaks = 1:4) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nPutting citations and references into Rmarkdown/Distill documents\nThis is something I do from time to time and I’m not claiming this is the best way to do it but it worked for me doing the post Jacobson #1 which, as is my usual, I did in Rstudio. This may not, perhaps probably won’t work if you using a different editor/environment. This also assumes you are using Zotero as your bibliographic database manager … which I do because it’s open source and excellent. You need the Better Bibtex for Zotero plugin for Zotero. That’s what the rbbt package connects to when it’s finding references.\nI installed the rbbt library. It’s not in CRAN so:\nremotes::install_github(\"paleolimbot/rbbt\")\nThat adds addins to the Rstudio addin menu and if you use the “Insert Zotero Citation” you get into the usual Zotero plugin lookup to select the reference(s) you want. The “Insert Zotero bibliography from Zotero Selection” puts the bibliography in at the end of the document so you generally want to end the document with a top level heading “References”\nThe really neat bits are that the plugin will collect just the references you cited into a bib format bibliography for you. To get that you put this\nbibliography: tmpBib.bib\nin the yaml header of the Rmarkdown document. You can call the bib file anything you like of course but it has to be in the same directory as the source file. (So for a distill blog post it goes in the directory for the post, not in the project root. Don’t worry if you don’t use distill as life’s simple then: just keep the bib file with the Rmd file.)\nTo get this to work, first make sure you have saved your Rmarkdown document since you last added a citation and then use the third (and last) addin call created by the rbbt package: “Update bibliography for current document from Zotero” which does what that says and tells you in the console tab how many references it has added.\nWorking like this allows you to use all the Rmarkdown citation tricks like ommitting the author(s) names, adding a reference so it will appear in the reference list despite not being cited in the document. See 4.5 Bibliographies and citations in bookdown.org for more on that.\nThe final neat bit is that one of these strengths is that you can set the formatting for the citations and reference list. I tend to use, despite a bit of grinding of teeth at the rigidity of it all, the APA rules for that but you can get a wide range of csl format files for different journals and rules. That is set by having this in the yaml header.\ncsl: apa.csl\nYou can get csl files from https://github.com/citation-style-language/styles.\nChanging plot with in Distill\nUpdate history of this post\n* 16.vi.23: Started this just with the first two headings for clipping ggplot axes and referencing in Rmarkdown. Updated with Distill bit 9.viii.23\n\n\n\n",
    "preview": "posts/2023-06-15-r-things-i-and-perhaps-others-forget/r-things-i-and-perhaps-others-forget_files/figure-html5/plot1-1.png",
    "last_modified": "2023-08-25T14:27:59+02:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2023-06-10-jacobson1/",
    "title": "Jacobson #1",
    "description": "The Jacobson plot and RCSC (Reliable and Clinical Change) methods for those who have never met them before or don't feel confident they understand them.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-06-10",
    "categories": [
      "RCSC paradigm",
      "R graphics",
      "Jacobson plot",
      "granularity of scores"
    ],
    "contents": "\n\nContents\nBackground\nIntroduction\nHistory\nStarting high and finishing high\nStarted high but finished low\nLow to high\nStayed low\n\nFrom CSC dichotomisation and the quadrants to add reliable change: from CSC to RCSC\nDual plot: trajectory plot and Jacobson\nTypical Jacobson summary table\nFinal Jacobson plot\n\nSummary\nNotes on the code for users of R\nUpdate history of this post\n\n\n\n\nBackground\nThis is my first post here for a long time so it’s serving a lot of purposes:\nReminding me how to use the R distill package to add things here!\nIt’s been triggered by excellent peer reviews to a paper of ours so it uses real data and may be a “supplementary” to that paper.\nMore importantly I hope it will take people through the construction of the Jacobson plot of start/finish therapy change scores showing the logic.\nThat expands on, but links to, what Jo-anne and I had about the RCSC and Jacobson plot in the OMbook and its slowly expanding online glossary. If you don’t know about the book and glossary, I recommend that you look at the pages about the book at some point as it could help go beyond the particulars here to wider issues about therapy change data.\nI am putting some cautions in about the assumptions in the RCSC model and about some of the intentions behind it, i.e. to make therapy research change data more meaningful to clinicians and some of the value of the plot to contextualise individual client change data can get lost.\nI hope that writing the code for the plots and tables will be a major step to putting a set of RCSC/Jacobson functions to generate the plots and tables into the CECPfuns R package.\nTechnicalities\nI don’t think this presentation is going to work on a mobile ’phone and you may need to play around resizing you browser window to get the best visibility for you. The other technical point is that you will see buttons saying “Show code”. If you’re not interested in R code, just ignore those; if you are interested in the R code then just clicking on those will show you the code which you are welcome to copy and amend as much as you like but please if you are publishing something that was helped by the code, then please put a link back to this post acknowledging this.\nIntroduction\nOK. Here is a simple Jacobson plot of our data from the paper.\n\n\n\nThat shows data for 182 clients from our paper. Let’s go into the construction of the plot starting without the clients’ data. The Jacobson plot creates a map with the x axis (horizontal axis) being the clients’ first assessment score and the y, the vertical, axis being the finishing score.\nHistory\nThe plot was first described in (Jacobson et al., 1984). Jacobson and colleagues had a mistake in one of the calculations that was pointed out (Christensen & Mendoza, 1986) and accepted (Jacobson et al., 1986) (more that below). Although it’s not the historically canonical reference for the plot, (Jacobson & Truax, 1991) is a nice summary of the method that is often cited for it. I’ve contributed to the literature on it with our attempt to make it easier to follow in (Evans et al., 1998) which a lot of people have told me they found helpful! There is also a shorter explanation of the plot than this one here in Chapter 5 of (Evans & Carlyle, 2021).\nThat’s the beginning of the plot in history but this blog post is about building the its beginnings as blank graph. So this is the canvas onto which we put our change scores.\n\n\nShow code\n\n### set the score limits (implicit in previous plot from the polygon vertices there)\nvalMinPoss <- 0\nvalMaxPoss <- 4\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            hjust = 1,\n            size = 6,\n            vjust = 0) +  \n  ### set limits (this way of setting the axis limits doesn't clip the plotting area)\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"The vertical and horizontal reference lines mark the CSC criterion\")\n\n\n\nThose lines are used to dichotomise the scores, starting and finishing scores into “clinical” and “non-clinical” (>= CSC and < CSC respectively). That means the area is split into four quadrants by those two lines which mark the CSC (Clinically Significant Change) criterion for the measure (here it was the CORE-OM but the principles apply to any measure of change.) There are many ways to split scores into two levels: “clinical” and “non-clinical” and many, many reasons to be cautious about such dichotomisation. Having said that, it seems that there is a huge and diverse wish to have such categories and Jacobson and his colleagues based their “RCSC” (Reliable and Clinically Significant Change) on such dichotomisation. (And they proposed three ways to determine the CSC score for any measure, one of which, their method c, has pretty overwhelming advantages on their other two and has become very widely used.) Here’s how those lines dichotomise the field.\n\n\nShow code\n\n### more polygon vertices\ndatPolyStartedHigh <- data.frame(x = c(csc, csc, valMaxPoss, valMaxPoss),\n                                 y = c(valMinPoss, valMaxPoss, valMaxPoss, valMinPoss))\ndatPolyStartedLow <- data.frame(x = c(valMinPoss, valMinPoss, csc, csc),\n                                y = c(valMinPoss, valMaxPoss, valMaxPoss, valMinPoss))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### starting scores high\n  geom_polygon(data = datPolyStartedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +\n  ### label that \n  annotate(\"label\",\n           x = csc + ((valMaxPoss - csc) / 2),\n           y = ((valMinPoss + valMaxPoss) / 2),\n           size = 6,\n           label = \"Points in here mark clients who\\n started above the CSC\") +\n  ### starting scores low\n  geom_polygon(data = datPolyStartedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +\n  ### label that \n  annotate(\"label\",\n           x = csc / 2,\n           y = ((valMinPoss + valMaxPoss) / 2),\n           size = 6,\n           label = \"Points in here mark clients who\\n started below the CSC\") +  \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  # geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Clients starting scores against the CSC\")\n\n\n\nThe same applies for the finishing scores.\n\n\nShow code\n\n### more vertices for geom_poly()\ndatPolyFinishedHigh <- data.frame(x = c(valMinPoss, valMinPoss, valMaxPoss, valMaxPoss),\n                                  y = c(csc, valMaxPoss, valMaxPoss, csc))\ndatPolyFinishedLow <- data.frame(x = c(valMinPoss, valMinPoss, valMaxPoss, valMaxPoss),\n                                 y = c(valMinPoss, csc, csc, valMinPoss))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### starting scores high\n  geom_polygon(data = datPolyFinishedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = (valMinPoss + valMaxPoss) / 2,\n           y = (csc + valMaxPoss) / 2,\n           size = 6,\n           label = \"Points in here mark clients who\\n finished above the CSC\") +\n  ### starting scores low\n  geom_polygon(data = datPolyFinishedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = (valMinPoss + valMaxPoss) / 2,\n           y = csc / 2,\n           size = 6,\n           label = \"Points in here mark clients who\\n finished below the CSC\") +  \n  ### put in CSC line\n  geom_hline(yintercept = csc) +\n  ### label that\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45),\n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +\n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Clients finishing scores against the CSC\")\n\n\n\nOf course the actual scores remain the actual scores! One thing to watch with all dichotomisation is not to lose sight of that. That will be a theme through this post. If we think of the scores as continuous this shows the starting scores as a colour gradient\n\n\nShow code\n\ndatPolyAll <- data.frame(x = c(valMinPoss, valMinPoss, valMaxPoss, valMaxPoss),\n                         y = c(valMinPoss, csc, csc, valMinPoss))\n\n### The CORE-OM has 41 possible score levels\nvalNlevels <- 41\n### rather crude way to create a full range of possible first/last score pairs\nas_tibble(data.frame(x = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), each = valNlevels),\n                     y = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), times = valNlevels))) -> tibFill\n\nggplot(tibFill,\n       aes(x = x,\n           y = y)) +\n  ### starting score gradient\n  geom_raster(aes(fill = x)) +\n  scale_fill_gradient(low = \"green\", high = \"red\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Continuous starting scores\")\n\n\n\nAnother thing to remember is that our scores aren’t truly continuous. Here are the possible scores for the CORE-10 with no prorating.\n\n\nShow code\n\nggplot(tibFill,\n       aes(x = x,\n           y = y)) +\n  geom_point(aes(colour = y),\n             size = 3) +\n  scale_colour_gradient(low = \"green\", high = \"red\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = 1.05 * valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Discrete starting scores (model of CORE-10 with no prorating)\")\n\n\n\nThis next shows the same but for the CORE-OM with the full 34 items completed, no prorating again. The granularity is clearly much greater. The issue about our scores not being truly continuous does start to be an issue to hold in mind but only when the number of possible scores gets quite low. Here are the possible scores with no pro-rating for the GAD-7 with the UK IAPT cutting score of 8.\n\n\nShow code\n\n### compute the number of possible scores on the GAD-7\nvalNlevels <- 4 * 7 + 1\n### reset limits\nvalMinPoss <- 0\nvalMaxPoss <- 21\ncsc <- 8\n### and now create the full set of possible first/last scores for the GAD-7\nas_tibble(data.frame(x = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), each = valNlevels),\n                     y = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), times = valNlevels))) -> tibFill\n\nggplot(tibFill,\n       aes(x = x,\n           y = y)) +\n  geom_point(aes(colour = y),\n             size = 3) +\n  scale_colour_gradient(low = \"green\", high = \"red\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = 1.05 * valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Discrete starting scores (model of GAD-7 with no prorating)\")\n\n\n\nEven with only seven items and four response levels we have 22 possible scores, 14 about that cutting point and eight below it.\n\n\nShow code\n\n### reset things to the CORE-OM\nvalNlevels <- 4 * 34 + 1\n### reset limits\nvalMinPoss <- 0\nvalMaxPoss <- 4\ncsc <- 1.26\n\nas_tibble(data.frame(x = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), each = valNlevels),\n                     y = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), times = valNlevels))) -> tibFill\n\nggplot(tibFill,\n       aes(x = x,\n           y = y)) +\n  geom_point(aes(colour = y),\n             size = 1) +\n  scale_colour_gradient(low = \"green\", high = \"red\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, \n            y = 1.03 * valMaxPoss, \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Discrete starting scores (model of CORE-OM with no prorating)\")\n\n\n\nSo that’s how the plot relates to the starting and finishing scores. When we look at both scores we have four quadrants. These next four blocks show each quadrant.\nStarting high and finishing high\n\n\nShow code\n\n### another polygon\ndatPolyStayedHigh <- data.frame(x = c(csc, csc, valMaxPoss, valMaxPoss),\n                                y = c(csc, valMaxPoss, valMaxPoss, csc))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### quadrants\n  geom_polygon(data = datPolyStayedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"orange\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = csc + ((valMaxPoss - csc) / 2),\n           y = csc + ((valMaxPoss - csc) / 2),\n           size = 6,\n           label = \"Points in here mark clients who\\n started above CSC and ended above CSC\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"'Stayed high' quadrant\")\n\n\n\nStarted high but finished low\n\n\nShow code\n\ndatPolyHighToLow <- data.frame(x = c(csc, csc, valMaxPoss, valMaxPoss),\n                               y = c(valMinPoss, csc, csc, valMinPoss))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### quadrants\n  geom_polygon(data = datPolyHighToLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = csc + ((valMaxPoss - csc) / 2),\n           y = (csc / 2),\n           size = 6,\n           label = \"Points in here mark clients who\\n started above CSC and ended below CSC\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"'High to low' quadrant\")\n\n\n\nLow to high\n\n\nShow code\n\ndatPolyLowToHigh <- data.frame(x = c(valMinPoss, valMinPoss, csc, csc),\n                               y = c(csc, valMaxPoss, valMaxPoss, csc))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### quadrants\n  geom_polygon(data = datPolyLowToHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = csc / 2,\n           y = csc + (valMaxPoss - csc) / 2,\n           size = 6,\n           label = \"Points in here mark clients\\n who started below CSC\\nand ended above CSC\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"'Low to high' quadrant\")\n\n\n\nStayed low\n\n\nShow code\n\ndatPolyStayedLow <- data.frame(x = c(valMinPoss, valMinPoss, csc, csc),\n                               y = c(valMinPoss, csc, csc, valMinPoss))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### quadrants\n  geom_polygon(data = datPolyStayedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"yellow\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = csc / 2,\n           y = csc / 2,\n           size = 6,\n           label = \"Points in here mark clients\\n who started below CSC\\nand ended below CSC\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Stayed low quadrant\")\n\n\n\nHere’s what that looks like for our real data.\n\n\nShow code\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### quadrants\n  geom_polygon(data = datPolyStayedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"yellow\") +\n  geom_polygon(data = datPolyStayedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"orange\") +\n  geom_polygon(data = datPolyHighToLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +  \n  geom_polygon(data = datPolyLowToHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  ### put in the points\n  geom_point(alpha = .5) +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Real data\")\n\n\n\nDichotomising the two scores using the CSC gives us those four quadrants and what one sometimes sees is the data being tabulated by those quadrants either as a first/last crosstabulation like this.\n\n\nShow code\n\ntibData %>%\n  filter(occasion == 1) %>%\n  select(id, firstScore, lastScore) %>%\n  ### categorise change\n  mutate(firstCSCcategory = if_else(firstScore >= csc, \"startHigh\", \"startLow\"),\n         lastCSCcategory = if_else(lastScore >= csc, \"endHigh\", \"endLow\"),\n         CSCchangeCategory = case_when(\n           firstCSCcategory == \"startHigh\" & lastCSCcategory == \"endHigh\" ~ \"Stayed high\",\n           firstCSCcategory == \"startHigh\" & lastCSCcategory == \"endLow\" ~ \"Clinically improved\",\n           firstCSCcategory == \"startLow\" & lastCSCcategory == \"endHigh\" ~ \"Clinically deteriorated\",\n           firstCSCcategory == \"startLow\" & lastCSCcategory == \"endLow\" ~ \"Stayed low\")) -> tmpTibCSC\n\ntmpTibCSC %>%\n  tabyl(firstCSCcategory, lastCSCcategory) %>%\n  adorn_percentages() %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns() %>%\n  flextable() %>%\n  ### flextable::flextable() uses bg() to set background colour i are rows, j are columns\n  bg(i = 1, j = 2, bg = \"orange\") %>%\n  bg(i = 1, j = 3, bg = \"green\") %>%\n  bg(i = 2, j = 2, bg = \"red\") %>%\n  bg(i = 2, j = 3, bg = \"yellow\")\n\nfirstCSCcategoryendHighendLowstartHigh66.9% (113)33.1% (56)startLow7.7%   (1)92.3% (12)\n\nOr just listing the categories.\n\n\nShow code\n\ntmpTibCSC %>%\n  mutate(CSCchangeCategory = ordered(CSCchangeCategory,\n                                     levels = c(\"Stayed high\",\n                                                \"Stayed low\",\n                                                \"Clinically improved\",\n                                                \"Clinically deteriorated\"),\n                                     labels = c(\"Stayed high\",\n                                                \"Stayed low\",\n                                                \"Clinically improved\",\n                                                \"Clinically deteriorated\"))) %>%\n  tabyl(CSCchangeCategory) %>%\n  adorn_pct_formatting(digits = 1) %>%\n  flextable() %>%\n  bg(i = 1, j = 1:3, bg = \"orange\") %>%\n  bg(i = 2, j = 1:3, bg = \"yellow\") %>%\n  bg(i = 3, j = 1:3, bg = \"green\") %>%\n  bg(i = 4, j = 1:3, bg = \"red\")\n\nCSCchangeCategorynpercentStayed high11362.1%Stayed low126.6%Clinically improved5630.8%Clinically deteriorated10.5%\n\nFrom CSC dichotomisation and the quadrants to add reliable change: from CSC to RCSC\nHowever, that clearly reduces the complexity of the scores perhaps a bit too far even for a quadrant classification based on dichotomising the first and last scores. The key thing that fails to consider is whether the changes, whatever quadrant they put the client into, are large enough that we should be interested!\nThe first step really is just add the no change line to the plot.\n\n\nShow code\n\nas_tibble(data.frame(x = seq(0, 4, length = 41),\n                     y = seq(0, 4, length = 41))) -> tibNoChange\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### quadrants\n  geom_polygon(data = datPolyStayedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"yellow\") +\n  geom_polygon(data = datPolyStayedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"orange\") +\n  geom_polygon(data = datPolyHighToLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +  \n  geom_polygon(data = datPolyLowToHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  ### put in the points\n  geom_point(alpha = .5) +\n  ### put in no change line\n  geom_line(data = tibNoChange,\n            aes(x = x, y = y)) +\n  ### label that\n  ggtext::geom_richtext(inherit.aes = FALSE,\n                       x = .9 * valMaxPoss, \n                       y = .9 * valMaxPoss,\n                       label = \"Line of no change\",\n                       angle = 45,\n                       hjust = 1,\n                       vjust = .5) +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Real data with no change line\")\n\n\nShow code\n\ntibData %>% \n  filter(firstLastChange == 0) %>% \n  select(id) %>% \n  distinct() %>%\n  nrow() -> valNnoChange\n\n\nOK so we can now see that the emerging Jacobson plot contextualises each client’s start and finish scores into the quadrants and adding the no change line clarifies which people showed no change (here there are\nfive) with the same starting and ending scores and lying on that no change line.\nHowever, just being reminded that points lying exactly on that line had exactly the same first and last scores would add little to our understanding of our data. Fortunately, there is more to the Jacobson plot. The next important aspect of the Jacobson plot addresses the question of how much change is meaningful. There are no perfect answers to this, just as there are no perfect ways to set the CSC cutting point, but the Jacobson plot uses a method called the Reliable Change Index (RCI). This was where there was the error in the original paper, leaving out the square root of two, sqrt(2) in R code, or 1.414 to three decimal places so making the criterion quite a bit easier to exceed than it is. The beauty of the method is that it allows us to add “tramlines” either side of the no change line like this.\n\n\nShow code\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### add leading diagonal of no change\n  geom_abline(slope = 1, intercept = 0) +\n  ### add RCI tramlines\n  geom_abline(slope = 1, intercept = -rci) +\n  geom_abline(slope = 1, intercept = rci) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  ### need to change or remove these if doing monochrome version\n  scale_color_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_fill_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_shape_manual(values = vecShapesRCI, name = \"Reliable Change Index\") +\n  scale_x_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_y_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1)\n\n\n\nThose tramlines are where the change was less than the RCI, here a change of less than\n0.398 and it tells us that this amount of change could very possibly have arisen simply from the fact that all our measures are imperfect: “unreliable” in psychometric jargon. Strictly the RCI says that given the unreliability of the particular measure used and the scatter of the starting scores you would expect 95% of the changes to lie within those tramlines *had nothing else been impinging” … including had therapy had no impact.\nSo now we can colour areas in terms of the level of change.\n\n\nShow code\n\ndata.frame(x = c(0, 0, valMaxPoss - rci, valMaxPoss - rci),\n           y = c(rci, valMaxPoss, valMaxPoss, valMaxPoss)) -> datRelDetVertices\ndata.frame(x = c(rci, valMaxPoss, valMaxPoss),\n           y = c(0, valMaxPoss - rci, 0)) -> datRelImpVertices\n\n### create data frame for the RCI tramlines\ndatTramlineVertices <- data.frame(x = c(0, 0, rci, 4, 4, 4 - rci),\n                           y = c(rci, 0, 0, 4 - rci, 4, 4))\n### create data frame for the recovered area of the plot\ndatRecoveredVertices <- data.frame(x =c(csc, csc, 4, 4, csc + rci),\n                            y = c(csc - rci, 0, 0, csc, csc))\nc(\"Reliable deterioration\" = 24, \n  \"No reliable change\" = 22, \n  \"Reliable improvement\" = 25) -> vecShapesRCI\n\nc(\"Reliable deterioration\" = \"black\", \n  \"No reliable change\" = \"grey70\", \n  \"Reliable improvement\" = \"grey45\") -> vecColoursRCI\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### add reliable change polygons\n  geom_polygon(inherit.aes = FALSE,\n               data = datRelDetVertices,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  geom_polygon(inherit.aes = FALSE,\n               data = datRelImpVertices,\n               aes(x = x, y = y),\n               fill = \"green\") +  \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### add leading diagonal of no change\n  geom_abline(slope = 1, intercept = 0) +\n  ### add RCI tramlines\n  geom_abline(slope = 1, intercept = -rci) +\n  geom_abline(slope = 1, intercept = rci) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  ### need to change or remove these if doing monochrome version\n  scale_color_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_fill_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_shape_manual(values = vecShapesRCI, name = \"Reliable Change Index\") +\n  scale_x_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_y_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1)\n\n\n\nSo here is the same with the real data.\n\n\nShow code\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### add reliable change polygons\n  geom_polygon(inherit.aes = FALSE,\n               data = datRelDetVertices,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  geom_polygon(inherit.aes = FALSE,\n               data = datRelImpVertices,\n               aes(x = x, y = y),\n               fill = \"green\") +    \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### add leading diagonal of no change\n  geom_abline(slope = 1, intercept = 0) +\n  ### add RCI tramlines\n  geom_abline(slope = 1, intercept = -rci) +\n  geom_abline(slope = 1, intercept = rci) +  \n  geom_point(alpha = .5) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  ### need to change or remove these if doing monochrome version\n  scale_color_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_fill_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_shape_manual(values = vecShapesRCI, name = \"Reliable Change Index\") +\n  scale_x_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_y_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1)\n\n\n\nNot infrequently the three reliable change categories are tabulated.\n\n\nShow code\n\ntibData %>%\n  filter(occasion == 1) %>%\n  select(id, RCIchange) %>%\n  tabyl(RCIchange) %>%\n  adorn_pct_formatting(digits = 1) %>%\n  flextable() %>%\n  bg(i = 1, j = 1:3, bg = \"red\") %>%\n  bg(i = 2, j = 1:3, bg = \"grey\") %>%\n  bg(i = 3, j = 1:3, bg = \"green\") \n\nRCIchangenpercentReliable deterioration21.1%No reliable change8345.6%Reliable improvement9753.3%\n\nDual plot: trajectory plot and Jacobson\nThis next plot recaps on how the Jacobson plot is formed from the first and last scores. I have taken a few clients from different areas of the Jacobson plot. The left hand plot shows their start and finish scores as a very simple “cat’s cradle plot” and the same clients’ scores are shown on the Jacobson plot on the right so you can map between the two plots.\n\n\nShow code\n\n### To create a spurious ID code (probably excessive anonymisation)\n### create tibble of the numbers from 1 to the number of clients in the data\nvalNtot <- n_distinct(tibData$id)\n1:valNtot %>%\n  as_tibble() %>%\n  ### randomise those\n  mutate(id2 = sample(value, valNtot)) -> tmpIDs ### checking ### count(id2) %>% count(n)\n\n### I want an example from within each change category\ntibData %>%\n  filter(occasion == 1) %>%\n  ### now merge in the CSC categories\n  select(-c(firstScore, lastScore)) %>% # they get reinserted by the left_join()\n  left_join(tmpTibCSC, by = \"id\") %>%\n  ### create a spurious ID code (probably excessive anonymisation)\n  mutate(id2 = tmpIDs$id2) %>% ### checking ### count(id2) %>% count(n)\n  ### get rid of old ID codes\n  select(-id) %>%\n  mutate(absChange = abs(firstLastChange)) %>%\n  select(id2, firstScore, lastScore, firstLastChange, absChange, occasion, RCIchange, CSCchangeCategory) %>%\n  ### get for each RCSC category\n  group_by(RCIchange, CSCchangeCategory) %>% \n  mutate(minChange = min(firstLastChange), \n         maxChange = max(firstLastChange),\n         maxAbsChange = max(absChange)) %>%\n  ungroup() %>%\n  filter(absChange == maxAbsChange) -> tmpTib2\n\n### obsessionall, purge tmpIDs\nrm(tmpIDs)\n\n### pivot longer to get a simple cat's cradle plot\ntmpTib2 %>%\n  select(id2, firstScore, lastScore) %>%\n  pivot_longer(cols = -id2, names_to = \"whichOcc\", values_to = \"score\") %>%\n  ### clean up occasion name and get numeric code for it\n  mutate(whichOcc = str_to_sentence(whichOcc),\n         whichOcc = str_replace(whichOcc, fixed(\"score\"), \"\"),\n         whichOccN = if_else(whichOcc == \"First\", 1, 2)) -> tmpTib2long\n\n### using tribble() to create polygon vertices, nicer than my earlier method\ntribble(~x, ~y,\n        .9, 0,\n        .9, csc,\n        2.1, csc,\n        2.1, 0) -> tmpTibLowVertices\n\ntribble(~x, ~y,\n        .9, csc,\n        .9, valMaxPoss,\n        2.1, valMaxPoss,\n        2.1, csc) -> tmpTibHighVertices\n\nggplot(data = tmpTib2long,\n       aes(x = whichOccN, y = score, \n           group = id2)) +\n  ### colour the plot area\n  geom_hline(yintercept = csc) +\n  geom_polygon(inherit.aes = FALSE,\n               data = tmpTibLowVertices,\n               aes(x = x, y = y),\n                fill = \"green\") +\n  geom_polygon(inherit.aes = FALSE,\n               data = tmpTibHighVertices,\n               aes(x = x, y = y),\n                fill = \"red\") +\n  geom_point() +\n  ### now label the points with their id2 values\n  ### rather clumsy to get justification different for first and last points\n  geom_text(data = filter(tmpTib2long, whichOcc == \"First\"),\n            aes(label = id2),\n            colour = \"black\",\n            size = 6,\n            hjust = 1,\n            nudge_x = -.02,\n            size = 4) +\n  ### but actually I dropped these labels on the last scores\n  # geom_text(data = filter(tmpTib2long, whichOcc == \"Last\"),\n  #           aes(label = id2),\n  #           colour = \"black\",\n  #           size = 6,\n  #           hjust = 0,\n  #           nudge_x = .02,\n  #           size = 4) +\n  geom_line() +\n  ylim(c(0, 4)) +\n  ylab(\"Score\") +\n  xlab(\"Occasion\") +\n  ### colour RCI categories of improvement\n  scale_color_manual(values = vecColoursRCI) +\n  scale_x_continuous(breaks = 1:2,\n                     limits = c(.90, 2.1), \n                     labels = c(\"First\", \"Last\")) +\n  theme(legend.position = \"none\") +\n  theme(aspect.ratio = 1) -> ggplot1\n\ntribble(~x, ~y,\n        rci, 0,\n        csc, csc - rci,\n        csc, 0) -> tibRelImpStayedLow\n\ntribble(~x, ~y,\n        csc + rci,csc,\n        valMaxPoss, valMaxPoss - rci,\n        valMaxPoss, csc) -> tibRelImpStayedHigh\n\ntribble(~x, ~y,\n       csc, 0,\n       csc, csc - rci,\n       csc + rci, csc,\n       valMaxPoss, csc,\n       valMaxPoss, 0) -> tibRelClinSig \n\nggplot(tmpTib2,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### add reliable change polygons\n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelImpStayedLow,\n               aes(x = x, y = y),\n               fill = \"yellow\") +  \n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelClinSig,\n               aes(x = x, y = y),\n               fill = \"green\") +    \n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelImpStayedHigh,\n               aes(x = x, y = y),\n               fill = \"#8BC34A\") +    \n  geom_polygon(inherit.aes = FALSE,\n               data = datRelDetVertices,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### no change line\n  geom_segment(inherit.aes = FALSE,\n               x = valMinPoss, xend = valMaxPoss, y = valMinPoss, yend = valMaxPoss) +\n  ### upper tramline\n  geom_segment(inherit.aes = FALSE,\n               x = valMinPoss, xend = valMaxPoss - rci, y = valMinPoss + rci, yend = valMaxPoss) +\n  ### lower tramline\n  geom_segment(inherit.aes = FALSE,\n               x = valMinPoss + rci, xend = valMaxPoss, y = valMinPoss, yend = valMaxPoss - rci) +\n  geom_point() +\n  geom_text(aes(label = id2),\n            colour = \"black\",\n            size = 6,\n            hjust = 0,\n            nudge_x = .03,\n            size = 3) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  scale_x_continuous(limits = c(0, 4)) +\n  scale_y_continuous(limits = c(0, 4)) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) -> ggplot2\n\n### patchwork is a package in the tidyverse that allows you to combine ggplot grobs\n### with hindsight I could have done this with cowplot or ggextra::grid()\nlibrary(patchwork)\nggplot1 + ggplot2 -> patchwork1\n\npatchwork1 +\n  plot_annotation(title = \"Selected data in a cat's cradle plot (left) and in a Jacobson plot (right)\",\n                  theme = theme(plot.title = element_text(size = 20)))\n\n\n\nReading from the top left in the cat’s cradle plot, that client showed a dramatic improvement in score, from above the CSC to just below it and you can see how that maps into the Jacobson plot from the ID code (I can’t reference the ID codes here as I have, ultra obsessionally, randomised them). The next from the top again shows a large score drop but stays above the CSC so mapping to a different quadrant … and so on.\nTypical Jacobson summary table\nAnd the tabulation most often give is the full Jacobson table of clinical change and reliable change.\n\n\nShow code\n\ntibData %>%\n  filter(occasion == 1) %>%\n  select(id, RCIchange) %>%\n  left_join(tmpTibCSC, by = \"id\") %>%\n  mutate(CSCchangeCategory = ordered(CSCchangeCategory,\n                                     levels = c(\"Clinically deteriorated\",\n                                                \"Stayed low\",\n                                                \"Stayed high\",\n                                                \"Clinically improved\"),\n                                     labels = c(\"Clinically deteriorated\",\n                                                \"Stayed low\",\n                                                \"Stayed high\",\n                                                \"Clinically improved\"))) %>%\n  tabyl(CSCchangeCategory, RCIchange) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"all\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns() %>%\n  flextable() %>%\n  ### this is a way that flextable allows you to reset the contents of individual cells\n  flextable::compose(i = 1, j = 4, as_paragraph(as_chunk(''))) %>%\n  flextable::compose(i = 4, j = 2, as_paragraph(as_chunk(''))) %>%\n  bg(i = 1, j = 2, bg = \"red\") %>%\n  bg(i = 4, j = 4, bg = \"green\") %>%\n  bg(i = 1:4, j = 3, bg = \"grey\") %>%\n  bg(i = 2:3, j = 4, bg = \"#8BC34A\") %>%\n  bg(i = 2:3, j = 2, bg = \"#EF6C00\")\n\nCSCchangeCategoryReliable deteriorationNo reliable changeReliable improvementTotalClinically deteriorated0.0% (0)0.5%  (1)0.5%   (1)Stayed low0.0% (0)6.6% (12)0.0%  (0)6.6%  (12)Stayed high1.1% (2)34.1% (62)26.9% (49)62.1% (113)Clinically improved4.4%  (8)26.4% (48)30.8%  (56)Total1.1% (2)45.6% (83)53.3% (97)100.0% (182)\n\nThe blank cells are logically impossible: no-one can show reliable improvement and clinical deterioration nor vice versa.\nFinal Jacobson plot\nThis next plot shows a five area (five polygon if you’re feeling geometrical) summary of our data.\n\n\nShow code\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### add reliable change polygons\n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelImpStayedLow,\n               aes(x = x, y = y),\n               fill = \"yellow\") +  \n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelClinSig,\n               aes(x = x, y = y),\n               fill = \"green\") +    \n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelImpStayedHigh,\n               aes(x = x, y = y),\n               fill = \"#8BC34A\") +    \n  geom_polygon(inherit.aes = FALSE,\n               data = datRelDetVertices,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### add leading diagonal of no change\n  geom_abline(slope = 1, intercept = 0) +\n  ### add RCI tramlines\n  geom_abline(slope = 1, intercept = -rci) +\n  geom_abline(slope = 1, intercept = rci) +  \n  geom_point(alpha = .5) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  ### need to change or remove these if doing monochrome version\n  scale_color_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_fill_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_shape_manual(values = vecShapesRCI, name = \"Reliable Change Index\") +\n  scale_x_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_y_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1)\n\n\n\nThat shows these areas:\n[White] No reliable change. The level of change fell within the tramlines, so the absolute change (i.e. ignoring the sign/direction of change) fell below the RCI, here 0.398.\n[Red] Reliable deterioration. The scores got worse and by more than the RCI. Regardless of where the client started and finished on the measure, a therapist or service might want to think hard about these.\nThe next three all shows reliable improvement but fall into three groups:\n[Pale green] Reliable improvement but stayed above the CSC. Clearly there can be many reasons for this but again, these bear some thought.\n[Yellow] Reliable improvement but stayed below CSC. We didn’t have any of these but they certainly do occur, at least in services that don’t, utterly wrongly in my view, simply refuse to offer therapies to clients starting below the CSC. Clearly one important question is about why the starting score was below the CSC, were the client’s problems ones not covered well or at all by the measure used?\n[Bright green] Reliable improvement and score moved from above the CSC to below it. “Reliable and clinical improvement” or “Clinical and reliable improvement”. These are now, e.g. in the UK IAPT programme, called “reliably recovered” a term I dislike for many reasons. Certainly these are good outcomes in terms of the measure used and that’s not nothing but even here it is probably wise for therapists/services to consider these clients. One way to conduct a service/therapist audit can be to look at the red reliable deterioration cases and an equal number of those in this reliably and clinically improved group, perhaps the ones who showed the greatest score improvement. That can help a case review audit becoming persecutory.\n\nSummary\nI hope this is useful for anyone puzzled by the RCSC framework and tabulations and the Jacobson plot. If you feel that there could be improvements do contact me and I’d be happy to discuss the issues and very happy to improve this.\nI also hope that it may help people who understand the framework and plot but want code to implement it and find that that code is not readily available in statistics packages and not that easy to implement in spreadsheets (heaven forbid!) If you are in this category you will want to look at the next section about the code and I hope that better things, like functions that just do all this, will appear in CECPfuns and that online interactive shiny apps will follow those.\nMore generally I was struck going back to the basics myself that I think there are two viewpoints on the RCSC framework, or really, on the Jacobson plot:\nIt can be used simply to summarise bodies of first/last change data and generally that is just done by presenting the final table, or even by reducing this to counts of “reliably recovered” (ugh, horrible term).\nNot to discourage a full RCSC table, there is much more that a service or therapist can get from looking at their Jacobson plot: it puts individual clients’ change into a very simple but actually very useful 2D map and allows people to think more about their clients in the context of all the other clients in the plot and the referential lines.\n\nThat could take us into some serious thinking about the CSC and even more about the RCI but I will keep that for other posts and, I hope, papers. I have touched on the issue of our scores being discrete not continuous not to open up the statistical issues that creates (which are not severe even for short measures) but to remind us of the realities of our data, which I think are often hidden in tabulations and plots.\nWe should always remember that these are just questionnaire scores: we should neither undervalue, nor overvalue them, striking that balance is an ongoing process for our field.\nNotes on the code for users of R\nI’m absolutely not a professional statistician nor a good programmer. As with all of my code I provide zero guarantees about it. If you find errors please tell me and I will fix it (and credit you somehow).\nI now use R in the tidyverse way so the code may seem very strange if you don’t. Sorry, but it works for me.\nGenerally the style I use conforms to the typical tidyverse but there are few deviations, for instance I tend to prefix the names of objects with “tmp” if they are not intended to persist across code blocks and then I prefix with three letters describing the object: “val” for a value (i.e. a vector of length 1), “vec” for longer vectors and “tib” for a tibble.\nI have peppered the code with comments, too many by the rules of formal style, but I think people likely to look at this code will be, like me, not trained R coders and they will probably appreciate the comments.\nLooking forward things to add to the final code here for the tables and the plot include:\nThere were few overprinting clients with the same first and last scores in this dataset so I chose not to handle those. Clearly options are to jitter, use transparency or to use geom_count() to scale the points. No one of those is perfect for all datasets so best to allow for all three (and combinations of jittering and transparency?)\nI have ignored gender but there can be situations in which gender, and age, change the CSC (or, in principle the RCI). Tables can be aggregate or separated by gender/age etc. but plots are a bit more complex.\nIf we only have referential data or binary gender can handle this with colour for points and lines, or shape and line type for monochrome. Can also facet by gender.\nThis gets more complicated if we ever have larger enough referential datasets to have three or more gender categories and it gets quite complicated for adolescents where the CSC and the RCI may vary quite markedly with both age, down to year, and gender. That needs a lookup table for the CSC and RCI and I think facetting becomes the only realistic way to plot things.\n\n\nUpdate history of this post\n* 14/6/23: Updated to improve labelling in plots.\n* 15/6/23: Updated to add citations and references.\n* 13/8/23: Updated to add categories to the post.\n\n\n\nChristensen, L., & Mendoza, J. L. (1986). A method of assessing change in a single subject: An alteration of the RC index. Behavior Therapy, 17, 305–308.\n\n\nEvans, C., & Carlyle, J. (2021). Outcome measures and evaluation in counselling and psychotherapy (1st ed.). SAGE Publishing. https://ombook.psyctc.org/book/\n\n\nEvans, C., Margison, F., & Barkham, M. (1998). The contribution of reliable and clinically significant change methods to evidence-based mental health. Evidence Based Mental Health, 1, 70–72. https://doi.org/0.1136/ebmh.1.3.70\n\n\nJacobson, N. S., Follette, W. C., & Revenstorf, D. (1984). Psychotherapy outcome research: Methods for reporting variability and evaluating clinical significance. Behavior Therapy, 15, 336–352.\n\n\nJacobson, N. S., Follette, W. C., & Revenstorf, D. (1986). Towards a standard definition of clinically significant change. Behavior Therapy, 17, 308–311.\n\n\nJacobson, N. S., & Truax, P. (1991). Clinical significance: A statistical approach to defining meaningful change in psychotherapy research. Journal of Consulting and Clinical Psychology, 59(1), 12–19.\n\n\n\n\n",
    "preview": "posts/2023-06-10-jacobson1/jacobson1_files/figure-html5/startHere-1.png",
    "last_modified": "2023-08-25T14:22:35+02:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2022-09-09-drawing-psyctcorg-logo-and-other-circulargroup-things/",
    "title": "Drawing a PSYCTC.org logo (and other circular things)",
    "description": "As title says: very simple stuff about circles in ggplot, does open up somne ideas about drawing therapy groups.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-09-11",
    "categories": [
      "R graphics",
      "R tricks"
    ],
    "contents": "\n\nContents\nArguments to the function that enable you to change the pictures\nNumber of points\nSizes of points\nFill colour\nLine colour\n\n\nWays this might evolve\n\nI have used a rather horrible graphic for some PSYCTC things since the dark ages but I wanted a logo to use for the Zenodo community/repository I have created and this …\nancient gif… ancient 248x248 gif just wasn’t nice enough! To be fair, that’s the Distill/Rmarkdown blowing it up having inserted it just with\n![ancient gif](https://www.psyctc.org/psyctc/wp-content/uploads/2019/01/cropped-g2_256-1.gif)\nWhat happens if I insert it using simple html code and supplying dimensions with this inline html code?\n<img src=\"https://www.psyctc.org/psyctc/wp-content/uploads/2019/01/cropped-g2_256-1.gif\" width=\"248\" height=\"248\"/>\n\nHm. I don’t understand that, clearly I could do some more reading about embedding graphics files into distill output but that’s not the focus here. Enough playing around with that now!\nWhat I really want is a function that will draw a nicer version of that but could also be used by people writing about groups and wanting simple graphic depictions of the group.\nHere’s what I got in place on zenodo, and here it is here. More on it, and where I might got to extend this to something more generally useful, below.\n\n\nShow code\n\nmakeLogo <- function(nPoints = 8,\n                     centre = c(0, 0), # might want to change \n                     diameter = 1,\n                     nCircumfPoints = 500,\n                     circumfThickness = 1,\n                     circumfColour = \"black\",\n                     shape = 21,\n                     pointSize = 3,\n                     pointColour = \"black\",\n                     pointFill = \"white\",\n                     lineThickness = 2,\n                     lineColour = \"black\") {\n  \n  ### start by making tibble of points that make the circumference\n  makeCircumfTib <- function(centre = centre,\n                             diameter = diameter,\n                             npoints = nCircumfPoints){\n    r <- diameter / 2\n    ### sequence of points (in radians)\n    tt <- seq(0,2*pi, length.out = npoints)\n    ### get the coordinates on Cartesian plot using \n    ### elemenary trigonometry!\n    xx <- centre[1] + r * cos(tt)\n    yy <- centre[2] + r * sin(tt)\n    ### make a data frame of those values\n    tmp <- data.frame(x = xx, \n                      y = yy)\n    ### return it as a tibble\n    return(as_tibble(tmp))\n  }\n  ### use that to make the points making up the circumference\n  makeCircumfTib(centre = centre,\n                             diameter = diameter,\n                             npoints = nCircumfPoints) -> tibCircle\n  \n  ### similar to create the locations of the required numbers of points\n  makePointsTib <- function(nPoints = 8, \n                            centre = c(0,0),\n                            diameter = 1) {\n    r <- diameter / 2\n    ### this is the only difference from the circumference (which actually plots the first and last points\n    ###     at essentially the same point (except for rounding inaccuracies))\n    tt <- seq(0, 2*pi, length.out = nPoints + 1) # to get nPoints points need nPoints + 1 steps\n    xx <- centre[1] + r * cos(tt)\n    yy <- centre[2] + r * sin(tt)\n    tmp <- data.frame(x = xx, \n                      y = yy)\n    ### create a tibble of these but with an index for the points, i\n    tmp %>%\n      as_tibble() %>% \n      mutate(i = row_number()) %>%\n      select(i, x, y) -> tmpTib\n    return(tmpTib) \n  }\n  ### use that to make the tibble\n  makePointsTib(nPoints = nPoints,\n                centre = centre,\n                diameter = diameter) -> tibPoints\n  \n  ### now make the tibble for the lines starting from that last tibble\n  makeLinesTib <- function(tibPoints){\n    tibPoints %>%\n      ### rename to create a row for each line termination on the points\n      rename(j = i,\n             x1 = x,\n             y1 = y) %>%\n      mutate(nPoints = max(j)) %>% # get the number of points\n      ### and use that to replicate each row above that number of times\n      uncount(nPoints, .remove = FALSE) %>% \n      ### create a new index for each origin point for each line\n      mutate(i = row_number() %% nPoints, # modulo arithmetic to get that\n             i = if_else(j == 0, nPoints, i)) %>% # sort out first of those\n      ### now we have a row for all n^2 combinations of points\n      ### this next line avoids having each line twice and \n      ### removes the rows that create a line from the point to itself (i == j)\n      filter(i < j) %>% \n      ### now merge that tibble of n*(n-1)/2 rows with the original n point locations\n      left_join(tibPoints, by = \"i\") %>%\n      ### rename to get the second points for each line\n      rename(x2 = x,\n             y2 = y) -> tibLines\n    return(tibLines)\n  }\n  ### use that to make the tibble  of the lines\n  makeLinesTib(tibPoints) -> tibLines\n  \n  ### finally draw the logo/map\n  ggplot(tibCircle,\n         aes(x = x, y = y)) + \n    ### draw the out circle\n    geom_path(colour = circumfColour,\n              size = circumfThickness) +\n    ### draw the lines connecting the points \n    ### do this before drawing the points so the points overlay the lines\n    geom_segment(data = tibLines,\n                 aes(x = x1, y = y1, xend = x2, yend = y2),\n                 size = lineThickness,\n                 colour = lineColour) +\n    ### draw the points\n    geom_point(data = tibPoints,\n               aes(x = x, y = y),\n               shape = 21,\n               size = pointSize,\n               colour = pointColour,\n               fill = pointFill) +\n    ### a bit of fixing of the canvas\n    coord_fixed() + # get square geometry\n    theme_no_axes() + # what it says and get rid of border ...\n    theme(panel.border = element_blank()) -> gLogo\n  print(gLogo)\n}\n\n### so now we use this for nine points\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .5,\n         circumfColour = \"black\",\n         pointSize = 1,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .5,\n         lineColour = \"black\")\n\n\n\nI wanted that as a file that I could upload to Zenodo. The ggsave() function is the way I save ggplot output. I thought I should generate different sizes of file and that actually means tweaking the line widths for the graphic to look OK at the different file sizes. In the next blockI’ve put the code I used to got reasonable looking images at 256x256px, 512x512px, 1024x1024px and 2048x2048px. For each size the png file was smaller and seemed to look no worse than the jpeg. I’ve marked the block not to be run as this is just about exporting files, we don’t need the output, but I have left them here in case they might be useful to others.\n\n\nShow code\n\n### save png files\n### I have kept these lines here so people who want can copy them\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .2,\n         circumfColour = \"black\",\n         pointSize = .6,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .2,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_256x256.png\",\n       device = \"png\",\n       scale = 1,\n       units = \"px\",\n       width = 256,\n       height = 256)\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .4,\n         circumfColour = \"black\",\n         pointSize = .9,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .4,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_512x512.png\",\n       device = \"png\",\n       scale = 1,\n       units = \"px\",\n       width = 512,\n       height = 512)\n\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .5,\n         circumfColour = \"black\",\n         pointSize = 1,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .5,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_1024x1024.png\",\n       device = \"png\",\n       scale = 1,\n       units = \"px\",\n       width = 1024,\n       height = 1024)\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 2,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = 1,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_2048x2048.png\",\n       device = \"png\",\n       scale = 1,\n       units = \"px\",\n       width = 2048,\n       height = 2048)\n\n\n### save jpeg files\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .2,\n         circumfColour = \"black\",\n         pointSize = .6,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .2,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_256x256.jpeg\",\n       device = \"jpeg\",\n       scale = 1,\n       units = \"px\",\n       width = 256,\n       height = 256)\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .4,\n         circumfColour = \"black\",\n         pointSize = .9,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .4,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_512x512.jpeg\",\n       device = \"jpeg\",\n       scale = 1,\n       units = \"px\",\n       width = 512,\n       height = 512)\n\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .5,\n         circumfColour = \"black\",\n         pointSize = 1,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .5,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_1024x1024.jpeg\",\n       device = \"jpeg\",\n       scale = 1,\n       units = \"px\",\n       width = 1024,\n       height = 1024)\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 2,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = 1,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_2048x2048.jpeg\",\n       device = \"jpeg\",\n       scale = 1,\n       units = \"px\",\n       width = 2048,\n       height = 2048)\n\n\nArguments to the function that enable you to change the pictures\nThis is very simple stuff. The function prints the plot, it might be better to return the ggplot object rather than printing it, or to offer the choice between the two as a parameter or two (“print” TRUE/FALSE, or “printOrReturn”) with options “p” or “r” or two arguments, “print” TRUE/FALSE and “return” TRUE/FALSE). The arguments the function has at the moment are:\nmakeLogo(nPoints = 9, # number of points/bodies/group members\n         centre = c(0, 0),        # where to put the centre of the circle, only useful if overlaying a plot on another\n         diameter = 1,            # diameter, again only useful if overlaying or combining plots\n         nCircumfPoints = 500,    # use enough to get a smooth enough circumference at the size of your plot\n         circumfThickness = .5,   # what it says: thickness of the circumference line\n         circumfColour = \"black\", # what it says\n         pointSize = 1,           # allows you to vary this\n         pointColour = \"black\",   # what it says\n         pointFill = \"white\",     # the points default to filled circles\n         lineThickness = .5,      # thickness of the connecting lines between points (same scale as for circumference)\n         lineColour = \"black\")    # colour of the connecting lines\nHere’s some playing around with some of these.\nNumber of points\n\n\nShow code\n\nfor (nPoints in 3:15) {\n  print(paste0(\"nPoints = \", nPoints))\n  makeLogo(nPoints,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 2,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = 1,\n         lineColour = \"black\")\n}\n\n[1] \"nPoints = 3\"\n\n[1] \"nPoints = 4\"\n\n[1] \"nPoints = 5\"\n\n[1] \"nPoints = 6\"\n\n[1] \"nPoints = 7\"\n\n[1] \"nPoints = 8\"\n\n[1] \"nPoints = 9\"\n\n[1] \"nPoints = 10\"\n\n[1] \"nPoints = 11\"\n\n[1] \"nPoints = 12\"\n\n[1] \"nPoints = 13\"\n\n[1] \"nPoints = 14\"\n\n[1] \"nPoints = 15\"\n\n\nQuite pretty, I like the way that the centre is empty for odd numbers of points and a star for even numbers of points.\nSizes of points\n\n\nShow code\n\nfor (pointSize in seq(.5, 7.5, 1)) {\n  print(paste0(\"pointSize = \", pointSize))\n  makeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = pointSize,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = 1,\n         lineColour = \"black\")\n}\n\n[1] \"pointSize = 0.5\"\n\n[1] \"pointSize = 1.5\"\n\n[1] \"pointSize = 2.5\"\n\n[1] \"pointSize = 3.5\"\n\n[1] \"pointSize = 4.5\"\n\n[1] \"pointSize = 5.5\"\n\n[1] \"pointSize = 6.5\"\n\n[1] \"pointSize = 7.5\"\n\n\nFill colour\n\n\nShow code\n\nfor (pointFill in c(\"black\", \"white\", \"red\", \"green\", \"blue\")) {\n  print(paste0(\"pointFill = \", pointFill))\n  makeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 8,\n         pointColour = \"black\",\n         pointFill = pointFill,\n         lineThickness = .3,\n         lineColour = \"black\")\n}\n\n[1] \"pointFill = black\"\n\n[1] \"pointFill = white\"\n\n[1] \"pointFill = red\"\n\n[1] \"pointFill = green\"\n\n[1] \"pointFill = blue\"\n\n\nLine colour\n\n\nShow code\n\nfor (lineColour in c(\"black\", \"white\", \"red\", \"green\", \"blue\")) {\n  print(paste0(\"lineColour = \", lineColour))\n  makeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 2,\n         pointColour = \"black\",\n         pointFill = \"black\",\n         lineThickness = 2,\n         lineColour = lineColour)\n}\n\n[1] \"lineColour = black\"\n\n[1] \"lineColour = white\"\n\n[1] \"lineColour = red\"\n\n[1] \"lineColour = green\"\n\n[1] \"lineColour = blue\"\n\n\nWays this might evolve\nI don’t think this is of enough general utility to do the necessary additions to roll it into CECPfuns now but I may.\nHowever, I do think this creates a base from which I might build something of more general use to group therapists. Sensible additions might be:\nallow different shapes (it’s typical to mark gender usually in binary as square and circle, use square, circle and diamond?), supply as a vector or list\nallow different shapes for therapists and clients (using fill colour?)\nallow addition of labels to the points (probably best to put these outside the circle)\nallow omission of lines or different colours of lines (omissiong can be done by setting colour to background colour). If so, best to supply as a tibble of the lines and sensible to add an auxiliary function making it easy to stipulate colours of bunches of lines: needs some thought.\nallow addition of arrowheads to particular lines (same issues as last point)\nallow setting of thickness of lines (same issues, watch for sensible thicknesses)\nallow omission of points (i.e. keep number of placeholders/chairs, but blank out missing person). Might make omission of lines from/to absent people automatic … or not, sometimes a lot is said about a missing person!) Could just make omission of arrowheads from the missing person/people automatic.\nallow addition of notes/labels to the whole, e.g. “Group 17, 11/9/22”. Perhaps allow four sets of such annotations: top left, top right …\nallow addition of notes/lables outside the naming/identity labels on points that would allow comments like “arrived 30 minutes late” or “very unhappy this week”\nanimate these\nmight be possible to create a standard spreadsheet that would make it easy for therapists to feed these things in for all the members for all the group occasions\nHm. All of that would be a lot of work but perhaps of very real utility. It would probably be evolving beyond just some R to something that might need some wider collaboration on the data to store on groups’ composition and activity (in the sense of those linking lines) and perhaps beyond that to an open data standard for group data more widely (e.g. adding changes in self-appraisals or group appraisals). My experience of the last decade tells me it would then really need an open standard, open source data entry system. Hm, that’s getting beyond my time and skills.\nDo contact me if you might be interested in collaborating to develop things for group therapies along the lines above.\n\n\n\n",
    "preview": "posts/2022-09-09-drawing-psyctcorg-logo-and-other-circulargroup-things/PSYCTC_logo_512x512.png",
    "last_modified": "2023-08-25T14:22:13+02:00",
    "input_file": {},
    "preview_width": 512,
    "preview_height": 512
  },
  {
    "path": "posts/2022-07-23-derangements-2/",
    "title": "Derangements #2",
    "description": "Follows on from 'Scores from matching things'",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-07-23",
    "categories": [
      "rigorous idiography",
      "method of derangements"
    ],
    "contents": "\n\n\nShow code\n\nas_tibble(list(x = 1,\n               y = 1)) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"Derangements #2\",\n            size = 20,\n            colour = \"red\",\n            angle = 30,\n            lineheight = 1) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\n\nUpdated to add contact me 11.ix.22\nIn my last post here, Scores from matching things I gave the background to the probabilities of achieving certain scores on matching tasks by chance alone to help explain the perhaps counter-intuitive finding that matching four or more things correctly is unlikely by chance alone at p < .05 regardless of the number of objects to be matched.\nThis just adds a bit more to that, mostly as plots and complements both that Rblog post and an “ordinary” blog post, Sometimes n=4 is enough.\nWhat I wanted was to show how rapidly the probabilities of achieving any particular score stabilise to an asymptotic value as n increases. Here we are for n from 4 to 15 and scores from 4 to 10.\n\n\nShow code\n\n### create some functions (as in previous post)\nall.derangements <- function(n){\n  cumprob <- prob <- number <- term <- score <- rev(0:n)\n  for (m in 1:n) {\n    i <- m+1\n    s <- n-m\n    term[i] <- ((-1)^(m))/(factorial(m))\n  }  \n  term[1] <- 1\n  for (i in 0:n) {\n    s <- i+1\n    prob[s] <- (sum(term[1:s]))/factorial(n-i)\n  }\n  number <- factorial(n)*prob\n  for (s in 0:n) {\n    m <- n-s\n    i <- m+1\n    cumprob[i] <- sum(prob[1:i])\n  }\n  tmp <- cbind(n, score,number,prob,cumprob)\n  tmp\n}\n\np.derange.score <- function(score,n){\n  if (score > n) stop(\"Score cannot be greater than n\")\n  if (score == (n-1)) stop (\"Score cannot be n-1\")\n  cumprob <- prob <- term <- rev(0:n)\n  for (m in 1:n) {\n    i <- m+1\n    s <- n-m\n    term[i] <- ((-1)^(m))/(factorial(m))\n  }  \n  term[1] <- 1\n  for (i in 0:n) {\n    s <- i+1\n    prob[s] <- (sum(term[1:s]))/factorial(n-i)\n  }\n  for (s in 0:n) {\n    m <- n-s\n    i <- m+1    \n    cumprob[i] <- sum(prob[1:i])\n  }\n  cumprob[n+1-score]\n}\n\n### now let's go a bit further\n### get all the possible scores for n from 4 to 30\nlapply(4:30, FUN = all.derangements) -> tmpList\n### I always forget this nice little bit of base R and I'm a bit surprised that there doesn't seem to be a nice tidyverse alternative\ndo.call(rbind.data.frame, tmpList) %>%\n  as_tibble() -> tmpTib\n### this was just to produce some tables for my blog post at https://www.psyctc.org/psyctc/2022/07/23/sometimes-n4-is-enough/\n# tmpTib %>% \n#   write_csv(file = \"derangements.csv\")\n\n### ditto\n# 1:14 %>% \n#   as_tibble() %>%\n#   rename(n = value) %>%\n#   mutate(PossibleWays = factorial(n),\n#          PossibleWays = prettyNum(PossibleWays, big.mark = \",\")) %>%\n#   write_csv(file = \"numbers.csv\")\n\n### but Vectorizing the function seemed cleaner so ...\nVectorize(FUN = all.derangements) -> All.derangements\nAll.derangements(4:14) -> tmpList\n### back to the do.call() just for the tables \n# do.call(rbind, tmpList) %>%\n#   as_tibble() %>%\n#   filter(score == 4) %>%\n#   select(n, number) %>%\n#   mutate(number = prettyNum(number, big.mark = \",\")) %>%\n#   write_csv(\"correct.csv\")\n\n# do.call(rbind, tmpList) %>%\n#   as_tibble() %>%\n#   filter(score == 4) %>%\n#   mutate(totalPerms = factorial(n)) %>%\n#   select(-prob) %>%\n#   select(n, totalPerms, everything()) %>%\n#   write_csv(\"final.csv\")\n  \n\n### OK, now for this Rblog post!\nAll.derangements(4:15) -> tmpList\ndo.call(rbind, tmpList) %>%\n  as_tibble() %>%\n  filter(score > 3 & score < 11) %>%\n  mutate(score = ordered(score,\n                         levels = 4:10)) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = n, y = cumprob, colour = score, group = score)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 3:15,\n                     minor_breaks = 3:15,\n                     limits = c(3, 15)) +\n  ylab(\"p\") +\n  theme_bw() \n\n\n\nHere’s the same on a log10 y axis to separate the p values for the higher scores.\n\n\nShow code\n\nggplot(data = tmpTib,\n       aes(x = n, y = cumprob, colour = score, group = score)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 3:15,\n                     minor_breaks = 3:15,\n                     limits = c(3, 15)) +\n  scale_y_continuous(trans = \"log10\") +\n  ylab(\"p\") +\n  theme_bw() \n\n\n\nThis next table shows how rapidly p values of real interest stabilise. The table is ordered by number of objects (n) within score. The column p is the probability of getting that score or better by chance alone, diffProb is the absolute change in that p value from the one for the previous n, diffPerc is the difference as a percentage of the previous p value. diffProbLT001 flags when the change in absolute p vaue is below .001 at which point I think in my realm any further precision is spurious. However, diffLT1pct flags when the change in p value is below 1% of the previous p value just in case someone wants that sort precise convergence.\n\n\nShow code\n\nAll.derangements(4:20) -> tmpList\ndo.call(rbind, tmpList) %>%\n  as_tibble() %>%\n  filter(score > 3 & score < 11)  -> tmpTib\n\n\n### just working out how stable the p values get how soon\ntmpTib %>%\n  arrange(score) %>%\n  group_by(score) %>%\n  mutate(diffProb = abs(cumprob - lag(cumprob)),\n         diffProbLT001 = if_else(diffProb < .001, \"Y\", \"N\"),\n         diffPerc = 100 * diffProb /lag(cumprob),\n         diffLT1pct = if_else(diffPerc < 1, \"Y\", \"N\")) %>% \n  ungroup() %>%\n  select(-c(number, prob)) %>%\n  rename(p = cumprob) %>%\n  select(score, everything()) %>%\n  as_hux() %>%\n  set_position(\"left\") %>% # left align the whole table\n  set_bold(row = everywhere, col = everywhere) %>% # everything into bold\n  set_align(everywhere, everywhere, \"center\") %>% # everything centred\n  set_align(everywhere, 1:2, \"right\") %>% # but now right justify the first two columns\n  map_text_color(by_values(\"Y\" = \"green\")) %>% # colour matches by text recognition\n  map_text_color(by_values(\"N\" = \"red\"))\n\nscorenpdiffProbdiffProbLT001diffPercdiffLT1pct440.0417450.008330.0333N80N460.02220.0139N167N470.01830.00397N17.9N480.01910.000868Y4.76N490.0190.000154Y0.807Y4100.0192.31e-05Y0.122Y4110.0193.01e-06Y0.0158Y4120.0193.44e-07Y0.00181Y4130.0193.53e-08Y0.000186Y4140.0193.28e-09Y1.73e-05Y4150.0192.78e-10Y1.47e-06Y4160.0192.17e-11Y1.15e-07Y4170.0191.57e-12Y8.29e-09Y4180.0191.06e-13Y5.59e-10Y4190.0196.71e-15Y3.53e-11Y4200.0193.99e-16Y2.1e-12Y550.00833560.001390.00694N83.3N570.004370.00298N214N580.00350.000868Y19.9N590.003690.000193Y5.52N5100.003663.47e-05Y0.941Y5110.003665.26e-06Y0.144Y5120.003666.89e-07Y0.0188Y5130.003667.95e-08Y0.00217Y5140.003668.2e-09Y0.000224Y5150.003667.65e-10Y2.09e-05Y5160.003666.52e-11Y1.78e-06Y5170.003665.12e-12Y1.4e-07Y5180.003663.72e-13Y1.02e-08Y5190.003662.52e-14Y6.87e-10Y5200.003661.59e-15Y4.35e-11Y660.00139670.0001980.00119N85.7N680.0007190.000521Y262N690.0005650.000154Y21.5N6100.00063.47e-05Y6.15N6110.0005936.31e-06Y1.05N6120.0005949.65e-07Y0.163Y6130.0005941.27e-07Y0.0214Y6140.0005941.48e-08Y0.00248Y6150.0005941.53e-09Y0.000258Y6160.0005941.44e-10Y2.42e-05Y6170.0005941.23e-11Y2.07e-06Y6180.0005949.67e-13Y1.63e-07Y6190.0005947.04e-14Y1.19e-08Y6200.0005944.78e-15Y8.04e-10Y770.000198782.48e-050.000174Y87.5N790.0001027.72e-05Y311N7107.88e-052.31e-05Y22.7N7118.41e-055.26e-06Y6.68N7128.31e-059.65e-07Y1.15N7138.33e-051.48e-07Y0.179Y7148.32e-051.97e-08Y0.0236Y7158.32e-052.3e-09Y0.00276Y7168.32e-052.39e-10Y0.000287Y7178.32e-052.25e-11Y2.7e-05Y7188.32e-051.93e-12Y2.32e-06Y7198.32e-051.53e-13Y1.83e-07Y7208.32e-051.12e-14Y1.34e-08Y882.48e-05892.76e-062.2e-05Y88.9N8101.27e-059.92e-06Y360N8119.67e-063.01e-06Y23.7N8121.04e-056.89e-07Y7.12N8131.02e-051.27e-07Y1.23N8141.03e-051.97e-08Y0.192Y8151.02e-052.62e-09Y0.0256Y8161.02e-053.08e-10Y0.003Y8171.02e-053.22e-11Y0.000314Y8181.02e-053.04e-12Y2.96e-05Y8191.02e-052.62e-13Y2.55e-06Y8201.02e-052.07e-14Y2.02e-07Y992.76e-069102.76e-072.48e-06Y90N9111.4e-061.13e-06Y409N9121.06e-063.44e-07Y24.6N9131.14e-067.95e-08Y7.51N9141.12e-061.48e-08Y1.3N9151.13e-062.3e-09Y0.204Y9161.13e-063.08e-10Y0.0273Y9171.13e-063.62e-11Y0.00322Y9181.13e-063.8e-12Y0.000337Y9191.13e-063.6e-13Y3.2e-05Y9201.13e-063.11e-14Y2.76e-06Y10102.76e-0710112.51e-082.51e-07Y90.9N10121.4e-071.15e-07Y458N10131.05e-073.53e-08Y25.3N10141.13e-078.2e-09Y7.85N10151.11e-071.53e-09Y1.36N10161.11e-072.39e-10Y0.215Y10171.11e-073.22e-11Y0.0289Y10181.11e-073.8e-12Y0.00341Y10191.11e-074e-13Y0.000359Y10201.11e-073.8e-14Y3.41e-05Y\nDo contact me if this interests you and if you might want to use the method with real data.\n\n\n\n",
    "preview": "posts/2022-07-23-derangements-2/derangements-2_files/figure-html5/createGraphic-1.png",
    "last_modified": "2023-08-25T14:21:42+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-07-15-matching-scores/",
    "title": "Scores from matching things",
    "description": "Mostly about the method of derangements but some huxtable!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-07-15",
    "categories": [
      "rigorous idiography",
      "method of derangements"
    ],
    "contents": "\n\nContents\nn(objects) = 3\nn(objects) = 4\nn(objects) = 5\nSummary\nContact me if you are interested in using this and want help\nHistorical footnote\n\n\n\nShow code\n\nas_tibble(list(x = 1,\n               y = 1)) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"Derangements #1\",\n            size = 20,\n            colour = \"red\",\n            angle = 30,\n            lineheight = 1) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\n\n[Created 15.vii.22, tweaked 23.vii.22 and 11.ix.22, neither changing code or outputs.]\nThe theory behind this is fully described in:\nEvans, C., Hughes, J., & Houston, J. (2002). Significance testing the validity of ideographic methods: A little derangement goes a long way.\nBritish Journal of Mathematical and Statistical Psychology, 55(2), 385–390. https://doi.org/10.1348/000711002760554525\nDo contact me through my work site if you would like a copy of that.\nThe idea is of matching things which might be purely idiographic. For example in that original paper the matching task presented\nto therapists from a prison therapy group was to see if they could match the two dimensional principal component plots from person\nrepertory grids created with elicited contructs and varying elements by each of the six members of the group. Both therapists matched\nfour of the six pre-therapy grids successfully; one therapist matched all six post-therapy grids and the other matched three of the six.\nThe paper showed that the probability of matching four or more objects correctly is always unlikely to happen by chance alone with\np < .05 regardless of the number of objects.\nAll I am doing here is using a bit of R, specifically the function permutations() from the admisc package to get all the possible permutations (i.e. ways of chosing) n objects and using a bit of tidyverse to feed this into a huxtable … i.e. into one of R’s various ways of prettifying and managing tables.\nn(objects) = 3\nLet’s start with the situation where you only have three objects (as it makes things small and simple). There are six ways of rearranging three objects, three ways to pick the first, two ways to pick the second and then of course the third one is picked for you.\nThis table shows the six possible permutations of three objects in columns 2 to 4. Then in columns 5 to 7 it shows the matching scores as “Y” or “N” depending on whether each chosen object has been put in the correct place. (Imagine that you had been given three repertory grid plots created from grids from people you knew well and you are trying to match each grid to the person who created it with no other clues.) Finally it shows the matching score.\n\n\nShow code\n\noptions(width = 160)\noptions(huxtable.knitr_output_format = \"html\") \ngetMatches <- function(vec) {\n  ### litle function that returns a vector of zero or one\n  ### depending whether the number in the vector matches its\n  ### position in the vector\n  ### I could have put some input error trapping \n  ### but no need given that I'm only using this here\n  return(as.numeric(vec == 1:length(vec)))\n}\n# getMatches(1:3)\n# getMatches(1:59)\n# getMatches(c(3, 2, 1))\n# getMatches(c(3, 1, 2))\n\n\nmatchScore <- function(vec) {\n  ### similar function to getMatches but this time returns\n  ### total score of matches\n  return(sum(vec == 1:length(vec)))\n}\n# matchScore(1:3)\n# matchScore(1:59)\n# matchScore(c(3, 2, 1))\n# matchScore(c(3, 1, 2))\n\n### I've wrapped this in suppressMessages to get rid of the irritating renaming messages from dplyr\nsuppressMessages(admisc::permutations(1:3) %>%\n                   ### that got me all the permtations of 1:3 \n                   ### but as a matrix\n                   as.data.frame() %>% # go to df (avoids warning from dplyr)\n                   as_tibble() %>% # and then to tibble!\n                   rowwise() %>% # go to rowwise mode\n                   ### and compute the matches as a list/vector\n                   mutate(matches = list(getMatches(across(everything())))) %>%\n                   ungroup() %>% # come out of rowwise (not strictly necessary)\n                   ### unnest that to separate columns\n                   unnest_wider(matches, names_sep = \"_\") %>%\n                   ### do some renaming to make things clearer\n                   rename_with( ~ gsub(\"V\", \"Choice\", .x, fixed = TRUE)) %>%\n                   rename_with( ~ gsub(\"...\", \"Match\", .x, fixed = TRUE)) %>%\n                   mutate(across(starts_with(\"Match\"), ~ if_else(.x == 1, \"Y\", \"N\"))) %>%\n                   ### back into rowwise mode\n                   rowwise() %>%\n                   ### to get the score\n                   mutate(score = matchScore(c_across(starts_with(\"Choice\")))) %>%\n                   ungroup() %>% \n                   ### create permutation number\n                   mutate(permutationN = row_number()) %>%\n                   ### rearrange order of columns\n                   select(permutationN, everything()) -> tmpTib3)\n\n\ntmpTib3 %>%\n  as_hux() %>%\n  set_position(\"left\") %>% # left align the whole table\n  set_bold(row = everywhere, col = everywhere) %>% # everything into bold\n  set_align(everywhere, everywhere, \"center\") %>% # everything centred\n  set_align(everywhere, 1, \"right\") %>% # but now right justify the first column\n  map_text_color(by_values(\"Y\" = \"green\")) %>% # colour matches by text recognition\n  map_text_color(by_values(\"N\" = \"red\"))\n\npermutationNChoice1Choice2Choice3matches_1matches_2matches_3score1123YYY32132YNN13213NNY14231NNN05312NNN06321NYN1\n(Sorry: the colour scheme isn’t great on the yellow I’ve used for this blog/site.) We can see that there is, as there will be for any number of objects, only one way of getting all of them matched correctly. There are three ways to get one matched correctly and that leaves two ways of scoring zero correct matches. There are no ways of scoring two correct matches: if you match the first two correctly then you are left with the last one which you then have to put in the correct place.\nSo nothing very impressive even about getting all three correct: you had a one in six probability of doing that by chance. Let’s go up to n = 4.\nn(objects) = 4\n\n\nShow code\n\nsuppressMessages(admisc::permutations(1:4) %>%\n                   as.data.frame() %>%\n                   as_tibble() %>%\n                   rowwise() %>%\n                   mutate(matches = list(getMatches(across(everything())))) %>%\n                   unnest_wider(matches, names_sep = \"_\") %>%\n                   rename_with( ~ gsub(\"V\", \"Choice\", .x, fixed = TRUE)) %>%\n                   rename_with( ~ gsub(\"...\", \"Match\", .x, fixed = TRUE)) %>%\n                   mutate(across(starts_with(\"Match\"), ~ if_else(.x == 1, \"Y\", \"N\"))) %>%\n                   rowwise() %>%\n                   mutate(score = matchScore(c_across(starts_with(\"Choice\")))) %>%\n                   ungroup() %>% \n                   mutate(permutationN = row_number()) %>%\n                   select(permutationN, everything()) -> tmpTib4)\n\n\ntmpTib4 %>%\n  as_hux() %>%\n  set_position(\"left\") %>%\n  set_bold(row = everywhere, col = everywhere) %>%\n  set_align(everywhere, everywhere, \"center\") %>%\n  set_align(everywhere, 1, \"right\") %>%\n  map_text_color(by_values(\"Y\" = \"green\")) %>%\n  map_text_color(by_values(\"N\" = \"red\"))\n\npermutationNChoice1Choice2Choice3Choice4matches_1matches_2matches_3matches_4score11234YYYY421243YYNN231324YNNY241342YNNN151423YNNN161432YNYN272134NNYY282143NNNN092314NNNY1102341NNNN0112413NNNN0122431NNYN1133124NNNY1143142NNNN0153214NYNY2163241NYNN1173412NNNN0183421NNNN0194123NNNN0204132NNYN1214213NYNN1224231NYYN2234312NNNN0244321NNNN0\nNow we have 24 ways of permuting the objects and still just the one correct matching of all four. As ever it’s impossible to score n - 1, i.e. three here. There are six ways of scoring two correct matches and eight ways of scoring one correct match leaving nine ways of scoring zero correct matches.\nHere’s that score breakdown.\n\n\nShow code\n\ntmpTib4 %>%\n  tabyl(score) %>%\n  adorn_pct_formatting(digits = 2) %>%\n  arrange(desc(score))\n\nscorenpercent414.17%2625.00%1833.33%0937.50%\nSo the chances of getting all four correct by chance alone was p = 1/24 = 0.04, below the conventional p < .05 criterion.\nn(objects) = 5\n\n\nShow code\n\nsuppressMessages(admisc::permutations(1:5) %>%\n                   as.data.frame() %>%\n                   as_tibble() %>%\n                   rowwise() %>%\n                   mutate(matches = list(getMatches(across(everything())))) %>%\n                   unnest_wider(matches, names_sep = \"_\") %>%\n                   rename_with( ~ gsub(\"V\", \"Choice\", .x, fixed = TRUE)) %>%\n                   rename_with( ~ gsub(\"...\", \"Match\", .x, fixed = TRUE)) %>%\n                   mutate(across(starts_with(\"Match\"), ~ if_else(.x == 1, \"Y\", \"N\"))) %>%\n                   rowwise() %>%\n                   mutate(score = matchScore(c_across(starts_with(\"Choice\")))) %>%\n                   ungroup() %>% \n                   mutate(permutationN = row_number()) %>%\n                   select(permutationN, everything()) -> tmpTib5)\n\n\ntmpTib5 %>%\n  as_hux() %>%\n  set_position(\"left\") %>%\n  set_bold(row = everywhere, col = everywhere) %>%\n  set_align(everywhere, everywhere, \"center\") %>%\n  set_align(everywhere, 1, \"right\") %>%\n  map_text_color(by_values(\"Y\" = \"green\")) %>%\n  map_text_color(by_values(\"N\" = \"red\"))\n\npermutationNChoice1Choice2Choice3Choice4Choice5matches_1matches_2matches_3matches_4matches_5score112345YYYYY5212354YYYNN3312435YYNNY3412453YYNNN2512534YYNNN2612543YYNYN3713245YNNYY3813254YNNNN1913425YNNNY21013452YNNNN11113524YNNNN11213542YNNYN21314235YNNNY21414253YNNNN11514325YNYNY31614352YNYNN21714523YNNNN11814532YNNNN11915234YNNNN12015243YNNYN22115324YNYNN22215342YNYYN32315423YNNNN12415432YNNNN12521345NNYYY32621354NNYNN12721435NNNNY12821453NNNNN02921534NNNNN03021543NNNYN13123145NNNYY23223154NNNNN03323415NNNNY13423451NNNNN03523514NNNNN03623541NNNYN13724135NNNNY13824153NNNNN03924315NNYNY24024351NNYNN14124513NNNNN04224531NNNNN04325134NNNNN04425143NNNYN14525314NNYNN14625341NNYYN24725413NNNNN04825431NNNNN04931245NNNYY25031254NNNNN05131425NNNNY15231452NNNNN05331524NNNNN05431542NNNYN15532145NYNYY35632154NYNNN15732415NYNNY25832451NYNNN15932514NYNNN16032541NYNYN26134125NNNNY16234152NNNNN06334215NNNNY16434251NNNNN06534512NNNNN06634521NNNNN06735124NNNNN06835142NNNYN16935214NNNNN07035241NNNYN17135412NNNNN07235421NNNNN07341235NNNNY17441253NNNNN07541325NNYNY27641352NNYNN17741523NNNNN07841532NNNNN07942135NYNNY28042153NYNNN18142315NYYNY38242351NYYNN28342513NYNNN18442531NYNNN18543125NNNNY18643152NNNNN08743215NNNNY18843251NNNNN08943512NNNNN09043521NNNNN09145123NNNNN09245132NNNNN09345213NNNNN09445231NNNNN09545312NNYNN19645321NNYNN19751234NNNNN09851243NNNYN19951324NNYNN110051342NNYYN210151423NNNNN010251432NNNNN010352134NYNNN110452143NYNYN210552314NYYNN210652341NYYYN310752413NYNNN110852431NYNNN110953124NNNNN011053142NNNYN111153214NNNNN011253241NNNYN111353412NNNNN011453421NNNNN011554123NNNNN011654132NNNNN011754213NNNNN011854231NNNNN011954312NNYNN112054321NNYNN1\nSo now we have 120 ways of permuting the objects and still just the one correct matching of all of them. Here’s the score breakdown.\n\n\nShow code\n\ntmpTib5 %>%\n  tabyl(score) %>%\n  adorn_pct_formatting(digits = 2) %>%\n  arrange(desc(score))\n\nscorenpercent510.83%3108.33%22016.67%14537.50%04436.67%\nIt was impossible to score four matches but getting all five correct was unlikely by chance alone at p = 1/120 = 0.008\nSummary\nIt can be seen that the number of possible ways to permute n objects goes up rapidly as n increases. That increasing number of ways of permuting things means that getting four or more correctly matched is always unlikely at p < .05 regardless of n. There’s a lookup table at https://link.psyctc.org/derangements where you can look up the scores and their probabilities for n <= 30.\nContact me if you are interested in using this and want help\nContact me here\nHistorical footnote\nThis was in my ancient derangements.R file that I clearly created while I still had access to S+:\nThis program differs from a program for S+ only in having to declare a function, factorial() which comes with S+ but not\nthe version of R on which I’m testing this (1.7.1) and in explicitly declaring tmp at the end of all.derangements() since\nR won’t return it to the console (does return it for assignment) if you just end the function with the assignment to tmp\n# factorial <- function(n) {\n#   gamma(n+1)\n# }\nI’ve often wondered which was my first R release, so it was 1.7.1 or earlier. R has long since acquired a factorial()\nfunction in the base functions.\n\n\n\n",
    "preview": "posts/2022-07-15-matching-scores/matching-scores_files/figure-html5/createGraphic-1.png",
    "last_modified": "2023-08-25T14:20:48+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-06-19-reading-data-into-r-from-msoft-access-mdb/",
    "title": "Reading data into R from M$oft Access mdb",
    "description": "Work in progress about reading data from M$ mdb file into R",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-06-19",
    "categories": [
      "R tricks",
      "data import in R",
      "database handling"
    ],
    "contents": "\n\nContents\nOS dependent issues\nWindoze\nWindoze 10\nWindoze 11\n\nOpening the mdb file from within R (windoze version)\nLinux\nOpening the mdb file from within R (Linux version)\n\nMore on using the database in R\n\n\n\nShow code\n\n# as_tibble(list(x = 1,\n#                y = 1)) -> tibDat\n# \n# ggplot(data = tibDat,\n#        aes(x = x, y = y)) +\n#   geom_text(label = \"M$oft Access mdb\",\n#             size = 20,\n#             colour = \"red\",\n#             angle = 30) +\n#   xlab(\"\") +\n#   ylab(\"\") +\n#   theme_bw() +\n#   theme(panel.grid.major = element_blank(),\n#         panel.grid.minor = element_blank(),\n#         panel.border = element_blank(),\n#         panel.background = element_blank(),\n#         axis.line = element_blank(),\n#         axis.text = element_blank(),\n#         axis.ticks = element_blank()) \n\n\nHm, second time in a row. Warning: this is pretty geeky stuff but this time it’s more general IT than statistical\nThe situation is that I have received a large lump of fascinating routine service data in M$ Access mdb format and I want to yank it into R to use it.\nThis is a “Work in progress page” as I am not yet where I want to be for this. At the moment (19.vi.22) I am going to split the issues into three:\nOS dependent issues about setting up what R needs to be done in the OS to allow it to find the data.\nWindoze (10 and 11).\nLinux (Unbuntu 22.04 LTS but probably fairly generic)\n\nIssues when you get through those issues and have R accessing the data.\nFor all this I am using the RODBC package as I’ve used it for this sort of task in the past. I think there is at least one alternative package and if someone thinks others are better, do [contact me] (https://www.coresystemtrust.org.uk/contact-form/).\nThe generic issue is whatever OS you use you need something outside R, in the OS, that creates a pipeline that can open the mdb file and offer the data there for use by R, strictly by the RODBC package. As I understand this the pipeline can be called an “ODBC” driver/connection. ODBC means Open DataBase Connectivity, see a typically good Wikipedia article at https://en.wikipedia.org/wiki/Open_Database_Connectivity to get some of the history.\nThe connections look like this.\n\n\nShow code\n\nlibrary(DiagrammeR)\n# png(filename = \"/media/chris/Clevo_SSD2/Data/MyR/R/distill_blog/test2/pipeline.png\", height = 800, width = 600, units = \"px\", bg = \"white\")\ngrViz(\"digraph flowchart {\n      # node definitions with substituted label text\n      node [fontname = Helvetica, shape = rectangle]   \n      \n      # I confess that I hate creating diagrams this way.  I can see that DiagrammeR and the tools \n      # it leans on are powerful but the syntax strikes me as archaic and doesn't lend itself to \n      # reproducability\n\n      ## Level 0 (total who opened links)\n      R [label = '@@1']\n      # edge definitions with the node IDs\n      R -> RODBC\n      RODBC -> R\n      RODBC [label = '@@2']\n      RODBC -> Connector\n      Connector -> RODBC\n      Connector [label = '@@3']\n      Connector -> mdbfile\n      mdbfile -> Connector\n      mdbfile [label = '@@4']\n      }\n\n      ### these are the names mapping to the @@@# above\n      [1]: 'R'\n      [2]: 'RODBC package'\n      [3]: 'ODBC connector (OS specific)'\n      [4]: 'Access mdb file'\n      \")\n\n\n\nShow code\n\n      # dev.off()\n\n\nThe downward arrows pass commands and the upward arrows return information. The commands include the crucial ones to open a connection and to close it but mainly will be requests for content from the data or comamands to change it. I don’t change the mdb data at all preferring to keep any data manipulation I need for execution in R when I have pulled the information in. Information coming back back up that pipeline can be purely contextually informative, e.g. that the connection has been successfully opened/closed, or may include warnings or errors, but when everything is fine, is usually just data. You can (I did) hit the challenge that some of the data may be scored in ways that are understood by Access and SQL but not liked by R. I actually only hit that when I tried to pull tables (the name for rectangular lumps of data in databases) through to tibbles. Tibbles don’t like “binary” data which is a legitimate column content in Access. That pulls through to R dataframes but as_tibble() spits at it saying it will only accept vector data. So brace yourself if you like using tibbles as I do, to pull the data into dataframes and then work out what to do with that little challenge.\nSo what is “SQL”? To a greater or lesser extent the command side of things may or may not be using SQL. I’m not clear whether it is always formally true that it is or not but I think it’s safe to behave as though it is. SQL is Structured Query Language (https://en.wikipedia.org/wiki/SQL). You can ignore this if working in Windoze but I think that you have to use a tiny bit of SQL if working in Linux to make sure the Linux database system you use as part of the connector there is set up to work for you.\nOS dependent issues\nWindoze\nI hate to admit it but this is much easier in Windoze than I have found it so far in Ubuntu. The connector is specific the mdb file you want to access and is called a DSN (Data Source Name) and is pretty much that, actually it gives a name to the file and, vitally, it tells Windoze which particular ODBC driver Windoze will need to access the file and create the link between client program (which doesn’t have to be R with RODBC but could be anything that understands Windoze DSNs). There are various routes to create a DSN but this works for me.\nWindoze 10\nStart menu >> Windoze >> Administrative tools >> ODBC Data Sources (64 bit) then that gets you something like this. (If you have a 32 bit version of Windoze 10, you will need ODBC Data Sources (32 bit)) I think but presumably that’s the only version it will be offering you. (Scratch the interweb to see how to find out if your version of Windoze is 64 bit or 32 bit.)\nODBC Data Sources Windoze 10, screenshot 1The great thing is that you can ignore almost everything in that rather packed little bit of screen space! The main thing is that listing in the middle. In that one there are four rows now and the one that starts “EOS” is the one I created for the M$ file I want to access which is called EOS.mdb (you can call the DSN anything you want). If you have never created any you won’t see that one but you should see the other lines. To create a new DSN you hit the Add button and get to this.\nODBC Data Sources Windoze 10, screenshotChoose the Access driver (selected by default, as you see there, I think). Then hit the “Finish” button (which is a very silly label as you’re not done yet!)\nODBC Data Sources Windoze 10, screenshot 2Now you fill in the name you want for the DSN (you will need this to access the data from R), the optional description and then you hit the Select button to get here.\nODBC Data Sources Windoze 10, screenshot 3Navigate to the mdb file you want to access and select it and hit the “OK” button which gets you back to this.\nODBC Data Sources Windoze 10, screenshot 4You can see that the path to the mdb file is now in there. Now hit that “OK” button and you are done: you have created your DSN and will be able to access it using RODBC from within R.\nWindoze 11\nYou find ODBC Data Sources (64 bit) from the search icon on the task bar, after that everthing is the same as for Windoze 10.\nOpening the mdb file from within R (windoze version)\nThis gets you the idea but I’m running out of time for this bit of “work in progress”. so I will have to come back to it to explain it and go into some wrinkles there. They may be unnecessary for you.\nlibrary(RODBC) # has the vital functions to access the database\nlibrary(tidyverse) # optional but I like working in the tidyverse manner if I can\n# rm(list = ls()) # you might want to do this if you aren't using a new session and are 100% certain you won't delete anything you'll miss\n\n\n### this opens the mdb file pointed to in the DSN created in Windoze\nconnEOS <- odbcConnect(\"EOS\")\n\nallTables3 <- sqlTables(connEOS) # get a list of all the tables in the database\nallTables3\n\n### pull one table through\nsqlFetch(connEOS, \"vw_Prodotti\") %>% \n  as_tibble()\n\n### close the connection\nodbcClose(connEOS)\nLinux\nThere are clearly many ways of doing this and I found much information on the interweb. Some I could ignore as it was for commercial options and I wanted a fully open source way of doing things. In the end I gave up after a few hours failing to get it to work but I will summarise what I did in the hope that someone will tell me how to get all the way there. If I find a way that works for me I will document it fully here as I really think a clear summary is not out there. In what folllows I am mostly following https://gist.github.com/amirkdv/9672857. I think everything needs libodbc1 but I think that’s installed by default in Ubuntu.\nchris@Clevo1:sudo apt-get install libodbc1\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nlibodbc1 is already the newest version (2.3.9-5).\n0 to upgrade, 0 to newly install, 0 to remove and 0 not to upgrade.\nI think the first thing we need that isn’t installed by default is the unixodbc package.\nsudo apt-get instal unixodbc\nshould get it.\nOne thing that is useful is the mdbtools package. You can (of course) install that with:\nsudo apt-get install mdbtools\nand it gives you the command mdb-tables which lists the tables in an mdb file:\n### find the tables in your mdb file\nchris@Clevo1:mdb-tables EOS.mdb \ndbo_Richieste dbo_TabAreaScala dbo_TabCollocazioneAmbientale dbo_TabInviante dbo_TabOperatori dbo_TabPrestazioni dbo_TabProdotti dbo_TabProfessioni dbo_TabQualifiche dbo_TabSettoriEconomici dbo_TabStatiCivili dbo_TabTipoDimissione dbo_TabTipoScala dbo_Valutazione dbo_ValutazioneRisposte dbo_vw_AnamnesiQuestionario vw_CartelleLista vw_ChiaviMultiple vw_Diagnosi vw_PazientiLista vw_Prestazioni vw_Prodotti vw_ProdottiPrestazioni vw_ScaleValutazione dbo_TabDistretto dbo_TabScolarita dbo_vw_AnamnesiRisposte\n\n### find the number of rows in any particular table\nchris@Clevo1:mdb-count EOS.mdb dbo_Richieste\n49600\n\n### export any table to CSV\nchris@Clevo1:mdb-export EOS.mdb dbo_Richieste > dbo_Richieste.csv\nThat was reassuring not least in confirming that my Ubuntu 22.04 had no problem opening and reading the mdb file. In principle I believe it should be possible to create a connector a bit like a DSN in Windoze using the odbc-mdbtools package so I install that in command terminal:\nsudo apt-get odbc-mdbtools\nIn principle what you need now is to add two blocks, one to /etc/obcinst.ini which defines the driver:\n[MDBTools]\nDescription = MDBTools Driver\nDriver      = libmdbodbc.so.1\nSetup       = libmdbodbc.so.1\nFileUsage   = 1\nUsageCount  = 1\nand then one in /etc/odbc.ini which defines the source file (EOS.mdb):\n[EOS]\nDescription=EOS database\nDriver=MDBTools\nDatabase=/media/chris/Clevo_SSD2/Data/CORE/translations/Italian/Modenaplus_CORE-OM/EOS.mdb\nIn principle that should work and the interactive sql command isql from the unixodbc package ought to open the file but for me it doesn’t:\nisql -v EOS\n[IM002][unixODBC][Driver Manager]Data source name not found and no default driver specified\n[ISQL]ERROR: Could not SQLConnect\nThe RDBOC route from R gives the same message. I have tried copying those ini files to my home directory ~, i.e. /home/chris/ but that doesn’t change anything. Permissions look fine to me:\nchris@Clevo1:ls -lsart /etc/odbc*\n4 -rw-r--r-- 1 root root 145 Jun 19 15:18 /etc/odbc.bak\n4 -rw-r--r-- 1 root root 530 Jun 19 15:18 /etc/odbcinst.bak\n4 -rw-r--r-- 1 root root 139 Jun 19 16:05 /etc/odbcinst.ini~\n4 -rw-r--r-- 1 root root 132 Jun 19 16:05 /etc/odbc.ini~\n4 -rw-r--r-- 1 root root 138 Jun 19 16:21 /etc/odbc.ini\n4 -rw-r--r-- 1 root root 134 Jun 19 16:21 /etc/odbcinst.ini\nchris@Clevo1:ls -lsart ~/odbc*\n4 -rw-r--r-- 1 chris chris 139 Jun 19 16:11 /home/chris/odbcinst.ini~\n4 -rw-r--r-- 1 chris chris 133 Jun 19 16:11 /home/chris/odbcinst.ini\n4 -rw-r--r-- 1 chris chris 530 Jun 19 16:11 /home/chris/odbcinst.bak\n4 -rw-r--r-- 1 chris chris 132 Jun 19 16:11 /home/chris/odbc.ini~\n4 -rw-r--r-- 1 chris chris 326 Jun 19 16:11 /home/chris/odbc.ini\n4 -rw-r--r-- 1 chris chris 145 Jun 19 16:11 /home/chris/odbc.bak\nIf anyone can put me straight about what I’m doing wrong, I’d really appreciate it, do [contact me] (https://www.coresystemtrust.org.uk/contact-form/).\nOpening the mdb file from within R (Linux version)\nSame as in Windoze, see above.\nMore on using the database in R\nTo come.\n\n\n\n",
    "preview": "posts/2022-06-19-reading-data-into-r-from-msoft-access-mdb/pipeline_exported.png",
    "last_modified": "2023-08-25T14:20:26+02:00",
    "input_file": {},
    "preview_width": 851,
    "preview_height": 734
  },
  {
    "path": "posts/2022-04-18-exact-confidence-intervals-for-difference-between-proportions/",
    "title": "Exact confidence intervals for difference between proportions",
    "description": "This describes the ExactCIdiff package, unpacks the arguments to the functions, looks at timings and notes an oddity, a typo I think, in the original paper about the package.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-04-18",
    "categories": [
      "confidence intervals",
      "proportions",
      "exact confidence intervals"
    ],
    "contents": "\n\nContents\nBack to the story: background\nDifference between proportions in two separate samples\nThe exact CI of the difference\n\nPaired difference of proportions\nDigression about tables in R and in Rmarkdown\nExact CI for a difference between paired proportions\n\n\nComputation times\nUnpaired example: CIs and timings\n\nSummary\n\nWarning: this is pretty geeky statistical stuff\nWarning2: the package is no longer on CRAN\n## Installing package from source\nUpdate added 26.vi.22\nThe ExactCIdiff package has dropped off CRAN because the authors/maintainers aren’t responding to the CRAN team and a URL in the package is not openning for CRAN. This doesn’t affect the code in the package.\nYou can download the source package from https://cran.r-project.org/src/contrib/Archive/ExactCIdiff/. It’s the last version, v1.3 you want which is https://cran.r-project.org/src/contrib/Archive/ExactCIdiff/ExactCIdiff_1.3.tar.gz\n### you can download it before launching R:\n### install.packages(\"~/Downloads/ExactCIdiff_1.3.tar.gz\", repos = NULL, type = \"source\")\n### replace \"~/Downloads/\" with the directory into which you downloaded it of course!\n\n### or, as I should have remembered, you can omit downloading the package beforehand and just grab it withing the \n### install.packages() call:\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/ExactCIdiff/ExactCIdiff_1.3.tar.gz\", repos = NULL, type = \"source\")\n\nThat’s that solved!\nBack to the story: background\nI came on this as Clara and I need to look at the differences in proportions between two arms of our “elephant” study (we call it that because it’s BIG!) I knew that I wasn’t confident about the best way to get 95% confidence intervals (CIs) for a difference between two proportions. I have been using Hmisc::binconf() for certainly over ten years for CIs around a single proportion but knew this was a bit different. A bit of searching led to the ExactCIdiff package and to the paper about it: Shan, G., & Wang, W. (2013). ExactCIdiff: An R Package for Computing Exact Confidence Intervals for the Difference of Two Proportions. The R Journal, 5(2), 62. https://doi.org/10.32614/RJ-2013-026.\nI can’t follow the maths of the method but I do follow the evidence that it does better in terms of coverage probability (the actual probability that it will include the population value) than other methods. It’s a clean package and a nice paper and I say it’s a clean package as it appears to do just two things and the two things it sets out to do, and to do them well.\nThe two things are to give you a CI around an observed difference in proportions for a paired sample (e.g. proportion above a cut-off at baseline and after therapy) or for the same but for unconnected samples (what we have: students versus non-student people of the same age group).\nI will start with the latter.\nDifference between proportions in two separate samples\nI’ll quote from their paper:\n\nThe second data set is from a two-arm randomized clinical trial for testing the effect of tobacco smoking on mice (Essenberg, 1952). In the treatment (smoking) group, the number of mice is n 1 = 23, and the number of mice which developed tumor is x = 21; in the control group, n 2 = 32 and y = 19.\n\nUgh, not my world but it’s their paper. So here’s their data, rows as smoking/non-smoking groups and columns as whether the poor things developed tumours.\n\n\nShow code\n\ntribble(~smoking, ~tumour, ~n,\n        1, 1, 21,\n        1, 0, 2,\n        0, 1, 19,\n        0, 0, 13) %>%\n  uncount(n) -> tibPoorMice\n\ntibPoorMice %>%\n  tabyl(smoking, tumour) %>%\n  adorn_totals(where = c(\"row\", \"col\")) \n\n smoking  0  1 Total\n       0 13 19    32\n       1  2 21    23\n   Total 15 40    55\n\nAnd with percentages.\n\n\nShow code\n\ntibPoorMice %>%\n  tabyl(smoking, tumour) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"row\") %>%\n  adorn_pct_formatting(digits = 1)\n\n smoking     0     1  Total\n       0 40.6% 59.4% 100.0%\n       1  8.7% 91.3% 100.0%\n   Total 27.3% 72.7% 100.0%\n\nPretty clear that’s going to be statistically significant … and it is.\n\n\nShow code\n\ntibPoorMice %>%\n  tabyl(smoking, tumour) %>%\n  chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  .\nX-squared = 5.3625, df = 1, p-value = 0.02057\n\nBut what we want is the confidence interval. Here are the separate intervals.\n\n\nShow code\n\ntibPoorMice %>% \n  group_by(smoking) %>%\n  ### bit back to square one given I stared with the raw numbers but \n  ### this is how it would be with real data\n  summarise(n = n(),\n            nTumour = sum(tumour == 1),\n            ### this is a bit messy, Hmisc::binconf returns a matrix\n            ### but I just want the first row, hence the \"[1,]\"\n            ### and of course I need the list as that's how dplyr\n            ### has to be told that what it's getting is a list or\n            ### a vector (as here)\n            binconf = list(Hmisc::binconf(nTumour, n)[1,])) %>%\n  ### OK, unnest that\n  unnest_wider(binconf) %>%\n  ### recode smoking to a factor\n  mutate(smoking = ordered(smoking,\n                           levels = 0:1,\n                           labels = c(\"Smoked mice\",\n                                      \"Lucky ones\"))) -> tmpTib \n\n### print that\ntmpTib %>%\n  ### do some rounding across the three values PointEst to Upper\n  mutate(across(PointEst:Upper, round, 2)) %>%\n  pander(justify = \"rrrrrr\")\n\nsmoking\nn\nnTumour\nPointEst\nLower\nUpper\nSmoked mice\n32\n19\n0.59\n0.42\n0.74\nLucky ones\n23\n21\n0.91\n0.73\n0.98\n\nAnd here as a plot as I am such a believer in offering both tabulated and plotted data where possible as some people find tables easier to digest (more precision) and others find the plots easier (more, hm, visual impact!)\n\n\nShow code\n\n### get overall proportion for reference line\ntibPoorMice %>%\n  summarise(nTumour = sum(tumour == 1),\n            n = n(),\n            prop = nTumour / n) %>%\n  select(prop) %>%\n  pull() -> tmpAllProp\n\nggplot(data = tmpTib,\n       aes(x = smoking, y = PointEst)) +\n  geom_point() +\n  geom_linerange(aes(ymin = Lower, ymax = Upper)) +\n  geom_hline(yintercept = tmpAllProp) +\n  scale_y_continuous(name = \"Proportion\", breaks = seq(0, 1, .1), limits = c(0,1)) +\n  xlab(\"Whether the poor mice were smoked or not!\") +\n  ggtitle(\"95% confidence intervals for proportions\")\n\n\n\nThe exact CI of the difference\nSo now (finally) we come to ExactCIdiff! The function is BinomCI and the syntax is that you give the four numbers from the crosstabulation, as n1, n2, count1, count2, so here 23, 32, 21, 19:\nBinomCI(23, 32, 21, 19, conf.level = ?, CItype = ?)\nI’m going to take things in the order that the authors do in their paper, starting with:\nuci <- BinomCI(23, 32, 21, 19, conf.level = 0.95, CItype = \"Upper\")$ExactCI so the one-sided upper 95% confidence limit.\n\n\nShow code\n\nSys.time() -> time1\nuci <- BinomCI(23, 32, 21, 19, conf.level = 0.95, CItype = \"Upper\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs1 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nuci\n\n[1] -1.00000  0.48595\n\nThat’s a one-sided interval telling me that the upper 95% confidence limit for the difference is 0.48595: big. The computations are CPU intensive, that took 1 minutes on a fairly powerful laptop. I will generally want a two-sided interval and their next three calls to BinomCI demonstrate the relationship between the two one-sided 97.5% confidence limits and the two-sided CI. First the upper 97.5% limit.\n\n\nShow code\n\nSys.time() -> time1\nu975 <- BinomCI(23, 32, 21, 19, conf.level = 0.975, CItype = \"Upper\")$ExactCI\nSys.time() -> time2\nu975\n\n[1] -1.00000  0.51259\n\nShow code\n\nelapsedTimeSecs2 <- as.numeric(difftime(time2, time1, units = \"secs\"))\n\n\nSo upper 97.5% limit 0.51259 (elapsed time 31.7 seconds).\n\n\nShow code\n\nSys.time() -> time1\nl975 <- BinomCI(23, 32, 21, 19, conf.level = 0.975, CItype = \"Lower\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs3 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nl975\n\n[1] 0.09468 1.00000\n\nShow code\n\n# [1] 0.09468 1.00000\n\n\nSo lower 97.5% limit 0.09468 (elapsed time 6.8 seconds).\n\n\nShow code\n\nSys.time() -> time1\nci95 <- BinomCI(23, 32, 21, 19)$ExactCI\nSys.time() -> time2\nelapsedTimeSecs4 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nci95\n\n[1] 0.09468 0.51259\n\nShow code\n\n# [1] 0.09468 0.51259\n\n\nAnd it can be seen there that the two-sided 95% CI is from 0.09468 to 0.51259, i.e. from the lower 97.5% CL to the upper 97.5% CL. (Elapsed time\n38.5 seconds).\nPaired difference of proportions\nHere again I’ll quote from the paper:\n\nWe illustrate the usage of the PairedCI() function to calculate the exact smallest lower one-sided confidence interval [LP , 1] for θP in (1) with the data from Karacan et al. (1976). In this study, 32 marijuana users are compared with 32 matched controls with respect to their sleeping difficulties, with n11 = 16, n12 = 9, n21 = 3, and n22 = 4. The second argument in the function is t = n11 + n22 = 20.\n\nThe “(1)” refers back to the first equation in the paper which I won’t copy in here as it would need some formatting and doesn’t really matter for our purposes.\n\n\nShow code\n\n# ```{r makeTable, results='asis'}\n# tmpVec <- c(\"\", \"Success at t2\", \"Failure at t2\", \"\",\n#             \"Success at t1\", \"N11, p11\", \"N12, p12\", \"p1 = p11 + p12\",\n#             \"Failure at t1\", \"N21, p21\", \"N22, p22\", \"\",\n#             \"\", \"p2 = p11 + p21\", \"\", \"Total, p = 1\")\n\ntmpVec <- c(\"\", \"Success at t2\", \"Failure at t2\", \"\",\n            \"Success at t1\", \"N11, p11\", \"N12, p12\", \"\",\n            \"Failure at t1\", \"N21, p21\", \"N22, p22\", \"\",\n            \"\", \"\", \"\", \"Total, p = 1\")\n\ntmpMat <- matrix(tmpVec, ncol = 4)\n# print(xtable::xtable(tmpMat, type = \"html\"))\n# print(xtable::xtable(tmpMat, getOption(\"xtable.type\", \"html\")))\n# knitr::kable(tmpMat, \"html\")\n\ntmpMat %>%\n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\")) %>%\n  # kable_styling() %>%\n  row_spec(1, align = \"c\", bold = TRUE) %>%\n  row_spec(2:4, align = \"c\") %>%\n  column_spec(1, bold = TRUE, border_right = TRUE) %>%\n  column_spec(2, border_right = TRUE) %>%\n  column_spec(3, border_right = TRUE) \n\n\n\n\nSuccess at t1\n\n\nFailure at t1\n\n\n\n\nSuccess at t2\n\n\nN11, p11\n\n\nN21, p21\n\n\n\n\nFailure at t2\n\n\nN12, p12\n\n\nN22, p22\n\n\n\n\n\n\n\n\n\n\nTotal, p = 1\n\n\nI do find this way of describing a contingency table pretty counterinuitive!\nDigression about tables in R and in Rmarkdown\nGRRrrrr!!! I continue to feel that table handling in R is almost its Achilles heel. I’ve just wasted the better part of an hour finding out a way to get that table in an even halfway, no quarterway, decent form. I think I first commented on this perhaps twenty years ago and the R team position has always been, I think, that nice tables are for packages to fix and so we have multiple packages that try to fix this, mostly incompatible and none of them working reliably in Rmarkdown and with all output formats from Rmarkdown. I think the R afficionados all love knocking up tables in LaTeX and I’m sure that’s fine if you are really familiar with LaTeX and I suspect that direct R to LaTeX is the most robust and general way to do things but many of us don’t know TeX/LaTeX and don’t really want to have to learn it. Aarghhhh! OK, flame over!\nBack to the data here.\n\n\nShow code\n\ntmpVec2 <- c(\"\", \"Sleep OK, no smokes\", \"Sleep poor, no smokes\", \"\",\n            \"Sleep OK, smokes\", \"16\", \"9\", \"\",\n            \"Sleep poor, smokes\", \"3\", \"4\", \"\",\n            \"\", \"\", \"\", \"Total, p = 1\")\n\ntmpMat2 <- matrix(tmpVec2, ncol = 4)\n# print(xtable::xtable(tmpMat, type = \"html\"))\n# print(xtable::xtable(tmpMat, getOption(\"xtable.type\", \"html\")))\n# knitr::kable(tmpMat, \"html\")\n\ntmpMat2 %>%\n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\")) %>%\n  # kable_styling() %>%\n  row_spec(1, align = \"c\", bold = TRUE) %>%\n  row_spec(2:4, align = \"c\") %>%\n  column_spec(1, bold = TRUE, border_right = TRUE) %>%\n  column_spec(2, border_right = TRUE) %>%\n  column_spec(3, border_right = TRUE) \n\n\n\n\nSleep OK, smokes\n\n\nSleep poor, smokes\n\n\n\n\nSleep OK, no smokes\n\n\n16\n\n\n3\n\n\n\n\nSleep poor, no smokes\n\n\n9\n\n\n4\n\n\n\n\n\n\n\n\n\n\nTotal, p = 1\n\n\nExact CI for a difference between paired proportions\nThat means that the code is:\nPairedCI(9, 20, 3, conf.level = 0.95)\nbecause the syntax is\nPairedCI(n12, t, n21, conf.level, CItype, precision, grid.one, grid.two)\nwhere we can ignore grid.one and grid.two for now and leave them at their default values of 30 and 20 and precision is, as the help says:\nPrecision of the confidence interval, default is 0.00001 rounded to 5 decimals.\nOK, so here we go with:\nPairedCI(9, 20, 3, conf.level = 0.95)\n\n\nShow code\n\nSys.time() -> time1\nlciall <- PairedCI(9, 20, 3, conf.level = 0.95) # store relevant quantities\nSys.time() -> time2\nelapsedTimeSecs5 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nlciall\n\n$conf.level\n[1] 0.95\n\n$CItype\n[1] \"Two.sided\"\n\n$estimate\n[1] 0.1875\n\n$ExactCI\n[1] -0.03564  0.39521\n\nShow code\n\n# $conf.level\n# [1] 0.95\n# \n# $CItype\n# [1] \"Two.sided\"\n# \n# $estimate\n# [1] 0.1875\n# \n# $ExactCI\n# [1] -0.03564  0.39521\n\n\n(Elapsed time\n15.8 seconds.)\nThe odd thing here is that this is not what the authors show in the paper:\nlciall  # print lciall  \n$conf.level  \n[1] 0.95    # confidence level  \n$CItype  \n[1] \"Lower\" # lower one-sided interval    \n$estimate  \n[1] 0.1875  # the mle of p1 - p2  \n$ExactCI \n[1] 0.00613 1.00000 # the lower one-sided 95% interval  \nlci <- lciall$ExactCI # extracting the lower one-sided 95% interval  \nlci         # print lci  \n[1] 0.00613 1.00000  \n\nThe use of marijuana helps sleeping because the interval [ 0.00613, 1 ] for θP is positive.\n\nWhich is clearly not what I just got. However, in the paper they go on:\nThe upper one-sided 95% interval and the two-sided 95% interval for θ~P~ are given below for illustration purpose.\nI think that’s a typo. I think what they are showing are the results of\nPairedCI(9, 20, 3, conf.level = 0.95, CItype = \"lower\")\nLet’s see:\n\n\nShow code\n\nSys.time() -> time1\nlciall <- PairedCI(9, 20, 3, conf.level = 0.95, CItype = \"Lower\") # store relevant quantities\nSys.time() -> time2\nelapsedTimeSecs5 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nlciall\n\n$conf.level\n[1] 0.95\n\n$CItype\n[1] \"Lower\"\n\n$estimate\n[1] 0.1875\n\n$ExactCI\n[1] 0.00613 1.00000\n\nYes! (Elapsed time\n3.5 seconds.)\nThey do go on to give us other things in the paper and that I think confirms that the above call was a typo.\nSo here is the upper 95% CL.\n\n\nShow code\n\nSys.time() -> time1\nuci <- PairedCI(9, 20, 3, conf.level = 0.95, CItype = \"Upper\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs7 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nuci\n\n[1] -1.00000  0.36234\n\nShow code\n\n# [1] -1.00000  0.36234\n\n\n(Elapsed time\n12.5 seconds.)\nThe upper 97.5% CL.\n\n\nShow code\n\nSys.time() -> time1\nu975 <- PairedCI(9, 20, 3, conf.level = 0.975, CItype = \"Upper\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs8 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nu975\n\n[1] -1.00000  0.39521\n\nShow code\n\n# [1] -1.00000  0.39521\n\n\n(Elapsed time\n12.2 seconds.)\nThe lower 97.5% CL.\n\n\nShow code\n\nSys.time() -> time1\nl975 <- PairedCI(9, 20, 3, conf.level = 0.975, CItype = \"Lower\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs9 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nl975\n\n[1] -0.03564  1.00000\n\nShow code\n\n# [1] -0.03564  1.00000\n\n\n(Elapsed time\n3.4 seconds.)\nAnd back to the two-sided 95% CI (and yes, I’m running it again just to be sure I get the same answer as last time!)\n\n\nShow code\n\nSys.time() -> time1\nci95 <- PairedCI(9, 20, 3, conf.level = 0.95)$ExactCI\nSys.time() -> time2\nelapsedTimeSecs10 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nci95\n\n[1] -0.03564  0.39521\n\nShow code\n\n# [1] -0.03564  0.39521\n\n\n(Elapsed time\n15.6 seconds.)\nYup, the same again and fits with what they say in the paper:\n[1] -0.03564 0.39521 # the two-sided 95% interval\n                      # it is equal to the intersection of two one-sided intervals\nComputation times\nClearly one issue here is that these are small total sample sizes in their two examples but the process is computationally expensive (though 40x faster than another approach with the same accuracy/coverage).\n\n\nShow code\n\nvecMultipliers <- 1:4 # check for sample sizes 1 to 8x the example above\nvecTimesPaired <- rep(NA, length(vecMultipliers))\nmatCI95paired <- matrix(rep(NA, length(vecMultipliers) * 2), ncol = 2)\n\nvecTimesPaired[1] <- elapsedTimeSecs10\nmatCI95paired[1, ] <- ci95\n\nfor (mult in vecMultipliers[-1]) {\n  Sys.time() -> time1\n  matCI95paired[mult, ] <- PairedCI(mult * 9, mult * 20, mult * 3, conf.level = 0.95)$ExactCI\n  Sys.time() -> time2\n  vecTimesPaired[mult] <- as.numeric(difftime(time2, time1, units = \"secs\"))\n}\n\n\nHere are those CIs getting tighter as the numbers go up.\n\n\nShow code\n\nmatCI95paired %>%\n  as_tibble() %>%\n  bind_cols(vecMultipliers) %>%\n  rename(nPairs = `...3`,\n         LCL = V1,\n         UCL = V2) %>%\n  mutate(nPairs = 32 * nPairs) %>%\n  select(nPairs, everything()) -> tibCI95paired\n\ntibCI95paired %>%\n  pander::pander()\n\nnPairs\nLCL\nUCL\n32\n-0.03564\n0.3952\n64\n0.03751\n0.3396\n96\n0.06301\n0.3067\n128\n0.07231\n0.2906\n\nShow code\n\nggplot(data = tibCI95paired,\n       aes(x = nPairs)) +\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) +\n  geom_hline(yintercept = 0.1875) +\n  ylab(\"Difference in proportions\") +\n  scale_x_continuous(name = \"Number of pairs\",\n                     breaks = vecMultipliers * 32) +\n  ggtitle(\"Two sided 95% CI tightening with increasing sample size\",\n          subtitle = \"Horizontal reference line is observed difference in proportions\")\n\n\n\nAnd here are the times (in seconds).\n\n\nShow code\n\nlibrary(tidyverse)\nvecTimesPaired %>%\n  as_tibble() %>%\n  rename(timeSecs = value) %>%\n  bind_cols(vecMultipliers) %>%\n  rename(nPairs = `...2`) %>%\n  mutate(nPairs = 32 * nPairs) %>% \n  select(nPairs, timeSecs) -> tibTimesPaired\n\ntibTimesPaired %>%\n  pander(justify = \"rr\")\n\nnPairs\ntimeSecs\n32\n15.63\n64\n167.3\n96\n243.8\n128\n550.7\n\nShow code\n\nggplot(data = tibTimesPaired,\n       aes(x = nPairs, y = timeSecs)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"Elapsed time (seconds)\") +\n  xlab(\"Number of pairs of participants\") +\n  ggtitle(\"Plot of computation time against sample size\")\n\n\n\nHm. That doesn’t look that far off linear which is not what I had expected.\nUnpaired example: CIs and timings\n\n\nShow code\n\nci95 <- BinomCI(23, 32, 21, 19)$ExactCI\n\nvecTimesUnpaired <- rep(NA, length(vecMultipliers))\nmatCI95unpaired <- matrix(rep(NA, length(vecMultipliers) * 2), ncol = 2)\n\nfor (mult in vecMultipliers) {\n  Sys.time() -> time1\n  matCI95unpaired[mult, ] <- PairedCI(mult * 9, mult * 20, mult * 3, conf.level = 0.95)$ExactCI\n  Sys.time() -> time2\n  vecTimesUnpaired[mult] <- as.numeric(difftime(time2, time1, units = \"secs\"))\n}\n\n\n\n\nShow code\n\nmatCI95unpaired %>%\n  as_tibble() %>%\n  bind_cols(vecMultipliers) %>%\n  rename(nTotal = `...3`,\n         LCL = V1,\n         UCL = V2) %>%\n  mutate(nTotal = 55 * nTotal) %>%\n  select(nTotal, everything()) -> tibCI95unpaired\n\ntibCI95unpaired %>%\n  pander::pander()\n\nnTotal\nLCL\nUCL\n55\n-0.03564\n0.3952\n110\n0.03751\n0.3396\n165\n0.06301\n0.3067\n220\n0.07231\n0.2906\n\nShow code\n\nggplot(data = tibCI95unpaired,\n       aes(x = nTotal)) +\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) +\n  geom_hline(yintercept = 0.1875) +\n  ylab(\"Difference in proportions\") +\n  scale_x_continuous(name = \"Total number of mice\",\n                     breaks = vecMultipliers * 32) +\n  ggtitle(\"Two sided 95% CI tightening with increasing sample size\",\n          subtitle = \"Horizontal reference line is observed difference in proportions\")\n\n\n\nAnd, again, the times.\n\n\nShow code\n\nlibrary(tidyverse)\nvecTimesUnpaired %>%\n  as_tibble() %>%\n  rename(timeSecs = value) %>%\n  bind_cols(vecMultipliers) %>%\n  rename(nTotal = `...2`) %>%\n  mutate(nTotal = 32 * nTotal) %>% \n  select(nTotal, timeSecs) -> tibTimesPaired\n\ntibTimesPaired %>%\n  pander(justify = \"rr\")\n\nnTotal\ntimeSecs\n32\n15.46\n64\n170.1\n96\n244.9\n128\n545.6\n\nShow code\n\nggplot(data = tibTimesPaired,\n       aes(x = nTotal, y = timeSecs)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"Elapsed time (seconds)\") +\n  xlab(\"Number of mice\") +\n  ggtitle(\"Plot of computation time against sample size\")\n\n\n\nAgain, looks fairly linear. Interesting.\nSummary\nThe R package ExactCIdiff provides two functions which give what appear to be the best confidence intervals for differences between two proportions, one function, BinomCI() for differences from unpaired samples and the other PairedCI() for paired samples. I think there’s a typo in the paper about the package and the syntax of the arguments isn’t particularly friendly (it’s even case sensitive so CItype = \"upper\" with throw an error, it has to be CItype = \"Upper\"). However, it’s not difficult to work those things out (and that’s partly why I’ve created this post) and it does seem that these really are the best ways to get these CIs. They’re faily computationally intensive but from my tiny simulation it looks as if the timing is linear across simple multiples of sample size. Thanks and kudos to Shan and Wang!\n\n\n\n",
    "preview": "posts/2022-04-18-exact-confidence-intervals-for-difference-between-proportions/exact-confidence-intervals-for-difference-between-proportions_files/figure-html5/separateIntervals2-1.png",
    "last_modified": "2023-08-25T14:20:02+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-05-prepost-analyses/",
    "title": "Pre/post analyses",
    "description": "Teaching myself about pre/post therapy change analyses using R.\nProbably the first of a series of posts.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-02-12",
    "categories": [
      "change scores",
      "pre/post change"
    ],
    "contents": "\n\nContents\nBackground\nPlan\nGeneration of the data\nExploration of the data\nGender\nAge\nNumbers of sessions\nLook at distributions of scores and how these relate to predictors\nGender\nAge\nCheck on change scores I’ve created\nWhat about change scores themselves?\nAge\n\n\n\nTesting for the effects\nStart over from simplest model and build up\n\n\nStarted 5.ii.22, latest update 12.ii.22, still work in progress but worth mounting to illustrate some of the issues\nBackground\nI was being very slow talking with Emily (Dr. Blackshaw now) about some pre/post analyses she is doing so I realised I should take my ageing brain for some gentle walking pace exercise about pre/post change analyses!\nHer dataset is fairly large and has first and last session CORE scores (CORE-10 or YP-CORE, analysing each dataset separately). She has been asked to look at the impacts on change of the numbers of sessions attended, gender and age.\nDoing this has been helpful to me in thinking through the challenges of testing for effects of even a very small set of predictor variables on pre/post change scores. I hope it may be useful to others on that level of exploring the issues. I also hope that the code both for the exploratory/descriptive graphics, and the effect testing, will be useful to others.\nPlan\nThis is an emerging document and I think it will spin off some separate posts, it’s also a large post and at the moment splits into three sections:\nGeneration of the data. This is really just here to make that process transparent and provide the code but unless you are particularly interested in this sort of thing you can ignore the code and get through this section very fast.\nExploration of the data. With real data I like to “see” the data and not just get jumped into ANOVA tables. Here I was also doing it to get a visual sense of the effects I had created and to be sure my simulation had worked. With real data this helps see where data doesn’t fit the distributional models in the analyses (usually assumptions of Gaussian distributions and linear effects). I have modelled in a quadratic effect, a U-shaped relationship between baseline score and age, but otherwise the data are simulated so are all pretty squeaky clean so skim this too if you want but I encourage you to look at your own data carefully with these sorts of graphics before jumping to modelling.\nTesting for the effects. This was what got me into this self-answered reassurance journey. I was hoping that effect plots would be helpful but got into the issue of interactions so this section is really still taking shape.\nGeneration of the data\nThis code block generates the baseline scores. I’m using Gaussian distributions which is just one of the many unrealistic aspects of this. However, I’m really doing this to explore different ways of displaying the effects and not aspiring to verisimilitude!\n\n\nShow code\n\n### these vectors create population proportions from which to sample with sample()\nvecSessions <- c(rep(2, 40), # I have made these up, I have no idea how realistic they are\n                 rep(3, 30),\n                 rep(4, 20),\n                 rep(3, 15),\n                 rep(4, 10),\n                 rep(5, 8),\n                 rep(6, 7),\n                 rep(7, 5),\n                 rep(8, 3),\n                 rep(9, 2),\n                 rep(10, 1))\nvecAge <- c(rep(15, 20), # ditto\n            rep(16, 25),\n            rep(17, 25),\n            rep(18, 30),\n            rep(19, 35),\n            rep(20, 30),\n            rep(21, 30),\n            rep(22, 33),\n            rep(23, 29),\n            rep(24, 20),\n            rep(25, 18))\nvecGender <- c(rep(\"F\", 63), # ditto\n               rep(\"M\", 30),\n               rep(\"Other\", 7))\n\nnGenders <- length(vecGender)\nnAges <- length(15:25) # lazy but does make tweaking the model later easier!\nnSessLengths <- length(2:20) # ditto\nnCells <- nGenders * nAges * nSessLengths\n\navCellSize <- 20 # trying to make things big enough\npopulnSize <- nCells * avCellSize\n\n### build scores from a Gaussian base variable\nlatentMean <- 0\nlatentSD <- 1\n\n### add baseline differences\n### gender has female as reference vale\neffBaseFemaleMean <- 0\neffBaseFemaleSD <- 1\neffBaseMaleMean <- -.25\neffBaseMaleSD <- 1.8\neffBaseOtherMean <- .35\neffBaseOtherSD <- 2\n### model age as a quadratic\nminAge <- min(vecAge)\nmidAge <- mean(vecAge)\neffBaseAgeMeanMult <- .03\neffBaseAgeSD <- 1\n\n### now create model for change effects\n### start with noise to add to baseline score\nchangeFuzzMean <- .1\nchangeFuzzSD <- .2\n### now gender effects on change\neffChangeFemaleMean <- -.8\neffChangeFemaleSD <- 1\neffChangeMaleMean <- effChangeFemaleMean + .2 # smaller improvement for men\neffChangeMaleSD <- 1.5 # more variance in male change\neffChangeOtherMean <- effChangeFemaleMean - .3 # better for \"other\"\neffChangeOtherSD <- 1\n\n### model age as a quadratic again\neffChangeAgeMeanMult <- .1\neffChangeAgeMeanSD <- 1\n\n### model effect of number of sessions as linear\nminSessions = min(vecSessions)\neffChangeSessionsMult <- -.15\neffChangeSessionsSD  <- 1\n\n### build the sample\nset.seed(12345) # reproducible sample\n### get the basics\nas_tibble(list(ID = 1:populnSize,\n               gender = sample(vecGender, populnSize, replace = TRUE),\n               age = sample(vecAge, populnSize, replace = TRUE),\n               nSessions = sample(vecSessions, populnSize, replace = TRUE),\n               ### now build baseline scores\n               baseLatent = rnorm(populnSize, mean = latentMean, sd = latentSD))) -> tibSimulnVars\n\n### now use those to build baseline data\ntibSimulnVars %>%\n  ### create effect of gender\n  mutate(gendEffect = case_when(gender == \"F\" ~ rnorm(populnSize, mean = effBaseFemaleMean, sd = effBaseFemaleSD),\n                                gender == \"M\" ~ rnorm(populnSize, mean = effBaseMaleMean, sd = effBaseMaleSD),\n                                gender == \"Other\" ~ rnorm(populnSize, mean = effBaseOtherMean, sd = effBaseOtherSD)),\n         ### this is just creating factors, useful when plotting\n         Age = factor(age),\n         facSessions = factor(nSessions)) %>%\n  rowwise() %>%\n  ### create effect of age\n  ### I am, a bit unrealistically, assuming that number of sessions doesn't affect baseline score (nor v.v.)\n  mutate(ageEffect = effBaseAgeMeanMult * rnorm(1, \n                                                mean = (age - midAge)^2, # centred quadratic\n                                                sd = effBaseAgeSD),\n         first = baseLatent + gendEffect + ageEffect) %>%\n  ungroup() -> tibBaselineScores\n\n\n\nNow create the change scores to create the final scores. First time around I went straight to create the final scores, easier to follow this way.\n\n\nShow code\n\ntibBaselineScores %>%\n  ### create basic change scores with noise to add to the baseline scores\n  mutate(change =rnorm(populnSize, \n                       mean = changeFuzzMean,\n                       sd = changeFuzzSD)) %>%\n  ### now add effect of gender on change\n  mutate(gendChangeEffect = case_when(gender == \"F\" ~ rnorm(populnSize, \n                                                            mean = effChangeFemaleMean, \n                                                            sd = effChangeFemaleSD),\n                                      gender == \"M\" ~ rnorm(populnSize, \n                                                            mean = effChangeMaleMean, \n                                                            sd = effChangeMaleSD),\n                                      gender == \"Other\" ~ rnorm(populnSize, \n                                                                mean = effChangeOtherMean, \n                                                                sd = effChangeOtherSD))) %>%\n  rowwise() %>%\n  ### add effect of age\n  mutate(ageChangeEffect = effChangeAgeMeanMult * rnorm(1, \n                                                        mean = (age - midAge)^2, \n                                                        sd = effChangeAgeMeanSD),\n         ### add effect of number of sessions\n         sessionChangeEffect = effChangeSessionsMult * rnorm(1,\n                                                             mean = nSessions - minSessions,\n                                                             sd = effChangeSessionsSD),\n         change = change + gendChangeEffect + ageChangeEffect + sessionChangeEffect,\n         last = first + change) %>%\n  ungroup() -> tibDat\n\n\n\nI’ve simulated a very unrealistic dataset of total size 418000 with a three way gender classification and age ranging from 15 to 25 and numbers of sessions from 2 to 10.\nExploration of the data\nThis was to check my simulation but I think it’s good practice with any dataset to explore it graphically and thoroughly yourself and ideally to make some of that exploration available to others, some in a main paper or report, some in supplementary materials anyone can get to.\nFirst check the breakdown by the predictor variables: gender, age and number of sessions. With a real world dataset I’d check for systematic associations between these variables, e.g. is the age distribution, or the numbers of sessions attended different by gender? Does the number of sessions attended relate to age? However, I built in no non-random associations so I haven’t done that exploration here.\nGender\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(n = n()) %>%\n  mutate(nText = str_c(\"n = \", n),\n         yPos = n * .8) -> tmpTibN\n\nggplot(data = tibDat,\n       aes(x = gender, fill = gender))+\n  geom_histogram(stat = \"count\") +\n  geom_text(data = tmpTibN,\n             aes(x = gender, y = yPos, label = nText))\n\n\n\n\nOK.\nAge\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = age, fill = gender))+\n  geom_histogram(stat = \"count\") +\n  scale_x_continuous(breaks = vecAge, labels = as.character(vecAge))\n\n\n\n\nI haven’t created any systematic association between age and gender.\nNumbers of sessions\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = nSessions, fill = gender))+\n  geom_histogram(stat = \"count\") +\n  scale_x_continuous(breaks = vecSessions, labels = as.character(vecSessions))\n\n\n\n\nI haven’t created any systematic association between number of sessions and gender, nor with age.\nLook at distributions of scores and how these relate to predictors\nChecking distributions is a bit silly here as we know I’ve created samples from Gaussian distributions, however with real world data really marked deviations from Gaussian distributions would clarify that caution would be needed for any tests or confidence intervals when we look at effects on change.\nStart with “first”, i.e. baseline score.\n\n\nShow code\n\n### using a tweak to be able to fit a Gaussian density using ggplot::geom_density() while wanting \n### counts on the y axis not density\n### found the answer at\n###   https://stackoverflow.com/questions/27611438/density-curve-overlay-on-histogram-where-vertical-axis-is-frequency-aka-count\n### it involves using a multiplier which is n * binwidth\n\n### this next is actually based on https://stackoverflow.com/questions/6967664/ggplot2-histogram-with-normal-curve\ntmpBinWidth <- .5\ntibDat %>%\n  summarise(n = n(),\n            mean = mean(first),\n            sd = sd(first)) -> tmpTibStats\n\nggplot(data = tibDat,\n       aes(x = first)) +\n  # geom_histogram(binwidth = tmpBinWidth) #+\n  geom_histogram(binwidth = tmpBinWidth) + #-> p\n  stat_function(fun = function(x) dnorm(x, mean = tmpTibStats$mean, sd = tmpTibStats$sd) * tmpTibStats$n * tmpBinWidth,\n    color = \"green\", \n    linewidth = 1) +\n  geom_vline(xintercept = mean(tibDat$first),\n             colour = \"blue\") +\n  xlab(\"Baseline scores\") +\n  ggtitle(\"Histogram of all baseline scores\",\n          subtitle = \"Blue vertical reference line marks mean\")\n\n\n\nI amused myself by adding the best Gaussian distribution fit so I can\nand tests of fit but clearly no major problem there but that’s all overkill here so I haven’t. Now what about the effects of predictors?\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = gender)) +\n  facet_grid(rows = vars(gender),\n             scales = \"free_y\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of baseline scores against gender\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nWe can see the relationship between mean baseline score and gender. I have set the Y axis as free, i.e. can be different in each facet of the plot, as numbers in each gender category vary a lot.\n\n\nShow code\n\ntibDat %>%\n  group_by(Age) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = Age)) +\n  facet_grid(rows = vars(Age),\n             scales = \"free_y\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of baseline scores against age\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nFine! And just to pander to obsessionality here’s a facetted histogram by both age and gender.\n\n\nShow code\n\ntibDat %>%\n  group_by(Age, gender) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = Age)) +\n  facet_grid(rows = vars(Age),\n             cols = vars(gender),\n             scales = \"free\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of baseline scores against age\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nThat’s just silly! (And it’s very small here, would be lovely if distill created large plots that would open if the small plot is clicked. I think that’s beyond my programming skills.)\n\n\nShow code\n\ntibDat %>%\n  group_by(nSessions) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = nSessions)) +\n  facet_grid(rows = vars(nSessions),\n             scales = \"free\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of baseline scores against number of sessions\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nOK. Now same for final scores.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = last)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tibDat$last),\n             colour = \"blue\") +\n  xlab(\"Final scores\") +\n  ggtitle(\"Histogram of all final scores\",\n          subtitle = \"Blue vertical reference line marks mean\")\n\n\n\n\nOK.\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = gender)) +\n  facet_grid(rows = vars(gender),\n             scales = \"free_y\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of final scores against gender\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nOK again.\n\n\nShow code\n\ntibDat %>%\n  group_by(Age) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = Age)) +\n  facet_grid(rows = vars(Age),\n             scales = \"free_y\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of final scores against age\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nAnd again.\n\n\nShow code\n\ntibDat %>%\n  group_by(Age, gender) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = Age)) +\n  facet_grid(rows = vars(Age),\n             cols = vars(gender),\n             scales = \"free\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of final scores against age\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nStill silly!\n\n\nShow code\n\ntibDat %>%\n  group_by(nSessions) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = nSessions)) +\n  facet_grid(rows = vars(nSessions),\n             scales = \"free\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of final scores against number of sessions\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nGetting lumpy where the cell sizes are getting small of course but fine.\nGender\n\n\nShow code\n\n### get means and bootstrap CIs for baseline gender effect\nset.seed(12345) # reproducible bootstrap\nsuppressWarnings(tibBaselineScores %>%\n                   group_by(gender) %>%\n                   summarise(mean = mean(first),\n                             CI = list(getBootCImean(first, \n                                                     nGT10kerr = FALSE,\n                                                     verbose = FALSE))) %>%\n                   unnest_wider(CI) -> tmpTibMeans)\n\n\nggplot(data = tibBaselineScores,\n       aes(x = gender, y = first)) +\n  geom_violin(aes(fill = gender),\n              scale = \"count\") +\n  geom_hline(yintercept = mean(tibBaselineScores$first)) +\n  geom_point(data = tmpTibMeans,\n             aes(y = mean)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = gender,\n                     ymin = LCLmean, \n                     ymax = UCLmean)) +\n  ylab(\"Baseline score\") +\n  xlab(\"Gender\") +\n  ggtitle(\"Violin plot to check baseline gender differences\",\n          subtitle = \"points are means, tiny vertical lines are 95% bootstrap CI of means\")\n\n\n\n\nOK. It’s not very visible but there is a small baseline gender effect and the confidence intervals are so tight that they are just about invisible.\nAge\n\n\nShow code\n\n### get means and bootstrap CIs for baseline age effect\nset.seed(12345) # reproducible bootstrap\nsuppressWarnings(tibBaselineScores %>%\n                   group_by(Age) %>%\n                   summarise(mean = mean(first),\n                             CI = list(getBootCImean(first, \n                                                     nGT10kerr = FALSE,\n                                                     verbose = FALSE))) %>%\n                   unnest_wider(CI) -> tmpTibMeans)\n\n\nggplot(data = tibBaselineScores,\n       aes(x = Age, y = first)) +\n  geom_violin(aes(fill = Age),\n              scale = \"count\") +\n  geom_hline(yintercept = mean(tibBaselineScores$first)) +\n  geom_point(data = tmpTibMeans,\n             aes(y = mean)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = Age,\n                     ymin = LCLmean, \n                     ymax = UCLmean)) +\n  ylab(\"Baseline score\") +\n  xlab(\"Age\") +\n  ggtitle(\"Violin plot to check baseline gender differences\",\n          subtitle = \"points are means, tiny vertical lines are 95% bootstrap CI of means\")\n\n\n\n\nSmall and very unrealistic quadratic (U shaped) effect of age on baseline scores.\nCheck on change scores I’ve created\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(meanFirst = mean(first),\n            meanLast = mean(last),\n            CIfirst = list(getBootCImean(first, \n                                         nGT10kerr = FALSE,\n                                         verbose = FALSE)),\n            CIlast = list(getBootCImean(last, \n                                        nGT10kerr = FALSE,\n                                        verbose = FALSE))) %>%\n  unnest_wider(CIfirst) %>%\n  ### got to rename to avoid name collision\n  rename(obsmeanFirst = obsmean,\n         LCLmeanFirst = LCLmean,\n         UCLmeanFirst = UCLmean) %>%\n  unnest_wider(CIlast) %>%\n  ### renaming now is just for clarity rather than necessity\n  rename(obsmeanLast = obsmean,\n         LCLmeanLast = LCLmean,\n         UCLmeanLast = UCLmean) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = gender, fill = gender)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  # geom_point(data = tmpTibMeans,\n  #            aes(x = meanFirst, y = meanLast),\n  #            size = 3) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = meanFirst,\n                     ymin = LCLmeanLast,\n                     ymax = UCLmeanLast)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(y = meanLast,\n                     xmin = LCLmeanFirst,\n                     xmax = UCLmeanFirst)) +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by gender\",\n          subtitle = \"Lines are linear regression by gender with 95% confidence intervals\\nCrosshairs are 95% confidence intervals of means\")\n\n\n\n\nNot a very informative plot here but it would be important with real data to plot something like this to see whether there are markedly non-linear relationships. Here it’s just about visible that I’ve created slight differences in slope of final session score on first session score by gender. I’ve put in the means (of first and last scores) by gender which helps remind us of the horizontal shift of the baseline score gender differences seen above. (Cross hairs in black as the ones for the men and for the women disappear if coloured by gender.)\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = gender, fill = gender)) +\n  facet_grid(rows = vars(gender)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  # geom_point(data = tmpTibMeans,\n  #            aes(x = meanFirst, y = meanLast),\n  #            size = 3) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = meanFirst,\n                     ymin = LCLmeanLast,\n                     ymax = UCLmeanLast)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(y = meanLast,\n                     xmin = LCLmeanFirst,\n                     xmax = UCLmeanFirst)) +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by gender\",\n          subtitle = \"Lines are linear regression by gender with 95% confidence intervals\\nCrosshairs are 95% confidence intervals of means\")\n\n\n\n\nAs the main objective here is to look for major problems with the relationship between x and y variables, best to complement that with the same but facetted by gender.\nOK, no issues of non-linearities there (of course they’re not, I didn’t model them so!)\nWhat about change scores themselves?\nPlotting final scores against baseline is vital to look for non-linearities in the relationship but we are as interested in change as final scores. (Actually, we’re interested in both and of course they’re mathematically completely linearly related but the give usefully different views on this whole issue of final score and change.)\nSo plot change against first score now we have seen that the relationships between first and last scores are not markedly non-linear.\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(meanFirst = mean(first),\n            meanChange = mean(change),\n            CIfirst = list(getBootCImean(first, \n                                         nGT10kerr = FALSE,\n                                         verbose = FALSE)),\n            CIchange = list(getBootCImean(change, \n                                          nGT10kerr = FALSE,\n                                          verbose = FALSE))) %>%\n  unnest_wider(CIfirst) %>%\n  ### got to rename to avoid name collision\n  rename(obsmeanFirst = obsmean,\n         LCLmeanFirst = LCLmean,\n         UCLmeanFirst = UCLmean) %>%\n  unnest_wider(CIchange) %>%\n  ### renaming now is just for clarity rather than necessity\n  rename(obsmeanChange = obsmean,\n         LCLmeanChange = LCLmean,\n         UCLmeanChange = UCLmean) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, y = change, colour = gender, fill = gender)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  geom_point(data = tmpTibMeans,\n             aes(x = meanFirst, y = meanChange),\n             size = 3) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = meanFirst,\n                     ymin = LCLmeanChange,\n                     ymax = UCLmeanChange)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(y = meanChange,\n                     xmin = LCLmeanFirst,\n                     xmax = UCLmeanFirst)) +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of change (last - first) against first scores by gender\",\n          subtitle = \"Lines are linear regression by gender with 95% confidence intervals\")\n\n\n\n\nNow the mean points show clearly both the horizontal shifts of baseline score gender differences, but also that the change scores are different. The CIs for the female subset are so tiny they disappear but it’s clear that the differences are systematic for the change scores as well as for the baseline scores. Very slight but clear linear relationship between baseline score and change, in real life datasets I’d expect more of a relationship and that’d be an important reason for doing this plot.\nAge\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = Age, fill = Age)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by age\",\n          subtitle = \"Lines are linear regression by age with 95% confidence intervals\")\n\n\n\n\nStrong relationships and no obvious non-linearities but not an easy plot to read. Facetted plot better.\n\n\nShow code\n\ntibDat %>%\n  mutate(xmean = mean(first)) %>% # centre on x axis\n  group_by(Age) %>%\n  summarise(xmean = first(xmean), # to retain that constant\n            last = mean(last)) -> tmpTibMeans\n\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = Age, fill = Age)) +\n  facet_grid(rows = vars(Age)) +\n  geom_point(alpha = .3, size = .5) +\n  geom_smooth(method = \"lm\") +\n  geom_hline(yintercept = mean(tibDat$first)) +\n  geom_point(data = tmpTibMeans,\n             inherit.aes = FALSE,\n             aes(x = xmean, y = last)) +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by age\",\n          subtitle = \"Lines are linear regression by age with 95% confidence intervals\\nBlack reference lines are overall mean final score, points are mean by age.\")\n\n\n\n\nMain thing here is that there are no obvious nonlinearities. I have added the overall mean as a horizontal reference and the facet (age) mean as a point so we can still see that the final score mean is related to age.\nNow change scores.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = change, colour = Age, fill = Age)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of change (last - first) against first scores by age\",\n          subtitle = \"Lines are linear regression by age with 95% confidence intervals\")\n\n\n\n\n@@@ put facetted plot here later, when I have time! @@@\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = facSessions, fill = facSessions)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by n(sessions)\",\n          subtitle = \"Lines are linear regression by n(sessions) with 95% confidence intervals\")\n\n\n\n\n@@@ put facetted plot here later, when I have time! @@@\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = change, colour = facSessions, fill = facSessions)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of change (last - first) against first scores by n(sessions)\",\n          subtitle = \"Lines are linear regression by n(sessions) with 95% confidence intervals\")\n\n\n\n\n@@@ put facetted plot here later, when I have time! @@@\n\n\nShow code\n\n### get means and bootstrap CIs for effect of n(sessions) on last score\nset.seed(12345) # reproducible bootstrap\nsuppressWarnings(tibDat %>%\n                   group_by(nSessions) %>%\n                   summarise(mean = mean(change),\n                             CI = list(getBootCImean(change, \n                                                     nGT10kerr = FALSE,\n                                                     verbose = FALSE))) %>%\n                   unnest_wider(CI) -> tmpTibMeans)\n\nggplot(data = tibDat,\n       aes(x = nSessions, y = change, colour = facSessions, fill = facSessions)) +\n  geom_violin(scale = \"count\") +\n  geom_point(data = tmpTibMeans,\n             inherit.aes = FALSE,\n             aes(x = nSessions, y = mean)) +\n  geom_linerange(data = tmpTibMeans,\n             inherit.aes = FALSE,\n             aes(x = nSessions, ymin = LCLmean, ymax = UCLmean),\n             size = 1) +\n  geom_smooth(inherit.aes = FALSE,\n    aes(x = nSessions, y = change),\n    method = \"lm\",\n    colour = \"black\") +\n  xlab(\"Number of sessions\") +\n  ylab(\"Score change\") +\n  ggtitle(\"Violin plot of change (last - first) against first scores by n(sessions)\",\n          subtitle = \"Line is linear regression with 95% confidence interval\\nPoints are means with vertical lines for their bootstrap 95% confidence intervals\")\n\n\n\nShow code\n\nggsave(\"prepost1.png\")\n\n\n\nTesting for the effects\nStart with linear regression of final score on baseline score with all predictors and interactions. Age as factor.\n\n\nShow code\n\nlm(last ~ first + gender + Age + nSessions + \n     ### add two way interactions\n     gender * Age + gender * nSessions + gender * Age + Age * nSessions +\n     ### add three way interaction\n     gender * Age * nSessions, data = tibDat) -> lisLMFull\n\nsummary(lisLMFull)\n\n\n\nCall:\nlm(formula = last ~ first + gender + Age + nSessions + gender * \n    Age + gender * nSessions + gender * Age + Age * nSessions + \n    gender * Age * nSessions, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9992 -0.7800 -0.0006  0.7767  7.3645 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  1.998892   0.021443  93.217  < 2e-16 ***\nfirst                        0.997563   0.001098 908.861  < 2e-16 ***\ngenderM                      0.275614   0.037636   7.323 2.43e-13 ***\ngenderOther                 -0.256914   0.065032  -3.951 7.80e-05 ***\nAge16                       -0.843495   0.028766 -29.323  < 2e-16 ***\nAge17                       -1.535725   0.028707 -53.497  < 2e-16 ***\nAge18                       -2.033553   0.027532 -73.862  < 2e-16 ***\nAge19                       -2.298519   0.026781 -85.828  < 2e-16 ***\nAge20                       -2.417854   0.027672 -87.375  < 2e-16 ***\nAge21                       -2.268483   0.027520 -82.431  < 2e-16 ***\nAge22                       -1.980524   0.027107 -73.063  < 2e-16 ***\nAge23                       -1.453611   0.027717 -52.445  < 2e-16 ***\nAge24                       -0.751826   0.030279 -24.830  < 2e-16 ***\nAge25                        0.167747   0.030820   5.443 5.25e-08 ***\nnSessions                   -0.138232   0.005449 -25.369  < 2e-16 ***\ngenderM:Age16               -0.057543   0.050395  -1.142  0.25353    \ngenderOther:Age16           -0.048794   0.088211  -0.553  0.58016    \ngenderM:Age17               -0.095745   0.050319  -1.903  0.05707 .  \ngenderOther:Age17           -0.069093   0.088763  -0.778  0.43634    \ngenderM:Age18               -0.096002   0.048294  -1.988  0.04683 *  \ngenderOther:Age18           -0.062146   0.084629  -0.734  0.46274    \ngenderM:Age19               -0.089492   0.047165  -1.897  0.05777 .  \ngenderOther:Age19           -0.050296   0.082522  -0.609  0.54220    \ngenderM:Age20               -0.045583   0.048587  -0.938  0.34815    \ngenderOther:Age20           -0.021258   0.085097  -0.250  0.80274    \ngenderM:Age21               -0.121299   0.048397  -2.506  0.01220 *  \ngenderOther:Age21           -0.130454   0.085207  -1.531  0.12577    \ngenderM:Age22               -0.144666   0.047561  -3.042  0.00235 ** \ngenderOther:Age22           -0.029070   0.083188  -0.349  0.72675    \ngenderM:Age23               -0.090242   0.048788  -1.850  0.06436 .  \ngenderOther:Age23           -0.044951   0.085015  -0.529  0.59699    \ngenderM:Age24               -0.056617   0.052857  -1.071  0.28411    \ngenderOther:Age24           -0.094066   0.093598  -1.005  0.31490    \ngenderM:Age25               -0.098099   0.054572  -1.798  0.07224 .  \ngenderOther:Age25           -0.208275   0.095924  -2.171  0.02991 *  \ngenderM:nSessions           -0.014893   0.009499  -1.568  0.11693    \ngenderOther:nSessions       -0.015892   0.016102  -0.987  0.32367    \nAge16:nSessions             -0.011043   0.007308  -1.511  0.13076    \nAge17:nSessions             -0.015418   0.007298  -2.113  0.03463 *  \nAge18:nSessions             -0.007741   0.006985  -1.108  0.26778    \nAge19:nSessions             -0.013239   0.006790  -1.950  0.05120 .  \nAge20:nSessions             -0.008232   0.007035  -1.170  0.24194    \nAge21:nSessions             -0.015335   0.006980  -2.197  0.02801 *  \nAge22:nSessions             -0.008215   0.006883  -1.193  0.23271    \nAge23:nSessions             -0.012965   0.007048  -1.839  0.06585 .  \nAge24:nSessions             -0.012906   0.007693  -1.678  0.09341 .  \nAge25:nSessions             -0.011482   0.007812  -1.470  0.14162    \ngenderM:Age16:nSessions      0.010376   0.012757   0.813  0.41600    \ngenderOther:Age16:nSessions  0.016470   0.022154   0.743  0.45724    \ngenderM:Age17:nSessions      0.022974   0.012738   1.804  0.07129 .  \ngenderOther:Age17:nSessions  0.029906   0.022300   1.341  0.17989    \ngenderM:Age18:nSessions      0.017580   0.012198   1.441  0.14954    \ngenderOther:Age18:nSessions  0.015737   0.021265   0.740  0.45927    \ngenderM:Age19:nSessions      0.016186   0.011926   1.357  0.17470    \ngenderOther:Age19:nSessions  0.012372   0.020598   0.601  0.54807    \ngenderM:Age20:nSessions      0.009190   0.012317   0.746  0.45563    \ngenderOther:Age20:nSessions  0.008813   0.021323   0.413  0.67939    \ngenderM:Age21:nSessions      0.029381   0.012220   2.404  0.01620 *  \ngenderOther:Age21:nSessions  0.036737   0.021375   1.719  0.08567 .  \ngenderM:Age22:nSessions      0.033377   0.012006   2.780  0.00543 ** \ngenderOther:Age22:nSessions  0.007113   0.020690   0.344  0.73100    \ngenderM:Age23:nSessions      0.020473   0.012334   1.660  0.09695 .  \ngenderOther:Age23:nSessions  0.020048   0.021269   0.943  0.34590    \ngenderM:Age24:nSessions      0.012999   0.013351   0.974  0.33025    \ngenderOther:Age24:nSessions  0.026838   0.023344   1.150  0.25027    \ngenderM:Age25:nSessions      0.021564   0.013813   1.561  0.11851    \ngenderOther:Age25:nSessions  0.046793   0.023952   1.954  0.05075 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.203 on 417933 degrees of freedom\nMultiple R-squared:  0.7375,    Adjusted R-squared:  0.7375 \nF-statistic: 1.779e+04 on 66 and 417933 DF,  p-value: < 2.2e-16\n\nShow code\n\n# lisLMFull$coefficients %>%\n#   as_tibble() %>% # that ignores the names so ...\n#   mutate(effect = names(lisLMFull$coefficients)) %>% # get them!\n#   select(effect, value) %>% # more sensible order\n#   rename(coefficient = value )\n\n### hm, that's done for me in broom\nbroom::tidy(lisLMFull) %>%\n  ### identify the order of the terms, i.e. two-way interaction has order 2 etc.\n  mutate(order = 1 + str_count(term, fixed(\":\")),\n         sig = if_else(p.value < .05, 1, 0)) -> tibLMFull\n\nvalNinteractions <- sum(tibLMFull$order > 1)\n\n\n\nThat’s not very digestible but it is, arguably, a sensible place to start. We can ignore the intercept really but it’s not zero!\nMore usefully, we have a very strong effect of initial score on final score, a statistically significant effect of male gender against the reference gender (female) and no statistically significant effect of gender “other” in this saturated model. The reference category for age is the lowest, age 15 and all the other ages show a statistically significantly different final score from that for age 15 except age 25. Finally, in the simple effects, we have a statistically significant effect of number of sessions on final score with coefficient estimate -0.138, i.e. a drop of about that in mean final score for every one more session attended. (Remember the final scores here distribute between -12.34 and 12.31 with SD 2.35 so I appear to have modelled in a pretty small effect of nSessions.\nThe complication is all those statistically significant interactions in this saturated model. We have 67 terms, including the intercept, 14 simple effects (ignoring the intercept) and 52 interactions, 32 two-way interactions and 20 three-way interactions. Here’s the breakdown of the numbers significant.\n\n\nShow code\n\ntibLMFull %>%\n  group_by(order) %>%\n  summarise(n = n(),\n            nSignif = sum(sig),\n            propn = round(nSignif / n, 3)) %>%\n  pander::pander(justify = \"lrrr\")\n\n\norder\nn\nnSignif\npropn\n1\n15\n15\n1\n2\n32\n6\n0.188\n3\n20\n2\n0.1\n\nWith 52 the probability that none of them would come out statistically significant at p < .05 given a true null population model would be .95^52, i.e. 0.069, pretty unlikely but the challenge is to know what to do about this. If we could treat age as linear we wouldn’t have all those effects for each age other than 15 and things would be much simpler, but we know I’ve modelled age as having a quadratic effect.\nCheat a bit and just fit the quadratic for age by centring and then squaring age.\n\n\nShow code\n\n# lm(last ~ first + gender + poly(Age, 2) + nSessions + \n#      gender * poly(Age, 2) + gender * nSessions + gender * poly(Age, 2) + poly(Age, 2) * nSessions +\n#      gender * poly(Age, 2) * nSessions, \n#    data = tibDat) -> lisLMAge2\n\ncentreVec <- function(x){\n  x - mean(x)\n}\ntibDat %>%\n  mutate(ageSquared = centreVec(age),\n         ageSquared = ageSquared^2,\n         ### recentre to get mean zero\n         ageSquared = centreVec(ageSquared)) -> tibDat\n\nlm(last ~ first + gender + ageSquared + nSessions + \n     gender * ageSquared + gender * nSessions + gender * ageSquared + ageSquared * nSessions +\n     gender * ageSquared * nSessions, \n   data = tibDat) -> lisLMAge2\n\nsummary(lisLMAge2)\n\n\n\nCall:\nlm(formula = last ~ first + gender + ageSquared + nSessions + \n    gender * ageSquared + gender * nSessions + gender * ageSquared + \n    ageSquared * nSessions + gender * ageSquared * nSessions, \n    data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9852 -0.7795 -0.0006  0.7767  7.3727 \n\nCoefficients:\n                                   Estimate Std. Error  t value\n(Intercept)                       0.4355888  0.0055454   78.549\nfirst                             0.9975685  0.0010975  908.949\ngenderM                           0.1900903  0.0097396   19.517\ngenderOther                      -0.3206512  0.0173832  -18.446\nageSquared                        0.0994679  0.0006739  147.600\nnSessions                        -0.1490117  0.0014044 -106.106\ngenderM:ageSquared                0.0021081  0.0011869    1.776\ngenderOther:ageSquared           -0.0014332  0.0021039   -0.681\ngenderM:nSessions                 0.0035952  0.0024671    1.457\ngenderOther:nSessions             0.0029519  0.0044030    0.670\nageSquared:nSessions              0.0001480  0.0001706    0.867\ngenderM:ageSquared:nSessions     -0.0004417  0.0003007   -1.469\ngenderOther:ageSquared:nSessions  0.0002704  0.0005293    0.511\n                                 Pr(>|t|)    \n(Intercept)                        <2e-16 ***\nfirst                              <2e-16 ***\ngenderM                            <2e-16 ***\ngenderOther                        <2e-16 ***\nageSquared                         <2e-16 ***\nnSessions                          <2e-16 ***\ngenderM:ageSquared                 0.0757 .  \ngenderOther:ageSquared             0.4958    \ngenderM:nSessions                  0.1450    \ngenderOther:nSessions              0.5026    \nageSquared:nSessions               0.3857    \ngenderM:ageSquared:nSessions       0.1418    \ngenderOther:ageSquared:nSessions   0.6095    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.203 on 417987 degrees of freedom\nMultiple R-squared:  0.7375,    Adjusted R-squared:  0.7375 \nF-statistic: 9.786e+04 on 12 and 417987 DF,  p-value: < 2.2e-16\n\nHm, better but no cigar!\nStart over from simplest model and build up\nBaseline of regression model.\n\n\nShow code\n\nlm(last ~ first, data = tibDat) -> lisLM1\nsummary(lisLM1)\n\n\n\nCall:\nlm(formula = last ~ first, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6682 -1.0108 -0.0606  0.9579  7.5304 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.070443   0.002304  -30.57   <2e-16 ***\nfirst        1.059520   0.001331  796.23   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.48 on 417998 degrees of freedom\nMultiple R-squared:  0.6027,    Adjusted R-squared:  0.6027 \nF-statistic: 6.34e+05 on 1 and 417998 DF,  p-value: < 2.2e-16\n\nOf course, highly significant.\nStart by adding nSessions.\n\n\nShow code\n\nlm(last ~ first + nSessions, data = tibDat) -> lisLMsessions\nsummary(lisLMsessions)\n\n\n\nCall:\nlm(formula = last ~ first + nSessions, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6054 -0.9964 -0.0671  0.9413  7.7406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.456206   0.005332   85.55   <2e-16 ***\nfirst        1.059541   0.001312  807.51   <2e-16 ***\nnSessions   -0.147375   0.001350 -109.17   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.459 on 417997 degrees of freedom\nMultiple R-squared:  0.6137,    Adjusted R-squared:  0.6137 \nF-statistic: 3.32e+05 on 2 and 417997 DF,  p-value: < 2.2e-16\n\nShow code\n\nanova(lisLM1, lisLMsessions)\n\n\nAnalysis of Variance Table\n\nModel 1: last ~ first\nModel 2: last ~ first + nSessions\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n1 417998 915044                                 \n2 417997 889678  1     25366 11918 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nMarked effect, add gender.\n\n\nShow code\n\nlm(last ~ first + nSessions + gender + \n     first * gender + nSessions * gender, data = tibDat) -> lisLMsessionsGend\nsummary(lisLMsessionsGend)\n\n\n\nCall:\nlm(formula = last ~ first + nSessions + gender + first * gender + \n    nSessions * gender, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7645 -0.9796 -0.0548  0.9426  7.5640 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            0.412336   0.006702  61.524   <2e-16 ***\nfirst                  1.097156   0.001968 557.374   <2e-16 ***\nnSessions             -0.149574   0.001695 -88.260   <2e-16 ***\ngenderM                0.209051   0.011759  17.778   <2e-16 ***\ngenderOther           -0.340363   0.021093 -16.136   <2e-16 ***\nfirst:genderM         -0.054331   0.002792 -19.461   <2e-16 ***\nfirst:genderOther     -0.053663   0.004284 -12.528   <2e-16 ***\nnSessions:genderM      0.005255   0.002977   1.765   0.0776 .  \nnSessions:genderOther  0.008217   0.005312   1.547   0.1219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.451 on 417991 degrees of freedom\nMultiple R-squared:  0.6177,    Adjusted R-squared:  0.6177 \nF-statistic: 8.443e+04 on 8 and 417991 DF,  p-value: < 2.2e-16\n\nShow code\n\nanova(lisLMsessions, lisLMsessionsGend)\n\n\nAnalysis of Variance Table\n\nModel 1: last ~ first + nSessions\nModel 2: last ~ first + nSessions + gender + first * gender + nSessions * \n    gender\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1 417997 889678                                  \n2 417991 880304  6      9374 741.84 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHighly significant effect of session count remains but odd effects of gender and an interaction!\n\n\nShow code\n\n### age effect treating age as continuous\n### short cut syntax for all interactions\nlm(last ~ first * nSessions * gender * age,\n   data = tibDat) -> lisLMAge\nsummary(lisLMAge)\n\n\n\nCall:\nlm(formula = last ~ first * nSessions * gender * age, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7660 -0.9798 -0.0545  0.9428  7.5527 \n\nCoefficients:\n                                  Estimate Std. Error t value\n(Intercept)                      3.774e-01  4.933e-02   7.650\nfirst                            1.093e+00  3.225e-02  33.906\nnSessions                       -1.436e-01  1.252e-02 -11.466\ngenderM                          2.416e-01  8.381e-02   2.883\ngenderOther                     -2.194e-01  1.547e-01  -1.418\nage                              1.818e-03  2.448e-03   0.743\nfirst:nSessions                 -1.848e-03  8.174e-03  -0.226\nfirst:genderM                   -5.082e-02  4.585e-02  -1.108\nfirst:genderOther               -5.180e-02  6.938e-02  -0.747\nnSessions:genderM               -9.518e-03  2.124e-02  -0.448\nnSessions:genderOther           -2.571e-02  3.898e-02  -0.660\nfirst:age                       -6.619e-05  1.597e-03  -0.041\nnSessions:age                   -3.190e-04  6.213e-04  -0.514\ngenderM:age                     -1.697e-03  4.159e-03  -0.408\ngenderOther:age                 -6.040e-03  7.680e-03  -0.786\nfirst:nSessions:genderM          1.961e-03  1.163e-02   0.169\nfirst:nSessions:genderOther      3.718e-03  1.726e-02   0.215\nfirst:nSessions:age              1.649e-04  4.048e-04   0.407\nfirst:genderM:age                2.636e-04  2.274e-03   0.116\nfirst:genderOther:age            6.470e-06  3.453e-03   0.002\nnSessions:genderM:age            7.593e-04  1.054e-03   0.721\nnSessions:genderOther:age        1.694e-03  1.934e-03   0.876\nfirst:nSessions:genderM:age     -2.211e-04  5.767e-04  -0.383\nfirst:nSessions:genderOther:age -2.149e-04  8.590e-04  -0.250\n                                Pr(>|t|)    \n(Intercept)                     2.02e-14 ***\nfirst                            < 2e-16 ***\nnSessions                        < 2e-16 ***\ngenderM                          0.00394 ** \ngenderOther                      0.15616    \nage                              0.45777    \nfirst:nSessions                  0.82114    \nfirst:genderM                    0.26767    \nfirst:genderOther                0.45530    \nnSessions:genderM                0.65401    \nnSessions:genderOther            0.50957    \nfirst:age                        0.96694    \nnSessions:age                    0.60758    \ngenderM:age                      0.68325    \ngenderOther:age                  0.43160    \nfirst:nSessions:genderM          0.86613    \nfirst:nSessions:genderOther      0.82942    \nfirst:nSessions:age              0.68369    \nfirst:genderM:age                0.90772    \nfirst:genderOther:age            0.99851    \nnSessions:genderM:age            0.47120    \nnSessions:genderOther:age        0.38117    \nfirst:nSessions:genderM:age      0.70141    \nfirst:nSessions:genderOther:age  0.80247    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.451 on 417976 degrees of freedom\nMultiple R-squared:  0.6177,    Adjusted R-squared:  0.6177 \nF-statistic: 2.937e+04 on 23 and 417976 DF,  p-value: < 2.2e-16\n\nShow code\n\nanova(lisLM1, lisLMAge)\n\n\nAnalysis of Variance Table\n\nModel 1: last ~ first\nModel 2: last ~ first * nSessions * gender * age\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1 417998 915044                                  \n2 417976 880290 22     34754 750.09 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNo effect of age as it’s got a quadratic effect in my model!\n\n\nShow code\n\n### age effect treating age as continuous but adding quadratic term\n### short cut syntax for all interactions again\nlm(last ~ first * nSessions * gender * ageSquared,\n   data = tibDat) -> lisLMAge\nsummary(lisLMAge2)\n\n\n\nCall:\nlm(formula = last ~ first + gender + ageSquared + nSessions + \n    gender * ageSquared + gender * nSessions + gender * ageSquared + \n    ageSquared * nSessions + gender * ageSquared * nSessions, \n    data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9852 -0.7795 -0.0006  0.7767  7.3727 \n\nCoefficients:\n                                   Estimate Std. Error  t value\n(Intercept)                       0.4355888  0.0055454   78.549\nfirst                             0.9975685  0.0010975  908.949\ngenderM                           0.1900903  0.0097396   19.517\ngenderOther                      -0.3206512  0.0173832  -18.446\nageSquared                        0.0994679  0.0006739  147.600\nnSessions                        -0.1490117  0.0014044 -106.106\ngenderM:ageSquared                0.0021081  0.0011869    1.776\ngenderOther:ageSquared           -0.0014332  0.0021039   -0.681\ngenderM:nSessions                 0.0035952  0.0024671    1.457\ngenderOther:nSessions             0.0029519  0.0044030    0.670\nageSquared:nSessions              0.0001480  0.0001706    0.867\ngenderM:ageSquared:nSessions     -0.0004417  0.0003007   -1.469\ngenderOther:ageSquared:nSessions  0.0002704  0.0005293    0.511\n                                 Pr(>|t|)    \n(Intercept)                        <2e-16 ***\nfirst                              <2e-16 ***\ngenderM                            <2e-16 ***\ngenderOther                        <2e-16 ***\nageSquared                         <2e-16 ***\nnSessions                          <2e-16 ***\ngenderM:ageSquared                 0.0757 .  \ngenderOther:ageSquared             0.4958    \ngenderM:nSessions                  0.1450    \ngenderOther:nSessions              0.5026    \nageSquared:nSessions               0.3857    \ngenderM:ageSquared:nSessions       0.1418    \ngenderOther:ageSquared:nSessions   0.6095    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.203 on 417987 degrees of freedom\nMultiple R-squared:  0.7375,    Adjusted R-squared:  0.7375 \nF-statistic: 9.786e+04 on 12 and 417987 DF,  p-value: < 2.2e-16\n\nShow code\n\nanova(lisLM1, lisLMAge2)\n\n\nAnalysis of Variance Table\n\nModel 1: last ~ first\nModel 2: last ~ first + gender + ageSquared + nSessions + gender * ageSquared + \n    gender * nSessions + gender * ageSquared + ageSquared * nSessions + \n    gender * ageSquared * nSessions\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n1 417998 915044                                 \n2 417987 604514 11    310531 19519 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nShow code\n\njtools::effect_plot(lisLMAge2, pred = ageSquared, interval = TRUE, rug = TRUE)\n\n\n\n\nI’ve added an effect plot with “rugs” for the y and x variables. Shows clear quadratic effect of age (looks linear because we’re plotting against squared age).\nBasically, this is a surprisingly real world mess! I will stop here as I want to check my hunch that I’ve created these (as I say, very real world) interactions by the way I created the final scores using a multiplier rather than a simple addition. However, this does demonstrate the complexities of disentangline effects with even a few predictors particularly when gender is treated not as binary and when age cannot be treated as a linear variable as it clearly has a quadratic effect.\n\n\n\n",
    "preview": "https://www.psyctc.org/psyctc/wp-content/uploads/2022/02/prepost1.png",
    "last_modified": "2023-08-25T14:15:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-24-chance-corrected-agreement/",
    "title": "Chance corrected agreement",
    "description": "Simple plotting of raw agreement and Cohen's kappa for various prevalences of the rated quality\nand only chance agreement",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-01-24",
    "categories": [
      "agreement",
      "Cohen's kappa"
    ],
    "contents": "\n\nContents\nThe issue about raw agreement and prevalence of what is rated\nAdding confidence interval around the observed kappa\nWhat happens with better than chance agreement?\nBut those are fixed agreement rates\n\nCreated 22.i.22, extended 25-26.i.22\nThis post is a bit different from most of my posts here which are mostly about R itself. This is perhaps the first of another theme I wanted to have here about using to R to illustrate general statistical or psychometric issues. This one was created to illustrate points I was making in a blog post Why kappa? or How simple agreement rates are deceptive on my psyctc.org/psyctc/ blog.\nThis starts with some trivial R to illustrate how the prevalence of the quality rated affects a raw agreement rate if agreement is truly random. Then it got into somewhat more challenging R (for me at least) as I explored better than chance agreement.\nThe issue about raw agreement and prevalence of what is rated\nThe code just computes raw agreement and kappa for chance agreement and prevalences from 1 in 1,000 to 999 in 1,000. It shows that the agreement rate rises to very near 1, i.e. 100% as the prevalence gets very high (or low) whereas Cohen’s kappa remains zero at all prevalences because it is a “chance corrected” agreement coefficient.\n\n\nShow code\n\nvalN <- 1000\n1:(valN - 1) %>%\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  mutate(prevalence = prevalence / valN) %>%\n  rowwise() %>%\n  mutate(valPosPos = round(valN * prevalence^2), # just the product of the prevalences and get as a number, not a rate\n         valNegNeg = round(valN * (1 - prevalence)^2), # product of the rate of the negatives\n         valPosNeg = (valN - valPosPos - valNegNeg) / 2, # must be half the difference\n         valNegPos = valPosNeg, # must be the same\n         checkSum = valPosPos + valNegNeg + valPosNeg + valNegPos, # just checking!\n         rawAgreement = (valPosPos + valNegNeg) / valN,\n         kappa = list(DescTools::CohenKappa(matrix(c(valPosPos,\n                                                     valPosNeg,\n                                                     valNegPos,\n                                                     valNegNeg),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = prevalence, y = rawAgreement)) +\n  geom_line(colour = \"red\") +\n  geom_line(aes(y = kappa), colour = \"green\") +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\nShow code\n\nggsave(\"ggsave1.png\")\nvalN2 <- 10^6\n\n\nI think that shows pretty clearly why raw agreement should never be used as a coefficient of agreement and why, despite some real arguments for other coefficients and known weaknesses (see good wikipedia entry), kappa is pretty good and likely to remain the most used such coefficient.\nOne perhaps suprising thing is that the kappa values aren’t all exactly zero: see the zigzag of the values towards the ends of the x axis. The biggest value is 0.0209603 and the smallest is -0.0224949. These non-zero values arise because counts are integers and I have plotted for values of prevalence between 0.001 and 0.999 and a sample size of 1000. Towards the ends of that prevalence range rounding to get integer counts means that kappa cannot be exactly zero.\nIf I don’t round the cell sizes to integers, in effect staying with probabilities, or simulating an infinitely large sample, the issue goes away as shown here.\n\n\nShow code\n\n### valN2 pulled through from block above\n1:(valN - 1) %>%\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  mutate(prevalence = prevalence / valN) %>%\n  rowwise() %>%\n  mutate(valPosPos = valN2 * prevalence^2, # just the product of the prevalences and get as a number, not a rate\n         valNegNeg = valN2 * (1 - prevalence)^2, # product of the rate of the negatives\n         valPosNeg = (valN2 - valPosPos - valNegNeg) / 2, # must be half the difference\n         valNegPos = valPosNeg, # must be the same\n         checkSum = valPosPos + valNegNeg + valPosNeg + valNegPos, # just checking!\n         rawAgreement = (valPosPos + valNegNeg) / valN2,\n         kappa = list(DescTools::CohenKappa(matrix(c(valPosPos,\n                                                     valPosNeg,\n                                                     valNegPos,\n                                                     valNegNeg),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tibDat2\n\nggplot(data = tibDat2,\n       aes(x = prevalence, y = rawAgreement)) +\n  geom_line(colour = \"red\") +\n  geom_line(aes(y = kappa), colour = \"green\") +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\n\nAdding confidence interval around the observed kappa\nConfidence intervals (CIs) are of course informative about imprecision of estimation, here of kappa and I love them for that. However, sometimes they can also alert you that something is being stretched to implausibilty in what you are trying to learn from your data. Here they are for a sample size of 1000.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = prevalence, y = rawAgreement)) +\n  geom_line(colour = \"red\") +\n  geom_linerange(aes(ymax = upr.ci, ymin = lwr.ci), colour = \"palegreen\") +\n  geom_line(aes(y = kappa), colour = \"green\") +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\n\nBetween say a prevalence of .1 and .9 things are sensible there: the confidence interval around the observed kappa widens as the smallest cell sizes in the 2x2 crosstabulation get smaller. That’s because, as with most statistics, it’s the smallest cell size, rather than the total sample size (which is of course constant here), that determine precision of estimation.\nHowever, going out towards a prevalence of .01 or of .99 something is very clearly wrong there as we have confidence limits on kappa that go above 1 and below -1: values that are impossible for a “real” kappa. Here the CI is telling us that it can’t give us real world answers for the CI: one or more cell sizes are simply too small. These impossible kappa confidence limits actually occur when one of the cell sizes is zero.\nHere are the confidence intervals if push the sample size up to 10^{6}.\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = prevalence, y = rawAgreement)) +\n  geom_line(colour = \"red\") +\n  geom_linerange(aes(ymax = upr.ci, ymin = lwr.ci), colour = \"palegreen\") +\n  geom_line(aes(y = kappa), colour = \"green\") +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\n\nVery tight and no confidence limits impossible.\nWhat happens with better than chance agreement?\nHere I am looking at agreement rates from .6 up to .90 with the agreement imposed on the sample and the cell sizes worked out to the nearest integer, all given the sample size of 1,000.\n\n\nShow code\n\nvalN <- 1000\nvecAgreeRate <- c(seq(60, 90, 10)) / 100\n1:valN %>%\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  mutate(posA = prevalence, # so change number of positives for rater A\n         negA = valN - posA, # and negative\n         prevalence = prevalence / valN, # get prevalence as a rate not a count\n         ### now put in the agreement rates from vecAgreeRate\n         agreeRate = list(vecAgreeRate)) %>%\n  unnest_longer(agreeRate) %>%\n  ### now create the rater B counts using those agreement rates\n  mutate(posAposB = round(posA * agreeRate),\n         posAnegB = round(posA * (1 - agreeRate)),\n         negAposB = round(negA * (1 - agreeRate)),\n         negAnegB = round(negA * agreeRate),\n         checkSum = posAposB + posAnegB + negAposB + negAnegB,\n         rawAgreement = (posAposB + negAnegB) / valN) %>%\n  rowwise() %>%\n  mutate(kappa = list(DescTools::CohenKappa(matrix(c(posAposB,\n                                                     negAposB,\n                                                     posAnegB,\n                                                     negAnegB),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tibDat3\n\ntibDat3 %>% \n  mutate(txtAgree = str_c(\"Sample agreement: \", agreeRate)) -> tibDat3\n\nggplot(data = tibDat3,\n       aes(x = prevalence, y = rawAgreement)) +\n  facet_wrap(facets = vars(txtAgree),\n             ncol = 2) +\n  geom_line(colour = \"red\") +\n  geom_linerange(aes(ymin = lwr.ci, ymax = upr.ci),\n                 colour = \"palegreen\") +\n  geom_line(aes(y = kappa),\n            colour = \"green\") +  \n  geom_hline(yintercept = 0)\n\n\n\nOf course agreement wouldn’t be exactly the same for every sample, this is a slightly more realistic simulation treating the actually sample agreement as a binomial variable with population value\n\n\nShow code\n\nvalN <- 1000\nvecAgreeRate <- c(seq(50, 90, 10)) / 100\n1:valN %>%\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  mutate(agreeRate = list(vecAgreeRate)) %>%\n  unnest_longer(agreeRate) %>%\n  rowwise() %>%\n  mutate(prevalence = prevalence / valN,\n         posA = rbinom(1, valN, prevalence),\n         negA = valN - posA,\n         posAposB = rbinom(1, posA, agreeRate),\n         posAnegB = posA - posAposB,\n         negAnegB = rbinom(1, negA, agreeRate),\n         negAposB = negA - negAnegB,\n         checkSum = posAposB + posAnegB + negAnegB + negAposB,\n         rawAgreement = (posAposB + negAnegB) / valN,\n         kappa = list(DescTools::CohenKappa(matrix(c(posAposB,\n                                                     negAposB,\n                                                     posAnegB,\n                                                     negAnegB),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tibDat4\n\ntibDat4 %>% \n  mutate(txtAgree = str_c(\"Population agreement: \", agreeRate)) -> tibDat4\n  \n\nggplot(data = tibDat4,\n       aes(x = prevalence, y = rawAgreement)) +\n  facet_wrap(facets = vars(txtAgree),\n             ncol = 2) +\n  geom_line(colour = \"red\") +\n  geom_linerange(aes(ymin = lwr.ci, ymax = upr.ci),\n                 colour = \"palegreen\") +\n  geom_line(aes(y = kappa),\n            colour = \"green\") +  \n  geom_hline(yintercept = 0)\n\n\n\nBut those are fixed agreement rates\nYou may have been wondering why the raw agreement rates don’t show the U shaped relationship with prevalence as they do, must do, when I modelled random agreement earlier. That’s because this was modelling a agreement rate in the sample so, even when I treated the agreement as a binomial distribution rather than a fixed rate, the relationship with prevalence was removed. It’s really a completely artificial representation of raw agreement.\nSo let’s have a population model. This was a bit more challenging to program. What I have done is first to simulate samples with bivariate Gaussian distributions from populations with fixed correlations between those Gaussian variables. I have set the population correlations at 0, .3, .6 and .9 (Pearson correlations). Then I created the binary data for different prevalences simply by dichotomising the Gaussian variables at the appropriate cuttings points on the Gaussian cumulative density curve setting prevalences of .01 to .99. The sample size is set at 10,000.\nThat gets us this.\n\n\nShow code\n\nmakeCorrMat <- function(corr) {\n  matrix(c(1, corr, corr, 1), ncol = 2)\n}\n# makeCorrMat(0)\n# makeCorrMat(.5)\n\n# valN <- 1000\nvalN <- 10000\n# vecCorr <- seq(0, .9, .1)\nvecCorr <- c(0, .3, .6, .9)\nvecMu <- c(0, 0) # set means for mvrnorm\nvecPrevalence <- 1:99 / 100\n\n### test\n# cor(MASS::mvrnorm(100, mu = vecMu, Sigma = makeCorrMat(.9)))\n\nset.seed(12345)\nvecPrevalence %>% # start from the prevalences to build tibble\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  ### get the cutting points on the cumulative Gaussian distribution per prevalence\n  mutate(cutPoint = qnorm(prevalence),\n         ### input the vector of correlations\n         corr = list(vecCorr)) %>%\n  ### unnest to create a row for each correlation\n  unnest_longer(corr) %>%\n  rowwise() %>%\n  ### now create a bivariate Gaussian distribution sample from those population correlations\n  mutate(rawDat = list(MASS::mvrnorm(valN, mu = vecMu, Sigma = makeCorrMat(corr))),\n         obsCorr = cor(rawDat)[1, 2]) %>%\n  ungroup() %>% \n  unnest(rawDat, names_repair = \"universal\") %>%\n  rowwise() %>%\n  ### I'm sure I ought to be able to do this more elegantly but this gets from the embedded dataframe to two column vectors\n  mutate(rawA = rawDat[[1]],\n         rawB = rawDat[[2]]) %>%\n  select(-rawDat) %>%\n  ### end of that mess!\n  mutate(binaryA = if_else(rawA > cutPoint, 1, 0),\n         binaryB = if_else(rawB > cutPoint, 1, 0),\n         sumBinaries = binaryA + binaryB,\n         posAposB = if_else(sumBinaries == 2, 1, 0),\n         negAnegB = if_else(sumBinaries == 0, 1, 0),\n         negAposB = if_else(binaryA == 0 & binaryB == 1, 1, 0),\n         posAnegB = if_else(binaryA == 1 & binaryB == 0, 1, 0),\n         checkSum = sum(posAposB:posAnegB)) %>%\n  ungroup() -> tibBigDat\n\ntibBigDat %>%\n  group_by(prevalence, corr) %>%\n  summarise(obsCorr = first(obsCorr),\n            across(posAposB:posAnegB, sum)) %>% \n  ungroup() %>%\n  rowwise() %>% \n  mutate(rawAgreement = (posAposB + negAnegB) / valN,\n         kappa = list(DescTools::CohenKappa(matrix(c(posAposB, posAnegB, \n                                                   negAposB, negAnegB),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tmpTib\n\n### improve labelling of corr for facets\ntmpTib %>%\n  mutate(txtCorr = str_c(\"Population correlation: \", corr)) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = prevalence, y = rawAgreement)) +\n  facet_wrap(facets = vars(txtCorr)) +\n  geom_point(colour = \"red\",\n             size = 1) +\n  geom_linerange(aes(ymin = lwr.ci, ymax = upr.ci),\n                 colour = \"palegreen\") +\n  geom_point(aes(y = kappa),\n             colour = \"green\",\n             size = 1) +\n  geom_hline(yintercept = c(0, 1)) +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\n\nThat shows correctly that the U shaped and misleading relationship between raw agreement and prevalence is not only true for random agreement, but is there as some real agreement is there, though the more the real agreement, the shallower the U curve as you’d expect.\nThis last is just me checking how tightly sample correlations approximate the population correlations (for n = 10,000).\n\n\nShow code\n\n### how well did the correlations work?\nset.seed(12345) # fix the jittering\nggplot(data = tmpTib,\n       aes(x= corr, y = obsCorr)) +\n  geom_jitter(height = 0, width = .05, alpha = .4) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"Population correlation\") +\n  ylab(\"Observed correlation\") +\n  ggtitle(\"Scatterplot: observed correlations against the population correlations\",\n          subtitle = \"Horizontal jittering and transparency used to handle overprinting.  Blue line is linear fit\")\n\n\n\nHere’s the raw linear of the observed correlations on the population ones.\n\n\nShow code\n\nlm(obsCorr ~ corr, data = tmpTib)\n\n\nCall:\nlm(formula = obsCorr ~ corr, data = tmpTib)\n\nCoefficients:\n(Intercept)         corr  \n  0.0007104    0.9991870  \n\nFine!\nOK. I hope all this is useful in explaining these issues. Do contact me if you have questions or suggestions for improvements.\n\n\n\n",
    "preview": "posts/2022-01-24-chance-corrected-agreement/chance-corrected-agreement_files/figure-html5/simulate-1.png",
    "last_modified": "2023-08-25T14:15:00+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-15-data-ellipses-and-confidence-ellipses/",
    "title": "Data ellipses and confidence ellipses",
    "description": "This just clarifies the distinction between a data ellipse and a confidence ellipse, \ni.e. an ellipse describing the joint confidence intervals on two parameters of a model",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "R graphics",
      "distributions",
      "correlation",
      "regression"
    ],
    "contents": "\n\nContents\nHistory\nggExtra::ggMarginal() adds marginal histograms\n“Densigram”\nBoxplot\nViolin plot\n\nData ellipses\n“Robust” data ellipse\nEllipsoid hulls (or ellipsoidhulls)\n\nConfidence ellipses\n\n\nHistory\nCreated 15.i.22\nUpdated 16.i.22 adding ggExtra::ggMarginal() plots\nFollows the typically generous and helpful post from John Fox on the R-help list:\nDear Paul,\n\nOn 2022-01-14 1:17 p.m., Paul Bernal wrote:\n> Dear John and R community friends,\n>\n> To be a little bit more specific, what I need to accomplish is the\n> creation of a confidence interval ellipse over a scatterplot at\n> different percentiles. The confidence interval ellipses should be drawn\n> over the scatterplot.\n\nI'm not sure what you mean. Confidence ellipses are for regression\ncoefficients and so are on the scale of the coefficients; data\n(concentration) ellipses are for and on the scale of the explanatory\nvariables. As it turns out, for a linear model, the former is the\nrescaled 90 degree rotation of the latter.\n\nBecause the scatterplot of the (two) variables has the variables on the\naxes, a data ellipse but not a confidence ellipse makes sense (i.e., is\nin the proper units). Data ellipses are drawn by car::dataEllipse() and\n(as explained by Martin Maechler) cluster::ellipsoidPoints(); confidence\nellipses are drawn by car::confidenceEllipse() and the various methods\nof ellipse::ellipse().\n\nI hope this helps,\n  John\nThat made me realise that I was only “sort of” sure I understood that and reminded me that I have so far never used ellipses either as a way to describe 2D data or to map the confidence intervals of two parameters from a model. I decided to get to grips with this, starting by creating some correlated data.\n\n\nShow code\n\nset.seed(12345) # get replicability\nvalN <- 300 # sample size (doh!)\nx <- rnorm(valN) # Gaussian distribution\ny <- x + rnorm(valN, sd = .3) # create correlated y variable\nbind_cols(x = x, y = y) -> tibDat # build into a tibble\n\n\nHere’s the head of that dataset.\n\n\nShow code\n\n### show the data\ntibDat\n\n# A tibble: 300 × 2\n        x       y\n    <dbl>   <dbl>\n 1  0.586  0.742 \n 2  0.709  0.712 \n 3 -0.109 -0.241 \n 4 -0.453 -0.0937\n 5  0.606  0.571 \n 6 -1.82  -1.81  \n 7  0.630  0.989 \n 8 -0.276 -0.173 \n 9 -0.284 -0.383 \n10 -0.919 -0.418 \n# ℹ 290 more rows\n\nAnd here is a simple ggplot scattergram of that using transparency to handle overprinting.\n\n\nShow code\n\n### simple scattergram\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point(alpha = .4) +\n  geom_smooth(method = \"lm\") +\n  xlim(c(-3, 3)) +\n  ylim(c(-3, 3)) +\n  coord_fixed(1) -> p\np\n\n\n\nggExtra::ggMarginal() adds marginal histograms\nThis is just so I remember where to find this and for the fun of it: ggExtra::ggMarginal() can add marginal histograms, density plots, boxplots, violin plots or “densigrams”, a combination of a histogram and a density plot, to the sides of a scattergram. I like this!\n“Densigram”\n\n\nShow code\n\nggExtra::ggMarginal(p, type = \"densigram\")\n\n\n\nBoxplot\n\n\nShow code\n\nggExtra::ggMarginal(p, type = \"boxplot\")\n\n\n\nViolin plot\n\n\nShow code\n\nggExtra::ggMarginal(p, type = \"violin\")\n\n\n\nOK, back to the main issue.\nData ellipses\nA 95% data ellipse is an ellipse expected to contain 95% of the joint population distributions of x and y based on the observed data and the assumption of bivariate Gaussian distributions. The area contained can be what you like really (within the logical restrictions of it being a positive proportion/percentage and lower than 100%!) Here are data ellipses for that dataset created with car::dataEllipse(). I’ve used its default confidence intervals of 50% and 95%.\n\n\nShow code\n\ncar::dataEllipse(tibDat$x, tibDat$y) -> retDat # collect up data for the lines\n\n\nShow code\n\n# str(retDat)\n### retDat is a list containing a mapping for the ellipses\n### 50% ellipse points\nretDat$`0.5` %>%\n  as_tibble() -> tib50\n### 95% ellipse points\nretDat$`0.95` %>%\n  as_tibble() -> tib95\n\n\nHere’s the same but using cluster::ellipsoidPoints(). A bit more work than car::dataEllipse().\n\n\nShow code\n\ntibDat %>%\n  as.data.frame() %>%\n  as.matrix() -> matDat\n\nmatCovLS <- cov(matDat)\nvecMeans <- colMeans(matDat)\nvecMeans <- colMeans(matDat)\n### get 95% CI ellipse\nd2.95 <- qchisq(0.95, df = 2)\ncluster::ellipsoidPoints(matCovLS, d2.95, loc = vecMeans) -> matEllipseHull95\n### and now 50%\nd2.50 <- qchisq(0.5, df = 2)\ncluster::ellipsoidPoints(matCovLS, d2.50, loc = vecMeans) -> matEllipseHull50\n\nplot(matDat, asp = 1, xlim = c(-3, 3))\nlines(matEllipseHull95, col=\"blue\")\nlines(matEllipseHull50, col=\"blue\")\n\n\n\nThat really is the same as the other, well, minus the 50% interval but it looks different because of the changed scales.\n“Robust” data ellipse\nJust to extend things, the help for cluster::ellipsoidPoints() shows that you can use it with a “robust covariance” estimate rather than the least squares lm() or cov() one. Turns out that this uses cov.rob() from the MASS package which essentially does some censoring off of perceived or potential outliers to get an covariance matrix that would be less sensitive to outliers. Here we go.\n\n\nShow code\n\nCxy <- MASS::cov.rob(cbind(x,y))\ncluster::ellipsoidPoints(Cxy$cov, d2 = d2.95, loc=Cxy$center) -> matEllipseHullRob\n\nplot(matDat, asp = 1, xlim = c(-3, 3))\nlines(matEllipseHull95, col=\"blue\")\nlines(matEllipseHullRob, col=\"green\")\n\n\n\nThat has the 95% ellipse from the robust covariance matrix in green and the simple least squares ellipse in blue. As you would expect the difference is negligible as these are bivariate Gaussian data so there are few real outliers.\nThese plots are reminding me that all that learning curve to understand ggplot was worth it! However, the corollary is that I have forgotten most of what I ever knew about improving base R graphic output. Fortunately, I can take the output from car::dataEllipse() and feed it into ggplot where I use geom_path() to plot it.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point(alpha = .4) +\n  geom_smooth(method = \"lm\") +\n  xlim(c(-3, 3)) +\n  ylim(c(-3, 3)) +\n  coord_fixed(1) +\n  geom_path(data = tib50,\n            aes(x = x, y = y), colour = \"red\") +\n  geom_path(data = tib95,\n            aes(x = x, y = y), colour = \"orange\") \n\n\n\nSo that’s same again, now just feeding the points created by car::dataEllipse() for each CI into tibbles and those into ggplot and overlaying them on the scattergram.\nEllipsoid hulls (or ellipsoidhulls)\nThis was an interesting extension of my learning. An ellipsoid hull is different from a data ellipse: it’s the ellipse that contains all the observed points (with some on the boundary of the ellipse). Here using cluster::ellipsoidhull() and base graphics.\n\n\nShow code\n\ntibDat %>%\n  as.data.frame() %>%\n  as.matrix() -> matDat\n\ncluster::ellipsoidhull(matDat) -> ellipseHull\n\nplot(matDat, asp = 1, xlim = c(-3, 3))\nlines(predict(ellipseHull), col=\"blue\")\n\n\n\nAnd the same spitting the data into ggplot.\n\n\nShow code\n\npredict(ellipseHull) %>%\n  as_tibble(.name_repair = \"universal\") %>%\n  rename(x = `...1`) -> tibEllipseHullPath\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point(alpha = .4) +\n  xlim(c(-4, 4)) +\n  ylim(c(-4, 4)) +\n  coord_fixed(1) +\n  geom_path(data = tibEllipseHullPath,\n            aes(x = x, y = y), colour = \"blue\")\n\n\n\nConfidence ellipses\nSo what are confidence ellipses? These are not about estimation the distribution of the population data but confidence ellipses for model parameters estimated from the data. Here the model is linear regression of y on x and assuming Gaussian distributions and here are the model parameters estimated using lm().\n\n\nShow code\n\nlm(y ~ x, data = tibDat)\n\n\nCall:\nlm(formula = y ~ x, data = tibDat)\n\nCoefficients:\n(Intercept)            x  \n    0.02265      1.00701  \n\nThe two parameters are the intercept and the slope and the confidence ellipse shows the area containing the desired joint CIs. The default interval is 95% and here it is constructed using car::confidenceEllipse(). The point in the middle marks the point estimates of intercept and slope and the ellipse the CI around that.\n\n\nShow code\n\ncar::confidenceEllipse(lm(y ~ x, data = tibDat))\n\n\n\nHere is the same ellipse created using ellipse::ellipse().\n\n\nShow code\n\nellipse::ellipse(lm(y ~ x, data = tibDat)) -> matEllipseEllipse\nplot(ellipse::ellipse(lm(y ~ x, data = tibDat)), type = \"l\")\n\n\n\nJust for completeness, it’s easy to get the ellipse path using ellipse::ellipse() and spit that into ggplot.\n\n\nShow code\n\nellipse::ellipse(lm(y ~ x, data = tibDat)) -> matEllipseEllipse\n\n### rather clumsy creation of tibble of parameters for ggplot\nlm(y ~ x, data = tibDat)$coefficients -> vecLM\nbind_cols(Intercept = vecLM[1], Slope = vecLM[2]) -> tibParms\n\n### slightly nicer creation of tibble of the points on the ellipse\nmatEllipseEllipse %>%\n  as_tibble() %>%\n  rename(Intercept = `(Intercept)`,\n         Slope = x) -> tmpTib\n\n### plot those\nggplot(data = tmpTib,\n       aes(x = Intercept, y = Slope)) +\n  geom_path() +\n  geom_point(data = tibParms,\n             colour = \"blue\", \n             size = 3)\n\n\n\nThat shows a joint distribution suggesting that the two estimated parameters are pretty much uncorrelated. I think that doesn’t have to be the case. Let’s try the very non-Gaussian joint distribution we get if we square both x and y. Here’s the scattergram and 50% and 95% data ellipses for that.\n\n\nShow code\n\ntibDat %>%\n  mutate(xSqrd = x^2,\n         ySqrd = y^2) -> tibDat\n\ncar::dataEllipse(tibDat$xSqrd, tibDat$ySqrd) -> retDat2 # collect up data for the lines\n\n\nShow code\n\n# str(retDat)\nretDat2$`0.5` %>%\n  as_tibble() -> tibSqrd50\n\nretDat2$`0.95` %>%\n  as_tibble() -> tibSqrd95\n\nggplot(data = tibDat,\n       aes(x = xSqrd, y = ySqrd)) +\n  geom_point(alpha = .4) +\n  geom_smooth(method = \"lm\") +\n  xlim(c(0, 9)) +\n  ylim(c(0, 9)) +\n  coord_fixed(1) +\n  geom_path(data = tib50,\n            aes(x = x, y = y), colour = \"red\") +\n  geom_path(data = tib95,\n            aes(x = x, y = y), colour = \"orange\")\n\n\n\nAnd here is the confidence ellipse from car::confidenceEllipse().\n\n\nShow code\n\ncar::confidenceEllipse(lm(ySqrd ~ xSqrd, data = tibDat))\n\n\n\nSame by ggplot.\n\n\nShow code\n\nellipse::ellipse(lm(ySqrd ~ xSqrd, data = tibDat)) -> matEllipseEllipse2\n\n### rather clumsy creation of tibble of parameters for ggplot\nlm(ySqrd ~ xSqrd, data = tibDat)$coefficients -> vecLM2\nbind_cols(Intercept = vecLM2[1], Slope = vecLM2[2]) -> tibParms2\n\n### slightly nicer creation of tibble of the points on the ellipse\nmatEllipseEllipse2 %>%\n  as_tibble() %>%\n  rename(Intercept = `(Intercept)`,\n         Slope = xSqrd) -> tmpTib2\n\n### plot those\nggplot(data = tmpTib2,\n       aes(x = Intercept, y = Slope)) +\n  geom_path() +\n  geom_point(data = tibParms2,\n             colour = \"blue\", \n             size = 3)\n\n\n\nOK. I think that’s enough on this!\n\n\n\n",
    "preview": "https://www.psyctc.org/psyctc/wp-content/uploads/2022/01/dataEllipse-scaled.jpg",
    "last_modified": "2023-08-25T13:41:17+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-12-compiling-packages-reporting-missing-headers-in-windows/",
    "title": "Compiling packages reporting missing headers in windows",
    "description": "For anyone else who hits this and doesn't want to wait for someone to put\nthe compiled package into CRAN",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "R geeky",
      "R packages",
      "reminder (for CE!)"
    ],
    "contents": "\n\nContents\nGeeky background\nThe issue\n“SOLVED”, “howto”, solution!\nMy unpacking of what I had to do\n\n\n\nShow code\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nas_tibble(list(x = 1,\n               y = 1)) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"Rtools, Bash and pacman:\\npackages for packages!\",\n            size = 12,\n            colour = \"red\",\n            angle = 30) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\n\nLanguage in the text tweaked a bit 9.viii.23\nThis really is pretty geeky and probably mainly for my benefit when, as I suspect I will, I hit it in the future and have forgotten these details.\nGeeky background\nIn case you don’t know, most software is written in “source code” and then compiled to create an executable: the program itself that can be run within an operating system if it compiles correctly. (Actually, some code is “interpreted” and run directly, line by line; R and most forms of BASIC on which many people learned to program a few decades ago are such interpreted languages.)\nR packages for Windoze exist in two forms: executable/binary packages which run directly within R if installed with\ninstall.packages(\"packagename\"))\nand packages which need to be compiled before they can be run within R.\nThe issue\nRecently I found that a number of R packages were reporting that there were upgrades available but only available as source packages, i.e. requiring me to compile them on my own machine if I wanted the latest upgrade. That happens regularly for me but I think that in the default R setup in Windoze R won’t waste your time telling you about the upgrade being available because the default R setup assumes you’re not going to want to do that and doesn’t install all the extra software necessary to compile source packages.\nTo compile R packages from source in Windoze you need software which is provided by the Rtools toolset. See https://cran.r-project.org/bin/windows/Rtools/. Rtools is not an R package, it’s a collection of programs that run under Windoze and a lot of other support materials. Because it’s not an R package you don’t install it with\ninstall.packages()\nbut you install it into Windoze like any other Windoze program: that confused me some years ago. As well as installing Rtools, for it to work from R to compile source packages you must also add the location of Rtools to the Windoze path so R knows where to find the tools: https://github.com/r-windows/docs/blob/master/rtools40.md explains this. (In my VM Windoze I found that I had to put the location of Rtools in the system path not the user path for the Rtools shell to find Rtools, we’ll come to that below.)\nRtools, as the name suggests, gives you all the tools necessary to compile many packages. All the tools are open source so there is no charge for Rtools. Furthermore, it ensures that compiling R packages is entirely automated: usually all you have to do if you have installed Rtools is to say “yes” when asked if you want local compiling of an R package where the source package is more up to date than the compiled version. Then R crunches through the compiling with a fascinating cascade of messages from the various stages and then you get the usual messages that installation has worked (in my experience it’s extremely rare that it doesn’t and probably means that your version of Rtools has drifted out of synch with the version of R you are using).\nSo if you have installed Rtools then if you use the menu option to update packages R will will give you the option to compile locally (i.e. on your own machine) if the source package is more up to date than the executable package. As I say, in my experience it’s very rare that compiling will fail if you have the correct version of Rtools for your version of R. When that happens I find that usually I only have to wait a few days and the compiled package, or a new source package that has fixed whatever failed, appears on CRAN and your package updating works for that package again. Even more occasionally you wait some days and still the issue doesn’t go away and you start to wonder if there is something wrong with your system!\nThis happened for me with three packages: gsl, igraph and nloptr. This is where I discovered that sometimes you don’t just need Rtools to compile source packages locally but you may also need some packages for packages.\nWhat was happening was that instead of the cascade of compilation messages (and the occasional warning) scooting past and ending up with a message that the package had been installed each was giving the message:\n   **********************************************\n   WARNING: this package has a configure script\n         It probably needs manual configuration\n   **********************************************\nAnd things like this:\n*** arch - i386\n\"C:/rtools40/mingw32/bin/\"gcc  -I\"C:/PROGRA~1/R/R-41~1.2/include\" -DNDEBUG -I/include         -O2 -Wall  -std=gnu99 -mfpmath=sse -msse2 -mstackrealign  -c airy.c -o airy.o\nairy.c:1:10: fatal error: gsl/gsl_sf_airy.h: No such file or directory\n #include <gsl/gsl_sf_airy.h>\n          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake: *** [C:/PROGRA~1/R/R-41~1.2/etc/i386/Makeconf:238: airy.o] Error 1\nERROR: compilation failed for package 'gsl'\nI know enough about computers and compiling source code to know that message is telling me that the compiler couldn’t find a “header” file, in this case gsl_sf_airy.h. (After all, that’s what it says!!) However, searching the interweb for that didn’t come up with anything recent about anyone having this problem under windows (beware things on the interweb with problems and solutions more than a year old: too often they’ve been superceded by subsequent developments).\nI was also puzzled by all three giving the message:\n   **********************************************\n   WARNING: this package has a configure script\n         It probably needs manual configuration\n   **********************************************\nAgain, I wasn’t finding answers about this. After a while I decided that the fact I had been seeing this for a week or so and on two really rather different Windoze systems (one sitting directly on an old laptop, the other running in a VirtualBox virtual machine under Ubuntu) meant I ought to try harder to work out what was wrong. I took a punt and Emailed Jeroen Ooms the maintainer of Rtools. I got a fast response pointing me back into https://github.com/r-windows/docs/blob/master/rtools40.md, specifically to https://github.com/r-windows/docs/blob/master/rtools40.md#example-installing-a-library-with-pacman and this time I persevered trying to understand it and got there in the end. Hence I’m writing this for others who might, like me, not be sufficiently immersed in these things as people likeas Jeroen. I struggled to understand not only the instructions but also that you have to be a bit lateral thinking and search a bit more if things aren’t quite as easy as the example he has there.\n“SOLVED”, “howto”, solution!\nFirstly, I think the “this package has a configure script” is a bit of red herring as I don’t think any of these packages or their supporting facilities actually have a configure script or need manual configuration. What they do need is installation of the necessary header (and no doubt other) files and actually that’s pretty easy and in Jeroen’s page on github. However, I needed to unpack it.\nMy unpacking of what I had to do\nYou have to use two tools that are installed by Rtools.\nOne is bash which is C:.exe (assuming you have installed Rtools in the default place on your Windoze machine). If your version of Rtools is higher than 4.0 beware of these instructions and make sure what is in that github page doesn’t contradict what I am writing here: things change.\nBash is the Bourne again shell (it replaced the Bourne shell) and it’s the command prompt of Linux. (Actually it’s rather more than that and there are other shells in Linux but if you’re a fairly ordinary Windoze user that’s probably a sensible analogy; if you dive a bit deeper into Windoze then the power shell is a better analogy).\nBash allows you to run a collection of Linux utilities that are provided in Rtools including the crucial pacman. But before we get there …\n… first launch bash. I made a shortcut to C:.exe using the Windoze file explorer and put onto the desktop because I like working that way but bash is in the Windoze app menu under Rtools so you can launch it by clicking on it there (and I think getting to it that way will make sure bash finds all the Rtools components, launching from the executable or a short cut to it I found I had to have the C: location in the system path.)\nPacman is a package manager (something in the name?) and it installs packages of software beyond those already in Rtools including the ones I was missing.\n[Note here: these are the “packages for packages”: the packages that pacman manages are packages of software, including header files, that are needed in order to compile the R packages. These are two analogical but completely different uses of “package”.]\nSo now you are in the Bash shell and can use pacman. Start by typing\npacman -Syu\nwhich updates pacman’s repositories of information.\nNow let’s get what we need for the gsl package:\npacman -S mingw-w64-{i686,x86_64}-gsl\nwhich pulls down (-S = synchronise) the package gsl where we need it and the “{i686,x86_64}” ensures that both the versions for 32 bit (i686) and for 64 bit R (x86_64 doh!) are pulled down.\nNow if you relaunch R and type\n\ninstall.packages(“gsl”, type = “source”)\n\nor if you use the “packages, Update packages …” menu entry, you should find that gsl compiles nicely.\nFor the nloptr R package it’s slightly less obvious: you need\npacman -S mingw-w64-{i686,x86_64}-gsl\n(not the crucial absence of the “r” on the end!) I had to do a bit of searching on the interweb to find that.\nFor igraph it’s even less obvious, what you need is\npacman -S mingw-w64-{i686,x86_64}-glpk\npacman -S mingw-w64-{i686,x86_64}-libxml2\nThat one took a bit more searching from the error message to get there but it wasn’t very difficult.\nThat’s it! Problem cracked. Huge thanks to Jeoem Ooms for a nearly instant response, for pushing me in the right direction but above all for his work on Rtools and the other open source projects he supports … and thanks to the package maintainers for the three packages and really everyone contributing to R.\n\n\n\n",
    "preview": "posts/2021-11-12-compiling-packages-reporting-missing-headers-in-windows/compiling-packages-reporting-missing-headers-in-windows_files/figure-html5/createGraphic-1.png",
    "last_modified": "2023-08-25T13:40:50+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-09-ombookglossary/",
    "title": "For OMbook glossary",
    "description": "Code used for entries in the glossary for the OMbook",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-11-09",
    "categories": [
      "OMbook & glossary"
    ],
    "contents": "\n\nContents\nExplanation\nNotes to self\nEntries\nIllustrating mean and median from modified (truncated) Gaussian: pseudo CORE-OM “clinical” scores.\nNow illustrate mean and median on positively skew data\n\nIllustrating quartiles and IQR\nMore on quartiles\nUniform distribution\nGaussian distribution\nThrowing dice and central limit theorem\nTossing coins and the central limit theorem\nVariance: Gaussian\nVariance: real, from CORE-OM items\nHistograms and barplots\nBoxplots: Gaussian\nBoxplots: real, age and CORE-OM scores\nNotched boxplots: Gaussian\nNotched boxplots: real, age & CORE-OM scores\nViolin plot\n\n\nExplanation\nThis file is simply a cumulative collection of code I wrote to create graphics (or occasionally raw numbers) for entries in the glossary for our book Outcome measures and evaluation in counselling and psychotherapy.\nInformation about the book is at https://ombook.psyctc.org/SAGE/\nSupplementary information is at https://ombook.psyctc.org/book/ and …\n… the glossary itself is at https://ombook.psyctc.org/glossary/\nI’m not claiming the code is good, some of it certainly isn’t, but it works. Some of it uses real data which for now I am not making available until I am sure that is safe and OK with others involved in collecting the data.\nNotes to self\nBest to store theme\nAlways set xlab and ylab explicitly\nUsing ggsave helps create standard size\nProbably correct not to bother with plot titles for simple plots\n\n\nShow code\n\nsetwd(\"/media/chris/Clevo_SSD2/Data/MyR/R/distill_blog/test2/_posts/2021-11-09-ombookglossary\")\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(pander)\nlibrary(CECPfuns)\n\n### set theme here\ntheme_set(theme_bw())\ntheme_update(plot.title = element_text(hjust = .5),\n             plot.subtitle = element_text(hjust = .5),\n             axis.title = element_text(size = 15))\n\n\nEntries\nIllustrating mean and median from modified (truncated) Gaussian: pseudo CORE-OM “clinical” scores.\n\n\nShow code\n\nset.seed(12345)\nvalN <- 350\nvalMean <- 14\nvalMin <- 0\nvalMax <- 40\nvalSD <- 7\n\nrnorm(valN, valMean, valSD) %>%\n  as_tibble() %>%\n  mutate(score = round(value),\n         ### reset out of range scores\n         score = if_else(score < 0, 0, score),\n         score = if_else(score > 40, 40, score)) -> tibDat\n\ntibDat %>%\n  summarise(min = min(score),\n            lQuart = quantile(score, .25),\n            mean = mean(score),\n            median = median(score),\n            uQuart = quantile(score, .75),\n            max = max(score),\n            SD = sd(score)) -> tibSummary\n\n\nggplot(data = tibDat,\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = tibSummary$mean, \n             colour = \"blue\",\n             size = 2) +\n    geom_vline(xintercept = tibSummary$median, \n             colour = \"green\",\n             size = 2) +\n  scale_x_continuous(breaks = seq(0, 40, 2)) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"mean.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nNow illustrate mean and median on positively skew data\n\n\nShow code\n\ntibDat %>%\n  ### skew by exponentiation\n  mutate(score2 = score^1.9) -> tibDat\n\nmax(tibDat$score2)\n\n[1] 767.6734\n\nShow code\n\nmean(tibDat$score2)\n\n[1] 200.9804\n\nShow code\n\nmedian(tibDat$score2)\n\n[1] 171.6222\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = score2)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = mean(tibDat$score2), \n             colour = \"blue\",\n             size = 2) +\n  geom_vline(xintercept = median(tibDat$score2), \n             colour = \"green\",\n             size = 2) +\n  scale_x_continuous(breaks = seq(0, 800, 50)) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"mean2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\n\n\nShow code\n\nset.seed(12345)\nvalN <- 1750\nvalMean <- 14\n\nrnorm(valN) %>%\n  as_tibble() %>%\n  rename(score = value) -> tibSymm\n\n\nggplot(data = tibSymm,\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"symm.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nstandardiseVar <- function(x){\n  x <- x - mean(x, na.rm = TRUE)\n  x / sd(x, na.rm = TRUE)\n}\ntibSymm %>%\n  mutate(score2 = (5 + score)^3,\n         score2 = standardiseVar(score2)) -> tibPosSkew\n\nggplot(data = tibPosSkew,\n       aes(x = score2)) + \n  geom_histogram(center = TRUE) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"posSkew.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### now negative skew\ntibSymm %>%\n  mutate(score3 = log(5 + score),\n         score3 = standardiseVar(score3)) -> tibNegSkew\n\nggplot(data = tibNegSkew,\n       aes(x = score3)) + \n  geom_histogram(center = TRUE) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"negSkew.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nIllustrating quartiles and IQR\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = tibSummary$mean, \n             colour = \"blue\",\n             size = 2) +\n    geom_vline(xintercept = tibSummary$median, \n             colour = \"green\",\n             size = 2) +\n  scale_x_continuous(breaks = seq(0, 40, 2)) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\ntibDat %>%\n  rename(symm = score,\n         skew = score2) %>%\n  pivot_longer(cols = symm:skew, names_to = \"dist\", values_to = \"score\") -> tibDatLong\n\ntibDatLong %>%\n  group_by(dist) %>%\n  summarise(min = min(score),\n            lQuart = quantile(score, .25),\n            mean = mean(score),\n            median = median(score),\n            uQuart = quantile(score, .75),\n            max = max(score),\n            SD = sd(score)) -> tibSummary2\n\ntibSummary2 %>%\n  select(dist, ends_with(\"Quart\")) %>%\n  pivot_longer(cols = ends_with(\"Quart\")) -> tibIQR\n\n\nggplot(data = tibDatLong,\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(data = tibSummary2,\n             aes(xintercept = median),\n             colour = \"green\",\n             size = 2) +\n  geom_vline(data = tibSummary2,\n             aes(xintercept = mean),\n             colour = \"red\",\n             size = 1) +\n  geom_vline(data = tibSummary2,\n             aes(xintercept = lQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(data = tibSummary2,\n             aes(xintercept = uQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_line(data = tibIQR,\n            aes(y = 5, x = value),\n            arrow = arrow(ends = \"both\"),\n            colour = \"blue\",\n            size = 1.2) +\n  facet_wrap(facets = vars(dist),\n             nrow = 2,\n             scales = \"free\") +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"IQR.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibSummary2 %>%\n  filter(dist == \"symm\") -> tibSummary2symm\ntibIQR %>%\n  filter(dist == \"symm\") -> tibIQRsymm\n\nggplot(data = filter(tibDatLong, dist == \"symm\"),\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(data = tibSummary2symm,\n             aes(xintercept = median),\n             colour = \"green\",\n             size = 2) +\n  geom_vline(data = tibSummary2symm,\n             aes(xintercept = mean),\n             colour = \"red\",\n             size = 1) +\n  geom_vline(data = tibSummary2symm,\n             aes(xintercept = lQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(data = tibSummary2symm,\n             aes(xintercept = uQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_line(data = tibIQRsymm,\n            aes(y = 5, x = value),\n            arrow = arrow(ends = \"both\"),\n            colour = \"blue\",\n            size = 1.2) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"IQRsymm.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\ntibSummary2 %>%\n  filter(dist == \"skew\") -> tibSummary2skew\ntibIQR %>%\n  filter(dist == \"skew\") -> tibIQRskew\n\nggplot(data = filter(tibDatLong, dist == \"skew\"),\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(data = tibSummary2skew,\n             aes(xintercept = median),\n             colour = \"green\",\n             size = 2) +\n  geom_vline(data = tibSummary2skew,\n             aes(xintercept = mean),\n             colour = \"red\",\n             size = 1) +\n  geom_vline(data = tibSummary2skew,\n             aes(xintercept = lQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(data = tibSummary2skew,\n             aes(xintercept = uQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_line(data = tibIQRskew,\n            aes(y = 5, x = value),\n            arrow = arrow(ends = \"both\"),\n            colour = \"blue\",\n            size = 1.2) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"IQRskew.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nMore on quartiles\n\n\nShow code\n\nvalN <- 300\nset.seed(12345)\n\nrnorm(valN, 40, 5) %>%\n  as_tibble() %>%\n  rename(score = value) -> tibDat1\n\nggplot(data = tibDat1,\n       aes(x = score)) +\n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = mean(tibDat1$score),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(xintercept = quantile(tibDat1$score, c(.25, .75)),\n             colour = \"green\",\n             size = 2) +\n  xlab(\"Score\") +\n  ylab(\"Count\") +\n  scale_x_continuous(breaks = seq(0, 100, 10),\n                     limits = c(0, 100))\n\n\nShow code\n\nggsave(filename = \"quartiles1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nmean(tibDat1$score)\n\n[1] 40.40703\n\nShow code\n\nquantile(tibDat1$score, c(.25, .75))\n\n     25%      75% \n36.87730 43.78248 \n\nShow code\n\nrnorm(valN, 40, 15) %>%\n  as_tibble() %>%\n  rename(score = value) -> tibDat1\n\nggplot(data = tibDat1,\n       aes(x = score)) +\n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = mean(tibDat1$score),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(xintercept = quantile(tibDat1$score, c(.25, .75)),\n             colour = \"green\",\n             size = 2) +\n  xlab(\"Score\") +\n  ylab(\"Count\") +\n  scale_x_continuous(breaks = seq(0, 100, 10),\n                     limits = c(0, 100))\n\n\nShow code\n\nggsave(filename = \"quartiles2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nmean(tibDat1$score)\n\n[1] 41.16123\n\nShow code\n\nquantile(tibDat1$score, c(.25, .75))\n\n     25%      75% \n32.48149 50.16902 \n\nUniform distribution\n\n\nShow code\n\nset.seed(12345)\nc(6, 6, 6, 6, 12, 18, 24, 50, 100, 200, 500, 1000, 1000, 5000, 10000) %>%\n  as_tibble() %>%\n  rename(sampSize = value) %>%\n  mutate(sampID = row_number()) %>%\n  # group_by(sampID) %>%\n  # uncount(weights = sampSize, .id  = \"indID\")\n  rowwise() %>%\n  mutate(values = list(sample(1:6, sampSize, replace = TRUE))) %>%\n  ungroup() %>%\n  unnest_longer(values) -> tibUnif\n\n# tibUnif\n\nggplot(data = filter(tibUnif, sampSize == 6),\n       aes(x = values)) +\n  # geom_histogram(binwidth = 1,\n  #                boundary = 0) +\n  geom_bar() +\n  facet_grid(rows = vars(sampID),\n             cols = NULL,\n             scales = \"free_y\") +\n  scale_y_continuous(breaks = 0:2) +\n  scale_x_continuous(name = \"Observed scores on each die\", breaks = 1:6)\n\n\nShow code\n\nggsave(filename = \"dice1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibUnif %>%\n  filter(sampSize > 6 & sampSize < 200) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = values)) +\n  geom_bar(aes(y = ..prop..)) +\n  facet_grid(rows = vars(sampSize),\n             cols = NULL,\n             scales = \"fixed\") +\n  scale_x_continuous(name = \"Observed scores on each die\", breaks = 1:6) +\n  scale_y_continuous(name = \"Proportion\", breaks = (0:5)/10)\n\n\nShow code\n\nggsave(filename = \"dice2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibUnif %>%\n  filter(sampSize > 200) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = values)) +\n  geom_bar(aes(y = ..prop..)) +\n  facet_grid(rows = vars(sampSize),\n             cols = NULL,\n             scales = \"fixed\") +\n  scale_x_continuous(name = \"Observed scores on each die\", breaks = 1:6) +\n  scale_y_continuous(name = \"Proportion\", breaks = (0:5)/10)\n\n\nShow code\n\nggsave(filename = \"dice3.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nGaussian distribution\n\n\nShow code\n\nseq(-5, 5, length = 5001) %>%\n  as_tibble() %>%\n  mutate(p = dnorm(value)) -> tibGauss1\n\nggplot(data = tibGauss1,\n       aes(x = value, y = p)) +\n  geom_line() +\n  xlab(\"Observed value\") +\n  ylab(\"Probability\")\n\n\nShow code\n\nggsave(filename = \"Gauss1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nThrowing dice and central limit theorem\n\n\nShow code\n\nthrowDice <- function(nThrows, nSides = 6, scoreSides = 1:6){\n  ### function to simulate throwing dice (or anything really)\n  ### defaults to six sided die with scores 1:6 but you can override that\n  if(nThrows <= 0) {\n    stop(\"nThrows must be positive\")\n  }\n  if(nThrows > 8) {\n    stop(\"nThrows must be under 9 (to keep things easy!)\")\n  }\n  if(nThrows - round(nThrows) > .Machine$double.eps) {\n    warning(\"nThrows wasn't integer, rounded to integer\")\n    nThrows <- round(nThrows)\n  }\n  newScores <- scoreSides\n  while(nThrows > 1) {\n    newScores <- as.vector(outer(newScores, scoreSides, FUN = \"+\"))\n    nThrows <- nThrows - 1\n  }\n  newScores\n}\n# throwDice(0)\n# throwDice(1)\n# throwDice(1.1)\n# throwDice(11)\n# throwDice(2)\n# length(throwDice(2))\n# min(throwDice(2))\n# max(throwDice(2))\n# nThrows <- 3\n# throwDice(nThrows)\n# length(throwDice(nThrows))\n# min(throwDice(nThrows))\n# max(throwDice(nThrows))\n# nThrows <- 4\n# throwDice(nThrows)\n# length(throwDice(nThrows))\n# min(throwDice(nThrows))\n# max(throwDice(nThrows))\n\n\n1:8 %>%\n  as_tibble() %>%\n  rename(nThrows = value) %>%\n  rowwise() %>%\n  mutate(score = list(throwDice(nThrows)),\n         nThrowsFac = factor(nThrows)) %>%\n  ungroup() %>%\n  unnest_longer(score) %>%\n  group_by(nThrowsFac) %>%\n  mutate(nScores = n()) %>%\n  ungroup() %>%\n  group_by(nThrowsFac, score) %>%\n  summarise(nThrows = first(nThrows),\n            nScores = first(nScores),\n            n = n(),\n            p = n / nScores) %>%\n  ungroup() -> tibDiceThrows\n\nggplot(data = tibDiceThrows,\n       aes(x = score, y = p, colour = nThrowsFac)) +\n  geom_point()  +\n  geom_line() +\n  ylab(\"Probability\") +\n  scale_x_continuous(name = paste0(\"Total score from n dice (with n from 1 to \",\n                                   max(tibDiceThrows$nThrows),\n                                   \")\"),\n                     breaks = seq(2, max(tibDiceThrows$score), 2)) +\n  scale_colour_discrete(name = \"n(throws)\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"throwingDice.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibDiceThrows %>% \n  filter(nThrows == 6) -> tmpTib\n\ntmpTib %>%\n  summarise(n = n(),\n            minScore = min(score),\n            meanScore = Hmisc::wtd.mean(score, weights = nScores),\n            maxScore = max(score),\n            SDScore = sqrt(Hmisc::wtd.var(score, weights = nScores)),\n            totP = sum(p)) -> tmpTibSumm\n\ntmpTib %>%\n  mutate(normScore = score - tmpTibSumm$meanScore,\n         normScore = normScore / 3,\n         normProb = dnorm(normScore) / 3) -> tmpTib\ntmpTib %>%\n  summarise(meanNormScore = mean(normScore),\n            SDNormScore = sd(normScore),\n            sumNormProb = sum(normProb))\n\n# A tibble: 1 × 3\n  meanNormScore SDNormScore sumNormProb\n          <dbl>       <dbl>       <dbl>\n1             0        3.03        1.00\n\nShow code\n\nggplot(data = tmpTib,\n       aes(x = score, y = p)) +\n  geom_point()  +\n  geom_line() +\n  ylab(\"Probability\") +\n  scale_x_continuous(name = \"Total score from 6 dice\",\n                     breaks = 6:36)\n\n\nShow code\n\nggsave(filename = \"throwing6Dice.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\nggplot(data = tmpTib,\n       aes(x = score, y = p)) +\n  geom_point()  +\n  geom_line() +\n  geom_point(aes(y = normProb), \n            colour = \"green\") +\n  geom_line(aes(y = normProb), \n            colour = \"green\") +    \n  ylab(\"Probability\") +\n  scale_x_continuous(name = \"Total score from 6 dice\",\n                     breaks = 2:max(tmpTib$score)) +\n  scale_colour_discrete(name = \"n(throws)\")\n\n\nShow code\n\nggsave(filename = \"throwing6DiceWithGaussian.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\nggplot(data = tmpTib,\n       aes(x = score, y = p)) +\n  geom_point()  +\n  geom_line() +\n  ylab(\"Probability\") +\n  scale_x_continuous(name = \"Total score from 6 dice\",\n                     breaks = 2:max(tmpTib$score)) +\n  scale_colour_discrete(name = \"n(throws)\")\n\n\nShow code\n\nggsave(filename = \"throwing6Dice.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nTossing coins and the central limit theorem\n\n\nShow code\n\nset.seed(12345)\nnReps <- 50000\nc(1:5, seq(10, 50, 10), 100, 200, 300, 400, 500) %>%\n  as_tibble() %>%\n  rename(nThrows = value) %>%\n  rowwise() %>%\n  mutate(score = list(rbinom(nReps, nThrows, .5))) %>%\n  ungroup() %>%\n  unnest_longer(score) %>%\n  group_by(nThrows, score) %>%\n  summarise(n = n(),\n            p = n / nReps,\n            nThrowsFac = factor(nThrows)) %>%\n  ungroup() %>%\n  group_by(nThrows) %>%\n  mutate(scaledScore = scale(score)) -> tibBinom\n\n\nggplot(data = tibBinom,\n       aes(x = score, y = p, colour = nThrowsFac)) +\n  geom_point() +\n  geom_line()\n\n\nShow code\n\nggsave(filename = \"tossingCoins1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nggplot(data = tibBinom,\n       aes(x = scaledScore, y = p, colour = nThrowsFac)) +\n  geom_point() +\n  geom_line()\n\n\nShow code\n\nggsave(filename = \"tossingCoinsScaled.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\npbinom(0:2, 2, .5)\n\n[1] 0.25 0.75 1.00\n\nShow code\n\nc(1:5, 10, 20, 50, 100) %>%\n  as_tibble() %>%\n  rename(nThrows = value) %>%\n  rowwise() %>%\n  mutate(pVals = list(pbinom(0:nThrows, nThrows, .5))) %>%\n  ungroup() %>%\n  unnest_longer(pVals) %>%\n  group_by(nThrows) %>%\n  mutate(score = row_number() - 1,\n         nThrowsFac = factor(first(nThrows)),\n         p = if_else(row_number() == 1, pVals, pVals - lag(pVals)),\n         meanScore = Hmisc::wtd.mean(score, weights = p),\n         sdScore = sqrt(Hmisc::wtd.var(score, weights = p, normwt = TRUE)),\n         stdScore = (score - meanScore) / sdScore) -> tibpBinom\n\n\n# tibGauss1 %>%\n#   summarise(max(p)) # .399\n# \n# tibpBinom %>%\n#   filter(nThrows == 100) %>%\n#   summarise(max(p)) # .0796\n\nggplot(data = tibpBinom,\n       aes(x = stdScore, y = p, colour = nThrowsFac)) +\n  geom_point() +\n  geom_line() +\n  geom_line(inherit.aes = FALSE,\n            data = tibGauss1,\n            aes(x = value, y = p * .0796 / .399)) +\n  ylab(\"Probability\") +\n  xlab(\"Standardised score across tossing n fair coins, heads = 1, tails = 0\") +\n  scale_colour_discrete(name = \"n(throws)\")\n\n\nShow code\n\nggsave(filename = \"pbinomWithGauss.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nVariance: Gaussian\n\n\nShow code\n\nseq(0, 20, length = 5001) %>%\n  as_tibble() %>%\n  mutate(NHS = dnorm(value, 8, 2),\n         HS = dnorm(value, 12, 1)) %>% \n  pivot_longer(cols = ends_with(\"HS\"), names_to = \"Population\", values_to = \"p\") -> tibGauss2\n\nggplot(data = tibGauss2,\n       aes(x = value, y = p, colour = Population)) +\n  geom_line(size = 2, alpha = .8) +\n  xlab(\"Observed score\") +\n  ylab(\"Probability\")\n\n\nShow code\n\nggsave(filename = \"Gauss2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nVariance: real, from CORE-OM items\n\n\nShow code\n\n# tmpDatDir <- \"~/internalHDD/Data/CORE/translations/SPA_other/Clara/Our_papers/NClinPaper\"\ntmpDatDir <- \"/media/chris/Clevo_SSD2/Data/CORE/translations/SPA_other/Clara/Our_papers/2020_NClinPaper\"\nreadxl::read_excel(paste0(tmpDatDir, \"/\", \"Muestra_NoClinica_CP180520.xlsx\")) %>%\n  mutate(Gender = recode(Genero,\n                         Masculino = \"Male\",\n                         Femenino = \"Female\")) -> tibRawDat\n\n### this is a silly way to get around having to up the na.rm = TRUE clause in the mutate\nmeanNotNA <- function(x){\n  mean(x, na.rm = TRUE)\n}\nmedianNotNA <- function(x){\n  median(x, na.rm = TRUE)\n}\nvarNotNA <- function(x){\n  var(x, na.rm = TRUE)\n}\nsdNotNA <- function(x){\n  sd(x, na.rm = TRUE)\n}\nminNotNA <- function(x){\n  min(x, na.rm = TRUE)\n}\nmaxNotNA <- function(x){\n  max(x, na.rm = TRUE)\n}\n\n# tibRawDat %>% \n#   select(starts_with(\"COREOM01\")) %>%\n#   corrr::correlate(diagonal = 1) %>%\n#   pivot_longer(cols = -term) %>%\n#   filter(term != name) %>%\n#   arrange(desc(value))\n### OK, all recoded\n\ntibRawDat %>%\n  filter(Excluded == \"NO\" & !is.na(Genero)) -> tibUseDat\n\ntibUseDat %>%\n  select(Gender, Edad, starts_with(\"COREOM01\")) %>%\n  select(-COREOM01_35) %>%\n  pivot_longer(cols = starts_with(\"COREOM01\"), names_to = \"Item\", values_to = \"Score\") %>%\n  # group_by(Gender, Item) %>%\n  group_by(Item) %>%\n  summarise(totN = n(),\n            nOK = getNOK(Score),\n            min = minNotNA(Score),\n            mean = meanNotNA(Score),\n            median = medianNotNA(Score),\n            max = maxNotNA(Score),                                      \n            var = varNotNA(Score),\n            sd = sdNotNA(Score)) %>%\n  ungroup() %>%\n  # filter(min != 0 | max != 4) ## OK full range on all items\n  arrange(var) %>%\n  filter(row_number() %in% c(1,2,3, 32, 33, 34)) -> tmpTibSummary\n\ntmpTibSummary %>%\n  select(Item) %>% \n  pull() -> tmpVecItems\n\nc(\"R: plans to end my life\",\n  \"R: hurt myself physically...\",\n  \"R: threatened or intimidated\",\n  \"P: difficulty getting to sleep ...\",\n  \"P: thought to blame ...\",\n  \"P: felt unhappy\") -> tmpVecNames\n\ntibUseDat %>%\n  select(all_of(tmpVecItems)) -> tmpTibRawScores\n\ntmpTibRawScores %>%\n  pivot_longer(cols = everything(), names_to = \"Item\", values_to = \"Score\") %>%\n  filter(!is.na(Score)) %>%\n  mutate(Item = ordered(Item,\n                        levels = tmpVecItems,\n                        labels = tmpVecNames)) -> tmpTibLongScores\n\ntmpTibSummary %>%\n  mutate(Item = ordered(Item,\n                        levels = tmpVecItems,\n                        labels = tmpVecNames),\n         meanMinusSD = mean - sd,\n         meanPlusSD = mean + sd) -> tmpTibSummary\n\n\n\nggplot(data = tmpTibLongScores,\n       aes(x = Score)) +\n  ylim(0, 1000) +\n  geom_bar(width = 1) +\n  geom_vline(data = tmpTibSummary,\n             aes(xintercept = mean),\n             colour = \"green\") +\n  facet_wrap(facets = vars(Item), ncol = 3)\n\n\nShow code\n\nggsave(filename = \"Variance1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nggplot(data = tmpTibLongScores,\n       aes(x = Score)) +\n  ylim(0, 1000) +\n  geom_bar(width = 1) +\n  geom_vline(data = tmpTibSummary,\n             aes(xintercept = mean),\n             colour = \"green\") +\n  geom_segment(inherit.aes = FALSE,\n               data = tmpTibSummary,\n               aes(y = 900, yend = 900,\n                   x = meanMinusSD,\n                   xend = meanPlusSD),\n               colour = \"blue\",\n               lineend = \"round\",\n               arrow = arrow(ends = \"both\",\n                             length = unit(2, \"mm\"))) +\n  geom_text(data = tmpTibSummary,\n            aes(y = 960, x = 4.5, label = paste0(\"Mean: \", \n                                                 round(mean, 2),\n                                                 \"\\nVar: \",\n                                                 round(var, 2),\n                                                 \"\\nSD: \",\n                                                 round(sd, 2))),\n            size = 2.5,\n            hjust = 1,\n            vjust = 1) +\n  facet_wrap(facets = vars(Item), ncol = 3)\n\n\nShow code\n\nggsave(filename = \"Variance2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nHistograms and barplots\n\n\nShow code\n\nggplot(data = tibRawDat,\n       aes(x = Gender)) +\n  geom_bar() +\n  xlab(\"Gender\") +\n  theme(axis.text = element_text(size = 40),\n        axis.title = element_text(size = 50)) -> ggGender1\n\nggplot(data = tibRawDat,\n       aes(x = Edad)) +\n  geom_histogram(center = TRUE,\n                 breaks = 18:80) +\n  xlab(\"Age\") +\n  theme(axis.text = element_text(size = 40),\n        axis.title = element_text(size = 50)) -> ggAge1\n\nggpubr::ggarrange(ggGender1, ggAge1) -> ggArrange1\n\nggpubr::ggexport(ggArrange1,\n         filename = \"Histogram1.png\",\n         width = 1700,\n         height = 1470,\n         units = \"px\")\n\n[1] \"Histogram1%03d.png\"\n\nShow code\n\nggplot(data = tibRawDat,\n       aes(x = Edad)) +\n  geom_histogram(center = TRUE,\n                 breaks = c(18, 20, 30, 40, 50, 80),\n                 closed = \"right\") +\n  xlab(\"Age\") +\n  scale_x_continuous(breaks = seq(10, 80, 10))\n\n\nShow code\n\n  # theme(axis.text = element_text(size = 40),\n  #       axis.title = element_text(size = 50))\n\nggsave(filename = \"HistAge.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tibRawDat,\n       aes(x = Edad)) +\n  geom_histogram(aes(y = stat(count) / sum(count)),\n                 center = TRUE,\n                 breaks = c(18, 20, 30, 40, 50, 80),\n                 closed = \"right\") +\n  xlab(\"Age\") +\n  ylab(\"Proportion\") +\n  scale_x_continuous(breaks = seq(10, 80, 10))\n\n\nShow code\n\n  # theme(axis.text = element_text(size = 40),\n  #       axis.title = element_text(size = 50))\n\nggsave(filename = \"HistAge2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tibRawDat,\n       aes(x = Edad, fill = Gender)) +\n  geom_histogram(aes(y = stat(count) / sum(count)),\n                 position = \"dodge2\",\n                 center = TRUE,\n                 breaks = c(18, seq(20, 80, 10)),\n                 closed = \"left\") +\n  xlab(\"Age\") +\n  ylab(\"Proportion\") +\n  scale_x_continuous(breaks = seq(20, 80, 5))\n\n\nShow code\n\nggsave(filename = \"HistAgeGendDodge.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nggplot(data = tibRawDat,\n       aes(x = Edad, fill = Gender)) +\n  geom_histogram(aes(y = stat(count) / sum(count)),\n                 position = \"stack\",\n                 center = TRUE,\n                 breaks = c(18, seq(20, 80, 10)),\n                 closed = \"left\") +\n  xlab(\"Age\") +\n  ylab(\"Proportion\") +\n  scale_x_continuous(breaks = seq(20, 80, 5))\n\n\nShow code\n\nggsave(filename = \"HistAgeGendStack.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\ntibRawDat %>%\n  filter(!is.na(Edad)) %>%\n  mutate(Age = case_when(Edad < 20 ~ \"<20\",\n                         Edad < 30 ~ \"20 to 29\",\n                         Edad < 40 ~ \"30 to 39\",\n                         Edad < 50 ~ \"40 to 49\",\n                         Edad >= 50 ~ \"50 and over\"),\n         Age = ordered(Age, \n                       levels = c(\"<20\",\n                                  \"20 to 29\",\n                                  \"30 to 39\",\n                                  \"40 to 49\",\n                                  \"50 and over\"))) -> tmpTib #%>% select(Edad, Age)\n\ntmpTib %>%\n  tabyl(Age) %>%\n  adorn_pct_formatting(digits = 1) %>%\n  pander(justify = \"lrr\")\n\nAge\nn\npercent\n<20\n148\n14.9%\n20 to 29\n545\n55.1%\n30 to 39\n131\n13.2%\n40 to 49\n80\n8.1%\n50 and over\n86\n8.7%\n\nShow code\n\nggplot(data = tmpTib,\n       aes(x = Age)) +\n  geom_bar() +\n  xlab(\"Age\") \n\n\nShow code\n\nggsave(filename = \"BarAge.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nBoxplots: Gaussian\n\n\nShow code\n\nset.seed(12345)\ntmpValN <- 10000\nrnorm(tmpValN) %>% \n  as_tibble() -> tmpTibGaussian\n\ntmpTibGaussian %>%\n  summarise(min = min(value),\n            lwrQ = quantile(value, .25),\n            median = median(value),\n            uprQ = quantile(value, .75),\n            max = max(value))\n\n# A tibble: 1 × 5\n    min   lwrQ   median  uprQ   max\n  <dbl>  <dbl>    <dbl> <dbl> <dbl>\n1 -3.88 -0.664 0.000494 0.663  3.36\n\nShow code\n\nggplot(data = tmpTibGaussian,\n       aes(y = value)) +\n  geom_boxplot(fill = \"grey\") +\n  ylim(4, 4) +\n  xlab(\"\") -> ggGaussBox1\n\nggGaussBox1\n\n\nShow code\n\nggsave(filename = \"GaussBox1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tmpTibGaussian,\n       aes(y = value)) +\n  geom_boxplot(fill = \"grey\") +\n  ylim(-4, 4) +\n  xlab(\"\") +\n  theme(axis.text = element_text(size = 40),\n        axis.title = element_text(size = 50)) -> ggGaussBox2\n\nggGaussBox2\n\n\nShow code\n\nggplot(data = tmpTibGaussian,\n       aes(x = value)) +\n  geom_histogram(aes(y = ..density..),\n                 fill = \"grey\") +\n  geom_vline(xintercept = median(tmpTibGaussian$value)) +\n  geom_line(data = tibGauss1,\n            aes(x = value, y = p)) +\n  ylab(\"Probability\") -> ggGaussHistOnX\n\nggGaussHistOnX\n\n\nShow code\n\nggsave(filename = \"GaussHistOnX.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tmpTibGaussian,\n       aes(y = value)) +\n  geom_histogram(aes(x = ..density..),\n                 fill = \"grey\") +\n  geom_hline(yintercept = median(tmpTibGaussian$value)) +\n  geom_line(data = tibGauss1,\n            aes(y = value, x = p),\n            orientation = \"y\") +\n  ylim(-4, 4) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(axis.text = element_text(size = 40),\n        axis.title = element_text(size = 50)) -> ggGaussHistOnY\n\nggGaussHistOnY\n\n\nShow code\n\nggpubr::ggarrange(ggGaussBox2, ggGaussHistOnY,\n                  ncol = 2) -> ggArrange2\n\nggArrange2\n\n\nShow code\n\nggpubr::ggexport(ggArrange2,\n         filename = \"BoxAndHist.png\",\n         width = 1700,\n         height = 1470,\n         units = \"px\")\n\n[1] \"BoxAndHist%03d.png\"\n\nBoxplots: real, age and CORE-OM scores\n\n\nShow code\n\nggplot(data = tibRawDat,\n       aes(y = Edad, x = Gender, fill = Gender)) +\n  geom_boxplot(varwidth = TRUE) +\n  ylab(\"Age\")\n\n\nShow code\n\nggsave(filename = \"AgeByGenderBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  mutate(SocState = `Estado civil`,\n         SocState = recode(SocState,\n                           `Casado/a` = \"Coupled\",\n                           `Soltero`  = \"Single\",\n                           `Separado/a` = \"Separated\",\n                           `Divorciado/a` = \"Divorced\",\n                           `Unido/a` = \"Coupled\",\n                           `Unido` = \"Coupled\",\n                           `Viudo/a` = \"Widowed/Widower\")) %>%\n  rowwise() %>%\n  mutate(meanCOREOM = meanNotNA(c_across(COREOM01_01:COREOM01_34))) %>%\n  ungroup() -> tibRawDat\n\ntibRawDat %>%\n  mutate(SocStatNA = if_else(is.na(SocState), \"NA\", SocState)) %>%\n  group_by(SocStatNA) %>%\n  summarise(tmpN = n(),\n            tmpMedianAge = median(Edad, na.rm = TRUE),\n            varAge = var(Edad, na.rm = TRUE),\n            SDAge = sqrt(varAge)) -> tmpTib\n\ntmpTib %>%\n  arrange(desc(tmpN)) %>%\n  select(SocStatNA) %>%\n  pull() -> tmpVecN\n\ntmpTib %>%\n  arrange(tmpMedianAge) %>%\n  select(SocStatNA) %>%\n  pull() -> tmpVecAge\n\n\ntibRawDat %>%\n  mutate(SocStatNA = if_else(is.na(SocState), \"NA\", SocState),\n         SocStatN = ordered(SocStatNA,\n                            levels = tmpVecN,\n                            labels = tmpVecN),\n         SocStateAge = ordered(SocStatNA,\n                               levels = tmpVecAge,\n                               labels = tmpVecAge)) -> tibRawDat\n\nggplot(data = tibRawDat,\n       aes(y = Edad, x = SocStatN, fill = SocStatN)) +\n  geom_boxplot(varwidth = TRUE) +\n  geom_text(data = tmpTib,\n            inherit.aes = FALSE,\n            aes(x = SocStatNA, y = 12, label = tmpN),\n            vjust = .5) +\n  geom_text(x = .5, y = 12, label = \"n: \",\n            vjust = .5,\n            hjust = 0) +\n  ylim(10, 80) +\n  scale_fill_discrete(name = \"Social status\") +\n  ylab(\"Age\") +\n  xlab(\"Social status\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"AgeBySocStatnBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tibRawDat,\n       aes(y = meanCOREOM, x = SocStatN, fill = SocStatN)) +\n  geom_boxplot(varwidth = TRUE) +\n  geom_text(data = tmpTib,\n            inherit.aes = FALSE,\n            aes(x = SocStatNA, y = -.2, label = tmpN),\n            vjust = .5) +\n  geom_text(x = .5, y = -.2, label = \"n: \",\n            vjust = .5,\n            hjust = 0) +\n  ylim(-.5, 4) +\n  scale_fill_discrete(name = \"Social status\") +\n  ylab(\"CORE-OM score (mean item score)\") +\n  xlab(\"Social status\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"COREOMBySocStatnBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  filter(!is.na(`Tiene Hijos`)) %>%\n  mutate(nChildren = `Especifique cuántos`,\n         nChildren = if_else(nChildren > 3, 4, nChildren),\n         nChildren = if_else(`Tiene Hijos` == \"NO\", 0, nChildren),\n         nChildren = ordered(nChildren,\n                             levels = 0:4,\n                             labels = c(as.character(0:3),\n                                        \"4 or more\"))) %>%  \n  filter(!is.na(nChildren)) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(y = Edad, x = nChildren, fill = nChildren)) +\n  geom_boxplot(varwidth = TRUE) \n\n\nShow code\n\ntmpTib %>%\n  rowwise() %>%\n  mutate(meanCOREOM = meanNotNA(c_across(COREOM01_01:COREOM01_34))) %>%\n  ungroup() -> tmpTib\n\nggplot(data = tmpTib,\n       aes(y = meanCOREOM, x = nChildren, fill = nChildren)) +\n  geom_boxplot(varwidth = TRUE) \n\n\n\nNotched boxplots: Gaussian\n\n\nShow code\n\ntmpTibGaussian %>%\n  filter(row_number() <= 500) %>%\n  mutate(samp250 = if_else(row_number() <= 250, value, NA_real_),\n         samp100 = if_else(row_number() <= 100, value, NA_real_),\n         samp50 = if_else(row_number() <= 50, value, NA_real_),\n         samp10 = if_else(row_number() <= 10, value, NA_real_)) %>%\n  rename(samp500 = value) -> tmpTib\n\ntmpTib %>%\n  pivot_longer(cols = everything(), names_to = \"sample\") %>%\n  mutate(sample = ordered(sample,\n                          levels = paste0(\"samp\", c(500, 250, 100, 50, 10)))) -> tmpTibLong\n\nggplot(data = tmpTib,\n       aes(y = samp250)) +\n  geom_boxplot(fill = \"grey\", varwidth = TRUE, notch = TRUE) +\n  ylim(-4, 4) +\n  xlab(\"\") +\n  ylab(\"value\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank()) \n\n\nShow code\n\nggsave(filename = \"notchedGaussianBox1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tmpTib,\n       aes(y = samp250)) +\n  geom_boxplot(fill = \"grey\", varwidth = TRUE) +\n  ylim(-4, 4) +\n  xlab(\"\") +\n  ylab(\"value\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank()) \n\n\nShow code\n\nggsave(filename = \"notNotchedGaussianBox1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tmpTibLong,\n       aes(y = value)) +\n  geom_boxplot(fill = \"grey\", varwidth = TRUE, notch = TRUE) +\n  geom_hline(yintercept = 0) +\n  ylim(-4, 4) +\n  facet_grid(cols = vars(sample)) +\n  xlab(\"\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank()) \n\n\nShow code\n\nggsave(filename = \"notchedGaussianBoxes2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntmpTibGaussian %>%\n  mutate(simulN = row_number() %% 20) %>% \n  group_by(simulN) %>%\n  mutate(samp250 = if_else(row_number() <= 250, value, NA_real_),\n         samp100 = if_else(row_number() <= 100, value, NA_real_),\n         samp50 = if_else(row_number() <= 50, value, NA_real_),\n         samp10 = if_else(row_number() <= 10, value, NA_real_)) %>%\n  rename(samp500 = value) %>%\n  ungroup() -> tmpTib\n\ntmpTib %>%\n  filter(simulN > 1 & simulN <= 5) %>%\n  pivot_longer(cols = -simulN, names_to = \"sample\") %>%\n  mutate(sample = ordered(sample,\n                          levels = paste0(\"samp\", c(500, 250, 100, 50, 10)))) -> tmpTibLong\n\nggplot(data = tmpTibLong,\n       aes(y = value)) +\n  geom_boxplot(fill = \"grey\", varwidth = TRUE, notch = TRUE) +\n  geom_hline(yintercept = 0) +\n  ylim(-4, 4) +\n  facet_grid(cols = vars(sample),\n             rows = vars(simulN)) +\n  xlab(\"\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank()) \n\n\nShow code\n\nggsave(filename = \"notchedGaussianBoxes3.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nNotched boxplots: real, age & CORE-OM scores\n\n\nShow code\n\ntibRawDat %>%\n  group_by(Gender) %>%\n  summarise(n = n(),\n            median = median(Edad, na.rm = TRUE)) -> tmpTibSummary\n\nggplot(data = tibRawDat,\n       aes(y = Edad, x = Gender, fill = Gender)) +\n  geom_boxplot(varwidth = TRUE, notch = TRUE) +\n  geom_text(data = tmpTibSummary,\n            aes(x = Gender, y = 14, label = n),\n            vjust = 0) +\n  geom_text(x = .90, y = 14, label = \"n: \",\n            hjust = 1,\n            vjust = 0) +\n  geom_text(data = tmpTibSummary,\n            aes(x = Gender, y = 11, label = median),\n            vjust = 0,\n            fontface = \"plain\") +\n  geom_text(x = .9, y = 11, label = \"median: \",\n            hjust = 1,\n            vjust = 0,\n            fontface = \"plain\") +\n  geom_hline(yintercept = median(tibRawDat$Edad, na.rm = TRUE)) +\n  ylab(\"Age\")\n\n\nShow code\n\nggsave(filename = \"AgeByGenderNotchedBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  mutate(SocStatNA = if_else(is.na(SocState), \"NA\", SocState)) %>%\n  group_by(SocStatNA) %>%\n  summarise(tmpN = n(),\n            tmpMedianAge = median(Edad, na.rm = TRUE),\n            varAge = var(Edad, na.rm = TRUE),\n            SDAge = sqrt(varAge)) -> tmpTib\n\ntmpTib %>%\n  arrange(desc(tmpN)) %>%\n  select(SocStatNA) %>%\n  pull() -> tmpVecN\n\ntmpTib %>%\n  arrange(tmpMedianAge) %>%\n  select(SocStatNA) %>%\n  pull() -> tmpVecAge\n\n\ntibRawDat %>%\n  mutate(SocStatNA = if_else(is.na(SocState), \"NA\", SocState),\n         SocStatN = ordered(SocStatNA,\n                            levels = tmpVecN,\n                            labels = tmpVecN),\n         SocStateAge = ordered(SocStatNA,\n                               levels = tmpVecAge,\n                               labels = tmpVecAge)) -> tibRawDat\n\nggplot(data = tibRawDat,\n       aes(y = Edad, x = SocStatN, fill = SocStatN)) +\n  geom_boxplot(varwidth = TRUE, notch = TRUE) +\n  geom_hline(yintercept = median(tibRawDat$Edad, na.rm = TRUE)) +\n  geom_text(data = tmpTib,\n            inherit.aes = FALSE,\n            aes(x = SocStatNA, y = 12, label = tmpN),\n            vjust = .5) +\n  geom_text(x = .5, y = 12, label = \"n: \",\n            vjust = .5,\n            hjust = 0) +\n  ylim(10, 80) +\n  scale_fill_discrete(name = \"Social status\") +\n  ylab(\"Age\") +\n  xlab(\"Social status\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"AgeBySocStatnNotchedBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  filter(!is.na(meanCOREOM)) %>%\n  group_by(SocStatN) %>%\n  summarise(n = n(),\n            median = round(median(meanCOREOM), 1)) -> tmpTibSummary\n\nggplot(data = tibRawDat,\n       aes(y = meanCOREOM, x = SocStatN, fill = SocStatN)) +\n  geom_boxplot(varwidth = TRUE, notch = TRUE) +\n  geom_hline(yintercept = median(tibRawDat$meanCOREOM, na.rm = TRUE)) +\n  geom_text(data = tmpTibSummary,\n            inherit.aes = FALSE,\n            aes(x = SocStatN, y = -.3, label = n),\n            vjust = .5) +\n  geom_text(x = .7, y = -.3, label = \"n: \",\n            vjust = .5,\n            hjust = 1) +\n  geom_text(data = tmpTibSummary,\n            inherit.aes = FALSE,\n            aes(x = SocStatN, y = -.5, label = median),\n            vjust = .5) +\n  geom_text(x = .7, y = -.5, label = \"median: \",\n            vjust = .5,\n            hjust = 1) +  \n  ylim(-.5, 4) +\n  expand_limits(x = -.7) +\n  scale_fill_discrete(name = \"Social status\") +\n  ylab(\"CORE-OM score (mean item score)\") +\n  xlab(\"Social status\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"COREOMBySocStatnBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  filter(!is.na(`Tiene Hijos`)) %>%\n  mutate(nChildren = `Especifique cuántos`,\n         nChildren = if_else(nChildren > 3, 4, nChildren),\n         nChildren = if_else(`Tiene Hijos` == \"NO\", 0, nChildren),\n         nChildren = ordered(nChildren,\n                             levels = 0:4,\n                             labels = c(as.character(0:3),\n                                        \"4 or more\"))) %>%  \n  filter(!is.na(nChildren)) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(y = Edad, x = nChildren, fill = nChildren)) +\n  geom_boxplot(varwidth = TRUE) \n\n\nShow code\n\ntmpTib %>%\n  rowwise() %>%\n  mutate(meanCOREOM = meanNotNA(c_across(COREOM01_01:COREOM01_34))) %>%\n  ungroup() -> tmpTib\n\nggplot(data = tmpTib,\n       aes(y = meanCOREOM, x = nChildren, fill = nChildren)) +\n  geom_boxplot(varwidth = TRUE) \n\n\n\nViolin plot\n\n\nShow code\n\n### start with simple Gaussian n = 50\nset.seed(12345)\ntmpValSampN <- 50\n1:5 %>%\n  as_tibble() %>%\n  rename(simN = value) %>%\n  group_by(simN) %>%\n  mutate(values = list(rnorm(tmpValSampN))) %>%\n  ungroup() %>%\n  unnest_longer(values) %>%\n  mutate(simN = ordered(simN)) -> tmpTib\n\nggplot(data = filter(tmpTib, simN == 1),\n       aes(x = simN, y = values)) +\n  geom_violin(fill = \"grey\") +\n  xlab(\"\")\n\n\nShow code\n\nggsave(filename = \"geomViolin1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### add jittered points to the plot\nggplot(data = filter(tmpTib, simN == 1),\n       aes(x = simN, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_jitter(height = 0, width = .07, alpha = .7) +\n  xlab(\"\")\n\n\nShow code\n\nggsave(filename = \"geomViolin2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### no jittering, just transparency\nggplot(data = filter(tmpTib, simN == 1),\n       aes(x = simN, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_point(alpha = .3) +\n  xlab(\"\")\n\n\nShow code\n\nggsave(filename = \"geomViolin3.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n### now use all five simulations\nggplot(data = tmpTib,\n       aes(x = simN, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_jitter(height = 0, width = .07, alpha = .7) +\n  geom_hline(yintercept = 0) +\n  xlab(\"Simulations\")\n\n\nShow code\n\nggsave(filename = \"geomViolin4.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### now move to different sample sizes\nc(50, 100, 200, 500, 1000, 5000, 50000, 500000) -> tmpVecN\n### get sample sizes into human readable rather than scientific format\nprettyNum(tmpVecN, big.mark = \",\", scientific = FALSE) -> tmpVecLabels\n\ntmpVecN %>%\n  as_tibble() %>%\n  rename(n = value) %>%\n  ### need rowwise() otherwise dplyr seems to use same seed each time\n  rowwise() %>%\n  mutate(values = list(rnorm(n))) %>%\n  ungroup() %>%\n  ### applying the labels to get discrete and readable variable\n  mutate(nFac = ordered(n,\n                     labels = tmpVecLabels)) %>%\n  unnest_longer(values) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = nFac, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_jitter(height = 0, width = .1, alpha = .3) +\n  geom_hline(yintercept = 0) +\n  scale_x_discrete()\n\n\nShow code\n\nggsave(filename = \"geomViolin5.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### but showing the points in the larger samples is mad so ...\nggplot(data = tmpTib,\n       aes(x = nFac, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_jitter(data = filter(tmpTib, n < 5000),\n              height = 0, width = .15, alpha = .1) +\n  geom_hline(yintercept = 0) +\n  scale_x_discrete()\n\n\nShow code\n\nggsave(filename = \"geomViolin6.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n### now some real data\ntibRawDat %>% \n  filter(!is.na(SocState)) %>%\n  filter(!is.na(Gender)) %>%\n  mutate(SocStat2 = recode(SocStatNA,\n                           \"NA\" = NA_character_,\n                           \"Divorced\" = \"Single\",\n                           \"Separated\" = \"Single\",\n                           \"Widowed/Widower\" = \"Single\")) -> tmpTib\n\ntmpTib %>%\n  group_by(Gender, SocStat2) %>%\n  summarise(CI = list(getBootCImean(meanCOREOM))) %>%\n  unnest_wider(CI) -> tmpSummary\ntmpSummary\n\n# A tibble: 4 × 5\n# Groups:   Gender [2]\n  Gender SocStat2 obsmean LCLmean UCLmean\n  <chr>  <chr>      <dbl>   <dbl>   <dbl>\n1 Female Coupled    0.778   0.695   0.861\n2 Female Single     1.03    0.978   1.08 \n3 Male   Coupled    0.728   0.670   0.793\n4 Male   Single     1.03    0.967   1.09 \n\nShow code\n\nggplot(data = tmpTib,\n       aes(x = interaction(Gender, SocStat2), y = meanCOREOM, fill = Gender)) +\n  geom_violin() +\n  geom_jitter(height = 0, width = .1, alpha = .2) +\n  geom_hline(yintercept = mean(tmpTib$meanCOREOM)) +\n  ### add summary statistics\n  geom_point(data = tmpSummary,\n             aes(y = obsmean),\n             position = position_nudge(x = -.15),\n             size = 2.5) +\n  geom_linerange(data = tmpSummary,\n                 inherit.aes = FALSE,\n                 aes(x = interaction(Gender, SocStat2),\n                     ymin = LCLmean, ymax = UCLmean),\n                 position = position_nudge(x = -.15),\n                 size = 1.5) +\n  xlab(\"Gender and social status\") +\n  ylab(\"CORE-OM score (mean item score)\")\n\n\nShow code\n\nggsave(filename = \"geomViolin7.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\n\nShow code\n\n### get the data\ntibUseDat %>%\n  mutate(SocState = `Estado civil`,\n         SocState = recode(SocState,\n                           `Casado/a` = \"Coupled\",\n                           `Soltero`  = \"Single\",\n                           `Separado/a` = \"Separated\",\n                           `Divorciado/a` = \"Divorced\",\n                           `Unido/a` = \"Coupled\",\n                           `Unido` = \"Coupled\",\n                           `Viudo/a` = \"Widowed/Widower\")) %>%\n  rowwise() %>%\n  mutate(meanCOREOM = meanNotNA(c_across(COREOM01_01:COREOM01_34))) %>%\n  ungroup() -> tibUseDat\n\n### histogram of the raw data\nggplot(data = tibUseDat,\n       aes(x = meanCOREOM)) +\n  geom_histogram() +\n  xlab(\"CORE-OM total score (item mean)\") +\n  ylab(\"Count\")\n\n\nShow code\n\nggsave(filename = \"jackknife1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### get the parametric CI\nmean(tibUseDat$meanCOREOM)\n\n[1] 0.9312496\n\nShow code\n\nvar(tibUseDat$meanCOREOM)\n\n[1] 0.2700728\n\nShow code\n\nsd(tibUseDat$meanCOREOM)\n\n[1] 0.5196853\n\nShow code\n\nsd(tibUseDat$meanCOREOM)/sqrt(nrow(tmpTib))\n\n[1] 0.01646686\n\nShow code\n\nmean(tibUseDat$meanCOREOM) - 1.96*sd(tibUseDat$meanCOREOM)/sqrt(nrow(tmpTib))\n\n[1] 0.8989745\n\nShow code\n\nmean(tibUseDat$meanCOREOM) + 1.96*sd(tibUseDat$meanCOREOM)/sqrt(nrow(tmpTib))\n\n[1] 0.9635246\n\nShow code\n\n# tibUseDat %>%\n#   select(Gender, SocState, meanCOREOM) %>%\n#   drop_na() -> tmpTib\n# \n# ggplot(data = tmpTib,\n#        aes(x = Gender, y = meanCOREOM)) +\n#   geom_boxplot(varwidth = TRUE, notch = TRUE)\n# \n# ggplot(data = tmpTib,\n#        aes(x = Gender, y = meanCOREOM)) +\n#   geom_violin()\n\n### get a well validated jackknife method\nbootstrap::jackknife(tmpTib$meanCOREOM, mean)\n\n$jack.se\n[1] 0.01688115\n\n$jack.bias\n[1] 0\n\n$jack.values\n  [1] 0.9630860 0.9628791 0.9629973 0.9629418 0.9634407 0.9629678\n  [7] 0.9630564 0.9627608 0.9628809 0.9634998 0.9629086 0.9631747\n [13] 0.9627904 0.9631747 0.9630564 0.9633225 0.9633225 0.9617263\n [19] 0.9632338 0.9634112 0.9633986 0.9629086 0.9632929 0.9632042\n [25] 0.9627608 0.9631156 0.9629382 0.9624948 0.9628200 0.9630269\n [31] 0.9633816 0.9625244 0.9634703 0.9629973 0.9629973 0.9631451\n [37] 0.9623327 0.9631451 0.9628495 0.9631451 0.9628495 0.9635294\n [43] 0.9629382 0.9625244 0.9628791 0.9632634 0.9625835 0.9629678\n [49] 0.9636181 0.9628791 0.9632634 0.9631156 0.9632042 0.9632634\n [55] 0.9633682 0.9625835 0.9623766 0.9635885 0.9625835 0.9628791\n [61] 0.9632929 0.9629418 0.9637954 0.9633520 0.9635885 0.9637067\n [67] 0.9618149 0.9637067 0.9637067 0.9637363 0.9627608 0.9635294\n [73] 0.9626722 0.9629113 0.9631451 0.9630269 0.9632338 0.9627608\n [79] 0.9632634 0.9633225 0.9632042 0.9636181 0.9620810 0.9630636\n [85] 0.9627017 0.9618741 0.9629113 0.9628200 0.9637336 0.9630860\n [91] 0.9627904 0.9627017 0.9633816 0.9637954 0.9628791 0.9632042\n [97] 0.9627608 0.9626130 0.9627904 0.9632929 0.9632929 0.9626068\n[103] 0.9631451 0.9632929 0.9635294 0.9636181 0.9630269 0.9627017\n[109] 0.9629770 0.9636476 0.9633225 0.9634703 0.9630860 0.9626426\n[115] 0.9634998 0.9636476 0.9635590 0.9632042 0.9632634 0.9631451\n[121] 0.9629678 0.9631156 0.9635885 0.9630564 0.9628200 0.9634703\n[127] 0.9629382 0.9634998 0.9611055 0.9627608 0.9629382 0.9632634\n[133] 0.9627017 0.9630564 0.9632338 0.9631451 0.9625835 0.9631451\n[139] 0.9633225 0.9630564 0.9631451 0.9634998 0.9629678 0.9617854\n[145] 0.9622879 0.9626722 0.9627904 0.9624357 0.9625835 0.9627608\n[151] 0.9628791 0.9626426 0.9632634 0.9633816 0.9627904 0.9629973\n[157] 0.9633816 0.9630269 0.9627608 0.9631451 0.9632929 0.9632338\n[163] 0.9623766 0.9634998 0.9635590 0.9633816 0.9625835 0.9629722\n[169] 0.9619036 0.9626426 0.9625539 0.9629678 0.9632929 0.9634112\n[175] 0.9623561 0.9632929 0.9630269 0.9629382 0.9634407 0.9637067\n[181] 0.9623174 0.9618149 0.9625539 0.9629678 0.9630860 0.9636181\n[187] 0.9632929 0.9630860 0.9624948 0.9627608 0.9627017 0.9629973\n[193] 0.9634112 0.9633816 0.9621992 0.9629086 0.9630860 0.9625835\n[199] 0.9615785 0.9629086 0.9632929 0.9637032 0.9624948 0.9631451\n[205] 0.9629973 0.9629678 0.9631156 0.9624061 0.9620219 0.9625835\n[211] 0.9627608 0.9635294 0.9624357 0.9630564 0.9631747 0.9633816\n[217] 0.9628495 0.9623631 0.9629276 0.9628504 0.9623661 0.9625244\n[223] 0.9630860 0.9637067 0.9633225 0.9631451 0.9637954 0.9632929\n[229] 0.9632042 0.9629086 0.9629382 0.9627017 0.9634703 0.9632463\n[235] 0.9634900 0.9636181 0.9622109 0.9627017 0.9629973 0.9633520\n[241] 0.9631451 0.9632042 0.9627608 0.9633225 0.9634703 0.9631156\n[247] 0.9637067 0.9637067 0.9629382 0.9635885 0.9636476 0.9628791\n[253] 0.9633520 0.9621105 0.9615785 0.9628200 0.9634703 0.9632929\n[259] 0.9631747 0.9633225 0.9629382 0.9631451 0.9630564 0.9632634\n[265] 0.9615785 0.9632634 0.9621105 0.9634407 0.9635885 0.9630564\n[271] 0.9624948 0.9634703 0.9635885 0.9636476 0.9635590 0.9632634\n[277] 0.9629973 0.9627017 0.9634703 0.9633520 0.9618741 0.9629418\n[283] 0.9635590 0.9627313 0.9631451 0.9632042 0.9620219 0.9632042\n[289] 0.9631451 0.9636727 0.9634998 0.9630860 0.9630880 0.9624948\n[295] 0.9635813 0.9631156 0.9628809 0.9615713 0.9628809 0.9626130\n[301] 0.9632634 0.9626722 0.9630564 0.9632042 0.9627591 0.9629973\n[307] 0.9637363 0.9626722 0.9635813 0.9631451 0.9632929 0.9634112\n[313] 0.9636181 0.9627608 0.9631747 0.9628200 0.9632634 0.9634112\n[319] 0.9629382 0.9624061 0.9634998 0.9620810 0.9628791 0.9629382\n[325] 0.9632042 0.9634703 0.9633520 0.9630269 0.9624652 0.9634112\n[331] 0.9627608 0.9631156 0.9629678 0.9630860 0.9635590 0.9631156\n[337] 0.9632929 0.9629678 0.9632929 0.9627017 0.9619332 0.9628495\n[343] 0.9623766 0.9635885 0.9629722 0.9633225 0.9627608 0.9632929\n[349] 0.9632042 0.9629086 0.9628495 0.9628791 0.9632634 0.9627904\n[355] 0.9629678 0.9637363 0.9625539 0.9629113 0.9630564 0.9634407\n[361] 0.9626130 0.9628200 0.9630860 0.9628791 0.9629382 0.9629678\n[367] 0.9632042 0.9634703 0.9634998 0.9629973 0.9635294 0.9626722\n[373] 0.9637659 0.9631747 0.9624948 0.9631654 0.9629113 0.9613438\n[379] 0.9635590 0.9622879 0.9632042 0.9615489 0.9629382 0.9637945\n[385] 0.9636118 0.9621105 0.9627608 0.9631451 0.9630269 0.9632042\n[391] 0.9633225 0.9634112 0.9620219 0.9634407 0.9623022 0.9633377\n[397] 0.9624061 0.9630564 0.9634112 0.9633520 0.9620586 0.9626130\n[403] 0.9614307 0.9624061 0.9628791 0.9627904 0.9618741 0.9627904\n[409] 0.9631747 0.9626068 0.9626130 0.9632634 0.9631451 0.9627313\n[415] 0.9627017 0.9626130 0.9632929 0.9620810 0.9633225 0.9620514\n[421] 0.9628495 0.9632634 0.9627608 0.9633816 0.9628200 0.9622457\n[427] 0.9625835 0.9627904 0.9629678 0.9621697 0.9634703 0.9629086\n[433] 0.9631747 0.9616376 0.9624357 0.9629418 0.9633225 0.9612829\n[439] 0.9625835 0.9627904 0.9628495 0.9632042 0.9632042 0.9631747\n[445] 0.9629973 0.9626130 0.9614898 0.9624061 0.9629086 0.9629973\n[451] 0.9633816 0.9626981 0.9629722 0.9629086 0.9627017 0.9632634\n[457] 0.9636181 0.9625539 0.9626130 0.9621401 0.9627904 0.9625539\n[463] 0.9630564 0.9631550 0.9624948 0.9614602 0.9630860 0.9625539\n[469] 0.9627017 0.9617558 0.9630269 0.9624357 0.9628200 0.9620219\n[475] 0.9621992 0.9625539 0.9622879 0.9622583 0.9625835 0.9633520\n[481] 0.9630860 0.9625835 0.9615785 0.9615489 0.9630564 0.9630269\n[487] 0.9624948 0.9625244 0.9621992 0.9629973 0.9624061 0.9624652\n[493] 0.9627313 0.9633816 0.9629086 0.9625539 0.9626722 0.9629678\n[499] 0.9625835 0.9632338 0.9630636 0.9626426 0.9626130 0.9631747\n[505] 0.9632929 0.9623174 0.9619923 0.9633225 0.9630564 0.9627313\n[511] 0.9631245 0.9622879 0.9631451 0.9633225 0.9627608 0.9621992\n[517] 0.9631156 0.9622879 0.9617854 0.9628200 0.9621992 0.9627904\n[523] 0.9623766 0.9627313 0.9623174 0.9623174 0.9616967 0.9626722\n[529] 0.9619036 0.9629678 0.9634703 0.9632929 0.9631451 0.9634703\n[535] 0.9631747 0.9634703 0.9626426 0.9628495 0.9634407 0.9615785\n[541] 0.9619332 0.9624545 0.9632634 0.9626722 0.9629086 0.9631747\n[547] 0.9625835 0.9632338 0.9623174 0.9629973 0.9623470 0.9632042\n[553] 0.9631156 0.9630564 0.9627313 0.9624948 0.9619627 0.9631550\n[559] 0.9623022 0.9627482 0.9624652 0.9632929 0.9622879 0.9621697\n[565] 0.9631156 0.9628791 0.9624061 0.9630860 0.9624357 0.9627608\n[571] 0.9630860 0.9625244 0.9630564 0.9632042 0.9636772 0.9628495\n[577] 0.9622879 0.9630269 0.9631747 0.9627608 0.9625539 0.9619036\n[583] 0.9625244 0.9624652 0.9617854 0.9616931 0.9629973 0.9634703\n[589] 0.9615104 0.9628809 0.9625835 0.9623470 0.9619332 0.9628495\n[595] 0.9618445 0.9632463 0.9616671 0.9626722 0.9624948 0.9620810\n[601] 0.9615489 0.9621992 0.9624745 0.9627904 0.9623766 0.9626722\n[607] 0.9621992 0.9624652 0.9628791 0.9629678 0.9620890 0.9631747\n[613] 0.9617263 0.9635885 0.9631451 0.9629973 0.9632338 0.9623470\n[619] 0.9626426 0.9627904 0.9628200 0.9633520 0.9627608 0.9622288\n[625] 0.9627608 0.9630269 0.9629678 0.9632929 0.9633816 0.9630269\n[631] 0.9634407 0.9632338 0.9623936 0.9620810 0.9635885 0.9632634\n[637] 0.9630564 0.9625244 0.9628495 0.9625835 0.9635885 0.9633816\n[643] 0.9636476 0.9629382 0.9624652 0.9624652 0.9623174 0.9616080\n[649] 0.9634998 0.9634703 0.9629678 0.9615193 0.9627608 0.9632338\n[655] 0.9632338 0.9631156 0.9632042 0.9632929 0.9631156 0.9626426\n[661] 0.9634112 0.9627608 0.9620810 0.9618149 0.9629382 0.9629973\n[667] 0.9623470 0.9630860 0.9636772 0.9627608 0.9635294 0.9625244\n[673] 0.9631156 0.9632338 0.9630860 0.9633520 0.9635885 0.9630564\n[679] 0.9627017 0.9627017 0.9638250 0.9633520 0.9633225 0.9632042\n[685] 0.9623470 0.9633986 0.9633816 0.9633225 0.9628495 0.9628791\n[691] 0.9626130 0.9629973 0.9625244 0.9626722 0.9628200 0.9635885\n[697] 0.9633816 0.9616671 0.9628504 0.9629086 0.9617263 0.9630636\n[703] 0.9629418 0.9633816 0.9623936 0.9624652 0.9633520 0.9634407\n[709] 0.9618741 0.9624948 0.9626426 0.9636181 0.9628791 0.9614898\n[715] 0.9613715 0.9621105 0.9631156 0.9630860 0.9623470 0.9624948\n[721] 0.9619332 0.9631451 0.9619332 0.9634112 0.9634998 0.9631451\n[727] 0.9632338 0.9615785 0.9621401 0.9634407 0.9630564 0.9626426\n[733] 0.9622879 0.9612829 0.9637954 0.9629382 0.9627904 0.9621401\n[739] 0.9636772 0.9634900 0.9636181 0.9633520 0.9630564 0.9627591\n[745] 0.9632634 0.9629382 0.9633816 0.9625835 0.9617558 0.9630564\n[751] 0.9631747 0.9635885 0.9612829 0.9631747 0.9627017 0.9630269\n[757] 0.9627017 0.9634112 0.9622583 0.9634112 0.9631854 0.9629382\n[763] 0.9634407 0.9631451 0.9636181 0.9628200 0.9632042 0.9615489\n[769] 0.9631156 0.9631747 0.9623766 0.9633816 0.9627904 0.9628791\n[775] 0.9630860 0.9627608 0.9626426 0.9622288 0.9632042 0.9630269\n[781] 0.9615489 0.9630860 0.9631747 0.9632929 0.9629382 0.9632929\n[787] 0.9628495 0.9628495 0.9632042 0.9630269 0.9627608 0.9631156\n[793] 0.9630941 0.9626426 0.9619332 0.9628495 0.9624357 0.9633225\n[799] 0.9619627 0.9629086 0.9634703 0.9630269 0.9634703 0.9623766\n[805] 0.9625539 0.9630860 0.9624948 0.9630269 0.9629678 0.9634998\n[811] 0.9628495 0.9619627 0.9630564 0.9622288 0.9625244 0.9618454\n[817] 0.9629973 0.9635294 0.9618741 0.9625539 0.9629086 0.9627313\n[823] 0.9630564 0.9633816 0.9627017 0.9633225 0.9629973 0.9625539\n[829] 0.9629973 0.9628791 0.9610464 0.9623470 0.9609873 0.9624357\n[835] 0.9619923 0.9632634 0.9615713 0.9620219 0.9628791 0.9624061\n[841] 0.9629678 0.9614602 0.9624357 0.9628791 0.9629678 0.9629382\n[847] 0.9630860 0.9623470 0.9619923 0.9629678 0.9622879 0.9631451\n[853] 0.9637363 0.9627904 0.9631747 0.9626068 0.9635590 0.9634998\n[859] 0.9632338 0.9629142 0.9628809 0.9626130 0.9629418 0.9629086\n[865] 0.9627904 0.9632634 0.9621697 0.9625244 0.9629382 0.9627904\n[871] 0.9631451 0.9630564 0.9626130 0.9632042 0.9619672 0.9631747\n[877] 0.9614602 0.9621697 0.9630564 0.9629678 0.9627904 0.9629973\n[883] 0.9633520 0.9608099 0.9629678 0.9627608 0.9634595 0.9628791\n[889] 0.9631451 0.9624357 0.9627904 0.9625835 0.9630860 0.9633520\n[895] 0.9621992 0.9619332 0.9625244 0.9630564 0.9628495 0.9621697\n[901] 0.9636476 0.9621697 0.9630564 0.9625244 0.9629382 0.9625154\n[907] 0.9625835 0.9617558 0.9628495 0.9620810 0.9618741 0.9625244\n[913] 0.9629973 0.9629678 0.9633520 0.9623470 0.9633225 0.9630269\n[919] 0.9633225 0.9630860 0.9628495 0.9628200 0.9631747 0.9626130\n[925] 0.9636476 0.9629086 0.9628200 0.9620219 0.9632929 0.9617263\n[931] 0.9620219 0.9627608 0.9622879 0.9636772 0.9630269 0.9628791\n[937] 0.9623174 0.9609873 0.9626426 0.9628495 0.9631451 0.9634998\n[943] 0.9637363 0.9636181 0.9631156 0.9630564 0.9630269 0.9633816\n[949] 0.9635590 0.9633520 0.9625835 0.9624652 0.9621992 0.9622583\n[955] 0.9624061 0.9632042 0.9631451 0.9629382 0.9624357 0.9633816\n[961] 0.9617854 0.9629086 0.9627904 0.9632929 0.9636476 0.9629456\n[967] 0.9627608 0.9627313 0.9629973 0.9625835 0.9626981 0.9622839\n[973] 0.9627313 0.9629086 0.9634998 0.9618149 0.9609281 0.9629382\n[979] 0.9625835 0.9623470 0.9624061 0.9626722 0.9625835 0.9619923\n[985] 0.9626426 0.9622879 0.9614898 0.9614602 0.9633816 0.9632634\n[991] 0.9630860 0.9620219 0.9632338 0.9631747 0.9630398 0.9612237\n\n$call\nbootstrap::jackknife(x = tmpTib$meanCOREOM, theta = mean)\n\nShow code\n\n### but I'm a masochistic, obsessional idiot so I replicate that!\njackMean <- function(x, confint = .95){\n  x <- na.omit(x)\n  valN <- length(x)\n  valSampMean <- mean(x)\n  vecJackMeans <- rep(NA, valN)\n  vecPseudoVals <- rep(NA, valN)\n  \n  for (i in 1:valN){\n    vecJackMeans[i] <- mean(x[-i])\n    vecPseudoVals[i] <- valN * valSampMean - (valN - 1)*vecJackMeans[i]\n  }\n  valBias <- (valN - 1) * (mean(vecJackMeans) - valSampMean)\n  valSE <- sqrt(((valN - 1) / valN) * sum((vecJackMeans - mean(vecJackMeans))^2))\n  valQt <- qt((1 - (1 - confint)/2), valN - 1)\n  print(valQt)\n  LCL <- valSampMean - (valSE * valQt)\n  UCL <- valSampMean + (valSE * valQt)\n  return(list(n = valN,\n              mean = valSampMean,\n              bias = valBias,\n              SE = valSE,\n              confint = confint,\n              LCL = LCL,\n              UCL = UCL,\n              CI = c(LCL, UCL),\n              vecJackMeans = vecJackMeans,\n              vecPseudoVals = vecPseudoVals))\n}\n\n# jackMean(1:8)\n# bootstrap::jackknife(1:8, mean)\n\njackMean(tmpTib$meanCOREOM) -> listJackknife\n\n[1] 1.962351\n\nShow code\n\nbootstrap::jackknife(tmpTib$meanCOREOM, mean)$jack.bias\n\n[1] 0\n\nShow code\n\nbootstrap::jackknife(tmpTib$meanCOREOM, mean)$jack.se\n\n[1] 0.01688115\n\nShow code\n\nlistJackknife$bias\n\n[1] 0\n\nShow code\n\nlistJackknife$SE\n\n[1] 0.01688115\n\nShow code\n\nlistJackknife$CI\n\n[1] 0.9297305 0.9959840\n\nShow code\n\nrange(listJackknife$vecJackMeans)\n\n[1] 0.9608099 0.9638250\n\nShow code\n\nlistJackknife$vecJackMeans %>%\n  as_tibble() -> tmpTibJackknife\n\nmean(tmpTibJackknife$value)\n\n[1] 0.9628573\n\nShow code\n\nmean(tmpTib$meanCOREOM)\n\n[1] 0.9628573\n\nShow code\n\nggplot(data = tmpTibJackknife,\n       aes(x = value)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tmpTibJackknife$value),\n             colour = \"green\",\n             size = 2) +\n  xlab(\"Jackknife means\") +\n  ylab(\"Count\")\n\n\nShow code\n\nggsave(filename = \"jackknife2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n### create an evenly spread small subsample\ntmpTib %>%\n  arrange(meanCOREOM) %>%\n  mutate(rowN = row_number()) %>%\n  filter(rowN %% 20 == 10) -> tmpTib2\n# max(tmpTib2$rowN)\n# nrow(tmpTib2)\n\nggplot(data = tmpTib2,\n       aes(x = meanCOREOM)) +\n  geom_histogram() +\n  xlab(\"CORE-OM total score (item mean)\") +\n  ylab(\"Count\")\n\n\nShow code\n\nggsave(filename = \"jackknife3.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### parametric CI\nmean(tmpTib2$meanCOREOM)\n\n[1] 0.9687147\n\nShow code\n\nvar(tmpTib2$meanCOREOM)\n\n[1] 0.296774\n\nShow code\n\nsd(tmpTib2$meanCOREOM)/sqrt(nrow(tmpTib))\n\n[1] 0.01726169\n\nShow code\n\nmean(tmpTib2$meanCOREOM) - 1.96*sd(tmpTib2$meanCOREOM)/sqrt(nrow(tmpTib2))\n\n[1] 0.8177122\n\nShow code\n\nmean(tmpTib2$meanCOREOM) + 1.96*sd(tmpTib2$meanCOREOM)/sqrt(nrow(tmpTib2))\n\n[1] 1.119717\n\nShow code\n\n### jackknifing\njackMean(tmpTib2$meanCOREOM) -> listJackknife2\n\n[1] 2.009575\n\nShow code\n\nbootstrap::jackknife(tmpTib2$meanCOREOM, mean)$jack.bias\n\n[1] 0\n\nShow code\n\nbootstrap::jackknife(tmpTib2$meanCOREOM, mean)$jack.se\n\n[1] 0.07704206\n\nShow code\n\nlistJackknife2$bias\n\n[1] 0\n\nShow code\n\nlistJackknife2$SE\n\n[1] 0.07704206\n\nShow code\n\nlistJackknife2$CI\n\n[1] 0.8138928 1.1235365\n\nShow code\n\nrange(listJackknife2$vecJackMeans)\n\n[1] 0.9356632 0.9866836\n\nShow code\n\nlistJackknife2$vecJackMeans %>%\n  as_tibble() -> tmpTibJackknife2\n\nggplot(data = tmpTibJackknife2,\n       aes(x = value)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tmpTibJackknife$value),\n             colour = \"green\",\n             size = 2) +\n  xlab(\"Jackknife means\") +\n  ylab(\"Count\")\n\n\nShow code\n\nggsave(filename = \"jackknife4.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\n\nShow code\n\nset.seed(12345)\n### function to use for bootstrapping\nmean4boot <- function(x, i){\n  mean(x[i])\n}\n\n### do the bootstrap (and time it)\nstart.time <- Sys.time()\ntmpRes <- boot::boot(tmpTib$meanCOREOM, mean4boot, 10000)\nend.time <- Sys.time()\nelapsed.time <- end.time - start.time\nelapsed.time\n\nTime difference of 0.3178759 secs\n\nShow code\n\n### get the data ready for plotting\ntmpRes$t[, 1] %>%\n  as_tibble() -> tmpTibBoot\n\nrange(tmpTibBoot$value)\n\n[1] 0.8807916 1.0319069\n\nShow code\n\nmean(tmpTibBoot$value)\n\n[1] 0.9630155\n\nShow code\n\ntmpRes$t0\n\n[1] 0.9628573\n\nShow code\n\nggplot(data = tmpTibBoot,\n       aes(x = value)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tmpTibBoot$value),\n             colour = \"green\",\n             size = 3.5) +\n  geom_vline(xintercept = mean(tmpTib$meanCOREOM),\n             colour = \"blue\")\n\n\nShow code\n\nggsave(filename = \"bootstrap1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntmpRes$t0\n\n[1] 0.9628573\n\nShow code\n\nboot::boot.ci(tmpRes)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = tmpRes)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.9292,  0.9962 )   ( 0.9293,  0.9954 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.9303,  0.9964 )   ( 0.9304,  0.9965 )  \nCalculations and Intervals on Original Scale\n\nShow code\n\n### now the same on the reduced dataset \n### do the bootstrap (and time it)\nstart.time <- Sys.time()\ntmpRes <- boot::boot(tmpTib2$meanCOREOM, mean4boot, 10000)\nend.time <- Sys.time()\nelapsed.time <- end.time - start.time\nelapsed.time\n\nTime difference of 0.08973837 secs\n\nShow code\n\n### get the data ready for plotting\ntmpRes$t[, 1] %>%\n  as_tibble() -> tmpTibBoot\n\nrange(tmpTibBoot$value)\n\n[1] 0.7056996 1.2611440\n\nShow code\n\nggplot(data = tmpTibBoot,\n       aes(x = value)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tmpTibBoot$value),\n             colour = \"green\",\n             size = 3.5) +\n  geom_vline(xintercept = mean(tmpTib$meanCOREOM),\n             colour = \"blue\",\n             size = 2)\n\n\nShow code\n\nggsave(filename = \"bootstrap2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntmpRes$t0\n\n[1] 0.9687147\n\nShow code\n\nboot::boot.ci(tmpRes)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = tmpRes)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.8214,  1.1181 )   ( 0.8157,  1.1128 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.8246,  1.1217 )   ( 0.8347,  1.1363 )  \nCalculations and Intervals on Original Scale\n\n\n\nShow code\n\ntribble(~DNA, ~Gender, ~n,\n        \"All\", \"F\", 125,\n        \"All\", \"M\", 125,\n        \"DNA...\", \"F\", 125,\n        \"DNA...\", \"M\", 125) -> tibAssoc125\ntibAssoc125 %>%\n  uncount(n) -> tibAssoc125long\n\n### good old fashioned, well hybrid, way to do this\ntibAssoc125long %>%\n  summarise(chisq = list(unlist(chisq.test(Gender, DNA)))) %>%\n  unnest_wider(chisq)\n\n# A tibble: 1 × 21\n  `statistic.X-squared` parameter.df p.value method          data.name\n  <chr>                 <chr>        <chr>   <chr>           <chr>    \n1 0                     1            1       Pearson's Chi-… Gender a…\n# ℹ 16 more variables: observed1 <chr>, observed2 <chr>,\n#   observed3 <chr>, observed4 <chr>, expected1 <chr>,\n#   expected2 <chr>, expected3 <chr>, expected4 <chr>,\n#   residuals1 <chr>, residuals2 <chr>, residuals3 <chr>,\n#   residuals4 <chr>, stdres1 <chr>, stdres2 <chr>, stdres3 <chr>,\n#   stdres4 <chr>\n\nShow code\n\n### let's see if I can get to understand the purrr and broom approach\ntibAssoc125long %>%\n  ### nest() makes all the data into a row with a dataframe in it of all the data\n  ### have to use \"data = everything\" when not using it after group_by() so it knows to use all columns\n  nest(data = everything()) %>%\n  ### so now we have a single row tibble so mutate() not summarise()\n  ### this says apply chisq.test to the data computing the chisq.test for Gender and DNA\n  ### this seems horrible syntax to me\n  mutate(chisq = purrr::map(data, ~chisq.test(.$Gender, .$DNA)),\n         ### this is similar: apply broom::tidy() to all of chisq\n         ### broom::tidy actually uses tidy.htest() seeing that chisq is an htest list\n         ### so it pulls the elements into a list\n         tidied = purrr::map(chisq, broom::tidy)) %>%\n  ### at this point you've still got all the data in a column data \n  ### and all the output of chisq.test() as a list\n  ### don't need all that!\n  select(tidied) %>%\n  ### now unnest the elements of tidied\n  unnest_wider(tidied)\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      <dbl>   <dbl>     <int> <chr>                     \n1         0       1         1 Pearson's Chi-squared test\n\nShow code\n\n### this was part of my abortive attempt to create tables I could easily \n### paste into WordPress by making them into jpegs or pngs using tools\n### it gridExtra.  I didn't crack it!\n# tt3 <- ttheme_default(core=list(fg_params=list(hjust=0, x=0.1)),\n#                       rowhead=list(fg_params=list(hjust=0, x=0)))\n# tibAssoc125long %>%\n#   tabyl(DNA, Gender) %>%\n#   as.data.frame() -> tmpDF\n# \n# \n# gridExtra::tableGrob(tmpDF, rows = NULL) -> tmpGrob\n# png(\"test.png\", height=1000, width=200)\n# gridExtra::grid.table(tmpDF, rows = NULL)\n# dev.off()\n\n\n\n\nShow code\n\n### trying to shift tables to WordPress using csv and the wpDataTables plugin\n### recode to long labels\ntibAssoc125long %>%\n  mutate(DNA = recode(DNA,\n                      \"All\" = \"Attended all sessions\",\n                      \"DNA...\" = \"DNAed at least one session\")) -> tibAssoc125long\n\n### simplest xtab\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(n = DNA) %>%\n  write_csv(file = \"xtab1.csv\")\n\n### simplest xtab\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(n = DNA) %>%\n  flextable::flextable() %>%\n  flextable::autofit() %>%\n  flextable::save_as_image(path = \"xtab1.png\", zoom = 3)\n\n[1] \"xtab1.png\"\n\nShow code\n\n### row percentages: CSV\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(`n (row %)` = DNA) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"row\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns(position = \"front\") %>%\n  write_csv(file = \"xtab2.csv\")\n\n### row percentages: png\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(`n (row %)` = DNA) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"row\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns(position = \"front\") %>%\n  flextable::flextable() %>%\n  flextable::autofit() %>%\n  flextable::save_as_image(path = \"xtab1rowperc.png\", zoom = 3)\n\n[1] \"xtab1rowperc.png\"\n\nShow code\n\n### col percentages\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(`n (col %)` = DNA) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"col\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns(position = \"front\") %>%\n  write_csv(file = \"xtab3.csv\")\n  \n### total percentages\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(`n (% of total)` = DNA) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"all\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns(position = \"front\") %>%\n  write_csv(file = \"xtab4.csv\")\n\n\n\n\nShow code\n\nvecCOREwbItems <- c(4,14,17,31)\nvecCOREprobItems <- c(2,5,8,11,13,15,18,20,23,27,28,30)\nvecCOREfuncItems <- c(1,3,7,10,12,19,21,25,26,29,32,33)\nvecCOREriskItems <- c(6,9,16,22,24,34)\nvecCOREnrItems <- c(vecCOREwbItems, vecCOREprobItems, vecCOREfuncItems)\n\nvecCOREwbItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREwbItems))\nvecCOREprobItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREprobItems))\nvecCOREfuncItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREfuncItems))\nvecCOREriskItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREriskItems))\nvecCOREnrItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREnrItems))\n\ntibUseDat %>% \n  rowwise() %>%\n  mutate(meanCOREwb = meanNotNA(c_across(all_of(vecCOREwbItems))),\n         meanCOREprob = meanNotNA(c_across(all_of(vecCOREprobItems))),\n         meanCOREfunc = meanNotNA(c_across(all_of(vecCOREfuncItems))),\n         meanCORErisk = meanNotNA(c_across(all_of(vecCOREriskItems))),\n         meanCOREnr = meanNotNA(c_across(all_of(vecCOREnrItems))),) %>%\n  ungroup() -> tibUseDat\n\n\n### had too much overprinting with the full dataset so:\nset.seed(12345)\ntibUseDat %>%\n  ### my first ever use of slice_sample(): nice\n  slice_sample(n = 88) -> tibSmallDat\n\nggplot(data = tibSmallDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_point() +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatterpoint.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nggplot(data = tibSmallDat,\n       aes(x = meanCOREnr, y = meanCORErisk, colour = Gender, fill = Gender)) +\n  geom_point() +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatterpointGender.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\ntibUseDat %>%\n  select(meanCORErisk, meanCOREnr) %>% \n  distinct() %>% \n  summarise(n())\n\n# A tibble: 1 × 1\n  `n()`\n  <int>\n1   346\n\nShow code\n\ntibUseDat %>%\n  select(meanCORErisk, meanCOREnr) %>% \n  group_by(meanCOREnr, meanCORErisk) %>%\n  summarise(n = n()) %>%\n  ungroup() %>%\n  filter(n > 1) %>%\n  arrange(desc(n))\n\n# A tibble: 148 × 3\n   meanCOREnr meanCORErisk     n\n        <dbl>        <dbl> <int>\n 1      0.643        0        19\n 2      0.536        0        16\n 3      0.429        0        14\n 4      0.75         0        14\n 5      0.607        0        13\n 6      0.679        0        13\n 7      1.07         0        13\n 8      0.286        0        12\n 9      0.393        0        12\n10      0.893        0.167    12\n# ℹ 138 more rows\n\nShow code\n\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_point() +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatter1point.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_point(alpha = .1) +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatter1pointAlph.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_count() +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatter2count.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_count(alpha = .3) +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatter3count.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\n\n\n\nShow code\n\nset.seed(12345)\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  # geom_point(colour = \"blue\", alpha = .2) +\n  geom_jitter(width = 0, height = .05, alpha = .2) +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"jitter1.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\ntibUseDat %>%\n  # select(starts_with(\"COREOM01_\")) %>%\n  # select(-COREOM01_35) %>%\n  select(all_of(vecCOREriskItems)) %>%\n  na.omit() %>%\n  pivot_longer(everything()) %>%\n  group_by(name) %>%\n  summarise(var = var(value)) %>% \n  arrange(desc(var)) %>%\n  slice_head()\n\n# A tibble: 1 × 2\n  name          var\n  <chr>       <dbl>\n1 COREOM01_06 0.591\n\nShow code\n\ntibUseDat %>%\n  # select(starts_with(\"COREOM01_\")) %>%\n  # select(-COREOM01_35) %>%\n  select(all_of(vecCOREnrItems)) %>%\n  na.omit() %>%\n  pivot_longer(everything()) %>%\n  group_by(name) %>%\n  summarise(var = var(value)) %>% \n  arrange(desc(var)) %>%\n  slice_head()\n\n# A tibble: 1 × 2\n  name          var\n  <chr>       <dbl>\n1 COREOM01_27  1.65\n\nShow code\n\nggplot(data = tibUseDat,\n       aes(x =  COREOM01_27, y = COREOM01_06)) +\n  geom_point() +\n  xlab(\"Scores on CORE-OM item 27:\\nI have felt unhappy\") +\n  ylab(\"Scores on CORE-OM item 6:\\nI have been physically violent to others\")\n\n\nShow code\n\nggsave(filename = \"items27and6_plot1.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nset.seed(12345)\nggplot(data = tibUseDat,\n       aes(x =  COREOM01_27, y = COREOM01_06)) +\n  geom_jitter(width = .4, height = .4) +\n  xlab(\"Scores on CORE-OM item 27:\\nI have felt unhappy\") +\n  ylab(\"Scores on CORE-OM item 6:\\nI have been physically violent to others\")\n\n\nShow code\n\nggsave(filename = \"items27and6_jitter1.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\n\nset.seed(12345)\nggplot(data = tibUseDat,\n       aes(x =  COREOM01_27, y = COREOM01_06)) +\n  geom_jitter(width = .4, height = .4, alpha = .3) +\n  xlab(\"Scores on CORE-OM item 27:\\nI have felt unhappy\") +\n  ylab(\"Scores on CORE-OM item 6:\\nI have been physically violent to others\")\n\n\nShow code\n\nggsave(filename = \"items27and6_jitter2.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\n\n\n\nShow code\n\nsave.image(file = \"Glossary.rda\")\n\nload(file = \"Glossary.rda\")\n\n\n\n\n\n",
    "preview": "posts/2021-11-09-ombookglossary/OMbook_glossary_files/figure-html5/mean1-1.png",
    "last_modified": "2023-08-28T21:18:07+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-31-subscaletotal-correlations/",
    "title": "Subscale/total correlations",
    "description": "A look at subscale/total correlations in the null model",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-10-31",
    "categories": [
      "correlation",
      "psychometrics",
      "regression"
    ],
    "contents": "\nThis came about from some work I am doing with colleagues looking at Authenticity Scale (AS; Wood et al., -Wood et al. (2008)). The AS has twelve items and a nicely balanced set of three subscales of four items each. The subscales are named Self-Alienation (SA), Accepting External Influence (AEI) and Authentic Living (AL). I was doing what I have always done before and looking at the simple correlations between the subscales and between them and the total score. As it happened a low correlation between one subscale and the other two took me back to something that has been in my mind a lot this year: when is a correlation structure that is not simply unidimensional/unifactorial, perhaps even fairly cleanly of two factors such that we shouldn’t report total scores but only the subscale (factor) scores?\nThat’s for another day and another blog post or several but I found myself aware that in a true null model in which correlations between the items of a measure are purely random, the correlations between subscale scores and the total score must be higher than zero as there is shared variance between the subscale score and the total score. That got me pondering why tradition has it (and like a slave, I have always followed it) that for subscale/total correlations we report the raw correlation but when looking item/total correlations we report “corrected” item/total correlations (CITCs), i.e. the correlation between the scores on the item and the scores on the whole scale corrected: with that item’s scores omitted.\nIf the items scores are Gaussian and uncorrelated and all have equal variance then it’s not rocket science to work out that the asymptotic Pearson correlation (i.e. the correlation as the sample size tends to \\(\\infty\\)) between the subscale score and the total score will be:\n\\[ \\sqrt{\\frac{k_{subscale}}{k_{total}}} \\]\nWhere \\(k_{subscale}\\) is the number of items in the subscale and \\(k_{total}\\) is the number of items in the entire measure. (Quick reductio ad absurdum checking: if \\(k_{subscale}\\) is zero then the correlation will be zero and if \\(k_{subscale}\\)\nis the same as \\(k_{total}\\)) then the correlation is one.)\nSo for the AS with four items per subscale the asymptotic correlation would be \\(\\sqrt{\\frac{4}{12}}\\), i.e. sqrt(1/3) = 0.577 (to 3 d.p.) were there no systematic covariance across the items.\nHere’s the relationship between the correlation and the fraction of the total number of items in the subscale (always assuming a null model that there is no covariance across the items). I have added reference lines for the proportions of items in the subscales of the CORE-OM and the AS assuming their were zero population item covariance.\n\n\nShow code\n\nlibrary(tidyverse)\nvalK <- 340\n0:340 %>%\n  as_tibble() %>%\n  rename(fraction = value) %>%\n  mutate(fraction = fraction / valK,\n         R = sqrt(fraction)) -> tibRvals\n\ntibble(scale = c(\"CORE-OM WB (4/34)\",\n                 \"CORE-OM Risk (6/34)\",\n                 \"CORE-OM Problems or Functioning (17/34)\",\n                 \"AS any subscale (4/12)\"),\n       fraction = c(4/34, 6/34, 18/34, 4/12)) %>%\n  mutate(R = sqrt(fraction)) -> tibCOREandAS\n\nggplot(data = tibRvals,\n       aes(x = fraction, y = R)) +\n  geom_point() +\n  geom_line() +\n  geom_linerange(data = tibCOREandAS,\n             aes(xmin = 0, xmax = fraction, y = R)) +\n  geom_linerange(data = tibCOREandAS,\n                 aes(x = fraction, ymin = 0, ymax = R)) +\n  geom_text(data = tibCOREandAS,\n             aes(x = 0, y = R + .015, label = scale),\n             hjust = 0,\n             size = 2.2) +\n  xlab(bquote(k[subscale]/k[total])) +\n  ylab(\"Asymptotic correlation\") +\n  ggtitle(\"Plot of asymptotic subscale/total correlation\\nagainst proportion of total items in subscale\") +\n  scale_x_continuous(breaks = (0:10/10)) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = .5))\n\n\n\nI amused myself simulating this for a sample size of 5000.\n\n\nShow code\n\nlibrary(tidyverse)\noptions(dplyr.summarise.inform = FALSE)\n\n### generate Gaussian null model data\nset.seed(12345) # set for reproducible results\nvalN <- 5000 # sample size\nvalK <- 12 # total number of items\n\n### now make up the data in long format, i.e.\n###   an item score\n###   an item label\n###   a person ID\nrnorm(valN * valK) %>% # gets uncorrelated Gaussian data\n  as_tibble() %>%\n  mutate(itemN = ((row_number() - 1) %% 12) + 1, # use modulo arithmetic to get item number\n         item = str_c(\"I\", sprintf(\"%02.0f\", itemN)), # format it nicely\n         ID = ((row_number() - 1) %/% 12) + 1, # use modulo arithmetic to get person ID \n         ID = sprintf(\"%03.0f\", ID)) %>% # and format that, can now dump itemN\n  select(-itemN) -> tibLongItemDat\n\n### now just pivot that to get it into wide format, valK items per row\ntibLongItemDat %>%\n  pivot_wider(id_cols = ID, names_from = item, values_from = value) -> tibWideItemDat\n\n### map items to scales (just sequentially here, that's not the AS mapping)\nvecItemsScale1 <- str_c(\"I\", sprintf(\"%02.0f\", 1:4))\nvecItemsScale2 <- str_c(\"I\", sprintf(\"%02.0f\", 5:8))\nvecItemsScale3 <- str_c(\"I\", sprintf(\"%02.0f\", 9:12))\n\n### now use those maps to get the subscale scores as well as the total score\ntibWideItemDat %>%\n  rowwise() %>%\n  mutate(scoreAll = mean(c_across(-ID)),\n         score1 = mean(c_across(all_of(vecItemsScale1))),\n         score2 = mean(c_across(all_of(vecItemsScale2))),\n         score3 = mean(c_across(all_of(vecItemsScale3)))) %>%\n  ungroup() -> tibWideAllDat\n\ntibWideAllDat %>%\n  select(starts_with(\"score\")) -> tibScores\n\n### corrr::correlate() has a message about the method and handling of missing\n### punches through markdown despite the block header having \"message=FALSE\"\n### I could have wrapped this in suppressMessages() however you can suppress \n### that with \"quiet = TRUE\", see below\ntibScores%>%\n  ### here is the \"quiet = TRUE\" suppression of the message\n  corrr::correlate(diagonal = 1, quiet = TRUE) %>%\n  mutate(across(starts_with(\"score\"), round, 2)) %>%\n  pander::pander(justify = \"lrrrr\", digits = 2)\n\nterm\nscoreAll\nscore1\nscore2\nscore3\nscoreAll\n1\n0.57\n0.58\n0.59\nscore1\n0.57\n1\n-0.01\n0.01\nscore2\n0.58\n-0.01\n1\n0.01\nscore3\n0.59\n0.01\n0.01\n1\n\nAnd here’s the plot of the simulated scores. The blue lines are the linear regression lines.\n\n\nShow code\n\nlm_fn <- function(data, mapping, ...){\n  p <- ggplot(data = data, mapping = mapping) + \n    geom_point(alpha = .05) + \n    geom_smooth(method=lm, fill=\"blue\", color=\"blue\", ...)\n  p\n}\n\n\nGGally::ggpairs(tibScores,\n                lower = list(continuous = lm_fn)) +\n  theme_bw()\n\n\n\nI am still not sure why we report CITCs for item analyses but raw subscale/total correlations for subscales. I keep trying to convince myself there’s a logic to my long entrenched behaviour but I’m not sure there is. I have a suspicion that we have all been doing it following others’ examples and that it started long ago when SPSS made CITCs easy to compute in its RELIABILITY function. I have long felt that RELIABILITY was one of the better parts of SPSS!\n\n\n\nWood, Alex M., P. Alex Linley, John Maltby, Michael Baliousis, and Stephen Joseph. 2008. “The Authentic Personality: A Theoretical and Empirical Conceptualization and the Development of the Authenticity Scale.” Journal of Counseling Psychology 55 (3): 385–99. https://doi.org/10.1037/0022-0167.55.3.385.\n\n\n\n\n",
    "preview": "posts/2021-10-31-subscaletotal-correlations/subscaletotal-correlations_files/figure-html5/plot1-1.png",
    "last_modified": "2023-08-25T13:40:05+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1536
  },
  {
    "path": "posts/2021-04-09-spearman-brown-formula/",
    "title": "Spearman-Brown formula",
    "description": "How does internal reliability relate to number of items?",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-04-09",
    "categories": [
      "correlation",
      "psychometrics",
      "regression"
    ],
    "contents": "\n\nContents\nBackground\nTheory behind the Spearman-Brown formula\nYou can use getRelBySpearmanBrown from the CECPfuns package\n\n[Created 10.iv.21, tweak 15.iv.21 to add “, build_manual = TRUE” to install_github call]\nThe Spearman-Brown formula:\n\\[{\\rho^{*}}=\\frac{n\\rho}{1 + (n-1)\\rho}\\]\ngives us this plot.\n\n\nShow code\n\nlibrary(CECPfuns) # for getRelBySpearmanBrown()\n### to get the CECPfuns package use:\n# remotes::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE)\n### for which you may have needed to do \n# install.packages(\"remotes\")\n### in order to get the remotes package\n### you can also use install_github(), essentially the same as in remotes\n### from the devtools package if you have installed that but if you aren't\n### making R packages then you probably don't want all of devtools\n### see https://www.psyctc.org/Rblog/posts/2021-02-10-making-my-first-usable-package/\nmaxK <- 60\nvecK <- 2:maxK\nvecK %>%\n  as_tibble() %>%\n  rename(k = value) %>%\n  rowwise() %>%\n  ### I have put the explict mapping of getRelBySpearmanBrown to my CECPfuns package here to avoid confusion\n  mutate(rel.1 = CECPfuns::getRelBySpearmanBrown(oldRel = .1, lengthRatio = k / 2,  verbose = FALSE),\n         rel.2 = CECPfuns::getRelBySpearmanBrown(oldRel = .2, lengthRatio = k / 2,  verbose = FALSE),\n         rel.3 = CECPfuns::getRelBySpearmanBrown(oldRel = .3, lengthRatio = k / 2,  verbose = FALSE), \n         rel.4 = CECPfuns::getRelBySpearmanBrown(oldRel = .4, lengthRatio = k / 2,  verbose = FALSE)) %>%\n  ungroup() -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = starts_with(\"rel.\"), names_to = \"IIC\", values_to = \"Reliability\") %>%\n  mutate(IIC = factor(str_sub(IIC, 4, 5))) -> tibDatLong\n  \nggplot(data = tibDatLong,\n       aes(x = k, y = Reliability, group = IIC, colour = IIC)) +\n  geom_point(size = 1) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = c(1, seq(2, 8, 2), seq(0, maxK, 10))) + # and I want the x axis with these tick marks and labels\n  scale_y_continuous(breaks = seq(0, 1, .1)) + # same for the y axis\n  ggtitle(\"Relationship of reliability to k, number of items in a measure\",\n          subtitle = \"for fixed inter-item correlation (IIC)\") +\n  theme_bw() + # I like this simple theme with white plot area\n  theme(plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5), # I like titles and subtitles centered\n        panel.grid.minor = element_blank(), # gets grid lines only where the axis tick marks are not adding minor ones between those\n        axis.text.x = element_text(angle = 80, # trying to get the axis point labels rotated for maximum clarity\n                                   hjust = 1,  # and aligning them, \n                                   vjust = .75)) # is there a bug in the ggplot code failing to handle the number of characters?\n\n\n\nBackground\nI must have discovered this neat little formula back in the very early 1990s thinking about the Body Shape Questionnaire (BSQ; Cooper et al. (1986)). That thinking led to a paper I still like quite a bit: Evans & Dolan -Evans and Dolan (1993). In the formula \\(\\rho\\) is the reliability of a test and the equation is predicting \\(\\rho^{*}\\) the reliability of a new test longer, or shorter, than the first by a ratio \\(n\\).\nIn fact, the title of this page could have been the more accurate: “How does internal reliability/consistency relate to number of items in Classical Test Theory (CTT) assuming that mean inter-item correlation remains the same?” That’s what the Spearman-Brown (prediction) formula tells us.\nThere’s a typically excellent wikipedia entry about the formula. As well as a very thorough explanation of the formula the page also has a fascinating bit of history about the factor that Spearman-Brown was neither a single person with a double barrelled surname, nor a working partnership. I do love the way wikipedia contributors often add these things.\nWhy am I posting about this now, thirty years later? Well, it came in useful recently looking at the psychometrics of the YP-CORE. The YP-CORE has ten items, seven negatively cued, e.g. “My problems have felt too much for me” and three that are positively cued, e.g. “I’ve felt able to cope when things go wrong”. Emily wanted to test whether the reliability of the positively cued items was the same as that of the negatively cued items and had discovered the excellent cocron package (see also http://comparingcronbachalphas.org). The cocron package implements in R formulae for inference testing one Cronbach alpha value, and for testing equality of more than one alpha (both for values from the same sample, i.e. a within participants test, as would have been the case for Emily’s question, and for the probably more common question where values from multiple groups are to be compared, e.g. is the alpha higher when women complete the measure than it is when men complete it. These are based on the parametric model of and developed by Feldt and summarised nicely in Feldt, Woodruff, and Salih (1987).\nThat looked to give the test Emily wanted, however, the truism that unless something is very wrong, a measure of a latent variable with more items will always have higher reliability than one with fewer. I avoid gambling like the plague but I should have offered a bet to Emily that the negative items would have the higher reliability, and given that she had an aggregated dataset with n in the thousands, that the difference would be highly statistically significant.\nTheory behind the Spearman-Brown formula\nWhy should a longer test have higher reliability? Simply because as you get more items each item’s error (unreliability) variance, by definition uncorrelated with any other item’s error variance will tend to cancel out while any systematic variance that each item captures from the latent variable will accumulate. (Why do I say that items’ error variances are uncorrelated with each other: that simply follows from the definition that unreliability is random contamination of scores: if errors were correlated they would be part of invalidity: systematic contamination of scores, not of unreliability.)\nSo it’s logical that longer tests will have higher reliability than shorter assuming the same or similar mean inter-item correlations which reflect the systematic variance across scores is similar or the same.\nThe Spearman-Brown formula tells us how reliability changes with k, the number of items. How does the relationship look? That was the plot above. Here it is again.\n\n\nShow code\n\nmaxK <- 60\nvecK <- 2:maxK\nvecK %>%\n  as_tibble() %>%\n  rename(k = value) %>%\n  rowwise() %>%\n  mutate(rel.1 = getRelBySpearmanBrown(oldRel = .1, lengthRatio = k / 2, verbose = FALSE),\n         rel.2 = getRelBySpearmanBrown(oldRel = .2, lengthRatio = k / 2, verbose = FALSE),\n         rel.3 = getRelBySpearmanBrown(oldRel = .3, lengthRatio = k / 2, verbose = FALSE), \n         rel.4 = getRelBySpearmanBrown(oldRel = .4, lengthRatio = k / 2, verbose = FALSE)) %>%\n  ungroup() -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = starts_with(\"rel.\"), names_to = \"IIC\", values_to = \"Reliability\") %>%\n  mutate(IIC = factor(str_sub(IIC, 4, 5))) -> tibDatLong\n  \n\nggplot(data = tibDatLong,\n       aes(x = k, y = Reliability, group = IIC, colour = IIC)) +\n  geom_point(size = 1) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = c(1, seq(2, 8, 2), seq(0, maxK, 10))) + # and I want the x axis with these tick marks and labels\n  scale_y_continuous(breaks = seq(0, 1, .1)) + # same for the y axis\n  ggtitle(\"Relationship of reliability to k, number of items in a measure\",\n          subtitle = \"for fixed inter-item correlation (IIC)\") +\n  theme_bw() + # I like this simple theme with white plot area\n  theme(plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5), # I like titles and subtitles centered\n        panel.grid.minor = element_blank(), # gets grid lines only where the axis tick marks are not adding minor ones between those\n        axis.text.x = element_text(angle = 80, # trying to get the axis point labels rotated for maximum clarity\n                                   hjust = 1,  # and aligning them, \n                                   vjust = .75)) # is there a bug in the ggplot code failing to handle the number of characters?\n\n\n\nThat shows clearly how reliability climbs very rapidly as you move from having two items (the minimum to have an internal reliability) through single figures and then how the improvement steadily slows and will never reach 1.0 (unless you start with a reliability of 1.0 which isn’t our real world and arguably isn’t any real world). It also shows that the curve depends on the inter-item correlation (IIC). I have plotted starting with a correlation between two items of 0.1, 0.2, 0.3 or 0.4.\nSo knowing that there are seven negatively cued items in the YP-CORE and three positively cued items, how would the ratio of the reliabilities of the two vary with the mean ICC assuming that it was the same in each set of items? Here, in red, is the plot of the Spearman-Brown predicted reliability for the negatively cued items given a range of reliabilty for the three positively cued items from .01 to .35.\n\n\nShow code\n\nnNeg <- 7\nnPos <- 3\nvecRelPos <- seq(.01, .35, .01)\nvecRelPos %>%\n  as_tibble() %>%\n  rename(relPos = value) %>%\n  rowwise() %>%\n  mutate(relNeg = getRelBySpearmanBrown(oldRel = relPos, lengthRatio = 7/3, verbose = FALSE),\n         relRatio = relPos / relNeg) %>%\n  ungroup() -> tibDat2\n\nggplot(data = tibDat2,\n       aes(x = relPos, y = relNeg)) +\n  geom_line(colour = \"red\",\n            size = 2) +\n  geom_abline(intercept = 0, slope = 7 / 3) +\n  geom_abline(intercept = 0, slope = 1,\n              colour = \"blue\") +\n  scale_x_continuous(breaks = seq(0, .55, .05), \n                     limits = c(0, .55)) +\n  scale_y_continuous(breaks = seq(0, .55, .05), \n                     limits = c(0, .55)) +\n  ylab(\"Reliability for negative items\") +\n  xlab(\"Reliability for positive items\") +\n  ggtitle(\"Plot of reliability for seven negatively cued items given reliability of three positively cued items\",\n          subtitle = \"Assumes same mean inter-item correlation, black line marks inaccurate prediction using just y = 7/3 * x not Spearman-Brown formula\\n\n          blue line is y = x\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5),\n        aspect.ratio = 1)\n\n\n\nWe can see that the values are always well above equality to the reliability of the three items (the blue line) and we can see that the relationship isn’t a simple proportion and that the values are always lower than 7/3 times the reliability from the positively cued items.\nYou can use getRelBySpearmanBrown from the CECPfuns package\nAs announced here, there is a developing CECPfuns package (https://cecpfuns.psyctc.org/) which contains the function getRelBySpearmanBrown and which was used in the code above. There is also the function SpearmanBrown in the psychometric package by Thomas D. Fletcher which does the same thing (and gives the same results!)\n\n\nCooper, P. J., M. J. Taylor, Z. Cooper, and C. G. Fairburn. 1986. “The Development and Validation of the Body Shape Questionnaire.” International Journal of Eating Disorders 6: 485–94.\n\n\nEvans, Chris, and Bridget Dolan. 1993. “Body Shape Questionnaire: Derivation of Shortened \"Alternate Forms\".” International Journal of Eating Disorders 13: 315–21.\n\n\nFeldt, Leonard S., David J. Woodruff, and Fathi A. Salih. 1987. “Statistical Inference for Coefficient Alpha.” Applied Psychological Measurement 11: 93–103.\n\n\n\n\n\n",
    "preview": "posts/2021-04-09-spearman-brown-formula/spearman-brown-formula_files/figure-html5/graphic1-1.png",
    "last_modified": "2023-08-25T13:39:35+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-26-compiling-r-on-a-raspberry-pi-4/",
    "title": "Compiling R on a Raspberry Pi 4",
    "description": "I thought I should document this process as it turned out to be fairly easy",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-03-26",
    "categories": [
      "Raspberry pi",
      "Geeky stuff"
    ],
    "contents": "\n\nContents\nGetting started with the machine\nCompiling the latest R from source\nAcknowledgement\n\n\n\nShow code\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nas_tibble(list(x = 1,\n               y = 1)) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"R 4.0.4 on Pi 4!\",\n            size = 20,\n            colour = \"red\",\n            angle = 30) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\n\n[update tweak 15.iv.21 to add “, build_manual = TRUE” to install_github call]\nI have recently spent a (very small) amount of money to have a Raspberry Pi 4 to play with to see if I can run the open source shiny server off it. I am using the lovely service my ISP, Mythic Beasts provide, see https://www.mythic-beasts.com/order/rpi. So this has got me a Pi 4 with 4Gb of RAM and a choice of three operating systems: Ubuntu, Raspbian and, my current choice “Raspberry Pi OS 64 bit”, Debian GNU/Linux 10 (buster) according to lsb_release -a. The nice way that Mythic Beasts do this uses NFS file storage rather than an SD card for the main storage and I have paid for 10Gb at this point. That may matter if someone is trying to follow this but using less storage.\nI am putting this up here in the hope it will help others. The combination of R and the Raspberry Pi, particularly the newer, really impressively powerful iterations of the Pi, strike me as an extremely low cost way to get yourself formidable number crunching power. However, my experience so far is that this is not a well documented path to take and that there can be real messes for you as things are different on ARM hardware from the commoner AMD or Intel processors and as, as always in the open source world, things change and documentation tends to lag behind the changes so that old documentation can create real problems. Like pretty much everyone else in the open source world, I’m not paid to do this so my page here will go out of date too. I will try to update it and please contact me if you find what I have put here doesn’t work for you and I’ll try to update this to reflect whatever has caused the issue.\nGetting started with the machine\nOK, so I started with a raw machine, logged in and ran:\napt-get update\napt-get upgrade\nto get things up to date. Then I ran:\napt-get install apt-file \n# helps finding packages for missing resources\napt-file update \n# initialises and in future will update the cache that apt-file uses\nThat was because\napt-file search missingThing\ncan be a very good way to find the particular package you need to install to find the missingThing you need!\nNext came:\napt-get install emacs #because I prefer it to vi[m]\nI think that got me python 2.7 as a byproduct.\nAnd then:\napt-get install curl\napt-get install wget\nas they are two ways of yanking things down from the internet and I don’t think they’re installed by default.\nThen I did this:\napt-get install r-base\nas I was told that would get some other Debian packages that I would need for R. I suspect that’s true and it was pretty fast, got me R version 3.5.2 and\nhaving that doesn’t seem to have interfered with the next stages.\nCompiling the latest R from source\nThe first thing is to get the latest source from CRAN. You can see the URL here and you should be tweaking these version numbers unless you are copying this in the next few days.\n[Update 13.iv.21 for R 4.0.5 on 32-bit Raspbian: obviously you change “4.0.4” below to “4.0.5”]\nwget https://cran.r-project.org/src/base/R-4/R-4.0.4.tar.gz\ngunzip R-4.0.4.tar.gz\ntar -xvf R-4.0.4.tar\nSo that’s yanked down the gzipped, tar packed, sources and then unzipped and unpacked them into a directory that, for this version, called R-4.0.4. Surprise, surprise!\nNow the key thing is the compiling. That means this but don’t do it yet …\ncd R-4.0.4\n./configure\nThat runs a stunning configuring script that checks out whether you have everything needed for the compilation. I had to keep running this until it stopped terminating with requests for resources. For example, the first error message for me was\nX11 headers/libs are not available\nwhich was satisfied by me doing apt-get install libxt-dev.\nWhen you have sorted all the missing resources that cause full errors there are still warnings. Again, my first was:\nconfigure: WARNING: you cannot build info or HTML versions of the R manuals.\nFinally, when you have got rid of all the warnings by adding things you are left with capabilities that are omitted. I had:\nCapabilities skipped:        TIFF, Cairo\nIt’s tedious and time wasting to keep going through these cycles of ./configure and correcting so to save yourself time I think you can safely do this lot before your first ./configure and then that run should work. Here are the things I pulled in.\napt-get install libxt-dev # supports x11 screen handling\napt-get install libpcre2-dev # gets the PCRE libraries used by grep and its relatives\napt-get install libcurl4-openssl-dev # adds SSL/TLS encrypted downloading\napt-get install libtiff-dev # for tiff graphic output\napt-get install libgtk-3-dev # may not have been necessary\napt-get install libghc-cairo-dev # for Cairo system for graphic output\napt-get install texinfo texlive texlive-fonts-extra # for creating of help/man pages\n### that pulled a huge amount but allows you got get TIFF and Cairo output, then\nfmtutil-sys --missing \n### rebuilds format files for new fonts (I think)\n[Update 13.iv.21 for R 4.0.5 on 32-bit Raspbian]\nInterestingly I had to add:\napt-get install libbz2-dev libreadline-dev\nOn Raspbian 32-bit, a.k.a. (also known as, healthcare slang?) Linux raspberrypi 5.10.17-v7l+ #1403 SMP Mon Feb 22 11:33:35 GMT 2021 armv7l GNU/Linux\nAt that point, i.e. after ./configure ran fine, I could finally go for\nmake -j4\nApparently the “-j4” allows the make process to use four processes which speeds things up. The compilation took less than 30 minutes on my machine.\nOne message I noticed as the compilation proceeded was a familiar one:\nmake[1]: Entering directory '/home/chris/R-4.0.4/doc'\nconfiguring Java ...\n\n*** Cannot find any Java interpreter\n*** Please make sure 'java' is on your PATH or set JAVA_HOME correspondingly\nI’ll come back to that.\nFinally we get to:\nmake install \nputs R into /usr/local/lib. To my surprise I had to copy the ./bin/R executable from the temporary directory to /usr/bin/R:\ncp ./bin/R /usr/bin/R\n\nand then I was away! R 4.0.[4|5] up and running in what I think was less than an hour.\n\nupdate.packages(ask = FALSE, checkBuilt = TRUE)\n\ngot the base and recommended packages installed by default updated. That through up one error:\nERROR: dependency ‘openssl’ is not available \nSo I added these from the OS prompt:\napt-get install libssl-dev\napt-get install libxml2-dev\napt-get install libgit2-dev     \nI use components of the tidyverse a lot so the next step was to go back into R and run the obvious\n> install.packages(\"tidyverse\") \nwhich pulls in the key tidyverse packages was vital for me. That took quite a while to get all the components compiled in. Then I could add my own little package:\nremotes::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE)\n### or \nremotes::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE)\n### to get the PDF manual as well\nThat pulled in some more other packages but all compiled without issues.\nFinally, I could come back to the Java issue. Back out of R and to the OS prompt. This seemed to get me the Java I wanted.\napt-get install default-jdk\nand then I could do\nR CMD javareconf\nwhich found all it wanted and so I could install the rJava package in R and check that it works: it does.\nThat’s it! R 4.0.[4|5] installed on a Raspberry Pi 4 and I’m now much more confident that I compile subsequent releases on the machine too.\nAcknowledgement\nI am very grateful for encouragement and tips from Denis Brion. I think some of his work with R on Raspberry Pi machines can be seen at https://qengineering.eu/deep-learning-examples-on-raspberry-32-64-os.html.\n\n\n\n",
    "preview": "posts/2021-03-26-compiling-r-on-a-raspberry-pi-4/compiling-r-on-a-raspberry-pi-4_files/figure-html5/createGraphic-1.png",
    "last_modified": "2023-08-25T13:39:17+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-28-where-to-store-different-settings-in-rmarkdown-files/",
    "title": "Where to store different settings in Rmarkdown files",
    "description": "This may be of use to others but it's partly for me as I keep forgetting these and searching around for the .Rmd files in which I used the one I want!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-28",
    "categories": [
      "R tricks",
      "Rmarkdown"
    ],
    "contents": "\n\nContents\nSetttings in the yaml header\nfor xelatex if using PDF and flextable\nyaml heading settings for distill\n… in index.Rmd\n… in “posts”\n… in articles/pages\n\n\nSettings in css block\nSettings in early/first R code block\nSetting ggplot defaults\n\nUpdated with improved information about ggplot defaults 25.x.21\nRmarkdown is brilliant as a framework in which to create reports using R and it’s often useful to reset various defaults at the start of a file. Increasingly I work from Rmarkdown to html so some of this only applies there. I find there are three places I set things:\nin the yaml header\nin a css block or separate file (only for html output)\nin the first or an early R code block\nsetting defaults for ggplot (usually in that same early block)\nSetttings in the yaml header\nThis is well documented in many places and https://bookdown.org/yihui/rmarkdown/html-document.html is probably the canonical reference but searching will provide much other advice. I often use:\n---\ntitle: \"A title here\"\nauthor: \"Xxxx Yxxx\"\ndate: \"03/01/2021\"\n\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    toc_depth: 4\n    fig_height: 8\n    fig_width: 11    \n# bibliography: references.bib\n---\n\nI think the main things to say about that is that I don’t find that the floating table of contents (toc_float: true) always works, with long documents and complex blocks with graphics and text I find it sometimes mangles the toc so I am using it less than I used to. This can be a useful place to set the figure heading if they might be the same for all your code blocks with graphic output. I am not sure how many other code block header settings you could set here. I must experiment more: could save me a lot of typing. The only other thing there is the bibliography line, commented out. I still haven’t got into regular use of the citation and referencing capacities built into Rmarkdown. Must try harder!\nHere is another\n---\ntitle: \"ICCs from multilevel models analysed with lme4 or nlmer\"\nauthor: \"CE\"\ndate: \"26/02/2021\"\noutput:\n  html_document:\n    # css: \"test.css\"\n    toc: TRUE\n    toc_float: FALSE\n    dpi: 200\n    out.width: \"100%\"\n    fig.height: 40\n---\nThat shows that you can call an external css file (see next section), so far I haven’t found that I have enough css to make that worth doing. More important here, and I’m still working on this, I have found that you can use out.width: \"100%\" to make the html avail itself of more of your screen width which I find useful. The dpi: 200 and huge fig.height: 40 settings were me trying to optimise my graphic output for some complex plots.\nfor xelatex if using PDF and flextable\n[added 1.xi.23]\nIn the last few months my long, long search for a single Rmarkdown (and now, tidyverse) compatible table generating package has sort of settled on flextable. It’s not perfect but for the moment at least it’s doing most of what I want. However, today, for the first in ages, I wanted to output to PDF and hit an issue which is that flextable really wants the xelatex PDF writer not the default (I forget what that is). So I installed xelatex (this is just to remind myself):\napt install texlive-xetex\n# or\napt-get install texlive-xetex\n# or\nsudo apt install texlive-xetex\n# if you're not in a root console session so also:\nsudo apt-get install texlive-xetex\nNow the yaml I need is\n---\ntitle: \"Setswana forward translations\"\nauthor: \"Chris Evans\"\ndate: \"2023-11-01\"\noutput:\n  pdf_document:\n    latex_engine: xelatex\n---\nI’m putting this here really just to remind myself of the indentation/nesting there.\nyaml heading settings for distill\n… in index.Rmd\nThis is all I have in my index.Rmd file. As yet I haven’t found any other options that can usefully be added here.\n---\ntitle: \"An R SAFAQ by Chris Evans\"\nsite: distill::distill_website\nlisting: posts\n---\n… in “posts”\nThis where most of the Distill extensions to routine Rmarkdown yaml header blocks go. Here’s an example.\n---\ntitle: \"Making the CECPfuns package: my own usable package\"\ndescription: |\n  This is very much work in progress so look for later posts about CECPfuns as well as this.\nbase_url: https://www.xxxx.org/psyctc/Rblog/  \npreview: https://www.xxxx.org/pelerinage2016/wp-content/uploads/2020/07/P1160474.jpg\nauthor:\n  - name: Xxxx Yyyy\n    url: https://www.xxxx.org/R_blog/\n    affiliation: xxxx.org\n    affiliation_url: https://www.xxxx.org/psyctc/\n    orcid_id: xxxx-xxxx-xxxx-xxxx\n\ndate: 2021-02-10\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 4\n    hightlight_downlit: true\n    self_contained: false\n    code_folding: true\ncreative_commons: CC BY-SA\n---\nI think that’s mostly self-explanatory and I hope I’ve messed up my own data with sufficient “xxxx” insertions that it’s safe for people to copy and paste to create their own extension on the basic yaml that distill:create_post(\"post title\") creates. The code_folding option means that blocks of code are “folded” away by default but have a “Show code” button so the reader can unfold the code and read it.\n… in articles/pages\nHere is one of my yaml headers:\n---\ntitle: \"Welcome to these pages\"\ndescription: |\n  Welcome to these pages which I hope will be useful to people using R to analyse data.\nbase_url: https://www.xxx.org/psyctc/Rblog/  \nauthor:\n  - name: Xxxx Yyyy\n    url: https://www.xxx.org/R_blog/\n    affiliation: Xxxx.org\n    affiliation_url: https://www.xxx.org/psyctc/\n    orcid_id: xxxx-xxxx-xxxx-xxxx\ndate: \"2023-11-05\"\noutput: \n  distill::distill_article:\n    self_contained: false\ncreative_commons: CC BY-SA    \n---\nI think that’s all pretty self-explanatory. I am sure you can see what to if copying and pasting this!\nSettings in css block\nCSS is definitely not my expert area but I have been using a block like this:\nclick to show css_chunk.txt\n(Apologies for this way of putting the code in here: I gave up on trying to work out how to escape characters or otherwise override things being mangled in knitting that!)\nThat is using the principles behind css (cascading style sheet) to set some html defaults. The first two stanzas allow raw R text output (which comes out in the html “pre” format) to come up in a horizontally scrollable window which can be useful where you find you are spitting out wide output and the next stanza I think determines the formatting of raw code (not sure about that!).\nThe body stanza is a recent discovery of mine. The “body” section of html is everything except the header information, i.e. it’s what the human reading an html document sees. That allows my html output to use more of my nice big screen.\nSettings in early/first R code block\nWhen you create a new Rmd file in Rstudio it always has this first R code block.\nclick to show default_setup_chunk.txt\nI often expand that to something like this:\nclick to show big_setup_chunk.txt\n[updated 5.xi.23]\nPerhaps most usefully, I realised that I can set my default Rmarkdown block settings, e.g.\nknitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 16, fig.height = 12, cache = FALSE)\nrather than the default setting:\nknitr::opts_chunk$set(echo = TRUE)\nSetting ggplot defaults\n[updated 25.x.21]\nI have my own preferences for some of the “theme” elements in ggplot and discovered that I can set these for a whole Rmarkdown files like this:\n### set ggplot defaults\ntheme_set(theme_bw())\ntheme_update(plot.title = element_text(hjust = .5),\n             plot.subtitle = element_text(hjust = .5))\nThat theme_set() sets the default theme that will be used by ggplot() for the rest of the session, here I have set it to theme_bw() and then the theme_update() updates that. You can also make that a named object\n### save whatever the current theme settings are to an object\ntheme_get() -> CEtheme\nWhich can make it easy to reinstate it with theme_set(CEtheme). And, of course, if you wanted to, you could even save that to a tiny file:\n### set ggplot defaults to file\nsave(CEtheme, \"CEtheme\")\nSo in any other R work you can load() that file and set theme.\nload(file = \"CEtheme\")\noldTheme <- theme_set(CEtheme) # uses invisible return of the pre-existing default theme by theme_set() to save that \nDo contact me if you have advice about setting Rmarkdown options and if have corrections to the above.\n\n\n\n",
    "preview": "posts/2021-02-28-where-to-store-different-settings-in-rmarkdown-files/css.png",
    "last_modified": "2023-11-05T20:17:09+01:00",
    "input_file": {},
    "preview_width": 260,
    "preview_height": 321
  },
  {
    "path": "posts/2021-02-10-making-my-first-usable-package/",
    "title": "Making the CECPfuns package: my own usable package",
    "description": "This is very much work in progress so look for later posts about CECPfuns as well as this.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-10",
    "categories": [
      "R packages",
      "R tricks"
    ],
    "contents": "\n\nContents\nLatest update\nBackground\nWhy do it?\nWarning\n\nCreate your package\nCreate your first function\nStart to insert help/documentation contents\n\nNow check and build your package\nThat’s it! You’re done!\nUsing functions from other packages\n\nHow I am synching my package to machines other than my main machine\nThings that are still work in progress for me!\nCECPfuns is a start\n\nMont BlancLatest update\n[Started 10.ii.21, tweak 15.iv.21 to add “, build_manual = TRUE” to install_github call]\nBackground\nI have been meaning to do this for years but I have still found it one of R’s more tough learning curves even by R’s sometimes challenging standards. The tidyverse is perhaps a higher and better known climb but making this package is K2 to the tidyverse Everest: nastier, colder, more dispiriting and my attempt of the ascent a few years ago hardly got beyond base camp. This time I’m sort at a first support camp level above base camp and I’m trying to document things here.\nWhy do it?\nI would have saved many hours over the last few years had I actually got on top of this when I first started. Why:\nit hugely simplifies keeping track of functions I’ve written and making sure I find the latest version,\nusing the devtools functions and the way these have been integrated into Rstudio actually makes it easy to create new functions, document them (at least minimally) and update them\nintegrating with git it becomes easy to keep track of changes you make\nif you integrate the local git repository to GitHub you can easily share the package, even if that’s only between your own individual machines, for me that’s my main laptop, my backup laptop, my lightweight and ageing Windoze machine and my web server: it’s easy to make sure they’re all looking at the same functions in the same package.\nWarning\nThere are hugely powerful tools to help the creation of R packages and many pages and PDFs on the web to help you. However, for me finding exactly the information I need, getting its context, being sure the advice isn’t outdated and sometimes just understanding what people have written has not always been easy. That’s partly why I’ve created this.\nPlease, I will try to remember to amend any mistakes I find in here, or things I discover change or can be done more easily than whatever I say here, but anything here doesn’t work for you, please:\nlook at the “Latest update” date above;\nuse the search function (in the navigation bar above) and search for “CECPfuns” and look for more recent posts about this;\nuse an advanced search on the web to search for the particular topic looking for things since that “Latest update” date;\ncontact me to tell me, ideally tell me how to fix what didn’t work for you;\nplease realise this is not my job, this, as with everything I put on the web is offered with no warranties, I accept no liabilities, and I probably will have very little time to try to help you explore anything … if I really have time on my hands though, I will try to help. I am doing this using the the Rstudio package building tools, it’s highly unlikely that I will be any help with any other ways of building a package (there are several but I see them as mostly for real R and software experts).\nHm, that I’m writing that probably conveys that this has been a bit tricky.\nOK, not K2, actually my view in the Alps, see (www.psyctc.org/pelerinage2016/Create your package\nOK, the first bit is easy: create a new package using a new directory and the “Create R package” option; give your package a name, e.g. “SillyDemoPackage”. There is the option to include some source (i.e. R code for our purposes) files here but I would recommend starting completely cleanly and creating new source files, one per function, and copying and pasting the code you already have into the new file.\nThat will have created a subdirectory of wherever you were titled named “SillyDemoPackage” and beneath it you have three more subdirectories:\nR (where you are going to put you R source files, one per function)\nman (where you will, using devtools::document(), create Rd files that in turn create the help for the package and functions)\n.Rproj.user (project information: can ignore it)\nCreate your first function\nThat’s your next step: create a new R script file; if your function is myFunction() then save the script into the R subdirectory that creating the project will have created.\nYou now have a single source file with a single function in it. (I think you can put more than one function in a single source file but I think it would be making your life more difficult so don’t).\nPut your cursor inside the function then go to the Code menu above and select “Insert Roxygen Skeleton”. Let’s say I start with this:\nmyFunction <- function(x){\n  return(paste(\"This is a silly function of\", x))\n}\nStart to insert help/documentation contents\nUsing Code, Insert Roxygen Skeleton changes that to this\n#' Title\n#'\n#' @param x \n#'\n#' @return\n#' @export\n#'\n#' @examples\nmyFunction <- function(x){\n  return(paste(\"This is a silly function of\", x))\n}\nAnd you now change that to this:\n#' Title\n#'    This is a very silly function purely for demonstration.\n#' @param x can be any printable object\n#'\n#' @return a string that pastes a silly comment before x\n#' @export\n#'\n#' @examples\n#' x <- \"stunning silliness\"\n#' myFunction(x)\n\nmyFunction <- function(x){\n  return(paste(\"This is a silly function of\", x))\n}\nYou see I have given a short description of the function, I have clarified the one parameter (argument) to the function and what the function returns and I have given a suitably inane example of how it might be used.\nNow put your cursor in the function and type devtools::document(). That will (essentially invisibly) create a new file myFunction.Rd in the /man subdirectory I mentioned above. **Remember to rerun devtools::document() within the function every time you tweak the documentation in those header lines and every time you tweak the function otherwise the help will lag behind what you’ve done (which might or might not be caught at the next stage, but better safe than sorry.)\nNow check and build your package\nNow the exciting bit: under the Build menu, pick “Check” and sit back and watch Rstudio and devtools (and perhaps other things for all I know) whiz through many checks on your package (in the top right hand pane of Rstudio in the usual layout, in the Build tab. I don’t think you can miss it. Those checks can flag up erorrs, warnings and notes and you hope to see an all green summary line at the end saying there were none of any of those. If the checks find issues some messages are very clear and helpful, some are more challenging but I have found that searching on the web usually translates them for me.\nI would then make sure you set up version control on the project using git and I would also recommend then pushing the package to GitHub if you want others to be able to find it easily.\nThat’s it! You’re done!\nOK, I lie. That’s it for the SillyDemoPackage and it’s one function, myFunction(). I think that’s a like a short afternoon stroll out of Kathmandu in summer. When you start to do something remotely useful the gradient goes up a bit and the air gets a little thinner.\nUsing functions from other packages\nThis is a huge issue but actually fairly easy to handle. Most useful functions will call on functions from packages outside of the base functions. Where you do this you need to handle declaring these in a way that means that the package will know what comes from where. There are simple and more sophisticated issues here and the Build, Clean error messages are pretty clear and helpful and there are good guides to the subtleties on the web. So far I have stayed with making the function calls explicit so instead of cor(x, y) I write stats::cor(x, y) in the function and then I add:\nSuggests:\n  stats\nat the bottom of the DESCRIPTION file in the root directory of the package and\nimportFrom(\"stats\", \"cor\")\nat the bottom of the NAMESPACE file, also in the root directory of the package. I think usethis::use_package() helps with this but I have done it manually so far.\nThe other thing you have to do at the head of any such function instead of having a\nlibrary(sausages) # I wouldn't have had this for stats as, of course,\n### the stats package is launched by default when R starts, \n### imagine I am calling sausages::recipe() \n### NO! I made that up!\nyou use:\ninvisible(stopifnot(requireNamespace(\"sausages\")))\n### so a call that doesn't spit out a message but will stop things \n### if you don't have the sausages package on your system\n### requireNamespace() only checks if you have the package\n### it doesn't load the entire package as library() or \n### require() would so if you are only going to call one\n### or a few functions explicitly with sausages::functionName()\n### this is more efficient\nThat’s the lightest way to do things. If you are going to use lots of functions from a package you may be better with other options but this works for me for now.\nHow I am synching my package to machines other than my main machine\nAdded 28.ii.21: dept to Winston Change!\nIf you’re using M$ Windoze I think it’s best to ignore this section. Because Windoze won’t let anything alter a dll on disc that has been loaded into memory, with the really rather complicated way that R (and Rstudio too) pull things into memory as they launch and run .Rprofile this tends to lead to some package upgrading being blocked, e.g. of cachem which Winston maintains.\nI am developing my package on my main Linux laptop. As I can’t really survive without it, I have a near duplicate backup machine and a little, old Windows laptop and Windows in a VM on the main machine and I have R on my web server (serving up this blog, my CORE work https://www.coresystemtrust.org.uk/; my non-CORE work site https://www.psyctc.org/psyctc/; and my personal web site: https://www.psyctc.org/pelerinage2016/. Do go and have a look!)\nI wanted to make sure that every time I (or cron) launched R on any of the those machines it would automatically check for an update to the package on GitHub and install it if there were one. That meant putting a call to install it with devtools::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE) into .Rprofile.\nAdded evening 18.ii.21 with input from Clara\n### Locating your .Rprofile file\nYou should find, or create that in locations that are operating system dependent:\n* on linux machines it is /home/username/.Rprofile\n* on Windows machines it is C:/Users/username/Documents/.Rprofile\n* on Macs I am told it is /Users/username/.Rprofile and I am also told that as it is a hidden file, you will need cmd + shift + [.] in order to show the hidden files.\nAdded evening 10.ii.21, with help from Bill Dunlap via the R-help Email list\nHowever, my original addition to .Rprofile cause R to keep looping when launched. Bill Dunlap confirmed that’s because something, probably invoked by the devtools::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE) call, is restarting the R session and so rerunning the .Rprofile, and so on ad infinitum and Bill gave me the answer so my .Rprofile is now:\nif (Sys.getenv(\"INSTALLING_FROM_GITHUB\", unset = \"no\") == \"no\") {\n  Sys.setenv(INSTALLING_FROM_GITHUB = \"yes\")\n  devtools::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE)\n}\nAs I understand that code, it checks for an environment variable (i.e. a variable set in the operating system) called “INSTALLING_FROM_GITHUB” and if it finds its value is “no” it runs the the commands inside the brackets, resetting the variable to “yes” and then, the next line, checking if there has been an update of the package on GitHub and installing it if there has been. However, if/when .Rprofile is rerun in that R session the environment variable now has the value “yes” so the looping is prevented. Lovely!\nThings that are still work in progress for me!\nI am slowly learning about all the extras that transform the basic documentation, such as I created above, into really good help for a function.\nI haven’t worked out how to document a whole package yet. The function devtools::build_manual() seems to build at least the typical nice PDF about a package that you see on CRAN, e.g. https://cran.r-project.org/web/packages/boot/boot.pdf but it puts it in the directory above the package directory and the file doesn’t seem to get integrated into the package which seems puzzling and less than entirely helpful to me. I’m sure there must be an answer to that but I haven’t found it yet.\nI haven’t worked out how to create index files like https://cran.r-project.org/web/packages/boot/index.html though that may be because my package is so small that it doesn’t have most of the information that is in there. I can’t really believe that’s the whole reason though.\nCECPfuns is a start\nThis is pretty embarrassing but I will share that this first actual package of mine, probably the only one I’ll ever need to create, is available if you want to see what I’ve managed to create. It will develop into a package mainly of functions I and Clara Paz have found useful (with, I hope, comments and suggestions from Emily) It’s at https://github.com/cpsyctc/CECPfuns and there is a web site for the package at https://cecpfuns.psyctc.org/. You can use git on pretty much any operating system to pull a copy from github if you want to look at the all the raw constituent parts and I think if you do pull that you can see the commit history, i.e. of the changes and updating. (A graph of the commits against date is at https://github.com/cpsyctc/CECPfuns/graphs/commit-activity). I am not opening it to submissions as it’s too early in my learning, I may never reach that place, so, if you have suggestions or corrections and any comments really,contact me through my work site. I hope this helps someone and encourages them to create their own package. I do wish I’d done it earlier!\nMont Blanc from my Alpine balcony\n\n\n",
    "preview": "https://www.psyctc.org/pelerinage2016/wp-content/uploads/2020/07/P1160474.jpg",
    "last_modified": "2023-08-25T13:38:06+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-10-more-piping-introducing-rowwise/",
    "title": "More piping, and rowwise()",
    "description": "This extends https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/ and introduces rowwise()",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-10",
    "categories": [
      "R packages",
      "tidyverse",
      "piping",
      "R tricks"
    ],
    "contents": "\nThis is a slight adaptation of a file I did for Emily (https://www.researchgate.net/profile/Emily_Blackshaw2) back in October 2020 when she and wanted to look at whether Cronbach’s alpha for the YP-CORE varied from session to session across help-seeking clients data: a very basic exploration of longitudinal measurement invariance. I realised it was a good chance for me to pull together what I had been learning back then about piping and to share it with her.\nAs a page here it probably should have come before https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, or been woven into that, but I had managed to lose the file (!). However, I think it complements what I put in there and it does introduce the rowwise() function and c_across().\nAs is my wont, I prefer to explore methods with simulated data so the first step was to make such data. Here I am simulating 500 clients each having ten sessions and just a five item questionnaire (the YP-CORE has ten items but five is quicker and fits output more easily!)\n\n\nShow code\n\n### make some nonsense data \nlibrary(tidyverse)\nnParticipants <- 500\nnSessions <- 10\n### give myself something to start with: the sessions\nsession <- rep(1:nSessions, nParticipants) # 1,2,3 ...10, 1,23 ...10 ...\nsession %>%\n  as_tibble() %>%  # turn from vector to tibble, that means I have rename it back to the vector name!\n  rename(session = value) %>%\n  mutate(baseVar = rnorm(nParticipants*nSessions),  # this creates a new variable in the tibble and sort of reminds me that variables may be vectors\n         item1 = baseVar + 0.7*rnorm(nParticipants*nSessions), # creates a first item\n         item2 = baseVar + 0.7*rnorm(nParticipants*nSessions), # and a second\n         item3 = baseVar + 0.7*rnorm(nParticipants*nSessions), # and a third\n         item4 = baseVar + 0.7*rnorm(nParticipants*nSessions), # and a 4th ...\n         item5 = baseVar + 0.7*rnorm(nParticipants*nSessions)) -> tmpDat\n\n### look at it\ntmpDat\n\n# A tibble: 5,000 × 7\n   session baseVar   item1   item2  item3  item4   item5\n     <int>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n 1       1 -0.0834 -0.0989 -0.544   1.79   0.328  0.0842\n 2       2  2.55    1.75    1.33    2.30   1.27   1.69  \n 3       3 -1.77   -1.47   -1.81   -1.59  -2.37  -1.97  \n 4       4 -0.324  -0.533  -0.0590  0.633 -0.867 -0.453 \n 5       5 -0.413  -0.904  -0.987  -1.56  -0.202  0.756 \n 6       6  1.06    1.37    1.42    0.882 -0.457  1.34  \n 7       7  0.0410 -1.35    0.788   0.624  0.437 -0.0140\n 8       8  0.770  -0.252   0.753   0.865  1.82   0.364 \n 9       9  0.0827  0.285   0.269  -1.33   0.815  0.0147\n10      10 -1.07   -1.47   -0.803  -0.693 -2.50  -1.86  \n# ℹ 4,990 more rows\n\nShow code\n\n### check the simple correlation\ncor(tmpDat[, 3:7])\n\n          item1     item2     item3     item4     item5\nitem1 1.0000000 0.6685281 0.6638093 0.6743247 0.6602781\nitem2 0.6685281 1.0000000 0.6684304 0.6797483 0.6696608\nitem3 0.6638093 0.6684304 1.0000000 0.6730563 0.6645038\nitem4 0.6743247 0.6797483 0.6730563 1.0000000 0.6738977\nitem5 0.6602781 0.6696608 0.6645038 0.6738977 1.0000000\n\nShow code\n\n### OK, I can play with that, here's the overall alpha (meaningless even for the simulation really but just checking)\npsychometric::alpha(tmpDat[, 3:7])\n\n[1] 0.9101857\n\nOK. Now I could start playing with the data in the tidyverse/dplyr/piping way. The key thing to remember is that the default behaviour of mutate() or summarise() within group_by() in dplyr is for a function to act on a vertical vector, i.e. on a variable\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(mean1 = mean(item1))\n\n# A tibble: 10 × 2\n   session    mean1\n     <int>    <dbl>\n 1       1  0.00630\n 2       2  0.00253\n 3       3  0.0293 \n 4       4  0.0727 \n 5       5 -0.0505 \n 6       6 -0.0188 \n 7       7 -0.0348 \n 8       8 -0.0437 \n 9       9 -0.0403 \n10      10  0.0420 \n\nSo that simply got us the mean for item1 across all completions but broken down by session. Trivial dplyr/piping but I still find it satisfying in syntax and in its utility.\nAs introduced in https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, if I have a function that returns more than one value dplyr\nhandles this nicely but I have to tell it the function is creating a list (even if it’s just a vector), as below. The catch to remember is that you then have to unnest() the list to see its values, usually unnest_wider() is what I want but there is unnest_longer().\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(summary1 = list(summary(item1))) %>%\n  unnest_wider(summary1)\n\n# A tibble: 10 × 7\n   session Min.        `1st Qu.`   Median       Mean   `3rd Qu.` Max. \n     <int> <table[1d]> <table[1d]> <table[1d]>  <tabl> <table[1> <tab>\n 1       1 -2.885444   -0.8542505   0.008453520  0.00… 0.7741324 4.33…\n 2       2 -3.714294   -0.8196168  -0.013540736  0.00… 0.7568785 4.40…\n 3       3 -3.397608   -0.7093374   0.048696677  0.02… 0.7308804 3.65…\n 4       4 -3.282683   -0.7507610   0.111586870  0.07… 0.7971904 3.54…\n 5       5 -5.841905   -0.8259870  -0.001774436 -0.05… 0.8063849 3.16…\n 6       6 -3.885132   -0.8912593  -0.001092184 -0.01… 0.7633556 3.57…\n 7       7 -4.745932   -0.8515208   0.005372354 -0.03… 0.8805254 4.01…\n 8       8 -2.739551   -0.8400586  -0.082297507 -0.04… 0.7367041 3.84…\n 9       9 -3.423777   -0.7875004  -0.042422361 -0.04… 0.7737911 3.34…\n10      10 -3.231467   -0.7180346   0.070618081  0.04… 0.8567534 3.19…\n\nShow code\n\n###  names are messy but it is easy to solve that ...\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(summary1 = list(summary(item1))) %>%\n  unnest_wider(summary1) %>%\n  ###  sometimes you have to clean up names that start \n  ###  with numbers or include spaces if you want to avoid backtick quoting\n  rename(Q1 = `1st Qu.`,\n         Q3 = `3rd Qu.`)\n\n# A tibble: 10 × 7\n   session Min.        Q1          Median       Mean       Q3    Max. \n     <int> <table[1d]> <table[1d]> <table[1d]>  <table[1d> <tab> <tab>\n 1       1 -2.885444   -0.8542505   0.008453520  0.006301… 0.77… 4.33…\n 2       2 -3.714294   -0.8196168  -0.013540736  0.002530… 0.75… 4.40…\n 3       3 -3.397608   -0.7093374   0.048696677  0.029293… 0.73… 3.65…\n 4       4 -3.282683   -0.7507610   0.111586870  0.072742… 0.79… 3.54…\n 5       5 -5.841905   -0.8259870  -0.001774436 -0.050492… 0.80… 3.16…\n 6       6 -3.885132   -0.8912593  -0.001092184 -0.018803… 0.76… 3.57…\n 7       7 -4.745932   -0.8515208   0.005372354 -0.034829… 0.88… 4.01…\n 8       8 -2.739551   -0.8400586  -0.082297507 -0.043682… 0.73… 3.84…\n 9       9 -3.423777   -0.7875004  -0.042422361 -0.040280… 0.77… 3.34…\n10      10 -3.231467   -0.7180346   0.070618081  0.041990… 0.85… 3.19…\n\nAgain, as I introduced in https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, I can extend this to handle more than one vector/variable at a time if they’re similar and I’m doing the same to each.\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(across(starts_with(\"item\"), ~mean(.x)))\n\n# A tibble: 10 × 6\n   session    item1   item2   item3     item4     item5\n     <int>    <dbl>   <dbl>   <dbl>     <dbl>     <dbl>\n 1       1  0.00630 -0.0293 -0.0539 -0.0636   -0.0104  \n 2       2  0.00253  0.0218  0.0177 -0.00370   0.0116  \n 3       3  0.0293   0.0246 -0.0194 -0.0278    0.00223 \n 4       4  0.0727   0.0776  0.0686  0.0163    0.0642  \n 5       5 -0.0505  -0.0927 -0.0322 -0.0703   -0.0350  \n 6       6 -0.0188  -0.0989 -0.0497 -0.0347   -0.0573  \n 7       7 -0.0348   0.0310  0.0215  0.0750   -0.000846\n 8       8 -0.0437  -0.0405 -0.0383 -0.0482    0.00199 \n 9       9 -0.0403  -0.0204  0.0198  0.000882 -0.0117  \n10      10  0.0420   0.120   0.0578  0.0288    0.0585  \n\nI can also do that with the following syntax. I have not yet really understood why the help for across() gives that one with function syntax (“~”) and the explicit call of “.x) rather than this and I really ought to get my head around the pros and cons of each.\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(across(starts_with(\"item\"), mean))\n\n# A tibble: 10 × 6\n   session    item1   item2   item3     item4     item5\n     <int>    <dbl>   <dbl>   <dbl>     <dbl>     <dbl>\n 1       1  0.00630 -0.0293 -0.0539 -0.0636   -0.0104  \n 2       2  0.00253  0.0218  0.0177 -0.00370   0.0116  \n 3       3  0.0293   0.0246 -0.0194 -0.0278    0.00223 \n 4       4  0.0727   0.0776  0.0686  0.0163    0.0642  \n 5       5 -0.0505  -0.0927 -0.0322 -0.0703   -0.0350  \n 6       6 -0.0188  -0.0989 -0.0497 -0.0347   -0.0573  \n 7       7 -0.0348   0.0310  0.0215  0.0750   -0.000846\n 8       8 -0.0437  -0.0405 -0.0383 -0.0482    0.00199 \n 9       9 -0.0403  -0.0204  0.0198  0.000882 -0.0117  \n10      10  0.0420   0.120   0.0578  0.0288    0.0585  \n\nAgain, as I introduced in https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, I can do multiple functions of the same items\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(across(starts_with(\"item\"), list(mean = mean, sd = sd)))\n\n# A tibble: 10 × 11\n   session item1_mean item1_sd item2_mean item2_sd item3_mean item3_sd\n     <int>      <dbl>    <dbl>      <dbl>    <dbl>      <dbl>    <dbl>\n 1       1    0.00630     1.18    -0.0293     1.21    -0.0539     1.16\n 2       2    0.00253     1.21     0.0218     1.16     0.0177     1.23\n 3       3    0.0293      1.16     0.0246     1.20    -0.0194     1.18\n 4       4    0.0727      1.20     0.0776     1.21     0.0686     1.22\n 5       5   -0.0505      1.22    -0.0927     1.17    -0.0322     1.17\n 6       6   -0.0188      1.28    -0.0989     1.26    -0.0497     1.19\n 7       7   -0.0348      1.26     0.0310     1.24     0.0215     1.24\n 8       8   -0.0437      1.18    -0.0405     1.18    -0.0383     1.17\n 9       9   -0.0403      1.17    -0.0204     1.16     0.0198     1.22\n10      10    0.0420      1.20     0.120      1.29     0.0578     1.26\n# ℹ 4 more variables: item4_mean <dbl>, item4_sd <dbl>,\n#   item5_mean <dbl>, item5_sd <dbl>\n\nI like that that names things sensibly\nI said the default behaviour of mutate() and summarise() is to work on variables, i.e. vectors, whether that is to work on all the values of the variable if there is no group_by(), or within the groups if there is a grouping. If I want to do something on individual values, i.e. by rows, “rowwise”, then I have to use rowwise() which basically treats each row as a group.\nIf, as you often will in that situation, you want to use a function of more than one value, i.e. values from more than one variable, then you have to remember to use c_across() now, not across(): “c_” as it’s by column.\nYou also have to remember to ungroup() after any mutate() as you probably don’t want future functions to handle things one row at a time.\n\n\nShow code\n\ntmpDat %>% \n  filter(row_number() < 6) %>% # just for this example\n  rowwise() %>%\n  mutate(mean = mean(c_across(starts_with(\"item\")))) %>%\n  ungroup() # see above about ungrouping after rowwise() and mutate()\n\n# A tibble: 5 × 8\n  session baseVar   item1   item2  item3  item4   item5   mean\n    <int>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>  <dbl>\n1       1 -0.0834 -0.0989 -0.544   1.79   0.328  0.0842  0.311\n2       2  2.55    1.75    1.33    2.30   1.27   1.69    1.67 \n3       3 -1.77   -1.47   -1.81   -1.59  -2.37  -1.97   -1.84 \n4       4 -0.324  -0.533  -0.0590  0.633 -0.867 -0.453  -0.256\n5       5 -0.413  -0.904  -0.987  -1.56  -0.202  0.756  -0.579\n\nOK, so that’s recapped these things, now what about if I want to look at multiple columns and multiple rows?\nthe trick seems to be cur_data().\nThat gives me a sensible digression from Cronbach’s alpha here as I often find I’m wanting to get correlation matrices when I’m wanting to get alpha (and its CI)\nand I think getting correlation matrices from grouped data ought to be much easier than it is!\n\n\nShow code\n\ntmpDat %>% \n  select(item1:item5) %>%\n  summarise(cor = list(cor(cur_data()))) %>%\n  unnest_wider(cor) \n\n# A tibble: 1 × 5\n  item1[,\"item1\"] item2[,\"item1\"] item3[,\"item1\"] item4[,\"item1\"]\n            <dbl>           <dbl>           <dbl>           <dbl>\n1               1           0.669           0.664           0.674\n# ℹ 5 more variables: item1[2:5] <dbl>, item2[2:5] <dbl>,\n#   item3[2:5] <dbl>, item4[2:5] <dbl>, item5 <dbl[,5]>\n\nThat, as you can see, is a right old mess!\nbut we can use correlate() from the corrr package:\n\n\nShow code\n\ntmpDat %>% \n  select(item1:item5) %>%\n  corrr::correlate()\n\n# A tibble: 5 × 6\n  term   item1  item2  item3  item4  item5\n  <chr>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 item1 NA      0.669  0.664  0.674  0.660\n2 item2  0.669 NA      0.668  0.680  0.670\n3 item3  0.664  0.668 NA      0.673  0.665\n4 item4  0.674  0.680  0.673 NA      0.674\n5 item5  0.660  0.670  0.665  0.674 NA    \n\nAs you see, corrr::correlate() puts NA in the leading diagonal not 1.0. That does make finding the maximum off diagonal correlations easy but I confess it seems wrong to me!\nWhat about using that and group_by()?\n\n\nShow code\n\ntmpDat %>% \n  select(-baseVar) %>%\n  group_by(session) %>%\n  corrr::correlate()\n\n# A tibble: 6 × 7\n  term     session    item1    item2   item3   item4    item5\n  <chr>      <dbl>    <dbl>    <dbl>   <dbl>   <dbl>    <dbl>\n1 session NA       -0.00915  0.00828  0.0111  0.0139  0.00340\n2 item1   -0.00915 NA        0.669    0.664   0.674   0.660  \n3 item2    0.00828  0.669   NA        0.668   0.680   0.670  \n4 item3    0.0111   0.664    0.668   NA       0.673   0.665  \n5 item4    0.0139   0.674    0.680    0.673  NA       0.674  \n6 item5    0.00340  0.660    0.670    0.665   0.674  NA      \n\nHm, that completely ignores the group_by() and includes session variable. That seems plain wrong to me. I feel sure this is something the package will eventually\nchange but for now I need another way to get what I want.\n\n\nShow code\n\ntmpDat %>% \n  select(-baseVar) %>%\n  group_by(session) %>%\n  corrr::correlate(cur_data())\n\n\nI have not evaluated that as it stops with the moderately cryptic error message which I’m putting in here as I quite often forget the summarise(x = ) bit\n# Error: `cur_data()` must only be used inside dplyr verbs.\n# Run `rlang::last_error()` to see where the error occurred.\nSo let’s fix that.\n\n\nShow code\n\ntmpDat %>% \n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(cor = corrr::correlate(cur_data()))\n\n# A tibble: 50 × 2\n# Groups:   session [10]\n   session cor$term $item1 $item2 $item3 $item4 $item5\n     <int> <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1       1 item1    NA      0.702  0.632  0.654  0.634\n 2       1 item2     0.702 NA      0.658  0.704  0.672\n 3       1 item3     0.632  0.658 NA      0.655  0.647\n 4       1 item4     0.654  0.704  0.655 NA      0.684\n 5       1 item5     0.634  0.672  0.647  0.684 NA    \n 6       2 item1    NA      0.661  0.684  0.673  0.642\n 7       2 item2     0.661 NA      0.695  0.682  0.672\n 8       2 item3     0.684  0.695 NA      0.665  0.682\n 9       2 item4     0.673  0.682  0.665 NA      0.671\n10       2 item5     0.642  0.672  0.682  0.671 NA    \n# ℹ 40 more rows\n\nHm. That does get me the analyses I want but in what is, to my mind, a very odd structure.\nOK, after that digression into the corrr package, let’s get to what Emily actually wanted: Cronbach’s alpha across the items but per session.\n\n\nShow code\n\ntmpDat %>%\n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(alpha = psychometric::alpha(cur_data()))\n\n# A tibble: 10 × 2\n   session alpha\n     <int> <dbl>\n 1       1 0.908\n 2       2 0.911\n 3       3 0.908\n 4       4 0.910\n 5       5 0.904\n 6       6 0.924\n 7       7 0.915\n 8       8 0.907\n 9       9 0.899\n10      10 0.913\n\nI get my CI around alpha using the following code.\n\n\nShow code\n\npsychometric::alpha(tmpDat[, 3:7])\n\n[1] 0.9101857\n\nShow code\n\ngetAlphaForBoot <- function(dat, i) {\n  # a little function that happens to use psych::alpha to get alpha\n  # but indexes it with i as boot() will require\n  psychometric::alpha(na.omit(dat[i,]))\n}\ngetAlphaForBoot(tmpDat[, 3:7], 1:nrow(tmpDat)) # just checking that it works\n\n[1] 0.9101857\n\nShow code\n\nbootReps <- 1000\ngetCIAlphaDF3 <- function(dat, ciInt = .95, bootReps = 1000) {\n  tmpRes <- boot::boot(na.omit(dat), getAlphaForBoot, R = bootReps)\n  tmpCI <- boot::boot.ci(tmpRes, conf = ciInt, type = \"perc\")$percent[4:5]\n  return(data.frame(alpha = tmpRes$t0,\n                    LCL = tmpCI[1],\n                    UCL = tmpCI[2]))\n}\ngetCIAlphaDF3(tmpDat[, 3:7])\n\n      alpha       LCL      UCL\n1 0.9101857 0.9055701 0.914153\n\nActually, now I have my CECPfuns package I create\na better, more robust function for this, but later!\nSo that’s the overall Cronbach alpha with bootstrap confidence interval.\nCan also do that within a group_by() grouping.\n\n\nShow code\n\ntmpDat %>%\n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(alpha = list(getCIAlphaDF3(cur_data()))) %>% \n  unnest_wider(alpha)\n\n# A tibble: 10 × 4\n   session alpha   LCL   UCL\n     <int> <dbl> <dbl> <dbl>\n 1       1 0.908 0.892 0.920\n 2       2 0.911 0.896 0.923\n 3       3 0.908 0.890 0.921\n 4       4 0.910 0.897 0.923\n 5       5 0.904 0.890 0.916\n 6       6 0.924 0.912 0.934\n 7       7 0.915 0.902 0.927\n 8       8 0.907 0.891 0.920\n 9       9 0.899 0.884 0.911\n10      10 0.913 0.899 0.924\n\nAnd that was nice and easy to feed into a forest style plot, as follows.\n\n\nShow code\n\ntmpDat %>%\n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(alpha = list(getCIAlphaDF3(cur_data()))) %>% \n  unnest_wider(alpha) -> tmpTib\n\npsychometric::alpha(tmpDat[, 3:7]) -> tmpAlphaAll\n\nggplot(data = tmpTib,\n       aes(x = session, y = alpha)) +\n  geom_point() + # get the observed alphas in as points\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) + # add the CIs as lines\n  geom_hline(yintercept = tmpAlphaAll) + # not really very meaningful to have an overall alpha but \n    # perhaps better than not having a reference line\n  xlab(\"Session\") +\n  ylab(\"Cronbach alpha\") +\n  ggtitle(\"Forest plot of observed Cronbach alpha per session\",\n          subtitle = paste0(\"Vertical lines are 95% CIs, \",\n                            bootReps,\n                            \" bootstrap replications, percentile method.\")) +\n  theme_bw() + # nice clean theme\n  theme(plot.title = element_text(hjust = .5), # centre the title\n        plot.subtitle = element_text(hjust = .5)) # and subtitle\n\n\n\nWell, as you’d expect from the simulation method, no evidence of heterogeneity of Cronbach’s alpha across sessions!\nI hope this is a useful further introduction to piping, dplyr and some of the tidyverse approach. I guess it introduced the corrr package, cur_data() and rowwise() … and it finished with a, for me, typical use of ggplot() (from the ggplot2 package.)\nDo contact me if you have any comments, suggestions, corrections, improvements … anything!\n\n\n\n",
    "preview": "posts/2021-02-10-more-piping-introducing-rowwise/more-piping-introducing-rowwise_files/figure-html5/useDat14-1.png",
    "last_modified": "2023-08-25T13:38:29+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-07-why-pipe-why-the-tidyverse/",
    "title": "Why pipe?  Why the tidyverse?",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-07",
    "categories": [
      "R packages",
      "piping",
      "tidyverse"
    ],
    "contents": "\n\nContents\nSo what is piping?\nA worked example of R piping\nSummarising\n\n\nThis was a topic suggested by Emily who is nearing the end of her PhD on the YP-CORE as you can see from her/our RG pages about the PhD here . (More about the YP-CORE here and the CORE system here.) She and I have been on a learning curve moving from base R (https://www.r-project.org/) to increasing use of the tidyverse (https://www.tidyverse.org/) developments of R.\nSo what is piping?\nIt’s this sort of thing.\ndata %>%\n  group_by(gender) %>%\n  summarise(n = n(),\n            minAge = min(age),\n            meanAge = mean(age),\n            maxAge = max(age))\nTo me the idea of piping comes out unix/linux where you can pipe the output of one command into another, for example:\nfind . -name filename.here -printf \"%T@ %Tc %p\\n\" | sort -n\nThe pipe operator in linux is the vertical line “|” and what happens is the text output from the linux find command is fed straight into the sort command to give a sorted list of files matching “filename.here”.\nI think “piping” in R is a bit different (hence some in jokes: piping was introduced into R through a package [magittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html, and also. The “magittr” name was a lovely play on the famous print by Magritte which said firmly Ceci n’est pas un pipe (this is not a pipe) and depicted … a pipe.\nComing back to that R code I showed above …\ndata %>%\n  group_by(gender) %>%\n  summarise(n = n(),\n            minAge = min(age),\n            meanAge = mean(age),\n            maxAge = max(age))\nThat assumes you have a data frame or tibble named “data” and that it has variables gender and age within it. The “%>%” “pipes” those data to the next command, group_by() which in turn pipes the data, now grouped by gender into a four line command, summarise(). That creates a new tibble with four variables each with their one value, for each value of gender.\nThis is so well described by Shannon Pileggi in her page https://www.pipinghotdata.com/posts/2018-11-05-welcome-to-piping-hot-data/ that if you use R but so far haven’t used piping, go and look at her page and then come back here.\nAt first I wasn’t convinced by piping. That was partly because I thought the documentation I found wasn’t terribly clear (it has improved greatly) and didn’t convince me that the new syntax, new way of sequencing instructions, was worth the learning curve. It was also because it was, by the time I found it, very bound in with what has become the tidyverse in R: a lot of other, sometimes really quite radical, changes from base R. To me it felt like having to learn a new language and I’m neither a good linguist nor a good programmer.\nHowever, I have become a convert. I still sometimes go back to what I think of as “classical R”, I still sometimes find there are things I can do more easily with lists and lapply() and its relatives. That’s particularly true when my data isn’t rectangular or a relational data structure of linked rectangular sets of data. If I have a data on the members of families and the data may differ between the members and the families and the relational structures are messy, I will probably use lists and lapply(). However, the vast majority of the data I work with is, or can be, “tidy”. A very common example for me is to have an ID and some demographics per participant, and then answers from each participant on items of one or more questionnaires where every participant may have more than one set of responses. Here the data is a relational rectangular structure one rectangular structure with one row per participant then one or more rows in another rectangular structure for each time they completed the questionnaires with the ID codes enabling us to link the two.\nLong, long ago, when I used SPSS, I was very uncomfortable if I had to handle single rectangular data structures and I would have handled that by having one “wide” file with one row per participant and multiple sets of responses by questionnaire going off to the right and added to for each new completion. That’s doable when you might only have two completions per participant but when you have many per partipant, and numbers perhaps varying from one completion to hundreds, then that becomes a terrible structure.\nOf course, classical R handles structures like this fine. That was one thing that helped me move from SPSS to R (a minor contributor but certainly there in the long list). However, I didn’t shake of some uncertainty with my data handling as handled data linking across data frames.\nNow I have moved to piping and the tidyverse the effect for me has been liberating. I no longer think of data as having to be in a single rectangle, it’s become entirely natural to handle it in a series of linked data sets. I use the tidyverse tibbles: a fundamental building stone in the tidyverse and in many ways a relatively minor extension of the data frame in R. One difference from a data frame is that the default print function for a tibble only prints the first ten rows where the default print function for a data frame would try to print all of it, or until you run out of console lines. At first that seemed an annoyance until I started to use that printing to build what you want iteratively.\nA worked example of R piping\nIn what follows I haven’t folded the code away partly as I wanted to give a bit of the experience of writing code iterative, starting usually with one line and building on that.\nOK, let’s simulate some data\n\n\nlibrary(tidyverse) # get the main packages of the tidyverse\n### that gives us the pipe operator \"%>% and as_tibble and \n### a few other things I use below\n### the piping tools I'm using are in the dplyr package which\n### is within the set of packages called by the tidyverse \"superpackage\"\nset.seed(12345) # get replicable data\nn <- 500 # n(participants)\n\n### create some demographics\n### start by giving each participant a gender ...\nlist(gender = sample(c(\"M\", \"F\"), n, replace = TRUE),\n     age = sample(11:18, n, replace = TRUE), # and an age\n     ### and now a number of questionnaire completions\n     nCompletions = sample(1:30, n, replace = TRUE)\n     ) %>% # and pipe that list forward to convert it to a tibble\n  as_tibble() %>%\n  ### now use a piping trick, mutate() does something to each row\n  ### here a new variable, ID, is created and given the value of \n  ### the row number of each participant, i.e. 1:n\n  mutate(ID = row_number()) -> tibParticipants # give that tibble a name\n\n\nNow if I want to understand what I’ve created I just type its name:\n\n\ntibParticipants\n\n# A tibble: 500 × 4\n   gender   age nCompletions    ID\n   <chr>  <int>        <int> <int>\n 1 F         11            8     1\n 2 M         11           30     2\n 3 F         15            8     3\n 4 F         13            2     4\n 5 F         13           12     5\n 6 F         15           20     6\n 7 F         15            6     7\n 8 M         18            4     8\n 9 M         11           30     9\n10 F         14           15    10\n# ℹ 490 more rows\n\nInstead of showing me all 500 rows, I just get the top ten (like using head(dataFrame) in classical R) but I also get pretty much everything else I need to know about the data. Had there been too many variables to fit on the screen the listing would have ended with a line giving me the names of all the variables that wouldn’t fit on the screen.\n\n\ntibParticipants %>% \n  select(ID, nCompletions) %>% # get just the ID codes\n  ### I confess that I always have to look up uncount(): \n  ### I can never remember the name, let's just stop there\n  ### and see what it did ...\n  uncount(nCompletions) \n\n# A tibble: 7,679 × 1\n      ID\n   <int>\n 1     1\n 2     1\n 3     1\n 4     1\n 5     1\n 6     1\n 7     1\n 8     1\n 9     2\n10     2\n# ℹ 7,669 more rows\n\nOK, it’s replicated each ID value by the value in the variable nCompletions. Good, that’s what I wanted. Imagine I’m doing this interactively at the console …\n\n\ntibParticipants %>% \n  select(ID, nCompletions) %>% # get just the ID codes\n  uncount(nCompletions) %>%\n  group_by(ID) %>%\n  mutate(nCompletions2 = n(), \n         ### that gets me the number of completions per ID \n         ### (which is just getting back to nCompletions in\n         ### tibParticipants)\n         completionN = row_number()) %>%\n  ### that gets an index number for each completion \n  ### of the questionnaire ...\n  ### it's a very short questionnaire and so badly \n  ### designed the item are uncorrelated answers \n  ### between 0 and 5) ...\n  mutate(item1 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item2 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item3 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item4 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item5 = sample(0:5, nCompletions2[1], replace = TRUE))\n\n# A tibble: 7,679 × 8\n# Groups:   ID [500]\n      ID nCompletions2 completionN item1 item2 item3 item4 item5\n   <int>         <int>       <int> <int> <int> <int> <int> <int>\n 1     1             8           1     5     4     1     4     0\n 2     1             8           2     5     1     2     4     2\n 3     1             8           3     5     1     0     3     5\n 4     1             8           4     0     0     0     2     0\n 5     1             8           5     2     1     1     1     2\n 6     1             8           6     5     1     4     3     3\n 7     1             8           7     0     3     3     4     4\n 8     1             8           8     2     5     4     0     1\n 9     2            30           1     4     1     5     1     2\n10     2            30           2     4     0     0     0     3\n# ℹ 7,669 more rows\n\n### little note here, that used to work with just:\n###   item1 = sample(0:5, nCompletions2, replace = TRUE)\n### instead of what I have now:\n###   item1 = sample(0:5, nCompletions2[1], replace = TRUE)\n### i.e. I didn't have to instruct sample() to just use the first value for nCompletions2\n### for each group\n### now R throws an error \"! invalid 'size' argument\" \n### it took me a good few minutes to work out that the error messages was, perhaps very reasonably\n### complaining that it didn't know how to handle a vector for its size argument\n### I assume it used to default silently just to use the first element of the vector if given a\n### vector for size\n### This little historical diversion illustrates a number of things\n###   1. R does change and even very, very old and tested base functions like sample() can become\n###      more demanding about their arguments\n###   2. Sadly, I don't think the error messages R throws are always as helpful as they might be\n###      and this can bite you if you're not a programmer or very used to terse error messages\n###   3. More constructively, this is a reminder that the default behaviour of summarise() or of\n###      mutate() is using variables as vectors, not as single values, one per row\n###      If you want variables to be handled row by row, as single values, you need to use the \n###      particular version of group_by(): rowwise() which groups by row.  I'll come to that in \n###      another post here.\n###      But really it was sloppy code writing by me not to have trusted sample() to have just\n###      used the first value if given a vector (of length > 1) for an argument expecting a single\n###      value!\n\n\nOK. Looking like what I wanted so just put it into a tibble.\n\n\ntibParticipants %>% \n  select(ID, nCompletions) %>% # get just the ID codes\n  uncount(nCompletions) %>%\n  group_by(ID) %>%\n  mutate(nCompletions2 = n(), \n         ### that gets me the number of completions per ID \n         ### (which is just getting back to nCompletions in\n         ### tibParticipants)\n         completionN = row_number()) %>%\n  ### that gets an index number for each completion \n  ### of the questionnaire ...\n  ### it's a very short questionnaire and so badly \n  ### designed the item are uncorrelated answers \n  ### between 0 and 5) ...\n  mutate(item1 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item2 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item3 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item4 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item5 = sample(0:5, nCompletions2[1], replace = TRUE))%>%\n  ### this can catch you out, if you have used group_by() \n  ### before a mutate, the data stay grouped which is \n  ### probably not what you want so ungroup(), \n  ### rowwise() is just a particular group_by() but\n  ### grouping by row so that variables are treated \n  ### by each individual value, not as vectors\n  ungroup() -> tibQuaireData\n\n\nNow I want to join the demographics back into that so …\n\n\ntibQuaireData %>%\n  left_join(tibParticipants, by = \"ID\") \n\n# A tibble: 7,679 × 11\n      ID nCompletions2 completionN item1 item2 item3 item4 item5\n   <int>         <int>       <int> <int> <int> <int> <int> <int>\n 1     1             8           1     4     1     0     5     2\n 2     1             8           2     0     1     1     2     5\n 3     1             8           3     1     3     5     3     5\n 4     1             8           4     5     3     5     4     1\n 5     1             8           5     0     1     2     3     4\n 6     1             8           6     0     1     5     0     2\n 7     1             8           7     2     4     1     1     3\n 8     1             8           8     3     1     4     5     4\n 9     2            30           1     0     0     0     0     5\n10     2            30           2     2     5     1     3     0\n# ℹ 7,669 more rows\n# ℹ 3 more variables: gender <chr>, age <int>, nCompletions <int>\n\nI didn’t actually have to put the by = \"ID\" argument in there as left_join will join every row in tibQuaireData to any row with a matching value in any variable shared between tibQuaireData and tibParticipants and in my little example the only shared variable is ID. OK, that’s looking good.\n\nThere are a full set of join functions: inner_join(), left_join(), right_join() and full_join() that handle the full set of ways you might want to merge to data sets on index variables. They are making three bits of work I’m involved in, each of which involve relational database structures feel very easy.\n\n\ntibQuaireData %>%\n  left_join(tibParticipants, by = \"ID\") %>%\n  ### I will change the order of the variables, \n  ### this order seems better to me\n  ### everything() picks up any variables not already named as we see\n  select(ID, gender, age, nCompletions, nCompletions2, \n         everything()) \n\n# A tibble: 7,679 × 11\n      ID gender   age nCompletions nCompletions2 completionN item1\n   <int> <chr>  <int>        <int>         <int>       <int> <int>\n 1     1 F         11            8             8           1     4\n 2     1 F         11            8             8           2     0\n 3     1 F         11            8             8           3     1\n 4     1 F         11            8             8           4     5\n 5     1 F         11            8             8           5     0\n 6     1 F         11            8             8           6     0\n 7     1 F         11            8             8           7     2\n 8     1 F         11            8             8           8     3\n 9     2 M         11           30            30           1     0\n10     2 M         11           30            30           2     2\n# ℹ 7,669 more rows\n# ℹ 4 more variables: item2 <int>, item3 <int>, item4 <int>,\n#   item5 <int>\n\nOK, I’m working at the console (well actually, in a file and running the lines each time I finish tweaking them) so now assign that.\n\n\ntibQuaireData %>%\n  left_join(tibParticipants, by = \"ID\") %>%\n  ### I will change the order of the variables, \n  ### this order seems better to me\n  ### everything() picks up any variables not already named as we see\n  select(ID, gender, age, nCompletions, nCompletions2, \n         everything()) -> tibQuaireData\n\n\nNow I can do simple things with the data exploring it. I am going to stick to simple things that can be done just using pipes and dplyr.\n\n\n### gender breakdown of age\ntibQuaireData %>%\n  group_by(gender) %>%\n  summarise(n = n(), # gives the number of rows within the gender grouping\n            ### n_distinct() like length(unique()) in classical R, \n            ### gives number of distinct values of ID\n            nParticipants = n_distinct(ID), \n            minAge = min(age), # minimum age within the gender grouping\n            meanAge = mean(age), # ... similarly!\n            sdAge = sd(age),\n            maxAge = max(age))\n\n# A tibble: 2 × 7\n  gender     n nParticipants minAge meanAge sdAge maxAge\n  <chr>  <int>         <int>  <int>   <dbl> <dbl>  <int>\n1 F       3765           249     11    14.7  2.35     18\n2 M       3914           251     11    14.6  2.33     18\n\nNow I want to check the range of responses on the items. This introduces the across() selection and within it the starts_with(). They pretty much do what their names suggests. There is also an ends_with() selector. I could also have used item1:item5 as the colon gives all the variables from the left hand side to the right hand side, i.e. here from item1 to item5.\n\n\ntibQuaireData %>%\n  summarise(across(starts_with(\"item\"), # that was the selector, \n                                        # explained above, now we want done with those variables ...\n                   list(min = min, max = max)))\n\n# A tibble: 1 × 10\n  item1_min item1_max item2_min item2_max item3_min item3_max\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         0         5         0         5         0         5\n# ℹ 4 more variables: item4_min <int>, item4_max <int>,\n#   item5_min <int>, item5_max <int>\n\nOK, so the full range of scores was used for every item (doh!). Not the most obvious way to show that so that introduces pivoting. I think the name came from its use in spreadsheets but pivot_longer() and pivot_wider() give lovely control over converting data from wide to long (pivot_longer() … doh!) and pivot_wider() does the opposite.\n\n[This is called an aside in Rmarkdown]\npivot_longer() and pivot_wider() replaced two earlier components of the dplyr/tidyverse system: gather() and spread() respectively. I found gather() and spread() sometimes hard use and have found pivot_longer() and pivot_wider() much better. If you see examples using gather() and spread() in the web, I would strongly recommend that you ignore them and find more recent work.\n\n\ntibQuaireData %>%\n  pivot_longer(cols = starts_with(\"item\"))\n\n# A tibble: 38,395 × 8\n      ID gender   age nCompletions nCompletions2 completionN name \n   <int> <chr>  <int>        <int>         <int>       <int> <chr>\n 1     1 F         11            8             8           1 item1\n 2     1 F         11            8             8           1 item2\n 3     1 F         11            8             8           1 item3\n 4     1 F         11            8             8           1 item4\n 5     1 F         11            8             8           1 item5\n 6     1 F         11            8             8           2 item1\n 7     1 F         11            8             8           2 item2\n 8     1 F         11            8             8           2 item3\n 9     1 F         11            8             8           2 item4\n10     1 F         11            8             8           2 item5\n# ℹ 38,385 more rows\n# ℹ 1 more variable: value <int>\n\n### \"name\" and \"value\" are the default names for the variables \n### created by pivoting but you can override them ...\ntibQuaireData %>%\n  pivot_longer(cols = starts_with(\"item\"), \n               names_to = \"item\", \n               values_to = \"score\")\n\n# A tibble: 38,395 × 8\n      ID gender   age nCompletions nCompletions2 completionN item \n   <int> <chr>  <int>        <int>         <int>       <int> <chr>\n 1     1 F         11            8             8           1 item1\n 2     1 F         11            8             8           1 item2\n 3     1 F         11            8             8           1 item3\n 4     1 F         11            8             8           1 item4\n 5     1 F         11            8             8           1 item5\n 6     1 F         11            8             8           2 item1\n 7     1 F         11            8             8           2 item2\n 8     1 F         11            8             8           2 item3\n 9     1 F         11            8             8           2 item4\n10     1 F         11            8             8           2 item5\n# ℹ 38,385 more rows\n# ℹ 1 more variable: score <int>\n\n### now just group and get min and max\ntibQuaireData %>%\n  pivot_longer(cols = starts_with(\"item\"), \n               names_to = \"item\", \n               values_to = \"score\") %>%\n  group_by(item) %>%\n  summarise(minScore = min(score),\n            maxScore = max(score))\n\n# A tibble: 5 × 3\n  item  minScore maxScore\n  <chr>    <int>    <int>\n1 item1        0        5\n2 item2        0        5\n3 item3        0        5\n4 item4        0        5\n5 item5        0        5\n\nMuch easier to read that way around.\nSummarising\nThese have been trivial examples but they’ve introduced some of the fundamental powers of piping using the dplyr package in the R tidyverse. They grew on me and now, as I said at the beginning, they are how I do pretty much all my data manipulation and analyses. There was certainly a learning curve for me and I guess my conversion to piping really happened through 2020. The advantages I have found are:\nthe iterative building of code to do what I want, that I’ve tried to illustrate above, feels a very easy way to write code: you see the output of each step and build things step by step\nI am sure this has meant that I am writing better code, faster, with fewer mistakes\nthe method is very similar to creating graphics with the ggplot2 package and ggplot() so my conversion to working with pipes was perhaps helped by a slightly earlier decision to move from classical R graphics to ggplot() and I find the two complement each other … but I do occasionally forget that the piping operator in ggplot() is “+”, not “%>%”!\nI find code I wrote this way far, far easier to read when I come back to it after time has passed\nI think piping, and particularly pivoting, have really helped me break old “SPSS thinking” and made me comfortable with relational data structures\nThese really have been trivial examples, I’ll be making up pages here illustrating more complicated and much more powerful aspects of piping and the tidyverse over the months ahead.\n\n\n\n",
    "preview": "posts/2021-02-07-why-pipe-why-the-tidyverse/redpipe.png",
    "last_modified": "2023-08-25T13:37:52+02:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4800
  },
  {
    "path": "posts/2021-02-06-how-i-have-done-this/",
    "title": "How I have done this",
    "description": "Just documenting how I have created these pages with the Distill package.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-06",
    "categories": [
      "Distill package",
      "R graphics",
      "R tricks"
    ],
    "contents": "\n\nContents\nDistill\nAutomate transfer to my web server\nHow to get images into the preview\nHow to get wider images\nDefault plot width: l-body\nWider plot width: l-body-outset\nWider plot width still: l-page\nFull screen: l-screen\n\n\n\n\n\n\n\n\nDistill\nThese pages have been created using the distill package in R. To quote its authors: “Distill is a publication format for scientific and technical writing, native to the web. Learn more about using Distill at https://rstudio.github.io/distill.”\nThat’s a pretty good summary and the documentation there covers most of the powers of Distill. However, as with much software documentation, I also felt there were things missing that I needed or that would have speeded things up for me. It’s the usual problem that the people who write the code, and many of the people who use it, are very clever and know what they are doing but don’t always remember that we’re not all that clever or that some things had become so familiar to them that they don’t notice they haven’t put those things in the documentation.\nSo Distill is an R package and I suppose it could be run without Rstudio but it’s clearly designed to dovetail with Rstudio. So I installed the package and followed the instructions to create a site at https://rstudio.github.io/distill/#creating-an-article. The system is as they say “a publication format” and they frame it as a tool with which to make a blog. It actually has what I would call “base pages” as well as pages that it creates as “blog posts”. It took me a while to realise that I had to create pages and posts at the command line withdistill::create_article()\nanddistill::create_post() (with some arguments, pretty much all you need to do withdistill::create_post() is to give the post a title: distill::create_post(\"My latest post\")).\nThe package code creates a template page which is basically Rmarkdown, just as happens when you create a new Rmarkdown page in Rstudio. You have all the usual Rmarkdown capacities: “knitting” together blocks of code and blocks of text, embedded figures, inline code in text blocks, TeX/LaTeX equation formatting inline and in equation blocks, tables, citations and reference insertion, tables of contents etc. The help at https://rstudio.github.io/distill goes through the various very nice things the templates can do for you that go a bit beyond what Rmarkdown does:\ncode folding (which I have used throughout) which “folds” code away but allows the reader of the page to open it by just clicking\nnice syntax highlighting in the code blocks pretty much mimicking the syntax highlighting in Rstudio\nyou can change theme with css (so I have a Rblog.css file where I’ve reset the background colour)\nfootnotes\nThere’s a lot that has been done to make some of the things you want for open scientific/academic/research publishing easy that is set in “yaml” (a recursive acronym for “YAML Ain’t Markup Language”) … it’s a header block above the markdown/markup in many markdown/up files. My _site.yml file (as of 6.ii.21) is this:\n---\nname: \"test2\"\ntitle: \"Chris (Evans) R SAFAQ\"\nbase_url: https://www.psyctc.org/R_blog\ndescription: |\n  CE's pages \"blog posts\" about using R\noutput_dir: \"_site\"\nnavbar:\n  logo:\n    image: images/g2_128.gif\n    href: https://www.psyctc.org/Rblog/\n    icon: images/g2_128.gif\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Welcome\"\n      href: \"Welcome.html\"\n    - text: \"About\"\n      href: about.html\n    - text: \"Copyright/permissions\"\n      href: Copyright_and_permissions.html\noutput: \n  distill::distill_article:\n    theme: Rblog.css\ncitations: true\ncookie_consent:\n  style: simple\n  type: express\n  palette: light\n  lang: en\n  cookies_policy: url\n---\nLet’s break that up and comment it some things that are perhaps not totally obvious. (Hm, not sure if you can comment yaml, hm, yes I think you can.)\nThis first block is defining the whole site.\nname: \"test2\" # this is the directory\ntitle: \"Chris (Evans) R SAFAQ\"\nbase_url: https://www.psyctc.org/R_blog # this makes sure the pages index to that URL\ndescription: |\n  CE's pages \"blog posts\" about using R\noutput_dir: \"_site\" # and this is the directory in which the site is compiled by Distill\nDistill automatically creates a simple site structure with a navigation bar at the top. The next bits define that. This first bit just allows you to put an image and icon in. (I could do with a bigger one!)\nnavbar:\n  logo:\n    image: images/g2_128.gif\n    href: https://www.psyctc.org/Rblog/\n    icon: images/g2_128.gif\nAnd this bit puts in links to pages you may have created with distill::create_article() … you have to put these into the navigation bar manually by putting lines like these next ones.\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Welcome\"\n      href: \"Welcome.html\"\n    - text: \"About\"\n      href: about.html\n    - text: \"Copyright/permissions\"\n      href: Copyright_and_permissions.html\noutput: \n  distill::distill_article: # not sure what this does!\n    theme: Rblog.css # this is where I invoke my theme/style css\nThen some very nice convenience powers of the package.\ncitations: true # automatically inserts a nice footnote about citing things on the site\ncookie_consent: # and a nice cookie consent for you\n  style: simple\n  type: express\n  palette: light\n  lang: en\n  cookies_policy: url\nPages created with distill::create_article(), like all Rmarkdown, start with their own yaml blocks and again these allow some nice things.\ntitle: \"Welcome to CE blog test\"\ndescription: |\n  Welcome to my blog which I hope will be useful to people using R to analyse data.\nauthor:\n  - name: Chris Evans\n    url: https://www.psyctc.org/R_blog/\n    affiliation: PSYCTC.org                              # put your affiliation/organisation in\n    affiliation_url: https://www.psyctc.org/psyctc/      # URL for that\n    orcid_id: 0000-0002-4197-4202                        # put your ORCID ID in\ndate: \"2023-11-27\"\noutput: \n  distill::distill_article:\n    self_contained: false\nAutomate transfer to my web server\nThis took me some hours today to sort out but will save me many hours over the years ahead. I suspect that anyone who is more familiar with git than I was will manage to do this much more quickly than I did. What I’ve done is:\ninstall git on the machine on which I’m running Rstudio and storing the site pages\ntell Rstudio that git is there and is to be used for “version control”, i.e. automatic backing up of all changes that you “commit” keeping a full historical archive of the changes\ncreated a free personal account on gitHub (https://github.com/cpsyctc/) and create a respository in it (https://github.com/cpsyctc/Rblog)\ncreated a personal token which works instead of a password to log into my repository there and makes sure that I’m the only one who can write things to that repository (but anyone can download, “pull” in git terminology, from it) (I have now discovered from https://usethis.r-lib.org/reference/use_github.html that these bits might have bee expedited with a )\nuse that to “push” each new committed update to the pages to that github repository\ninstall git on my web server (pretty sure my ISP had already done this actually)\n[this bit, and the next, are linux specific but could be done, though the terminology is different, in Windoze] create a little shell script on the server that “pulls” a copy of the repository content down to the server from github (git handles the tracking of changes and makes sure that only the minimum necessary material is stored and transferred) and uses rsync to copy things to the web pages (rsync, a bit like git, will only copy changed files)\nput a call into crontab to run that little script every ten minutes\nSo I’ve now got a site/blog developing here as an Rstudio project that I can commit and push to github (where anyone can pull it if they want it) and which then automatically updates my server, at slowest, ten minutes later.\nNow I need to spend a bit more time creating more content but perhaps I’ll browse some other people’s examples first: see https://pkgs.rstudio.com/distill/articles/examples.html.\nHow to get images into the preview\n[Added 7.ii.21] I couldn’t work out how to get an ordinary image into listing of “posts” in the base of the “blog” but, courtesy of Shannon Pileggi of the excellent https://www.pipinghotdata.com/ site she created with Distill, I now have the trick: put the graphic in the directory holding the post and put a line in the yaml pointing to it. So here’s the YAML header of the Rmarkdown file that creates this page:\n---\ntitle: \"How I've done this\"\ndescription: |\n  Just documenting how I have created these pages\nbase_url: https://www.psyctc.org/psyctc/Rblog/\npreview: distill.png\nauthor:\n  - name: Chris Evans\n    url: https://www.psyctc.org/R_blog/\n    affiliation: PSYCTC.org\n    affiliation_url: https://www.psyctc.org/psyctc/\n    orcid_id: 0000-0002-4197-4202\n\ndate: 2021-02-06\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    self_contained: false\n---\nYou see the crucial preview: distill.png (I downloaded the graphic from https://blog.rstudio.com/2020/12/07/distill/distill.png). That’s it: thanks Shannon! Shannon also pointed me to her public github repository at https://github.com/shannonpileggi which has all the code for her blog at https://github.com/shannonpileggi/pipinghotdata_distill … I should have been able to find that without Emailing her.\nHow to get wider images\nThis is from https://rstudio.github.io/distill/figures.html. You use a “layout=” argument in the code block header.\nThe default format is l-body so the code block header:\n```{r codeBlock}\nis the same as:\n```{r codeBlock, layout=\"l-body\"}\nWider layouts are l-body-outset, l-page and l-screen. Let’s see!\nDefault plot width: l-body\n```{r plotECDFwithCIquantiles1, layout=\"l-body\"}\n\n\n\nWider plot width: l-body-outset\n```{r plotECDFwithCIquantiles2, layout=\"l-body-outset\"}\n\n\n\nWider plot width still: l-page\n```{r plotECDFwithCIquantiles3, layout=\"l-page\"}\n\n\n\nFull screen: l-screen\n```{r plotECDFwithCIquantiles4, layout=\"l-screen\"}\n\n\n\nThose were ECDF plots showing confidence intervals around the observed quartiles (and median) see Rblog post: Confidence intervals for quantiles. Plots created using plotQuantileCIsfromDat() from the CECPfuns package.\n\n\n\n",
    "preview": "posts/2021-02-06-how-i-have-done-this/distill.png",
    "last_modified": "2023-11-27T12:14:09+01:00",
    "input_file": {},
    "preview_width": 2521,
    "preview_height": 2911
  },
  {
    "path": "posts/2021-01-27-bootstrapspearman/",
    "title": "Bootstrap_Spearman",
    "description": "A quick exploration of bootstrapping a Spearman and why you might, or might not, want it.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [
      "confidence intervals",
      "bootstrapping"
    ],
    "contents": "\n\nContents\nBootstrapping Spearman correlation coefficient\n\nBootstrapping Spearman correlation coefficient\nTraditionally people used the Spearman correlation coefficient where the sample observed distributions of the variables being correlated were clearly not Gaussian. The logic is that as the Spearman correlation is a measure of correlation between the ranks of the values, the distribution of the scores, population or sample, was irrelevant to any inferential interpretation of the Spearman correlation. By contrast, inference about the statistical (im)probability of a Pearson correlation, or a confidence interval (CI) around an observed correlation, was based on maths which assumed that population values were Gaussian. This is simply and irrevocably true: so if the distributions of your sample scores are way off Gaussian then the p values and CIs for a Pearson can be very misleading.\nThe logic of doing a test of fit to Gaussian on your sample data (univariate and/or bivariate test of fit) is dodgy as if your sample is small then even a large deviation from Gaussian that may give very poor p values and CIs has a fair risk of not being flagged as statistically significantly different from Gaussian and with a large sample, even trivial deviations from Gaussian that would have no effect on the p values and CIs will show as statistically significant. How severe that problem is should really have simulation exploration and I haven’t searched for that but the theoretical/logical problem is crystal clear.\nKeen supporters of non-parametrical statistical methods sometimes argued, reasonably to my mind, that the simple answer was to use non-parametric tests regardless of sample distributions, their opponents argued that this threw away some statistical power: true but the loss wasn’t huge.\nAll this has been fairly much swept away, again, to my mind, by the arrival of bootstrapping which allows you, for anything above a very small sample and for pretty much all but very, very weird distributions, to get pretty robust CIs around observed Pearson correlations regardless of the distributions, population or sample distributions, of the variables.\nBecause of this I now report Pearson correlations with bootstrap CIs around them for any correlations unless there is something very weird about the data. This has all the advantages of CIs over p values and is robust to most distributions.\nHowever, I often want to compare with historical findings (including my own!) that were reported using Spearman’s correlation so I still often report Spearman correlations. However, there is no simple parametric CI for the Spearman correlation and I’m not sure there should be outside the edge case where you have perfect ranking (i.e. no ties on either variable). Then the Spearman correlation is the Pearson correlation of the ranks and I think the approximation of using the parametric Pearson CI computation for the Spearman is probably sensible. I am not at all sure that once you have ties that you can apply the same logic though probably putting in n as the lower of the number of distinct values of the two variables probably gives a safe but often madly wide CI. (“Safe” in the sense that it will include the true population correlation 95% of the time (assuming that you are computing the usual 95% CI)).\nFortunately, I can see reason why the bootstrap cannot be used to find a CI around an observed Spearman correlation and this is what I do now when I am reporting a Spearman correlation.\n\n\nShow code\n\ngetCISpearmanTxt <- function(x, bootReps = 999, conf = .95, digits = 2) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  in format \"rho (LCL to UCL)\"\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  ### digits sets the rounding\n  require(boot) # need boot package!\n  ### now we need a function that \n  spearmanForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i, 1], \n        x[i, 2],\n        method = \"spearman\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = spearmanForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### get observed Spearman correlation and confidence limits as vector\n  retVal <- (c(tmpBootRes$t0,\n           tmpCI$percent[4],\n           tmpCI$percent[5]))\n  ### round that\n  retVal <- round(retVal, digits)\n  ### return it as a single character variable\n  retVal <- paste0(retVal[1],\n                   \" (\",\n                   retVal[2],\n                   \" to \",\n                   retVal[3],\n                   \")\")\n  retVal\n}\n\ngetCISpearmanList <- function(x, bootReps = 999, conf = .95) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  returns a list with items obsCorr, LCL and UCL\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  require(boot) # need boot package!\n  ### now we need a function that \n  spearmanForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"spearman\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = spearmanForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### return observed Spearman correlation and confidence limits as a list\n  retVal <- list(obsCorrSpear = as.numeric(tmpBootRes$t0),\n                 LCLSpear = tmpCI$percent[4],\n                 UCLSpear = tmpCI$percent[5])\n  retVal\n}\n\ngetCIPearsonTxt <- function(x, bootReps = 999, conf = .95, digits = 2) {\n  ### function to give bootstrap CI around bivariate PearsonR\n  ###  in format \"R (LCL to UCL)\"\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  ### digits sets the rounding\n  require(boot) # need boot package!\n  ### now we need a function that \n  pearsonForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"pearson\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = pearsonForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### get observed Spearman correlation and confidence limits as vector\n  retVal <- (c(tmpBootRes$t0,\n           tmpCI$percent[4],\n           tmpCI$percent[5]))\n  ### round that\n  retVal <- round(retVal, digits)\n  ### return it as a single character variable\n  retVal <- paste0(retVal[1],\n                   \" (\",\n                   retVal[2],\n                   \" to \",\n                   retVal[3],\n                   \")\")\n  retVal\n}\n\ngetCIPearsonList <- function(x, bootReps = 999, conf = .95) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  returns a list with items obsCorr, LCL and UCL\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  require(boot) # need boot package!\n  ### now we need a function that \n  pearsonForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"pearson\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = pearsonForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### return observed Spearman correlation and confidence limits as a list\n  retVal <- list(obsCorrPears = as.numeric(tmpBootRes$t0),\n                 LCLPears = tmpCI$percent[4],\n                 UCLPears = tmpCI$percent[5])\n  retVal\n}\n\n\n\n\nShow code\n\n### generate some Gaussian and some non-Gaussian data\nn <- 5000 # sample size\nset.seed(12345) # get replicable results\n\nas_tibble(list(x = rnorm(n),\n               y = rnorm(n))) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) %>%\n  summarise(absMinVal = abs(min(value))) %>%\n  pull() -> varMinVal\n\ntibDat %>%\n  mutate(xSquared = x^2,\n         ySquared = y^2,\n         xLn = log(x + varMinVal + 0.2),\n         yLn = log(y + varMinVal + 0.2),\n         xInv = 1/(x + varMinVal + 0.1),\n         yInv = 1/(y + varMinVal + 0.1)) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) -> tibDatLong\n\nggplot(data = tibDatLong,\n       aes(x = value)) +\n  facet_wrap(facets = vars(name),\n             ncol = 2,\n             scales = \"free\",\n             dir = \"v\") +\n  geom_histogram(bins = 100) +\n  theme_bw()\n\n\nShow code\n\nggpairs(tibDat)\n\n\n\nGood! Got some weird variables there: x and y are Gaussian random variables and uncorrelated then we have their squares, a natural log (after adding enough to the values to avoid trying to get ln(0)) and their inverses (with the same tweak to avoid getting 1/0).\n\n\nShow code\n\noptions(dplyr.summarise.inform = FALSE)\ntibDatLong %>%\n  mutate(id = (1 + row_number()) %/% 2 ,\n         var = str_sub(name, 1, 1),\n         transform = str_sub(name, 2, 20),\n         transform = if_else(transform == \"\", \"none\", transform),\n         transform = ordered(transform,\n                             levels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"),\n                             labels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"))) %>%\n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = var) -> tibDatLong2\n\ntibDatLong2 %>%\n  group_by(transform) %>%\n  select(x, y) %>%\n  summarise(corrS = list(getCISpearmanList(cur_data())),\n            corrP = list(getCIPearsonList(cur_data()))) %>%\n  unnest_wider(corrS) %>%\n  unnest_wider(corrP) %>% \n  pander(justify = \"lrrrrrr\", split.tables = Inf)\n\ntransform\nobsCorrSpear\nLCLSpear\nUCLSpear\nobsCorrPears\nLCLPears\nUCLPears\nnone\n-0.03547\n-0.06164\n-0.005442\n-0.02368\n-0.0495\n0.003609\nLn\n-0.03547\n-0.06259\n-0.008897\n-0.01938\n-0.04774\n0.01032\nInv\n-0.03547\n-0.06106\n-0.006073\n0.0001815\n-0.0324\n0.0382\nSquared\n-0.00467\n-0.03253\n0.02274\n-0.009833\n-0.03985\n0.02017\n\nThat’s what we would expect to see: the observed Spearman correlations are the same for the raw data, the ln and inv transformed values (as these are transforms that preserve monotonic, i.e. ranked, ordered, relationships between values while changing the values a lot) but the value is different for the squared transform as that’s not monotonic. The values for the Pearson correlations change with ln and inv transforming as they should as the correlations between the transformed values are not the same as between the raw values. The CIs for the Spearman raw and ln and inv transformed values are not quite identical because the bootstrapping will have produced different bootstrapped samples for each. (I think there’s a way I could have got all three in the same call to boot() but that would have needed a different function to bootstrap.)\nReassuring that all the CIs include zero: you’d hope so really with n = 5000 and uncorrelated raw values.\nNow let’s get a moderately correlated pair of variables.\n\n\nShow code\n\n### generate some Gaussian and some non-Gaussian data\nn <- 5000 # sample size\nset.seed(12345) # get replicable results\n\ntibDat %>% \n  mutate(y = x + y) %>%\n  select(x, y) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) %>%\n  summarise(absMinVal = abs(min(value))) %>%\n  pull() -> varMinVal\n\ntibDat %>%\n  mutate(xSquared = x^2,\n         ySquared = y^2,\n         xLn = log(x + varMinVal + 0.2),\n         yLn = log(y + varMinVal + 0.2),\n         xInv = 1/(x + varMinVal + 0.1),\n         yInv = 1/(y + varMinVal + 0.1)) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) -> tibDatLong\n\nggplot(data = tibDatLong,\n       aes(x = value)) +\n  facet_wrap(facets = vars(name),\n             ncol = 2,\n             scales = \"free\",\n             dir = \"v\") +\n  geom_histogram(bins = 100) +\n  theme_bw()\n\n\nShow code\n\nggpairs(tibDat)\n\n\n\n\n\nShow code\n\noptions(dplyr.summarise.inform = FALSE)\ntibDatLong %>%\n  mutate(id = (1 + row_number()) %/% 2 ,\n         var = str_sub(name, 1, 1),\n         transform = str_sub(name, 2, 20),\n         transform = if_else(transform == \"\", \"none\", transform),\n         transform = ordered(transform,\n                             levels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"),\n                             labels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"))) %>%\n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = var) -> tibDatLong2\n\ntibDatLong2 %>%\n  group_by(transform) %>%\n  select(x, y) %>%\n  summarise(corrS = list(getCISpearmanList(cur_data())),\n            corrP = list(getCIPearsonList(cur_data()))) %>%\n  unnest_wider(corrS) %>%\n  unnest_wider(corrP) %>% \n  pander(justify = \"lrrrrrr\", split.tables = Inf)\n\ntransform\nobsCorrSpear\nLCLSpear\nUCLSpear\nobsCorrPears\nLCLPears\nUCLPears\nnone\n0.666\n0.6485\n0.6833\n0.6906\n0.6755\n0.7049\nLn\n0.666\n0.6496\n0.683\n0.679\n0.6625\n0.6956\nInv\n0.666\n0.6488\n0.6823\n0.279\n0.2516\n0.6728\nSquared\n0.3559\n0.3311\n0.3815\n0.4992\n0.4617\n0.5347\n\nGreat: exactly what we’d expect again.\n\n\n\n",
    "preview": "posts/2021-01-27-bootstrapspearman/bootstrapspearman_files/figure-html5/simulate1-1.png",
    "last_modified": "2023-08-25T13:36:50+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-27-handling-overprinting/",
    "title": "Handling overprinting",
    "description": "This is the first of my blog posts here, about the issue of overprinting and some ways to handle it \nusing R and ggplot().  There's a small spin off topic on the impact on bivariate correlations and on\nlinear regression of discretising continuous variables.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [
      "Graphics",
      "Overprinting",
      "Jittering",
      "R tricks"
    ],
    "contents": "\n\nContents\nOverprinting\nJittering\nUsing transparency\nUsing area to show counts: geom_count()\nHow do those methods work with the original data?\nJittering\nTransparency\nUsing area: geom_count()\n\n\nTangential issue: impact of discretising on relationship between variables\nQuestions and feedback\nTechnical footnote and thanks\nLicensing\n\nOverprinting\nOverprinting is where one point on a graph overlies another. It’s mainly a problem with scattergrams and if you have large numbers of points and few discrete values it can make a plot completely misleading. OK, let’s make up some data.\n\nI am showing the raw R within the Rmarkdown code blocks. I have tried to comment things liberally. Click on “Show code” to see the code.\n\n\nShow code\n\nn <- 5000 # a lot of points means that overprinting is inevitable \nnVals <- 5 # discretising continuous variables to this number of values (below) makes it even more certain\nset.seed(12345) # ensures we get the same result every time \n\n### now generate x and y variables as a tibble\nas_tibble(list(x = rnorm(n),\n               y = rnorm(n))) -> tibDat\n\n### create strong correlation between them by adding x to y (!)\ntibDat %>%\n  mutate(y = x + y) -> tibDat\n\n### now we want to discretise into equiprobable scores so find the empirical quantiles\nvecXcuts <- quantile(tibDat$x, probs = seq(0, 1, 1/nVals))\nvecYcuts <- quantile(tibDat$y, probs = seq(0, 1, 1/nVals))\n\n### now use those to transform the raw variables to equiprobable scores in range 1:5\ntibDat %>%\n  mutate(x5 = cut(x, breaks = vecXcuts, include.lowest = TRUE, labels = FALSE, right = TRUE),\n         y5 = cut(y, breaks = vecYcuts, include.lowest = TRUE, labels = FALSE, right = TRUE)) -> tibDat\n\n\nNow let’s have a simple scatterplot.\n\n\nShow code\n\n### use ggplot to generate the simple scattergram for the raw variables\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  theme_bw()\n\n\n\nThe relationship between the two variables is clear but we don’t know about any overprinting. We can add a loess smoothed regression which clarifies the relationship between the scores but doesn’t resolve the overprinting issue.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth() + # adding the loess smoother\n  theme_bw()\n\n\n\nHowever, to really drive home the point about overprinting, if those points are transformed and discretised to five equiprobable scores then things look like this.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) + # use discretised variables instead of raw variables\n  geom_point() +\n  theme_bw()\n\n\n\nWhoops: much overprinting as 5000 points have collapsed to 25 visible points on the scattergram but we can’t see how much and no apparent relationship between the variables at all.\nAgain we can add a regression to that plot for amusement and to show that the transform hasn’t removed the relationship. (Has to be a linear regression as the number of distinct points doesn’t allow for loess smoothing.)\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") + # linear regression fit\n  theme_bw()\n\n\n\nJittering\nOne way around overprinting it is to jitter the points. Here I have used geom_jitter(width = .2, height = .2) which adds random “jittering” to both x and y values spread across .2 of the “implied bins”. I’ve left the raw data in in blue.\nThere are situations in which you just want jittering on one axis and not the other so you can use geom_jitter(width = .2). Sometimes playing around with width helps get the what seems the best visual fit to the counts.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_jitter(width = .2, height = .2) + # jittered data\n  geom_point(data = tibDat,\n             aes(x = x5, y = y5),\n             colour = \"blue\") +\n  theme_bw()\n\n\n\nUsing transparency\nAnother approach is to use transparency. Here you just have the one parameter, alpha and again, sometimes you need to play with different values.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_point(alpha = .01) +\n  theme_bw()\n\n\n\nThat’s not working terribly well as we have so many points (n = 5000).\nUsing area to show counts: geom_count()\nAnd another approach, good when values are widely spaced as here, is geom_count().\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_count() +\n  scale_size_area(n.breaks = 10) +\n  theme_bw()\n\n\n\nI used a rather excessive number of breaks there but it makes the point.\nHow do those methods work with the original data?\nIn this next set of blocks I’ve applied the same three tricks but to both the raw data and the discretised data.\nJittering\n\n\nShow code\n\n### reshape data to make it easy to get plots side by side using facetting\ntibDat %>%\n  ### first pivot longer \n  pivot_longer(cols = everything()) %>%\n  ### which gets something like this\n  #   # A tibble: 20,000 x 2\n  #    name    value\n  #    <chr>   <dbl>\n  #  1 x      0.586 \n  #  2 y     -0.107 \n  #  3 x5     4     \n  #  4 y5     3     \n  #  5 x      0.709 \n  #  6 y      1.83  \n  #  7 x5     4     \n  #  8 y5     5     \n  #  9 x     -0.109 \n  # 10 y      0.0652\n  # # … with 19,990 more rows\n  ### now get new variables one for x and y\n  mutate(variable = str_sub(name, 1, 1),\n       ### and one for the transform\n       transform = str_sub(name, 2, 2),\n       transform = if_else(transform == \"5\", \"discretised\", \"raw\"),\n       transform = factor(transform,\n                          levels = c(\"raw\", \"discretised\")),\n       ### create an id variable clumping each set of four scores together\n       id = (3 + row_number()) %/% 4) %>%\n  ### so now we can pivot back \n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = variable) -> tibDat2\n### to get this\n# A tibble: 10,000 x 4\n#       id transform        x       y\n#    <dbl> <chr>        <dbl>   <dbl>\n#  1     1 raw          0.586 -0.107 \n#  2     1 discretised  4      3     \n#  3     2 raw          0.709  1.83  \n#  4     2 discretised  4      5     \n#  5     3 raw         -0.109  0.0652\n#  6     3 discretised  3      3     \n#  7     4 raw         -0.453 -2.42  \n#  8     4 discretised  2      1     \n#  9     5 raw          0.606 -1.04  \n# 10     5 discretised  4      2     \n# # … with 9,990 more rows\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = .2)  +\n  facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\nAmusing! I’ve put the linear regression fit line on both.\nTransparency\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n  facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_point(alpha = .01) +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\nTransparency that works on the right for the discretised data, well works up to a point, is a bit too thin for the raw data. I can’t see that as of now (26.i.21 and ggplot version 3.3.3) that you can map transparency, i.e. alpha to a variable as you can, say, for colour. So to get a side-by-side plot I’m using a different approach. There are various ways of doing this, a useful page seems to be: http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/81-ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page/\n\n\nShow code\n\nggplot(data = filter(tibDat2, transform == \"raw\"), # select just the raw data\n       aes(x = x, y = y)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() -> tmpPlot1\n\n\nggplot(data = filter(tibDat2, transform == \"discretised\"), # select just the raw data\n       aes(x = x, y = y)) +\n  geom_point(alpha = .01) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() -> tmpPlot2\n\n### use ggarrange from the ggpubr package, see the URL for other options\nggpubr::ggarrange(tmpPlot1, tmpPlot2)\n\n\n\nUsing area: geom_count()\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n    facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_count() +\n  geom_smooth(method = \"lm\") +\n  scale_size_area(n.breaks = 10) +\n  theme_bw()\n\n\n\nI used a rather excessive number of breaks there but it makes the point.\nTangential issue: impact of discretising on relationship between variables\n\n\nShow code\n\nvalRawCorr <- cor(tibDat$x, tibDat$y)\nvalDisc5Corr <- cor(tibDat$x5, tibDat$y5)\n\nvecRawCorrCI <- cor.test(tibDat$x, tibDat$y)$conf.int\nvecDisc5CorrCI <- cor.test(tibDat$x5, tibDat$y5)$conf.int\n\n### or here's another, tidyverse way to do this\n### seems like unnecessary faff except that it makes \n### it so easy to do a micro forest plot (see below)\ngetParmPearsonCI <- function(x, y){\n  ### little function to get parametric 95% CI from two vectors\n  obsCorr <- cor(x, y)\n  tmpCI <- cor.test(x, y)$conf.int\n  return(list(LCL = tmpCI[1],\n              obsCorr = obsCorr,\n              UCL = tmpCI[2]))\n}\ntibDat2 %>%\n  group_by(transform) %>%\n  summarise(pearson = list(getParmPearsonCI(x, y))) %>%\n  unnest_wider(pearson) -> tibCorrs\n### which gives us this\n# tibCorrs\n# # A tibble: 2 x 4\n#   transform     LCL obsCorr   UCL\n#   <fct>       <dbl>   <dbl> <dbl>\n# 1 raw         0.676   0.691 0.705\n# 2 discretised 0.609   0.626 0.643\n\n\nThe correlation between the original variables is\n0.691 with parametric 95% confidence interval (CI) from\n0.676 to\n0.705 whereas that between the discretised variables is\n0.626 with 95% CI from\n0.609 to\n0.643 so some clear attenuation there. Micro forest plot of that:\n\n\nShow code\n\nggplot(data = tibCorrs,\n       aes(x = transform, y = obsCorr)) +\n  geom_point() +\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) +\n  ylim(.5, 1) +\n  theme_bw()\n\n\n\nYup, that’s a fairly large and clear difference on a y scale from .5 to 1.0. What about the linear regression?\n\n\nShow code\n\n### raw variables\nlm(scale(y) ~ scale(x), data = tibDat)\n\n\nCall:\nlm(formula = scale(y) ~ scale(x), data = tibDat)\n\nCoefficients:\n(Intercept)     scale(x)  \n  1.066e-17    6.906e-01  \n\nShow code\n\n### discretised variables\nlm(scale(y5) ~ scale(x5), data = tibDat)\n\n\nCall:\nlm(formula = scale(y5) ~ scale(x5), data = tibDat)\n\nCoefficients:\n(Intercept)    scale(x5)  \n -1.726e-17    6.265e-01  \n\nShow code\n\n### or tidyverse way\n### I confess I haven't really got my head aound the broomverse but this is powerful\ntibDat2 %>% \n  group_by(transform) %>%\n  do(broom::tidy(lm(scale(y) ~ scale(x), data = .))) %>%\n  pander(justify = \"llrrrr\", split.tables = Inf)\n\ntransform\nterm\nestimate\nstd.error\nstatistic\np.value\nraw\n(Intercept)\n1.066e-17\n0.01023\n1.042e-15\n1\nraw\nscale(x)\n0.6906\n0.01023\n67.51\n0\ndiscretised\n(Intercept)\n-1.726e-17\n0.01102\n-1.566e-15\n1\ndiscretised\nscale(x)\n0.6265\n0.01102\n56.83\n0\n\nOh dear, oh dear! I think I should have known that the standardised regression (slope) coefficients of a simple, two variable linear regression are the Pearson correlations!\nQuestions and feedback\nhttps://www.psyctc.org/psyctc/ web site. In most browsers I think that will open in a new page and if you close it when you have sent your message I think that and most browsers will bring you back here.\nTechnical footnote and thanks\nThis has been created using the distill package in R (and in Rstudio). Distill is a publication format for scientific and technical writing, native to the web. There is a bit more information about distill at https://rstudio.github.io/distill but I found the youtube (ugh) presentation by Maëlle Salmon at https://www.youtube.com/watch?v=Xyc4-bJjdys much more useful than the very minimal notes at that github page. After watching (the first half of) that presentation the github documentation becomes useful.\nLicensing\nAs with most things I put on the web, I am putting this under the Creative Commons Attribution Share-Alike licence.\n\n\n\n",
    "preview": "posts/2021-01-27-handling-overprinting/handling-overprinting_files/figure-html5/scatter1-1.png",
    "last_modified": "2023-08-25T13:37:10+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-27-nudgingonaxes/",
    "title": "Nudging groupings on plot axes",
    "description": "How to nudge categories on an axis of a ggplot plot.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [
      "Graphics",
      "Nudging",
      "Jittering",
      "R tricks"
    ],
    "contents": "\n\nContents\nHow to “nudge” plots\n\nHow to “nudge” plots\nI can never remember how to do this and keep looking it up. Emily asked me about it so I thought I should crack it and make a file about it to remind myself.\nI’m going to use a little function to get bootstrap confidence intervals around observed means so here’s the code for that.\n\nI am showing the raw R within the Rmarkdown code blocks. I have tried to comment things liberally. Click on “Show code” to see the code.\n\n\nShow code\n\n### function using boot() and boot.ci() from the the boot package to get bootstrap CIs around observed means\ngetCIbootMean <- function(data, ciInt = .95, bootReps = 1000){\n  getMeanForBoot <- function(dat, ind) {mean(dat[ind])} # ind indexes the particular bootstrap sample of vector dat\n  tmpRes <- boot::boot(data, getMeanForBoot, R = bootReps)  # gets the boostrap results\n  tmpCI <- boot::boot.ci(tmpRes, type =  \"perc\")$percent[1,4:5] # gets the percentile method CI\n  return(list(LCL = tmpCI[1],\n              obsMean = tmpRes$t0,\n              UCL = tmpCI[2]))\n}\n# getCIbootMean(1:30) # testing!\n\n\nNow let’s get some demonstation data.\n\n\nShow code\n\nn <- 500 # sample size\nset.seed(1245) # get same result every run\ntibble(genderNum = sample(0:1, n, replace = TRUE), # generate gender\n       ageNum = sample(13:17, n, replace = TRUE), # generate age\n       gender = if_else(genderNum == 1, \"F\", \"M\"),\n       score = rnorm(n) + # get randomness unsystematically related to gender or age\n         genderNum*.1*rnorm(n) + # add a simple gender effect\n         ageNum*.1*rnorm(n) + # add a simple age effect\n         (genderNum*(ageNum - 15)*.5*rnorm(n))^2 + # and an interaction\n         20, # make sure values are positive\n       age = as.factor(ageNum)) %>%\n  group_by(age, gender) %>%\n  summarise(mean = list(getCIbootMean(score))) %>%\n  unnest_wider(mean) -> tibDat\n\n\nHere’s a crude way to separate things by nudging them on the x axis.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = interaction(age, gender), y = obsMean, colour = gender)) +\n       geom_point() +\n       geom_linerange(aes(ymin = LCL, ymax = UCL))\n\n\n\nBut that’s aesthetically and informatively rubbish as it’s not reflecting the grouping. I think what we want is something like this.\n\n\nShow code\n\nvalXdodge = .25 # setting it here makes it easier to try different values when you have multiple geoms you want to dodge\nggplot(data = tibDat,\n       aes(x = age, y = obsMean, colour = gender, group = gender)) + # key thing is that dodging is by the grouping\n  geom_point(position = position_dodge2(width = valXdodge)) +\n  geom_linerange(aes(ymin = LCL, ymax = UCL),\n                 position = position_dodge(width = valXdodge)) \n\n\n\nI think “nudge” would have been a much better term than “dodge” but that may be because dodging has a particular meaning in manual printing of photos (where it’s all about changing the darkness of particular areas of the image) which was something I learned about long, long ago.\nI also think the help for dodge is truly awful and is compounded by the fact that dodging works differently depending on the geom you are using (I’ve been lazy and not gotten to the bottom of that but the basic issue is that it works differently for geom_bar() and geom_histogram() where I think it assumes that the x aesthetic is a grouping whereas with geom_point(), geom_linerange() and geom_errorbar() (and probably geom_line()) it needs to be told the grouping on which you are dodging.\nNotwithstanding my grousing, it’s incredibly useful for depicting things. I guess it has something in common with my previous post here https://www.psyctc.org/Rblog/posts/2021-01-27-handling-overprinting/ as both tricks have in common that they actually distort the literal mappings to create mappings that are far more informative and less misleading than the simply “accurate” mapping.\n\n\n\n",
    "preview": "posts/2021-01-27-nudgingonaxes/nudgingonaxes_files/figure-html5/plot1-1.png",
    "last_modified": "2023-08-25T13:37:24+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
