[
  {
    "path": "posts/2021-04-09-spearman-brown-formula/",
    "title": "Spearman-Brown formula",
    "description": "How does internal reliability relate to number of items?",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\n\nContents\nBackground\nTheory behind the Spearman-Brown formula\nYou can use getRelBySpearmanBrown from the CECPfuns package\n\n[Created 10.iv.21, tweak to point to https://cecpfuns.psyctc.org/ rather than to github 11.iv.21]\nThe Spearman-Brown formula: \\[{\\rho^{*}}=\\frac{n\\rho}{1 + (n-1)\\rho}\\]\ngives us this plot.\n\n\nShow code\n\nlibrary(CECPfuns) # for getRelBySpearmanBrown()\n### to get the CECPfuns package use:\n# remotes::install_github(\"cpsyctc/CECPfuns\")\n### for which you may have needed to do \n# install.packages(\"remotes\")\n### in order to get the remotes package\n### you can also use install_github(), essentially the same as in remotes\n### from the devtools package if you have installed that but if you aren't\n### making R packages then you probably don't want all of devtools\n### see https://www.psyctc.org/Rblog/posts/2021-02-10-making-my-first-usable-package/\nmaxK <- 60\nvecK <- 2:maxK\nvecK %>%\n  as_tibble() %>%\n  rename(k = value) %>%\n  rowwise() %>%\n  ### I have put the explict mapping of getRelBySpearmanBrown to my CECPfuns package here to avoid confusion\n  mutate(rel.1 = CECPfuns::getRelBySpearmanBrown(oldRel = .1, lengthRatio = k / 2,  verbose = FALSE),\n         rel.2 = CECPfuns::getRelBySpearmanBrown(oldRel = .2, lengthRatio = k / 2,  verbose = FALSE),\n         rel.3 = CECPfuns::getRelBySpearmanBrown(oldRel = .3, lengthRatio = k / 2,  verbose = FALSE), \n         rel.4 = CECPfuns::getRelBySpearmanBrown(oldRel = .4, lengthRatio = k / 2,  verbose = FALSE)) %>%\n  ungroup() -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = starts_with(\"rel.\"), names_to = \"IIC\", values_to = \"Reliability\") %>%\n  mutate(IIC = factor(str_sub(IIC, 4, 5))) -> tibDatLong\n  \nggplot(data = tibDatLong,\n       aes(x = k, y = Reliability, group = IIC, colour = IIC)) +\n  geom_point(size = 1) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = c(1, seq(2, 8, 2), seq(0, maxK, 10))) + # and I want the x axis with these tick marks and labels\n  scale_y_continuous(breaks = seq(0, 1, .1)) + # same for the y axis\n  ggtitle(\"Relationship of reliability to k, number of items in a measure\",\n          subtitle = \"for fixed inter-item correlation (IIC)\") +\n  theme_bw() + # I like this simple theme with white plot area\n  theme(plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5), # I like titles and subtitles centered\n        panel.grid.minor = element_blank(), # gets grid lines only where the axis tick marks are not adding minor ones between those\n        axis.text.x = element_text(angle = 80, # trying to get the axis point labels rotated for maximum clarity\n                                   hjust = 1,  # and aligning them, \n                                   vjust = .75)) # is there a bug in the ggplot code failing to handle the number of characters?\n\n\n\n\nBackground\nI must have discovered this neat little formula back in the very early 1990s thinking about the Body Shape Questionnaire (BSQ; Cooper et al. (1986)). That thinking led to a paper I still like quite a bit: Evans & Dolan (1993). In the formula \\(\\rho\\) is the reliability of a test and the equation is predicting \\(\\rho^{*}\\) the reliability of a new test longer, or shorter, than the first by a ratio \\(n\\).\nIn fact, the title of this page could have been the more accurate: “How does internal reliability/consistency relate to number of items in Classical Test Theory (CTT) assuming that mean inter-item correlation remains the same?” That’s what the Spearman-Brown (prediction) formula tells us.\nThere’s a typically excellent wikipedia entry about the formula. As well as a very thorough explanation of the formula the page also has a fascinating bit of history about the factor that Spearman-Brown was neither a single person with a double barrelled surname, nor a working partnership. I do love the way wikipedia contributors often add these things.\nWhy am I posting about this now, thirty years later? Well, it came in useful recently looking at the psychometrics of the YP-CORE. The YP-CORE has ten items, seven negatively cued, e.g. “My problems have felt too much for me” and three that are positively cued, e.g. “I’ve felt able to cope when things go wrong.” Emily wanted to test whether the reliability of the positively cued items was the same as that of the negatively cued items and had discovered the excellent cocron package (see also http://comparingcronbachalphas.org). The cocron package implements in R formulae for inference testing one Cronbach alpha value, and for testing equality of more than one alpha (both for values from the same sample, i.e. a within participants test, as would have been the case for Emily’s question, and for the probably more common question where values from multiple groups are to be compared, e.g. is the alpha higher when women complete the measure than it is when men complete it. These are based on the parametric model of and developed by Feldt and summarised nicely in Feldt, Woodruff, and Salih (1987).\nThat looked to give the test Emily wanted, however, the truism that unless something is very wrong, a measure of a latent variable with more items will always have higher reliability than one with fewer. I avoid gambling like the plague but I should have offered a bet to Emily that the negative items would have the higher reliability, and given that she had an aggregated dataset with n in the thousands, that the difference would be highly statistically significant.\nTheory behind the Spearman-Brown formula\nWhy should a longer test have higher reliability? Simply because as you get more items each item’s error (unreliability) variance, by definition uncorrelated with any other item’s error variance will tend to cancel out while any systematic variance that each item captures from the latent variable will accumulate. (Why do I say that items’ error variances are uncorrelated with each other: that simply follows from the definition that unreliability is random contamination of scores: if errors were correlated they would be part of invalidity: systematic contamination of scores, not of unreliability.)\nSo it’s logical that longer tests will have higher reliability than shorter assuming the same or similar mean inter-item correlations which reflect the systematic variance across scores is similar or the same.\nThe Spearman-Brown formula tells us how reliability changes with k, the number of items. How does the relationship look? That was the plot above. Here it is again.\n\n\nShow code\n\nmaxK <- 60\nvecK <- 2:maxK\nvecK %>%\n  as_tibble() %>%\n  rename(k = value) %>%\n  rowwise() %>%\n  mutate(rel.1 = getRelBySpearmanBrown(oldRel = .1, lengthRatio = k / 2, verbose = FALSE),\n         rel.2 = getRelBySpearmanBrown(oldRel = .2, lengthRatio = k / 2, verbose = FALSE),\n         rel.3 = getRelBySpearmanBrown(oldRel = .3, lengthRatio = k / 2, verbose = FALSE), \n         rel.4 = getRelBySpearmanBrown(oldRel = .4, lengthRatio = k / 2, verbose = FALSE)) %>%\n  ungroup() -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = starts_with(\"rel.\"), names_to = \"IIC\", values_to = \"Reliability\") %>%\n  mutate(IIC = factor(str_sub(IIC, 4, 5))) -> tibDatLong\n  \n\nggplot(data = tibDatLong,\n       aes(x = k, y = Reliability, group = IIC, colour = IIC)) +\n  geom_point(size = 1) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = c(1, seq(2, 8, 2), seq(0, maxK, 10))) + # and I want the x axis with these tick marks and labels\n  scale_y_continuous(breaks = seq(0, 1, .1)) + # same for the y axis\n  ggtitle(\"Relationship of reliability to k, number of items in a measure\",\n          subtitle = \"for fixed inter-item correlation (IIC)\") +\n  theme_bw() + # I like this simple theme with white plot area\n  theme(plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5), # I like titles and subtitles centered\n        panel.grid.minor = element_blank(), # gets grid lines only where the axis tick marks are not adding minor ones between those\n        axis.text.x = element_text(angle = 80, # trying to get the axis point labels rotated for maximum clarity\n                                   hjust = 1,  # and aligning them, \n                                   vjust = .75)) # is there a bug in the ggplot code failing to handle the number of characters?\n\n\n\n\nThat shows clearly how reliability climbs very rapidly as you move from having two items (the minimum to have an internal reliability) through single figures and then how the improvement steadily slows and will never reach 1.0 (unless you start with a reliability of 1.0 which isn’t our real world and arguably isn’t any real world). It also shows that the curve depends on the inter-item correlation (IIC). I have plotted starting with a correlation between two items of 0.1, 0.2, 0.3 or 0.4.\nSo knowing that there are seven negatively cued items in the YP-CORE and three positively cued items, how would the ratio of the reliabilities of the two vary with the mean ICC assuming that it was the same in each set of items? Here, in red, is the plot of the Spearman-Brown predicted reliability for the negatively cued items given a range of reliabilty for the three positively cued items from .01 to .35.\n\n\nShow code\n\nnNeg <- 7\nnPos <- 3\nvecRelPos <- seq(.01, .35, .01)\nvecRelPos %>%\n  as_tibble() %>%\n  rename(relPos = value) %>%\n  rowwise() %>%\n  mutate(relNeg = getRelBySpearmanBrown(oldRel = relPos, lengthRatio = 7/3, verbose = FALSE),\n         relRatio = relPos / relNeg) %>%\n  ungroup() -> tibDat2\n\nggplot(data = tibDat2,\n       aes(x = relPos, y = relNeg)) +\n  geom_line(colour = \"red\",\n            size = 2) +\n  geom_abline(intercept = 0, slope = 7 / 3) +\n  geom_abline(intercept = 0, slope = 1,\n              colour = \"blue\") +\n  scale_x_continuous(breaks = seq(0, .55, .05), \n                     limits = c(0, .55)) +\n  scale_y_continuous(breaks = seq(0, .55, .05), \n                     limits = c(0, .55)) +\n  ylab(\"Reliability for negative items\") +\n  xlab(\"Reliability for positive items\") +\n  ggtitle(\"Plot of reliability for seven negatively cued items given reliability of three positively cued items\",\n          subtitle = \"Assumes same mean inter-item correlation, black line marks inaccurate prediction using just y = 7/3 * x not Spearman-Brown formula\\n\n          blue line is y = x\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5),\n        aspect.ratio = 1)\n\n\n\n\nWe can see that the values are always well above equality to the reliability of the three items (the blue line) and we can see that the relationship isn’t a simple proportion and that the values are always lower than 7/3 times the reliability from the positively cued items.\nYou can use getRelBySpearmanBrown from the CECPfuns package\nAs announced here, there is a developing CECPfuns package (https://cecpfuns.psyctc.org/) which contains the function getRelBySpearmanBrown and which was used in the code above. There is also the function SpearmanBrown in the psychometric package by Thomas D. Fletcher which does the same thing (and gives the same results!)\n\n\nCooper, P. J., M. J. Taylor, Z. Cooper, and C. G. Fairburn. 1986. “The Development and Validation of the Body Shape Questionnaire.” International Journal of Eating Disorders 6: 485–94.\n\n\nEvans, Chris, and Bridget Dolan. 1993. “Body Shape Questionnaire: Derivation of Shortened \"Alternate Forms\".” International Journal of Eating Disorders 13: 315–21.\n\n\nFeldt, Leonard S., David J. Woodruff, and Fathi A. Salih. 1987. “Statistical Inference for Coefficient Alpha.” Applied Psychological Measurement 11: 93–103.\n\n\n\n\n\n",
    "preview": "posts/2021-04-09-spearman-brown-formula/spearman-brown-formula_files/figure-html5/graphic1-1.png",
    "last_modified": "2021-04-11T14:05:41+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-26-compiling-r-on-a-raspberry-pi-4/",
    "title": "Compiling R on a Raspberry Pi 4",
    "description": "I thought I should document this process as it turned out to be fairly easy",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-03-26",
    "categories": [],
    "contents": "\n\nContents\nGetting started with the machine\nCompiling the latest R from source\nAcknowledgement\n\n\n\nShow code\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nas_tibble(list(x = 1,\n               y = 1)) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"R 4.0.4 on Pi 4!\",\n            size = 20,\n            colour = \"red\",\n            angle = 30) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\n\n\n[update tweak 6.iv.21, again 13.iv.21 for R 4.0.5 on 32-bit Raspbian]\nI have recently spent a (very small) amount of money to have a Raspberry Pi 4 to play with to see if I can run the open source shiny server off it. I am using the lovely service my ISP, Mythic Beasts provide, see https://www.mythic-beasts.com/order/rpi. So this has got me a Pi 4 with 4Gb of RAM and a choice of three operating systems: Ubuntu, Raspbian and, my current choice “Raspberry Pi OS 64 bit”, Debian GNU/Linux 10 (buster) according to lsb_release -a. The nice way that Mythic Beasts do this uses NFS file storage rather than an SD card for the main storage and I have paid for 10Gb at this point. That may matter if someone is trying to follow this but using less storage.\nI am putting this up here in the hope it will help others. The combination of R and the Raspberry Pi, particularly the newer, really impressively powerful iterations of the Pi, strike me as an extremely low cost way to get yourself formidable number crunching power. However, my experience so far is that this is not a well documented path to take and that there can be real messes for you as things are different on ARM hardware from the commoner AMD or Intel processors and as, as always in the open source world, things change and documentation tends to lag behind the changes so that old documentation can create real problems. Like pretty much everyone else in the open source world, I’m not paid to do this so my page here will go out of date too. I will try to update it and please contact me if you find what I have put here doesn’t work for you and I’ll try to update this to reflect whatever has caused the issue.\nGetting started with the machine\nOK, so I started with a raw machine, logged in and ran:\napt-get update\napt-get upgrade\nto get things up to date. Then I ran:\napt-get install apt-file \n# helps finding packages for missing resources\napt-file update \n# initialises and in future will update the cache that apt-file uses\nThat was because\napt-file search missingThing\ncan be a very good way to find the particular package you need to install to find the missingThing you need!\nNext came:\napt-get install emacs #because I prefer it to vi[m]\nI think that got me python 2.7 as a byproduct.\nAnd then:\napt-get install curl\napt-get install wget\nas they are two ways of yanking things down from the internet and I don’t think they’re installed by default.\nThen I did this:\napt-get install r-base\nas I was told that would get some other Debian packages that I would need for R. I suspect that’s true and it was pretty fast, got me R version 3.5.2 and having that doesn’t seem to have interfered with the next stages.\nCompiling the latest R from source\nThe first thing is to get the latest source from CRAN. You can see the URL here and you should be tweaking these version numbers unless you are copying this in the next few days.\n[Update 13.iv.21 for R 4.0.5 on 32-bit Raspbian: obviously you change “4.0.4” below to “4.0.5”]\nwget https://cran.r-project.org/src/base/R-4/R-4.0.4.tar.gz\ngunzip R-4.0.4.tar.gz\ntar -xvf R-4.0.4.tar\nSo that’s yanked down the gzipped, tar packed, sources and then unzipped and unpacked them into a directory that, for this version, called R-4.0.4. Surprise, surprise!\nNow the key thing is the compiling. That means this but don’t do it yet …\ncd R-4.0.4\n./configure\nThat runs a stunning configuring script that checks out whether you have everything needed for the compilation. I had to keep running this until it stopped terminating with requests for resources. For example, the first error message for me was X11 headers/libs are not available which was satisfied by me doing apt-get install libxt-dev.\nWhen you have sorted all the missing resources that cause full errors there are still warnings. Again, my first was: configure: WARNING: you cannot build info or HTML versions of the R manuals.\nFinally, when you have got rid of all the warnings by adding things you are left with capabilities that are omitted. I had: Capabilities skipped:        TIFF, Cairo\nIt’s tedious and time wasting to keep going through these cycles of ./configure and correcting so to save yourself time I think you can safely do this lot before your first ./configure and then that run should work. Here are the things I pulled in.\napt-get install libxt-dev # supports x11 screen handling\napt-get install libpcre2-dev # gets the PCRE libraries used by grep and its relatives\napt-get install libcurl4-openssl-dev # adds SSL/TLS encrypted downloading\napt-get install libtiff-dev # for tiff graphic output\napt-get install libgtk-3-dev # may not have been necessary\napt-get install libghc-cairo-dev # for Cairo system for graphic output\napt-get install texinfo texlive texlive-fonts-extra # for creating of help/man pages\n### that pulled a huge amount but allows you got get TIFF and Cairo output, then\nfmtutil-sys --missing \n### rebuilds format files for new fonts (I think)\n[Update 13.iv.21 for R 4.0.5 on 32-bit Raspbian] Interestingly I had to add:\napt-get install libbz2-dev libreadline-dev\nOn Raspbian 32-bit, a.k.a. (also known as, healthcare slang?) Linux raspberrypi 5.10.17-v7l+ #1403 SMP Mon Feb 22 11:33:35 GMT 2021 armv7l GNU/Linux\nAt that point, i.e. after ./configure ran fine, I could finally go for make -j4 Apparently the “-j4” allows the make process to use four processes which speeds things up. The compilation took less than 30 minutes on my machine.\nOne message I noticed as the compilation proceeded was a familiar one:\nmake[1]: Entering directory '/home/chris/R-4.0.4/doc'\nconfiguring Java ...\n\n*** Cannot find any Java interpreter\n*** Please make sure 'java' is on your PATH or set JAVA_HOME correspondingly\nI’ll come back to that.\nFinally we get to:\nmake install \nputs R into /usr/local/lib. To my surprise I had to copy the ./bin/R executable from the temporary directory to /usr/bin/R:\ncp ./bin/R /usr/bin/R\n\nand then I was away! R 4.0.[4|5] up and running in what I think was less than an hour.\n\nupdate.packages(ask = FALSE, checkBuilt = TRUE)\n\ngot the base and recommended packages installed by default updated. That through up one error:\nERROR: dependency ‘openssl’ is not available \nSo I added these from the OS prompt:\napt-get install libssl-dev\napt-get install libxml2-dev\napt-get install libgit2-dev     \nI use components of the tidyverse a lot so the next step was to go back into R and run the obvious\n> install.packages(\"tidyverse\") \nwhich pulls in the key tidyverse packages was vital for me. That took quite a while to get all the components compiled in. Then I could add my own little package:\n> remotes::install_github(\"cpsyctc/CECPfuns\")\nThat pulled in some more other packages but all compiled without issues.\nFinally, I could come back to the Java issue. Back out of R and to the OS prompt. This seemed to get me the Java I wanted.\napt-get install default-jdk\nand then I could do\nR CMD javareconf\nwhich found all it wanted and so I could install the rJava package in R and check that it works: it does.\nThat’s it! R 4.0.[4|5] installed on a Raspberry Pi 4 and I’m now much more confident that I compile subsequent releases on the machine too.\nAcknowledgement\nI am very grateful for encouragement and tips from Denis Brion. I think some of his work with R on Raspberry Pi machines can be seen at https://qengineering.eu/deep-learning-examples-on-raspberry-32-64-os.html.\n\n\n\n",
    "preview": "posts/2021-03-26-compiling-r-on-a-raspberry-pi-4/compiling-r-on-a-raspberry-pi-4_files/figure-html5/createGraphic-1.png",
    "last_modified": "2021-04-14T15:05:45+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-28-where-to-store-different-settings-in-rmarkdown-files/",
    "title": "Where to store different settings in Rmarkdown files",
    "description": "This may be of use to others but it's partly for me as I keep forgetting these and searching around for the .Rmd files in which I used the one I want!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-28",
    "categories": [],
    "contents": "\n\nContents\nSetttings in the yaml header\nyaml heading settings for distill\n… in index.Rmd\n… in “posts”\n… in articles/pages\n\n\nSettings in css block\nSettings in early/first R code block\nSetting ggplot defaults\n\nUpdated with information about ggplot defaults 19.iii.21\nRmarkdown is brilliant as a framework in which to create reports using R and it’s often useful to reset various defaults at the start of a file. Increasingly I work from Rmarkdown to html so some of this only applies there. I find there are three places I set things:\nin the yaml header\nin a css block or separate file (only for html output)\nin the first or an early R code block\nsetting defaults for ggplot (usually in that same early block)\nSetttings in the yaml header\nThis is well documented in many places and https://bookdown.org/yihui/rmarkdown/html-document.html is probably the canonical reference but searching will provide much other advice. I often use:\n---\ntitle: \"A title here\"\nauthor: \"Xxxx Yxxx\"\ndate: \"03/01/2021\"\n\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    toc_depth: 4\n    fig_height: 8\n    fig_width: 11    \n# bibliography: references.bib\n---\n\nI think the main things to say about that is that I don’t find that the floating table of contents (toc_float: true) always works, with long documents and complex blocks with graphics and text I find it sometimes mangles the toc so I am using it less than I used to. This can be a useful place to set the figure heading if they might be the same for all your code blocks with graphic output. I am not sure how many other code block header settings you could set here. I must experiment more: could save me a lot of typing. The only other thing there is the bibliography line, commented out. I still haven’t got into regular use of the citation and referencing capacities built into Rmarkdown. Must try harder!\nHere is another\n---\ntitle: \"ICCs from multilevel models analysed with lme4 or nlmer\"\nauthor: \"CE\"\ndate: \"26/02/2021\"\noutput:\n  html_document:\n    # css: \"test.css\"\n    toc: TRUE\n    toc_float: FALSE\n    dpi: 200\n    out.width: \"100%\"\n    fig.height: 40\n---\nThat shows that you can call an external css file (see next section), so far I haven’t found that I have enough css to make that worth doing. More important here, and I’m still working on this, I have found that you can use out.width: \"100%\" to make the html avail itself of more of your screen width which I find useful. The dpi: 200 and huge fig.height: 40 settings were me trying to optimise my graphic output for some complex plots.\nyaml heading settings for distill\n… in index.Rmd\nThis is all I have in my index.Rmd file. As yet I haven’t found any other options that can usefully be added here.\n---\ntitle: \"An R SAFAQ by Chris Evans\"\nsite: distill::distill_website\nlisting: posts\n---\n… in “posts”\nThis where most of the Distill extensions to routine Rmarkdown yaml header blocks go. Here’s an example.\n---\ntitle: \"Making the CECPfuns package: my own usable package\"\ndescription: |\n  This is very much work in progress so look for later posts about CECPfuns as well as this.\nbase_url: https://www.xxxx.org/psyctc/Rblog/  \npreview: https://www.xxxx.org/pelerinage2016/wp-content/uploads/2020/07/P1160474.jpg\nauthor:\n  - name: Xxxx Yyyy\n    url: https://www.xxxx.org/R_blog/\n    affiliation: xxxx.org\n    affiliation_url: https://www.xxxx.org/psyctc/\n    orcid_id: xxxx-xxxx-xxxx-xxxx\n\ndate: 2021-02-10\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 4\n    hightlight_downlit: true\n    self_contained: false\n    code_folding: true\ncreative_commons: CC BY-SA\n---\nI think that’s mostly self-explanatory and I hope I’ve messed up my own data with sufficient “xxxx” insertions that it’s safe for people to copy and paste to create their own extension on the basic yaml that distill:create_post(\"post title\") creates. The code_folding option means that blocks of code are “folded” away by default but have a “Show code” button so the reader can unfold the code and read it.\n… in articles/pages\nHere is one of my yaml headers:\n---\ntitle: \"Welcome to these pages\"\ndescription: |\n  Welcome to these pages which I hope will be useful to people using R to analyse data.\nbase_url: https://www.xxx.org/psyctc/Rblog/  \nauthor:\n  - name: Xxxx Yyyy\n    url: https://www.xxx.org/R_blog/\n    affiliation: Xxxx.org\n    affiliation_url: https://www.xxx.org/psyctc/\n    orcid_id: xxxx-xxxx-xxxx-xxxx\ndate: \"2021-03-19\"\noutput: \n  distill::distill_article:\n    self_contained: false\ncreative_commons: CC BY-SA    \n---\nI think that’s all pretty self-explanatory. I am sure you can see what to if copying and pasting this!\nSettings in css block\nThis is not my expert area but I have been using a block like this:\nclick to show css_chunk.txt\n(Apologies for this way of putting the code in here: I gave up on trying to work out how to escape characters or otherwise override things being mangled in knitting that!)\nThat is using the principles behind css (cascading style sheet) to set some html defaults. The first two stanzas allow raw R text output (which comes out in the html “pre” format) to come up in a horizontally scrollable window which can be useful where you find you are spitting out wide output and the next stanza I think determines the formatting of raw code (not sure about that!).\nThe body stanza is a recent discovery of mine. The “body” section of html is everything except the header information, i.e. it’s what the human reading an html document sees. That allows my html output to use more of my nice big screen.\nSettings in early/first R code block\nWhen you create a new Rmd file in Rstudio it always has this first R code block.\nclick to show default_setup_chunk.txt\nI often expand that to something like this:\nclick to show big_setup_chunk.txt\nSetting ggplot defaults\nI have my own preferences for some of the “theme” elements in ggplot and discovered that I can set these for a whole Rmarkdown files like this:\n### set ggplot defaults\ntheme_set(theme_bw())\ntheme_update(plot.title = element_text(hjust = .5),\n             plot.subtitle = element_text(hjust = .5))\nYou can also make that a named object:\n### set ggplot defaults\ntheme_set(theme_bw())\ntheme_update(plot.title = element_text(hjust = .5),\n             plot.subtitle = element_text(hjust = .5)) -> CEtheme\nAnd, of course, if you wanted to, you could even save that to a tiny file:\n### set ggplot defaults\ntheme_set(theme_bw())\ntheme_update(plot.title = element_text(hjust = .5),\n             plot.subtitle = element_text(hjust = .5)) -> CEtheme\nsave(CEtheme, \"CEtheme\")\nDo contact me if you have advice about setting Rmarkdown options and if have corrections to the above.\n\n\n\n",
    "preview": "posts/2021-02-28-where-to-store-different-settings-in-rmarkdown-files/css.png",
    "last_modified": "2021-03-19T13:58:00+00:00",
    "input_file": {},
    "preview_width": 260,
    "preview_height": 321
  },
  {
    "path": "posts/2021-02-16-wisdom-of-years/",
    "title": "Wisdom of years!",
    "description": "I've learned a lot about data analysis from my errors, here's what I wish I'd known earlier!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-16",
    "categories": [],
    "contents": "\n\n\n\nThis is just a little post to point to a new developing page “Wisdom1”(https://www.psyctc.org/Rblog/wisdom.html) in my little Rblog site. It’s a compilation of principles and rules to myself all of which I wish I’d learned earlier and which, I believe, save me weeks of time even though, sometimes, they can add minutes, occasionally hours and, once per project (writing DAPs & and DMPs: Data Analysis Plans and Data Management Plans) they may even take days. Very occasionally, when trying to simulate a project, they may take even longer but those, like long DAPs, may turn into papers in their own rights.\nThis will accumulate and I welcome comments and suggestions contact me, so I’ve made it a page not a post and I’m just using this to flag it up.\n\n\n\n",
    "preview": "posts/2021-02-16-wisdom-of-years/wisdom.png",
    "last_modified": "2021-02-16T12:09:31+00:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4800
  },
  {
    "path": "posts/2021-02-10-making-my-first-usable-package/",
    "title": "Making the CECPfuns package: my own usable package",
    "description": "This is very much work in progress so look for later posts about CECPfuns as well as this.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-10",
    "categories": [],
    "contents": "\n\nContents\nLatest update\nBackground\nWhy do it?\nWarning\n\nCreate your package\nCreate your first function\nStart to insert help/documentation contents\n\nNow check and build your package\nThat’s it! You’re done!\nUsing functions from other packages\n\nHow I am synching my package to machines other than my main machine\nThings that are still work in progress for me!\nCECPfuns is a start\n\nLatest update\n[Started 10.ii.21, update about use of .Rprofile added later, tweak to point to https://cecpfuns.psyctc.org/ 11.iv.21]\nBackground\nI have been meaning to do this for years but I have still found it one of R’s more tough learning curves even by R’s sometimes challenging standards. The tidyverse is perhaps a higher and better known climb but making this package is K2 to the tidyverse Everest: nastier, colder, more dispiriting and my attempt of the ascent a few years ago hardly got beyond base camp. This time I’m sort at a first support camp level above base camp and I’m trying to document things here.\nWhy do it?\nI would have saved many hours over the last few years had I actually got on top of this when I first started. Why:\nit hugely simplifies keeping track of functions I’ve written and making sure I find the latest version,\nusing the devtools functions and the way these have been integrated into Rstudio actually makes it easy to create new functions, document them (at least minimally) and update them\nintegrating with git it becomes easy to keep track of changes you make\nif you integrate the local git repository to GitHub you can easily share the package, even if that’s only between your own individual machines, for me that’s my main laptop, my backup laptop, my lightweight and ageing Windoze machine and my web server: it’s easy to make sure they’re all looking at the same functions in the same package.\nWarning\nThere are hugely powerful tools to help the creation of R packages and many pages and PDFs on the web to help you. However, for me finding exactly the information I need, getting its context, being sure the advice isn’t outdated and sometimes just understanding what people have written has not always been easy. That’s partly why I’ve created this.\nPlease, I will try to remember to amend any mistakes I find in here, or things I discover change or can be done more easily than whatever I say here, but anything here doesn’t work for you, please:\nlook at the “Latest update” date above;\nuse the search function (in the navigation bar above) and search for “CECPfuns” and look for more recent posts about this;\nuse an advanced search on the web to search for the particular topic looking for things since that “Latest update” date;\ncontact me to tell me, ideally tell me how to fix what didn’t work for you;\nplease realise this is not my job, this, as with everything I put on the web is offered with no warranties, I accept no liabilities, and I probably will have very little time to try to help you explore anything … if I really have time on my hands though, I will try to help. I am doing this using the the Rstudio package building tools, it’s highly unlikely that I will be any help with any other ways of building a package (there are several but I see them as mostly for real R and software experts).\nHm, that I’m writing that probably conveys that this has been a bit tricky.\nOK, not K2, actually my view in the Alps, see (www.psyctc.org/pelerinage2016/Create your package\nOK, the first bit is easy: create a new package using a new directory and the “Create R package” option; give your package a name, e.g. “SillyDemoPackage”. There is the option to include some source (i.e. R code for our purposes) files here but I would recommend starting completely cleanly and creating new source files, one per function, and copying and pasting the code you already have into the new file.\nThat will have created a subdirectory of wherever you were titled named “SillyDemoPackage” and beneath it you have three more subdirectories:\nR (where you are going to put you R source files, one per function)\nman (where you will, using devtools::document(), create Rd files that in turn create the help for the package and functions)\n.Rproj.user (project information: can ignore it)\nCreate your first function\nThat’s your next step: create a new R script file; if your function is myFunction() then save the script into the R subdirectory that creating the project will have created.\nYou now have a single source file with a single function in it. (I think you can put more than one function in a single source file but I think it would be making your life more difficult so don’t).\nPut your cursor inside the function then go to the Code menu above and select “Insert Roxygen Skeleton”. Let’s say I start with this:\nmyFunction <- function(x){\n  return(paste(\"This is a silly function of\", x))\n}\nStart to insert help/documentation contents\nUsing Code, Insert Roxygen Skeleton changes that to this\n#' Title\n#'\n#' @param x \n#'\n#' @return\n#' @export\n#'\n#' @examples\nmyFunction <- function(x){\n  return(paste(\"This is a silly function of\", x))\n}\nAnd you now change that to this:\n#' Title\n#'    This is a very silly function purely for demonstration.\n#' @param x can be any printable object\n#'\n#' @return a string that pastes a silly comment before x\n#' @export\n#'\n#' @examples\n#' x <- \"stunning silliness\"\n#' myFunction(x)\n\nmyFunction <- function(x){\n  return(paste(\"This is a silly function of\", x))\n}\nYou see I have given a short description of the function, I have clarified the one parameter (argument) to the function and what the function returns and I have given a suitably inane example of how it might be used.\nNow put your cursor in the function and type devtools::document(). That will (essentially invisibly) create a new file myFunction.Rd in the /man subdirectory I mentioned above. **Remember to rerun devtools::document() within the function every time you tweak the documentation in those header lines and every time you tweak the function otherwise the help will lag behind what you’ve done (which might or might not be caught at the next stage, but better safe than sorry.)\nNow check and build your package\nNow the exciting bit: under the Build menu, pick “Check” and sit back and watch Rstudio and devtools (and perhaps other things for all I know) whiz through many checks on your package (in the top right hand pane of Rstudio in the usual layout, in the Build tab. I don’t think you can miss it. Those checks can flag up erorrs, warnings and notes and you hope to see an all green summary line at the end saying there were none of any of those. If the checks find issues some messages are very clear and helpful, some are more challenging but I have found that searching on the web usually translates them for me.\nI would then make sure you set up version control on the project using git and I would also recommend then pushing the package to GitHub if you want others to be able to find it easily.\nThat’s it! You’re done!\nOK, I lie. That’s it for the SillyDemoPackage and it’s one function, myFunction(). I think that’s a like a short afternoon stroll out of Kathmandu in summer. When you start to do something remotely useful the gradient goes up a bit and the air gets a little thinner.\nUsing functions from other packages\nThis is a huge issue but actually fairly easy to handle. Most useful functions will call on functions from packages outside of the base functions. Where you do this you need to handle declaring these in a way that means that the package will know what comes from where. There are simple and more sophisticated issues here and the Build, Clean error messages are pretty clear and helpful and there are good guides to the subtleties on the web. So far I have stayed with making the function calls explicit so instead of cor(x, y) I write stats::cor(x, y) in the function and then I add:\nSuggests:\n  stats\nat the bottom of the DESCRIPTION file in the root directory of the package and\nimportFrom(\"stats\", \"cor\")\nat the bottom of the NAMESPACE file, also in the root directory of the package. I think usethis::use_package() helps with this but I have done it manually so far.\nThe other thing you have to do at the head of any such function instead of having a\nlibrary(sausages) # I wouldn't have had this for stats as, of course,\n### the stats package is launched by default when R starts, \n### imagine I am calling sausages::recipe() \n### NO! I made that up!\nyou use:\ninvisible(stopifnot(requireNamespace(\"sausages\")))\n### so a call that doesn't spit out a message but will stop things \n### if you don't have the sausages package on your system\n### requireNamespace() only checks if you have the package\n### it doesn't load the entire package as library() or \n### require() would so if you are only going to call one\n### or a few functions explicitly with sausages::functionName()\n### this is more efficient\nThat’s the lightest way to do things. If you are going to use lots of functions from a package you may be better with other options but this works for me for now.\nHow I am synching my package to machines other than my main machine\nAdded 28.ii.21: dept to Winston Change! If you’re using M$ Windoze I think it’s best to ignore this section. Because Windoze won’t let anything alter a dll on disc that has been loaded into memory, with the really rather complicated way that R (and Rstudio too) pull things into memory as they launch and run .Rprofile this tends to lead to some package upgrading being blocked, e.g. of cachem which Winston maintains.\nI am developing my package on my main Linux laptop. As I can’t really survive without it, I have a near duplicate backup machine and a little, old Windows laptop and Windows in a VM on the main machine and I have R on my web server (serving up this blog, my CORE work https://www.coresystemtrust.org.uk/; my non-CORE work site https://www.psyctc.org/psyctc/; and my personal web site: https://www.psyctc.org/pelerinage2016/. Do go and have a look!)\nI wanted to make sure that every time I (or cron) launched R on any of the those machines it would automatically check for an update to the package on GitHub and install it if there were one. That meant putting a call to install it with devtools::install_github(\"cpsyctc/CECPfuns\") into .Rprofile.\nAdded evening 18.ii.21 with input from Clara ### Locating your .Rprofile file You should find, or create that in locations that are operating system dependent:\n* on linux machines it is /home/username/.Rprofile\n* on Windows machines it is C:/Users/username/Documents/.Rprofile\n* on Macs I am told it is /Users/username/.Rprofile and I am also told that as it is a hidden file, you will need cmd + shift + [.] in order to show the hidden files.\nAdded evening 10.ii.21, with help from Bill Dunlap via the R-help Email list However, my original addition to .Rprofile cause R to keep looping when launched. Bill Dunlap confirmed that’s because something, probably invoked by the devtools::install_github(\"cpsyctc/CECPfuns\") call, is restarting the R session and so rerunning the .Rprofile, and so on ad infinitum and Bill gave me the answer so my .Rprofile is now:\nif (Sys.getenv(\"INSTALLING_FROM_GITHUB\", unset = \"no\") == \"no\") {\n  Sys.setenv(INSTALLING_FROM_GITHUB = \"yes\")\n  devtools::install_github(\"cpsyctc/CECPfuns\")\n}\nAs I understand that code, it checks for an environment variable (i.e. a variable set in the operating system) called “INSTALLING_FROM_GITHUB” and if it finds its value is “no” it runs the the commands inside the brackets, resetting the variable to “yes” and then, the next line, checking if there has been an update of the package on GitHub and installing it if there has been. However, if/when .Rprofile is rerun in that R session the environment variable now has the value “yes” so the looping is prevented. Lovely!\nThings that are still work in progress for me!\nI am slowly learning about all the extras that transform the basic documentation, such as I created above, into really good help for a function.\nI haven’t worked out how to document a whole package yet. The function devtools::build_manual() seems to build at least the typical nice PDF about a package that you see on CRAN, e.g. https://cran.r-project.org/web/packages/boot/boot.pdf but it puts it in the directory above the package directory and the file doesn’t seem to get integrated into the package which seems puzzling and less than entirely helpful to me. I’m sure there must be an answer to that but I haven’t found it yet.\nI haven’t worked out how to create index files like https://cran.r-project.org/web/packages/boot/index.html though that may be because my package is so small that it doesn’t have most of the information that is in there. I can’t really believe that’s the whole reason though.\nCECPfuns is a start\nThis is pretty embarrassing but I will share that this first actual package of mine, probably the only one I’ll ever need to create, is available if you want to see what I’ve managed to create. It will develop into a package mainly of functions I and Clara Paz have found useful (with, I hope, comments and suggestions from Emily) It’s at https://github.com/cpsyctc/CECPfuns and there is a web site for the package at https://cecpfuns.psyctc.org/. You can use git on pretty much any operating system to pull a copy from github if you want to look at the all the raw constituent parts and I think if you do pull that you can see the commit history, i.e. of the changes and updating. (A graph of the commits against date is at https://github.com/cpsyctc/CECPfuns/graphs/commit-activity). I am not opening it to submissions as it’s too early in my learning, I may never reach that place, so, if you have suggestions or corrections and any comments really,contact me through my work site. I hope this helps someone and encourages them to create their own package. I do wish I’d done it earlier!\nMont Blanc from my Alpine balcony\n\n\n",
    "preview": "https://www.psyctc.org/pelerinage2016/wp-content/uploads/2020/07/P1160474.jpg",
    "last_modified": "2021-04-11T14:09:22+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-10-more-piping-introducing-rowwise/",
    "title": "More piping, and rowwise()",
    "description": "This extends https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/ and introduces rowwise()",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-10",
    "categories": [],
    "contents": "\nThis is a slight adaptation of a file I did for Emily (https://www.researchgate.net/profile/Emily_Blackshaw2) back in October 2020 when she and wanted to look at whether Cronbach’s alpha for the YP-CORE varied from session to session across help-seeking clients data: a very basic exploration of longitudinal measurement invariance. I realised it was a good chance for me to pull together what I had been learning back then about piping and to share it with her.\nAs a page here it probably should have come before https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, or been woven into that, but I had managed to lose the file (worrying). However, I think it complements what I put in there and it does introduce the rowwise() function and c_across().\nAs is my wont, I prefer to explore methods with simulated data so the first step was to make such data. Here I am simulating 500 clients each having ten sessions and just a five item questionnaire (the YP-CORE has ten items but five is quicker and fits output more easily!)\n\n\nShow code\n\n### make some nonsense data \nlibrary(tidyverse)\nnParticipants <- 500\nnSessions <- 10\n### give myself something to start with: the sessions\nsession <- rep(1:nSessions, nParticipants) # 1,2,3 ...10, 1,23 ...10 ...\nsession %>%\n  as_tibble() %>%  # turn from vector to tibble, that means I have rename it back to the vector name!\n  rename(session = value) %>%\n  mutate(baseVar = rnorm(nParticipants*nSessions),  # this creates a new variable in the tibble and sort of reminds me that variables may be vectors\n         item1 = baseVar + 0.7*rnorm(nParticipants*nSessions), # creates a first item\n         item2 = baseVar + 0.7*rnorm(nParticipants*nSessions), # and a second\n         item3 = baseVar + 0.7*rnorm(nParticipants*nSessions), # and a third\n         item4 = baseVar + 0.7*rnorm(nParticipants*nSessions), # and a 4th ...\n         item5 = baseVar + 0.7*rnorm(nParticipants*nSessions)) -> tmpDat\n\n### look at it\ntmpDat\n\n\n# A tibble: 5,000 x 7\n   session  baseVar   item1  item2   item3   item4  item5\n     <int>    <dbl>   <dbl>  <dbl>   <dbl>   <dbl>  <dbl>\n 1       1  0.00631 -0.194  -0.280  0.0798 -0.331  -0.420\n 2       2  2.71     2.39    2.94   2.15    3.45    1.64 \n 3       3 -0.868   -0.413  -0.709 -0.938  -1.45   -0.662\n 4       4 -0.601   -1.51   -1.43  -0.177  -0.541  -0.979\n 5       5 -0.478   -1.26   -1.22   0.0396  0.667  -0.668\n 6       6  0.449    0.517   1.06   0.526   0.645  -0.695\n 7       7 -0.0757   0.413   0.868  0.163  -0.549   0.192\n 8       8 -0.599    0.0248 -1.97  -0.0925 -1.53    0.944\n 9       9 -0.0810   0.576   0.830 -0.121  -0.0841 -0.503\n10      10  1.79     0.944   1.41   2.31    1.28    1.97 \n# … with 4,990 more rows\n\nShow code\n\n### check the simple correlation\ncor(tmpDat[, 3:7])\n\n\n          item1     item2     item3     item4     item5\nitem1 1.0000000 0.6799230 0.6699420 0.6748932 0.6769445\nitem2 0.6799230 1.0000000 0.6661796 0.6793697 0.6839315\nitem3 0.6699420 0.6661796 1.0000000 0.6631769 0.6702323\nitem4 0.6748932 0.6793697 0.6631769 1.0000000 0.6729270\nitem5 0.6769445 0.6839315 0.6702323 0.6729270 1.0000000\n\nShow code\n\n### OK, I can play with that, here's the overall alpha (meaningless even for the simulation really but just checking)\npsychometric::alpha(tmpDat[, 3:7])\n\n\n[1] 0.9117041\n\nOK. Now I could start playing with the data in the tidyverse/dplyr/piping way. The key thing to remember is that the default behaviour of mutate() or summarise() within group_by() in dplyr is for a function to act on a vertical vector, i.e. on a variable\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(mean1 = mean(item1))\n\n\n# A tibble: 10 x 2\n   session    mean1\n *   <int>    <dbl>\n 1       1 -0.0335 \n 2       2 -0.00275\n 3       3  0.0434 \n 4       4 -0.0244 \n 5       5  0.0716 \n 6       6  0.0271 \n 7       7  0.103  \n 8       8 -0.0995 \n 9       9 -0.0311 \n10      10 -0.0835 \n\nSo that simply got us the mean for item1 across all completions but broken down by session. Trivial dplyr/piping but I still find it satisfying in syntax and in its utility.\nAs introduced in https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, if I have a function that returns more than one value dplyr handles this nicely but I have to tell it the function is creating a list (even if it’s just a vector), as below. The catch to remember is that you then have to unnest() the list to see its values, usually unnest_wider() is what I want but there is unnest_longer().\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(summary1 = list(summary(item1))) %>%\n  unnest_wider(summary1)\n\n\n# A tibble: 10 x 7\n   session  Min. `1st Qu.`   Median     Mean `3rd Qu.`  Max.\n     <int> <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <dbl>\n 1       1 -4.20    -0.941 -0.0607  -0.0335      0.847  4.29\n 2       2 -4.44    -0.853  0.0633  -0.00275     0.816  3.24\n 3       3 -3.34    -0.791  0.0556   0.0434      0.840  4.36\n 4       4 -3.74    -0.816 -0.0150  -0.0244      0.744  3.68\n 5       5 -3.26    -0.787  0.103    0.0716      1.02   3.61\n 6       6 -3.55    -0.731 -0.00969  0.0271      0.767  3.32\n 7       7 -3.32    -0.724  0.0880   0.103       0.959  4.52\n 8       8 -4.14    -0.910 -0.111   -0.0995      0.746  3.13\n 9       9 -4.62    -0.815 -0.0212  -0.0311      0.810  3.34\n10      10 -3.88    -0.908 -0.0993  -0.0835      0.691  3.60\n\nShow code\n\n###  names are messy but it is easy to solve that ...\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(summary1 = list(summary(item1))) %>%\n  unnest_wider(summary1) %>%\n  ###  sometimes you have to clean up names that start \n  ###  with numbers or include spaces if you want to avoid backtick quoting\n  rename(Q1 = `1st Qu.`,\n         Q3 = `3rd Qu.`)\n\n\n# A tibble: 10 x 7\n   session  Min.     Q1   Median     Mean    Q3  Max.\n     <int> <dbl>  <dbl>    <dbl>    <dbl> <dbl> <dbl>\n 1       1 -4.20 -0.941 -0.0607  -0.0335  0.847  4.29\n 2       2 -4.44 -0.853  0.0633  -0.00275 0.816  3.24\n 3       3 -3.34 -0.791  0.0556   0.0434  0.840  4.36\n 4       4 -3.74 -0.816 -0.0150  -0.0244  0.744  3.68\n 5       5 -3.26 -0.787  0.103    0.0716  1.02   3.61\n 6       6 -3.55 -0.731 -0.00969  0.0271  0.767  3.32\n 7       7 -3.32 -0.724  0.0880   0.103   0.959  4.52\n 8       8 -4.14 -0.910 -0.111   -0.0995  0.746  3.13\n 9       9 -4.62 -0.815 -0.0212  -0.0311  0.810  3.34\n10      10 -3.88 -0.908 -0.0993  -0.0835  0.691  3.60\n\nAgain, as I introduced in https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, I can extend this to handle more than one vector/variable at a time if they’re similar and I’m doing the same to each.\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(across(starts_with(\"item\"), ~mean(.x)))\n\n\n# A tibble: 10 x 6\n   session    item1    item2   item3    item4    item5\n *   <int>    <dbl>    <dbl>   <dbl>    <dbl>    <dbl>\n 1       1 -0.0335   0.0376   0.0144 -0.0350  -0.00901\n 2       2 -0.00275  0.0188   0.0404  0.00395  0.0376 \n 3       3  0.0434   0.0526   0.0794 -0.0348   0.00194\n 4       4 -0.0244  -0.0320  -0.0594 -0.0144  -0.0160 \n 5       5  0.0716   0.0197   0.0151  0.0777   0.0842 \n 6       6  0.0271  -0.00327  0.0127  0.0834   0.00580\n 7       7  0.103    0.0951   0.0690  0.0499   0.0918 \n 8       8 -0.0995  -0.0370  -0.0687 -0.106   -0.102  \n 9       9 -0.0311  -0.0550   0.0211 -0.0338   0.0361 \n10      10 -0.0835  -0.0144  -0.0844 -0.0772  -0.0163 \n\nI can also do that with the following syntax. I have not yet really understood why the help for across() gives that one with function syntax (“~”) and the explicit call of \".x) rather than this and I really ought to get my head around the pros and cons of each.\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(across(starts_with(\"item\"), mean))\n\n\n# A tibble: 10 x 6\n   session    item1    item2   item3    item4    item5\n *   <int>    <dbl>    <dbl>   <dbl>    <dbl>    <dbl>\n 1       1 -0.0335   0.0376   0.0144 -0.0350  -0.00901\n 2       2 -0.00275  0.0188   0.0404  0.00395  0.0376 \n 3       3  0.0434   0.0526   0.0794 -0.0348   0.00194\n 4       4 -0.0244  -0.0320  -0.0594 -0.0144  -0.0160 \n 5       5  0.0716   0.0197   0.0151  0.0777   0.0842 \n 6       6  0.0271  -0.00327  0.0127  0.0834   0.00580\n 7       7  0.103    0.0951   0.0690  0.0499   0.0918 \n 8       8 -0.0995  -0.0370  -0.0687 -0.106   -0.102  \n 9       9 -0.0311  -0.0550   0.0211 -0.0338   0.0361 \n10      10 -0.0835  -0.0144  -0.0844 -0.0772  -0.0163 \n\nAgain, as I introduced in https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, I can do multiple functions of the same items\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(across(starts_with(\"item\"), list(mean = mean, sd = sd)))\n\n\n# A tibble: 10 x 11\n   session item1_mean item1_sd item2_mean item2_sd item3_mean item3_sd\n *   <int>      <dbl>    <dbl>      <dbl>    <dbl>      <dbl>    <dbl>\n 1       1   -0.0335      1.25    0.0376      1.25     0.0144     1.22\n 2       2   -0.00275     1.28    0.0188      1.21     0.0404     1.21\n 3       3    0.0434      1.26    0.0526      1.25     0.0794     1.26\n 4       4   -0.0244      1.20   -0.0320      1.20    -0.0594     1.18\n 5       5    0.0716      1.25    0.0197      1.25     0.0151     1.21\n 6       6    0.0271      1.12   -0.00327     1.18     0.0127     1.14\n 7       7    0.103       1.25    0.0951      1.22     0.0690     1.20\n 8       8   -0.0995      1.25   -0.0370      1.25    -0.0687     1.23\n 9       9   -0.0311      1.18   -0.0550      1.23     0.0211     1.21\n10      10   -0.0835      1.18   -0.0144      1.24    -0.0844     1.19\n# … with 4 more variables: item4_mean <dbl>, item4_sd <dbl>,\n#   item5_mean <dbl>, item5_sd <dbl>\n\nI like that that names things sensibly\nI said the default behaviour of mutate() and summarise() is to work on variables, i.e. vectors, whether that is to work on all the values of the variable if there is no group_by(), or within the groups if there is a grouping. If I want to do something on individual values, i.e. by rows, “rowwise”, then I have to use rowwise() which basically treats each row as a group.\nIf, as you often will in that situation, you want to use a function of more than one value, i.e. values from more than one variable, then you have to remember to use c_across() now, not across(): “c_” as it’s by column.\nYou also have to remember to ungroup() after any mutate() as you probably don’t want future functions to handle things one row at a time.\n\n\nShow code\n\ntmpDat %>% \n  filter(row_number() < 6) %>% # just for this example\n  rowwise() %>%\n  mutate(mean = mean(c_across(starts_with(\"item\")))) %>%\n  ungroup() # see above about ungrouping after rowwise() and mutate()\n\n\n# A tibble: 5 x 8\n  session  baseVar  item1  item2   item3  item4  item5   mean\n    <int>    <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>\n1       1  0.00631 -0.194 -0.280  0.0798 -0.331 -0.420 -0.229\n2       2  2.71     2.39   2.94   2.15    3.45   1.64   2.51 \n3       3 -0.868   -0.413 -0.709 -0.938  -1.45  -0.662 -0.835\n4       4 -0.601   -1.51  -1.43  -0.177  -0.541 -0.979 -0.926\n5       5 -0.478   -1.26  -1.22   0.0396  0.667 -0.668 -0.488\n\nOK, so that’s recapped these things, now what about if I want to look at multiple columns and multiple rows? the trick seems to be cur_data().\nThat gives me a sensible digression from Cronbach’s alpha here as I often find I’m wanting to get correlation matrices when I’m wanting to get alpha (and its CI) and I think getting correlation matrices from grouped data ought to be much easier than it is!\n\n\nShow code\n\ntmpDat %>% \n  select(item1:item5) %>%\n  summarise(cor = list(cor(cur_data()))) %>%\n  unnest_wider(cor) \n\n\n# A tibble: 1 x 25\n   ...1  ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 ...10 ...11\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1 0.680 0.670 0.675 0.677 0.680     1 0.666 0.679 0.684 0.670\n# … with 14 more variables: ...12 <dbl>, ...13 <dbl>, ...14 <dbl>,\n#   ...15 <dbl>, ...16 <dbl>, ...17 <dbl>, ...18 <dbl>, ...19 <dbl>,\n#   ...20 <dbl>, ...21 <dbl>, ...22 <dbl>, ...23 <dbl>, ...24 <dbl>,\n#   ...25 <dbl>\n\nThat, as you can see, is a right old mess!\nbut we can use correlate() from the corrr package:\n\n\nShow code\n\ntmpDat %>% \n  select(item1:item5) %>%\n  corrr::correlate()\n\n\n# A tibble: 5 x 6\n  term   item1  item2  item3  item4  item5\n  <chr>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 item1 NA      0.680  0.670  0.675  0.677\n2 item2  0.680 NA      0.666  0.679  0.684\n3 item3  0.670  0.666 NA      0.663  0.670\n4 item4  0.675  0.679  0.663 NA      0.673\n5 item5  0.677  0.684  0.670  0.673 NA    \n\nAs you see, corrr::correlate() puts NA in the leading diagonal not 1.0. That does make finding the maximum off diagonal correlations easy but I confess it seems wrong to me!\nWhat about using that and group_by()?\n\n\nShow code\n\ntmpDat %>% \n  select(-baseVar) %>%\n  group_by(session) %>%\n  corrr::correlate()\n\n\n# A tibble: 6 x 7\n  term     session   item1   item2   item3   item4    item5\n  <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n1 session NA       -0.0146 -0.0152 -0.0200 -0.0114 -0.00498\n2 item1   -0.0146  NA       0.680   0.670   0.675   0.677  \n3 item2   -0.0152   0.680  NA       0.666   0.679   0.684  \n4 item3   -0.0200   0.670   0.666  NA       0.663   0.670  \n5 item4   -0.0114   0.675   0.679   0.663  NA       0.673  \n6 item5   -0.00498  0.677   0.684   0.670   0.673  NA      \n\nHm, that completely ignores the group_by() and includes session variable. That seems plain wrong to me. I feel sure this is something the package will eventually change but for now I need another way to get what I want.\n\n\nShow code\n\ntmpDat %>% \n  select(-baseVar) %>%\n  group_by(session) %>%\n  corrr::correlate(cur_data())\n\n\n\nI have not evaluated that as it stops with the moderately cryptic error message which I’m putting in here as I quite often forget the summarise(x = ) bit\n# Error: `cur_data()` must only be used inside dplyr verbs.\n# Run `rlang::last_error()` to see where the error occurred.\nSo let’s fix that.\n\n\nShow code\n\ntmpDat %>% \n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(cor = corrr::correlate(cur_data()))\n\n\n# A tibble: 50 x 2\n# Groups:   session [10]\n   session cor$term $item1 $item2 $item3 $item4 $item5\n     <int> <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1       1 item1    NA      0.711  0.711  0.715  0.678\n 2       1 item2     0.711 NA      0.704  0.699  0.714\n 3       1 item3     0.711  0.704 NA      0.700  0.677\n 4       1 item4     0.715  0.699  0.700 NA      0.693\n 5       1 item5     0.678  0.714  0.677  0.693 NA    \n 6       2 item1    NA      0.669  0.670  0.690  0.679\n 7       2 item2     0.669 NA      0.644  0.679  0.680\n 8       2 item3     0.670  0.644 NA      0.664  0.704\n 9       2 item4     0.690  0.679  0.664 NA      0.679\n10       2 item5     0.679  0.680  0.704  0.679 NA    \n# … with 40 more rows\n\nHm. That does get me the analyses I want but in what is, to my mind, a very odd structure.\nOK, after that digression into the corrr package, let’s get to what Emily actually wanted: Cronbach’s alpha across the items but per session.\n\n\nShow code\n\ntmpDat %>%\n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(alpha = psychometric::alpha(cur_data()))\n\n\n# A tibble: 10 x 2\n   session alpha\n *   <int> <dbl>\n 1       1 0.921\n 2       2 0.912\n 3       3 0.919\n 4       4 0.903\n 5       5 0.908\n 6       6 0.901\n 7       7 0.911\n 8       8 0.912\n 9       9 0.910\n10      10 0.915\n\nI get my CI around alpha using the following code.\n\n\nShow code\n\npsychometric::alpha(tmpDat[, 3:7])\n\n\n[1] 0.9117041\n\nShow code\n\ngetAlphaForBoot <- function(dat, i) {\n  # a little function that happens to use psych::alpha to get alpha\n  # but indexes it with i as boot() will require\n  psychometric::alpha(na.omit(dat[i,]))\n}\ngetAlphaForBoot(tmpDat[, 3:7], 1:nrow(tmpDat)) # just checking that it works\n\n\n[1] 0.9117041\n\nShow code\n\nbootReps <- 1000\ngetCIAlphaDF3 <- function(dat, ciInt = .95, bootReps = 1000) {\n  tmpRes <- boot::boot(na.omit(dat), getAlphaForBoot, R = bootReps)\n  tmpCI <- boot::boot.ci(tmpRes, conf = ciInt, type = \"perc\")$percent[4:5]\n  return(data.frame(alpha = tmpRes$t0,\n                    LCL = tmpCI[1],\n                    UCL = tmpCI[2]))\n}\ngetCIAlphaDF3(tmpDat[, 3:7])\n\n\n      alpha       LCL      UCL\n1 0.9117041 0.9079198 0.915301\n\nActually, now I have my CECPfuns package I create a better, more robust function for this, but later!\nSo that’s the overall Cronbach alpha with bootstrap confidence interval.\nCan also do that within a group_by() grouping.\n\n\nShow code\n\ntmpDat %>%\n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(alpha = list(getCIAlphaDF3(cur_data()))) %>% \n  unnest_wider(alpha)\n\n\n# A tibble: 10 x 4\n   session alpha   LCL   UCL\n     <int> <dbl> <dbl> <dbl>\n 1       1 0.921 0.910 0.931\n 2       2 0.912 0.900 0.923\n 3       3 0.919 0.908 0.929\n 4       4 0.903 0.889 0.916\n 5       5 0.908 0.894 0.919\n 6       6 0.901 0.885 0.915\n 7       7 0.911 0.895 0.923\n 8       8 0.912 0.899 0.923\n 9       9 0.910 0.895 0.921\n10      10 0.915 0.903 0.926\n\nAnd that was nice and easy to feed into a forest style plot, as follows.\n\n\nShow code\n\ntmpDat %>%\n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(alpha = list(getCIAlphaDF3(cur_data()))) %>% \n  unnest_wider(alpha) -> tmpTib\n\npsychometric::alpha(tmpDat[, 3:7]) -> tmpAlphaAll\n\nggplot(data = tmpTib,\n       aes(x = session, y = alpha)) +\n  geom_point() + # get the observed alphas in as points\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) + # add the CIs as lines\n  geom_hline(yintercept = tmpAlphaAll) + # not really very meaningful to have an overall alpha but \n    # perhaps better than not having a reference line\n  xlab(\"Session\") +\n  ylab(\"Cronbach alpha\") +\n  ggtitle(\"Forest plot of observed Cronbach alpha per session\",\n          subtitle = paste0(\"Vertical lines are 95% CIs, \",\n                            bootReps,\n                            \" bootstrap replications, percentile method.\")) +\n  theme_bw() + # nice clean theme\n  theme(plot.title = element_text(hjust = .5), # centre the title\n        plot.subtitle = element_text(hjust = .5)) # and subtitle\n\n\n\n\nWell, as you’d expect from the simulation method, no evidence of heterogeneity of Cronbach’s alpha across sessions!\nI hope this is a useful further introduction to piping, dplyr and some of the tidyverse approach. I guess it introduced the corrr package, cur_data() and rowwise() … and it finished with a, for me, typical use of ggplot() (from the ggplot2 package.)\nDo contact me if you have any comments, suggestions, corrections, improvements … anything!\n\n\n\n",
    "preview": "posts/2021-02-10-more-piping-introducing-rowwise/more-piping-introducing-rowwise_files/figure-html5/useDat14-1.png",
    "last_modified": "2021-02-10T20:01:17+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-07-why-pipe-why-the-tidyverse/",
    "title": "Why pipe?  Why the tidyverse?",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-07",
    "categories": [],
    "contents": "\n\nContents\nSo what is piping?\nA worked example of R piping\nSummarising\n\n\nThis was a topic suggested by Emily who is nearing the end of her PhD on the YP-CORE as you can see from her/our RG pages about the PhD here . (More about the YP-CORE here and the CORE system here.) She and I have been on a learning curve moving from base R (https://www.r-project.org/) to increasing use of the tidyverse (https://www.tidyverse.org/) developments of R.\nSo what is piping?\nIt’s this sort of thing.\ndata %>%\n  group_by(gender) %>%\n  summarise(n = n(),\n            minAge = min(age),\n            meanAge = mean(age),\n            maxAge = max(age))\nTo me the idea of piping comes out unix/linux where you can pipe the output of one command into another, for example:\nfind . -name filename.here -printf \"%T@ %Tc %p\\n\" | sort -n\nThe pipe operator in linux is the vertical line “|” and what happens is the text output from the linux find command is fed straight into the sort command to give a sorted list of files matching “filename.here”.\nI think “piping” in R is a bit different (hence some in jokes: piping was introduced into R through a package [magittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html, and also. The “magittr” name was a lovely play on the famous print by Magritte which said firmly Ceci n’est pas un pipe (this is not a pipe) and depicted … a pipe.\nComing back to that R code I showed above …\ndata %>%\n  group_by(gender) %>%\n  summarise(n = n(),\n            minAge = min(age),\n            meanAge = mean(age),\n            maxAge = max(age))\nThat assumes you have a data frame or tibble named “data” and that it has variables gender and age within it. The “%>%” “pipes” those data to the next command, group_by() which in turn pipes the data, now grouped by gender into a four line command, summarise(). That creates a new tibble with four variables each with their one value, for each value of gender.\nThis is so well described by Shannon Pileggi in her page https://www.pipinghotdata.com/posts/2018-11-05-welcome-to-piping-hot-data/ that if you use R but so far haven’t used piping, go and look at her page and then come back here.\nAt first I wasn’t convinced by piping. That was partly because I thought the documentation I found wasn’t terribly clear (it has improved greatly) and didn’t convince me that the new syntax, new way of sequencing instructions, was worth the learning curve. It was also because it was, by the time I found it, very bound in with what has become the tidyverse in R: a lot of other, sometimes really quite radical, changes from base R. To me it felt like having to learn a new language and I’m neither a good linguist nor a good programmer.\nHowever, I have become a convert. I still sometimes go back to what I think of as “classical R”, I still sometimes find there are things I can do more easily with lists and lapply() and its relatives. That’s particularly true when my data isn’t rectangular or a relational data structure of linked rectangular sets of data. If I have a data on the members of families and the data may differ between the members and the families and the relational structures are messy, I will probably use lists and lapply(). However, the vast majority of the data I work with is, or can be, “tidy”. A very common example for me is to have an ID and some demographics per participant, and then answers from each participant on items of one or more questionnaires where every participant may have more than one set of responses. Here the data is a relational rectangular structure one rectangular structure with one row per participant then one or more rows in another rectangular structure for each time they completed the questionnaires with the ID codes enabling us to link the two.\nLong, long ago, when I used SPSS, I was very uncomfortable if I had to handle single rectangular data structures and I would have handled that by having one “wide” file with one row per participant and multiple sets of responses by questionnaire going off to the right and added to for each new completion. That’s doable when you might only have two completions per participant but when you have many per partipant, and numbers perhaps varying from one completion to hundreds, then that becomes a terrible structure.\nOf course, classical R handles structures like this fine. That was one thing that helped me move from SPSS to R (a minor contributor but certainly there in the long list). However, I didn’t shake of some uncertainty with my data handling as handled data linking across data frames.\nNow I have moved to piping and the tidyverse the effect for me has been liberating. I no longer think of data as having to be in a single rectangle, it’s become entirely natural to handle it in a series of linked data sets. I use the tidyverse tibbles: a fundamental building stone in the tidyverse and in many ways a relatively minor extension of the data frame in R. One difference from a data frame is that the default print function for a tibble only prints the first ten rows where the default print function for a data frame would try to print all of it, or until you run out of console lines. At first that seemed an annoyance until I started to use that printing to build what you want iteratively.\nA worked example of R piping\nIn what follows I haven’t folded the code away partly as I wanted to give a bit of the experience of writing code iterative, starting usually with one line and building on that.\nOK, let’s simulate some data\n\n\nlibrary(tidyverse) # get the main packages of the tidyverse\n### that gives us the pipe operator \"%>% and as_tibble and \n### a few other things I use below\n### the piping tools I'm using are in the dplyr package which\n### is within the set of packages called by the tidyverse \"superpackage\"\nset.seed(12345) # get replicable data\nn <- 500 # n(participants)\n\n### create some demographics\n### start by giving each participant a gender ...\nlist(gender = sample(c(\"M\", \"F\"), n, replace = TRUE),\n     age = sample(11:18, n, replace = TRUE), # and an age\n     ### and now a number of questionnaire completions\n     nCompletions = sample(1:30, n, replace = TRUE)\n     ) %>% # and pipe that list forward to convert it to a tibble\n  as_tibble() %>%\n  ### now use a piping trick, mutate() does something to each row\n  ### here a new variable, ID, is created and given the value of \n  ### the row number of each participant, i.e. 1:n\n  mutate(ID = row_number()) -> tibParticipants # give that tibble a name\n\n\n\nNow if I want to understand what I’ve created I just type its name:\n\n\ntibParticipants\n\n\n# A tibble: 500 x 4\n   gender   age nCompletions    ID\n   <chr>  <int>        <int> <int>\n 1 F         11            8     1\n 2 M         11           30     2\n 3 F         15            8     3\n 4 F         13            2     4\n 5 F         13           12     5\n 6 F         15           20     6\n 7 F         15            6     7\n 8 M         18            4     8\n 9 M         11           30     9\n10 F         14           15    10\n# … with 490 more rows\n\nInstead of showing me all 500 rows, I just get the top ten (like using head(dataFrame) in classical R) but I also get pretty much everything else I need to know about the data. Had there been too many variables to fit on the screen the listing would have ended with a line giving me the names of all the variables that wouldn’t fit on the screen.\n\n\ntibParticipants %>% \n  select(ID, nCompletions) %>% # get just the ID codes\n  ### I confess that I always have to look up uncount(): \n  ### I can never remember the name, let's just stop there\n  ### and see what it did ...\n  uncount(nCompletions) \n\n\n# A tibble: 7,679 x 1\n      ID\n   <int>\n 1     1\n 2     1\n 3     1\n 4     1\n 5     1\n 6     1\n 7     1\n 8     1\n 9     2\n10     2\n# … with 7,669 more rows\n\nOK, it’s replicated each ID value by the value in the variable nCompletions. Good, that’s what I wanted. Imagine I’m doing this interactively at the console …\n\n\ntibParticipants %>% \n  select(ID, nCompletions) %>% # get just the ID codes\n  uncount(nCompletions) %>%\n  group_by(ID) %>%\n  mutate(nCompletions2 = n(), \n         ### that gets me the number of completions per ID \n         ### (which was nCompletions in tibParticipants)\n         completionN = row_number(), \n         ### that gets an index number for each completion \n         ### of the questionnaire ...\n         ### it's a very short questionnaire and so badly \n         ### designed the item are uncorrelated answers \n         ### between 0 and 5) ...\n         item1 = sample(0:5, nCompletions2, replace = TRUE),\n         item2 = sample(0:5, nCompletions2, replace = TRUE),\n         item3 = sample(0:5, nCompletions2, replace = TRUE),\n         item4 = sample(0:5, nCompletions2, replace = TRUE),\n         item5 = sample(0:5, nCompletions2, replace = TRUE))\n\n\n# A tibble: 7,679 x 8\n# Groups:   ID [500]\n      ID nCompletions2 completionN item1 item2 item3 item4 item5\n   <int>         <int>       <int> <int> <int> <int> <int> <int>\n 1     1             8           1     5     4     1     4     0\n 2     1             8           2     5     1     2     4     2\n 3     1             8           3     5     1     0     3     5\n 4     1             8           4     0     0     0     2     0\n 5     1             8           5     2     1     1     1     2\n 6     1             8           6     5     1     4     3     3\n 7     1             8           7     0     3     3     4     4\n 8     1             8           8     2     5     4     0     1\n 9     2            30           1     4     1     5     1     2\n10     2            30           2     4     0     0     0     3\n# … with 7,669 more rows\n\nOK. Looking like what I wanted so just put it into a tibble.\n\n\ntibParticipants %>% \n  select(ID, nCompletions) %>% # get just the ID codes\n  uncount(nCompletions) %>%\n  group_by(ID) %>%\n  mutate(nCompletions2 = n(), \n         ### that gets me the number of completions per ID \n         ### (which was nCompletions in tibParticipants)\n         completionN = row_number(), \n         ### that gets an index number for each completion \n         ### of the questionnaire ...\n         ### it's a very short questionnaire and so badly \n         ### designed the item are uncorrelated answers \n         ### between 0 and 5) ...\n         item1 = sample(0:5, nCompletions2, replace = TRUE),\n         item2 = sample(0:5, nCompletions2, replace = TRUE),\n         item3 = sample(0:5, nCompletions2, replace = TRUE),\n         item4 = sample(0:5, nCompletions2, replace = TRUE),\n         item5 = sample(0:5, nCompletions2, replace = TRUE)) %>%\n  ### this can catch you out, if you have used group_by() \n  ### before a mutate, the data stay grouped which is \n  ### probably not what you want so ungroup()\n  ungroup() -> tibQuaireData\n\n\n\nNow I want to join the demographics back into that so …\n\n\ntibQuaireData %>%\n  left_join(tibParticipants, by = \"ID\") \n\n\n# A tibble: 7,679 x 11\n      ID nCompletions2 completionN item1 item2 item3 item4 item5\n   <int>         <int>       <int> <int> <int> <int> <int> <int>\n 1     1             8           1     4     1     0     5     2\n 2     1             8           2     0     1     1     2     5\n 3     1             8           3     1     3     5     3     5\n 4     1             8           4     5     3     5     4     1\n 5     1             8           5     0     1     2     3     4\n 6     1             8           6     0     1     5     0     2\n 7     1             8           7     2     4     1     1     3\n 8     1             8           8     3     1     4     5     4\n 9     2            30           1     0     0     0     0     5\n10     2            30           2     2     5     1     3     0\n# … with 7,669 more rows, and 3 more variables: gender <chr>,\n#   age <int>, nCompletions <int>\n\nI didn’t actually have to put the by = \"ID\" argument in there as left_join will join every row in tibQuaireData to any row with a matching value in any variable shared between tibQuaireData and tibParticipants and in my little example the only shared variable is ID. OK, that’s looking good.\n\nThere are a full set of join functions: inner_join(), left_join(), right_join() and full_join() that handle the full set of ways you might want to merge to data sets on index variables. They are making three bits of work I’m involved in, each of which involve relational database structures feel very easy.\n\n\ntibQuaireData %>%\n  left_join(tibParticipants, by = \"ID\") %>%\n  ### I will change the order of the variables, \n  ### this order seems better to me\n  ### everything() picks up any variables not already named as we see\n  select(ID, gender, age, nCompletions, nCompletions2, \n         everything()) \n\n\n# A tibble: 7,679 x 11\n      ID gender   age nCompletions nCompletions2 completionN item1\n   <int> <chr>  <int>        <int>         <int>       <int> <int>\n 1     1 F         11            8             8           1     4\n 2     1 F         11            8             8           2     0\n 3     1 F         11            8             8           3     1\n 4     1 F         11            8             8           4     5\n 5     1 F         11            8             8           5     0\n 6     1 F         11            8             8           6     0\n 7     1 F         11            8             8           7     2\n 8     1 F         11            8             8           8     3\n 9     2 M         11           30            30           1     0\n10     2 M         11           30            30           2     2\n# … with 7,669 more rows, and 4 more variables: item2 <int>,\n#   item3 <int>, item4 <int>, item5 <int>\n\nOK, I’m working at the console (well actually, in a file and running the lines each time I finish tweaking them) so now assign that.\n\n\ntibQuaireData %>%\n  left_join(tibParticipants, by = \"ID\") %>%\n  ### I will change the order of the variables, \n  ### this order seems better to me\n  ### everything() picks up any variables not already named as we see\n  select(ID, gender, age, nCompletions, nCompletions2, \n         everything()) -> tibQuaireData\n\n\n\nNow I can do simple things with the data exploring it. I am going to stick to simple things that can be done just using pipes and dplyr.\n\n\n### gender breakdown of age\ntibQuaireData %>%\n  group_by(gender) %>%\n  summarise(n = n(), # gives the number of rows within the gender grouping\n            ### n_distinct() like length(unique()) in classical R, \n            ### gives number of distinct values of ID\n            nParticipants = n_distinct(ID), \n            minAge = min(age), # minimum age within the gender grouping\n            meanAge = mean(age), # ... similarly!\n            sdAge = sd(age),\n            maxAge = max(age))\n\n\n# A tibble: 2 x 7\n  gender     n nParticipants minAge meanAge sdAge maxAge\n* <chr>  <int>         <int>  <int>   <dbl> <dbl>  <int>\n1 F       3765           249     11    14.7  2.35     18\n2 M       3914           251     11    14.6  2.33     18\n\nNow I want to check the range of responses on the items. This introduces the across() selection and within it the starts_with(). They pretty much do what their names suggests. There is also an ends_with() selector. I could also have used item1:item5 as the colon gives all the variables from the left hand side to the right hand side, i.e. here from item1 to item5.\n\n\ntibQuaireData %>%\n  summarise(across(starts_with(\"item\"), # that was the selector, explained above, now we want done with those variables ...\n                   list(min = min, max = max)))\n\n\n# A tibble: 1 x 10\n  item1_min item1_max item2_min item2_max item3_min item3_max\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         0         5         0         5         0         5\n# … with 4 more variables: item4_min <int>, item4_max <int>,\n#   item5_min <int>, item5_max <int>\n\nOK, so the full range of scores was used for every item (doh!). Not the most obvious way to show that so that introduces pivoting. I think the name came from its use in spreadsheets but pivot_longer() and pivot_wider() give lovely control over converting data from wide to long (pivot_longer() … doh!) and pivot_wider() does the opposite.\npivot_longer() and pivot_wider() replaced two earlier components of the dplyr/tidyverse system: gather() and spread() respectively. I found gather() and spread() sometimes hard use and have found pivot_longer() and pivot_wider() much better. If you see examples using gather() and spread() in the web, I would strongly recommend that you ignore them and find more recent work.\n\n\ntibQuaireData %>%\n  pivot_longer(cols = starts_with(\"item\"))\n\n\n# A tibble: 38,395 x 8\n      ID gender   age nCompletions nCompletions2 completionN name \n   <int> <chr>  <int>        <int>         <int>       <int> <chr>\n 1     1 F         11            8             8           1 item1\n 2     1 F         11            8             8           1 item2\n 3     1 F         11            8             8           1 item3\n 4     1 F         11            8             8           1 item4\n 5     1 F         11            8             8           1 item5\n 6     1 F         11            8             8           2 item1\n 7     1 F         11            8             8           2 item2\n 8     1 F         11            8             8           2 item3\n 9     1 F         11            8             8           2 item4\n10     1 F         11            8             8           2 item5\n# … with 38,385 more rows, and 1 more variable: value <int>\n\n### \"name\" and \"value\" are the default names for the variables \n### created by pivoting but you can override them ...\ntibQuaireData %>%\n  pivot_longer(cols = starts_with(\"item\"), \n               names_to = \"item\", \n               values_to = \"score\")\n\n\n# A tibble: 38,395 x 8\n      ID gender   age nCompletions nCompletions2 completionN item \n   <int> <chr>  <int>        <int>         <int>       <int> <chr>\n 1     1 F         11            8             8           1 item1\n 2     1 F         11            8             8           1 item2\n 3     1 F         11            8             8           1 item3\n 4     1 F         11            8             8           1 item4\n 5     1 F         11            8             8           1 item5\n 6     1 F         11            8             8           2 item1\n 7     1 F         11            8             8           2 item2\n 8     1 F         11            8             8           2 item3\n 9     1 F         11            8             8           2 item4\n10     1 F         11            8             8           2 item5\n# … with 38,385 more rows, and 1 more variable: score <int>\n\n### now just group and get min and max\ntibQuaireData %>%\n  pivot_longer(cols = starts_with(\"item\"), \n               names_to = \"item\", \n               values_to = \"score\") %>%\n  group_by(item) %>%\n  summarise(minScore = min(score),\n            maxScore = max(score))\n\n\n# A tibble: 5 x 3\n  item  minScore maxScore\n* <chr>    <int>    <int>\n1 item1        0        5\n2 item2        0        5\n3 item3        0        5\n4 item4        0        5\n5 item5        0        5\n\nMuch easier to read that way around.\nSummarising\nThese have been trivial examples but they’ve introduced some of the fundamental powers of piping using the dplyr package in the R tidyverse. They grew on me and now, as I said at the beginning, they are how I do pretty much all my data manipulation and analyses. There was certainly a learning curve for me and I guess my conversion to piping really happened through 2020. The advantages I have found are:\nthe iterative building of code to do what I want, that I’ve tried to illustrate above, feels a very easy way to write code: you see the output of each step and build things step by step\nI am sure this has meant that I am writing better code, faster, with fewer mistakes\nthe method is very similar to creating graphics with the ggplot2 package and ggplot() so my conversion to working with pipes was perhaps helped by a slightly earlier decision to move from classical R graphics to ggplot() and I find the two complement each other … but I do occasionally forget that the piping operator in ggplot() is “+”, not “%>%”!\nI find code I wrote this way far, far easier to read when I come back to it after time has passed\nI think piping, and particularly pivoting, have really helped me break old “SPSS thinking” and made me comfortable with relational data structures\nThese really have been trivial examples, I’ll be making up pages here illustrating more complicated and much more powerful aspects of piping and the tidyverse over the months ahead.\n\n\n\n",
    "preview": "posts/2021-02-07-why-pipe-why-the-tidyverse/redpipe.png",
    "last_modified": "2021-02-08T11:35:48+00:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4800
  },
  {
    "path": "posts/2021-02-06-how-ive-done-this/",
    "title": "How I've done this",
    "description": "Just documenting how I have created these pages",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-06",
    "categories": [],
    "contents": "\n\nContents\nDistill\nAutomate transfer to my web server\nHow to get images into the preview\n\n\n\n\nDistill\nThese pages have been created using the distill package in R. To quote its authors: “Distill is a publication format for scientific and technical writing, native to the web. Learn more about using Distill at https://rstudio.github.io/distill.”\nThat’s a pretty good summary and the documentation there covers most of the powers of Distill. However, as with much software documentation, I also felt there were things missing that I needed or that would have speeded things up for me. It’s the usual problem that the people who write the code, and many of the people who use it, are very clever and know what they are doing but don’t always remember that we’re not all that clever or that some things had become so familiar to them that they don’t notice they haven’t put those things in the documentation.\nSo Distill is an R package and I suppose it could be run without Rstudio but it’s clearly designed to dovetail with Rstudio. So I installed the package and followed the instructions to create a site at https://rstudio.github.io/distill/#creating-an-article. The system is as they say “a publication format” and they frame it as a tool with which to make a blog. It actually has what I would call “base pages” as well as pages that it creates as “blog posts”. It took me a while to realise that I had to create pages and posts at the command line withdistill::create_article()\nanddistill::create_post() (with some arguments, pretty much all you need to do withdistill::create_post() is to give the post a title: distill::create_post(\"My latest post\")).\nThe package code creates a template page which is basically Rmarkdown, just as happens when you create a new Rmarkdown page in Rstudio. You have all the usual Rmarkdown capacities: “knitting” together blocks of code and blocks of text, embedded figures, inline code in text blocks, TeX/LaTeX equation formatting inline and in equation blocks, tables, citations and reference insertion, tables of contents etc. The help at https://rstudio.github.io/distill goes through the various very nice things the templates can do for you that go a bit beyond what Rmarkdown does:\ncode folding (which I have used throughout) which “folds” code away but allows the reader of the page to open it by just clicking\nnice syntax highlighting in the code blocks pretty much mimicking the syntax highlighting in Rstudio\nyou can change theme with css (so I have a Rblog.css file where I’ve reset the background colour)\nfootnotes\nThere’s a lot that has been done to make some of the things you want for open scientific/academic/research publishing easy that is set in “yaml” (a recursive acronym for “YAML Ain’t Markup Language”) … it’s a header block above the markdown/markup in many markdown/up files. My _site.yml file (as of 6.ii.21) is this:\nname: \"test2\"\ntitle: \"Chris (Evans) R SAFAQ\"\nbase_url: https://www.psyctc.org/R_blog\ndescription: |\n  CE's pages \"blog posts\" about using R\noutput_dir: \"_site\"\nnavbar:\n  logo:\n    image: images/g2_128.gif\n    href: https://www.psyctc.org/Rblog/\n    icon: images/g2_128.gif\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Welcome\"\n      href: \"Welcome.html\"\n    - text: \"About\"\n      href: about.html\n    - text: \"Copyright/permissions\"\n      href: Copyright_and_permissions.html\noutput: \n  distill::distill_article:\n    theme: Rblog.css\ncitations: true\ncookie_consent:\n  style: simple\n  type: express\n  palette: light\n  lang: en\n  cookies_policy: url\nLet’s break that up and comment it some things that are perhaps not totally obvious. (Hm, not sure if you can comment yaml, hm, yes I think you can.)\nThis first block is defining the whole site.\nname: \"test2\" # this is the directory\ntitle: \"Chris (Evans) R SAFAQ\"\nbase_url: https://www.psyctc.org/R_blog # this makes sure the pages index to that URL\ndescription: |\n  CE's pages \"blog posts\" about using R\noutput_dir: \"_site\" # and this is the directory in which the site is compiled by Distill\nDistill automatically creates a simple site structure with a navigation bar at the top. The next bits define that. This first bit just allows you to put an image and icon in. (I could do with a bigger one!)\nnavbar:\n  logo:\n    image: images/g2_128.gif\n    href: https://www.psyctc.org/Rblog/\n    icon: images/g2_128.gif\nAnd this bit puts in links to pages you may have created with distill::create_article() … you have to put these into the navigation bar manually by putting lines like these next ones.\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Welcome\"\n      href: \"Welcome.html\"\n    - text: \"About\"\n      href: about.html\n    - text: \"Copyright/permissions\"\n      href: Copyright_and_permissions.html\noutput: \n  distill::distill_article: # not sure what this does!\n    theme: Rblog.css # this is where I invoke my theme/style css\nThen some very nice convenience powers of the package.\ncitations: true # automatically inserts a nice footnote about citing things on the site\ncookie_consent: # and a nice cookie consent for you\n  style: simple\n  type: express\n  palette: light\n  lang: en\n  cookies_policy: url\nPages created with distill::create_article(), like all Rmarkdown, start with their own yaml blocks and again these allow some nice things.\nAutomate transfer to my web server\nThis took me some hours today to sort out but will save me many hours over the years ahead. I suspect that anyone who is more familiar with git than I was will manage to do this much more quickly than I did. What I’ve done is:\ninstall git on the machine on which I’m running Rstudio and storing the site pages\ntell Rstudio that git is there and is to be used for “version control”, i.e. automatic backing up of all changes that you “commit” keeping a full historical archive of the changes\ncreated a free personal account on gitHub (https://github.com/cpsyctc/) and create a respository in it (https://github.com/cpsyctc/Rblog)\ncreated a personal token which works instead of a password to log into my repository there and makes sure that I’m the only one who can write things to that repository (but anyone can download, “pull” in git terminology, from it) (I have now discovered from https://usethis.r-lib.org/reference/use_github.html that these bits might have bee expedited with a )\nuse that to “push” each new committed update to the pages to that github repository\ninstall git on my web server (pretty sure my ISP had already done this actually)\n[this bit, and the next, are linux specific but could be done, though the terminology is different, in Windoze] create a little shell script on the server that “pulls” a copy of the repository content down to the server from github (git handles the tracking of changes and makes sure that only the minimum necessary material is stored and transferred) and uses rsync to copy things to the web pages (rsync, a bit like git, will only copy changed files)\nput a call into crontab to run that little script every ten minutes\nSo I’ve now got a site/blog developing here as an Rstudio project that I can commit and push to github (where anyone can pull it if they want it) and which then automatically updates my server, at slowest, ten minutes later.\nNow I need to spend a bit more time creating more content but perhaps I’ll browse some other people’s examples first: see https://pkgs.rstudio.com/distill/articles/examples.html.\nHow to get images into the preview\n[Added 7.ii.21] I couldn’t work out how to get an ordinary image into listing of “posts” in the base of the “blog” but, courtesy of Shannon Pileggi of the excellent https://www.pipinghotdata.com/ site she created with Distill, I now have the trick: put the graphic in the directory holding the post and put a line in the yaml pointing to it. So here’s the YAML header of the Rmarkdown file that creates this page:\n---\ntitle: \"How I've done this\"\ndescription: |\n  Just documenting how I have created these pages\nbase_url: https://www.psyctc.org/psyctc/Rblog/\npreview: distill.png\nauthor:\n  - name: Chris Evans\n    url: https://www.psyctc.org/R_blog/\n    affiliation: PSYCTC.org\n    affiliation_url: https://www.psyctc.org/psyctc/\n    orcid_id: 0000-0002-4197-4202\n\ndate: 2021-02-06\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    self_contained: false\n---\nYou see the crucial preview: distill.png (I downloaded the graphic from https://blog.rstudio.com/2020/12/07/distill/distill.png). That’s it: thanks Shannon! Shannon also pointed me to her public github repository at https://github.com/shannonpileggi which has all the code for her blog at https://github.com/shannonpileggi/pipinghotdata_distill … I should have been able to find that without Emailing her.\n\n\n\n",
    "preview": "posts/2021-02-06-how-ive-done-this/distill.png",
    "last_modified": "2021-02-07T16:52:13+00:00",
    "input_file": {},
    "preview_width": 2521,
    "preview_height": 2911
  },
  {
    "path": "posts/2021-01-27-bootstrapspearman/",
    "title": "Bootstrap_Spearman",
    "description": "A quick exploration of bootstrapping a Spearman and why you might, or might not, want it.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [],
    "contents": "\n\nContents\nBootstrapping Spearman correlation coefficient\n\nBootstrapping Spearman correlation coefficient\nTraditionally people used the Spearman correlation coefficient where the sample observed distributions of the variables being correlated were clearly not Gaussian. The logic is that as the Spearman correlation is a measure of correlation between the ranks of the values, the distribution of the scores, population or sample, was irrelevant to any inferential interpretation of the Spearman correlation. By contrast, inference about the statistical (im)probability of a Pearson correlation, or a confidence interval (CI) around an observed correlation, was based on maths which assumed that population values were Gaussian. This is simply and irrevocably true: so if the distributions of your sample scores are way off Gaussian then the p values and CIs for a Pearson can be very misleading.\nThe logic of doing a test of fit to Gaussian on your sample data (univariate and/or bivariate test of fit) is dodgy as if your sample is small then even a large deviation from Gaussian that may give very poor p values and CIs has a fair risk of not being flagged as statistically significantly different from Gaussian and with a large sample, even trivial deviations from Gaussian that would have no effect on the p values and CIs will show as statistically significant. How severe that problem is should really have simulation exploration and I haven’t searched for that but the theoretical/logical problem is crystal clear.\nKeen supporters of non-parametrical statistical methods sometimes argued, reasonably to my mind, that the simple answer was to use non-parametric tests regardless of sample distributions, their opponents argued that this threw away some statistical power: true but the loss wasn’t huge.\nAll this has been fairly much swept away, again, to my mind, by the arrival of bootstrapping which allows you, for anything above a very small sample and for pretty much all but very, very weird distributions, to get pretty robust CIs around observed Pearson correlations regardless of the distributions, population or sample distributions, of the variables.\nBecause of this I now report Pearson correlations with bootstrap CIs around them for any correlations unless there is something very weird about the data. This has all the advantages of CIs over p values and is robust to most distributions.\nHowever, I often want to compare with historical findings (including my own!) that were reported using Spearman’s correlation so I still often report Spearman correlations. However, there is no simple parametric CI for the Spearman correlation and I’m not sure there should be outside the edge case where you have perfect ranking (i.e. no ties on either variable). Then the Spearman correlation is the Pearson correlation of the ranks and I think the approximation of using the parametric Pearson CI computation for the Spearman is probably sensible. I am not at all sure that once you have ties that you can apply the same logic though probably putting in n as the lower of the number of distinct values of the two variables probably gives a safe but often madly wide CI. (“Safe” in the sense that it will include the true population correlation 95% of the time (assuming that you are computing the usual 95% CI)).\nFortunately, I can see reason why the bootstrap cannot be used to find a CI around an observed Spearman correlation and this is what I do now when I am reporting a Spearman correlation.\n\n\nShow code\n\ngetCISpearmanTxt <- function(x, bootReps = 999, conf = .95, digits = 2) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  in format \"rho (LCL to UCL)\"\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  ### digits sets the rounding\n  require(boot) # need boot package!\n  ### now we need a function that \n  spearmanForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i, 1], \n        x[i, 2],\n        method = \"spearman\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = spearmanForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### get observed Spearman correlation and confidence limits as vector\n  retVal <- (c(tmpBootRes$t0,\n           tmpCI$percent[4],\n           tmpCI$percent[5]))\n  ### round that\n  retVal <- round(retVal, digits)\n  ### return it as a single character variable\n  retVal <- paste0(retVal[1],\n                   \" (\",\n                   retVal[2],\n                   \" to \",\n                   retVal[3],\n                   \")\")\n  retVal\n}\n\ngetCISpearmanList <- function(x, bootReps = 999, conf = .95) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  returns a list with items obsCorr, LCL and UCL\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  require(boot) # need boot package!\n  ### now we need a function that \n  spearmanForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"spearman\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = spearmanForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### return observed Spearman correlation and confidence limits as a list\n  retVal <- list(obsCorrSpear = as.numeric(tmpBootRes$t0),\n                 LCLSpear = tmpCI$percent[4],\n                 UCLSpear = tmpCI$percent[5])\n  retVal\n}\n\ngetCIPearsonTxt <- function(x, bootReps = 999, conf = .95, digits = 2) {\n  ### function to give bootstrap CI around bivariate PearsonR\n  ###  in format \"R (LCL to UCL)\"\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  ### digits sets the rounding\n  require(boot) # need boot package!\n  ### now we need a function that \n  pearsonForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"pearson\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = pearsonForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### get observed Spearman correlation and confidence limits as vector\n  retVal <- (c(tmpBootRes$t0,\n           tmpCI$percent[4],\n           tmpCI$percent[5]))\n  ### round that\n  retVal <- round(retVal, digits)\n  ### return it as a single character variable\n  retVal <- paste0(retVal[1],\n                   \" (\",\n                   retVal[2],\n                   \" to \",\n                   retVal[3],\n                   \")\")\n  retVal\n}\n\ngetCIPearsonList <- function(x, bootReps = 999, conf = .95) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  returns a list with items obsCorr, LCL and UCL\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  require(boot) # need boot package!\n  ### now we need a function that \n  pearsonForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"pearson\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = pearsonForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### return observed Spearman correlation and confidence limits as a list\n  retVal <- list(obsCorrPears = as.numeric(tmpBootRes$t0),\n                 LCLPears = tmpCI$percent[4],\n                 UCLPears = tmpCI$percent[5])\n  retVal\n}\n\n\n\n\n\nShow code\n\n### generate some Gaussian and some non-Gaussian data\nn <- 5000 # sample size\nset.seed(12345) # get replicable results\n\nas_tibble(list(x = rnorm(n),\n               y = rnorm(n))) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) %>%\n  summarise(absMinVal = abs(min(value))) %>%\n  pull() -> varMinVal\n\ntibDat %>%\n  mutate(xSquared = x^2,\n         ySquared = y^2,\n         xLn = log(x + varMinVal + 0.2),\n         yLn = log(y + varMinVal + 0.2),\n         xInv = 1/(x + varMinVal + 0.1),\n         yInv = 1/(y + varMinVal + 0.1)) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) -> tibDatLong\n\nggplot(data = tibDatLong,\n       aes(x = value)) +\n  facet_wrap(facets = vars(name),\n             ncol = 2,\n             scales = \"free\",\n             dir = \"v\") +\n  geom_histogram(bins = 100) +\n  theme_bw()\n\n\n\nShow code\n\nggpairs(tibDat)\n\n\n\n\nGood! Got some weird variables there: x and y are Gaussian random variables and uncorrelated then we have their squares, a natural log (after adding enough to the values to avoid trying to get ln(0)) and their inverses (with the same tweak to avoid getting 1/0).\n\n\nShow code\n\noptions(dplyr.summarise.inform = FALSE)\ntibDatLong %>%\n  mutate(id = (1 + row_number()) %/% 2 ,\n         var = str_sub(name, 1, 1),\n         transform = str_sub(name, 2, 20),\n         transform = if_else(transform == \"\", \"none\", transform),\n         transform = ordered(transform,\n                             levels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"),\n                             labels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"))) %>%\n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = var) -> tibDatLong2\n\ntibDatLong2 %>%\n  group_by(transform) %>%\n  select(x, y) %>%\n  summarise(corrS = list(getCISpearmanList(cur_data())),\n            corrP = list(getCIPearsonList(cur_data()))) %>%\n  unnest_wider(corrS) %>%\n  unnest_wider(corrP) %>% \n  pander(justify = \"lrrrrrr\", split.tables = Inf)\n\n\nAdding missing grouping variables: transform\ntransform\nobsCorrSpear\nLCLSpear\nUCLSpear\nobsCorrPears\nLCLPears\nUCLPears\nnone\n-0.03547\n-0.06164\n-0.005442\n-0.02368\n-0.0495\n0.003609\nLn\n-0.03547\n-0.06259\n-0.008897\n-0.01938\n-0.04774\n0.01032\nInv\n-0.03547\n-0.06106\n-0.006073\n0.0001815\n-0.0324\n0.0382\nSquared\n-0.00467\n-0.03253\n0.02274\n-0.009833\n-0.03985\n0.02017\n\nThat’s what we would expect to see: the observed Spearman correlations are the same for the raw data, the ln and inv transformed values (as these are transforms that preserve monotonic, i.e. ranked, ordered, relationships between values while changing the values a lot) but the value is different for the squared transform as that’s not monotonic. The values for the Pearson correlations change with ln and inv transforming as they should as the correlations between the transformed values are not the same as between the raw values. The CIs for the Spearman raw and ln and inv transformed values are not quite identical because the bootstrapping will have produced different bootstrapped samples for each. (I think there’s a way I could have got all three in the same call to boot() but that would have needed a different function to bootstrap.)\nReassuring that all the CIs include zero: you’d hope so really with n = 5000 and uncorrelated raw values.\nNow let’s get a moderately correlated pair of variables.\n\n\nShow code\n\n### generate some Gaussian and some non-Gaussian data\nn <- 5000 # sample size\nset.seed(12345) # get replicable results\n\ntibDat %>% \n  mutate(y = x + y) %>%\n  select(x, y) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) %>%\n  summarise(absMinVal = abs(min(value))) %>%\n  pull() -> varMinVal\n\ntibDat %>%\n  mutate(xSquared = x^2,\n         ySquared = y^2,\n         xLn = log(x + varMinVal + 0.2),\n         yLn = log(y + varMinVal + 0.2),\n         xInv = 1/(x + varMinVal + 0.1),\n         yInv = 1/(y + varMinVal + 0.1)) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) -> tibDatLong\n\nggplot(data = tibDatLong,\n       aes(x = value)) +\n  facet_wrap(facets = vars(name),\n             ncol = 2,\n             scales = \"free\",\n             dir = \"v\") +\n  geom_histogram(bins = 100) +\n  theme_bw()\n\n\n\nShow code\n\nggpairs(tibDat)\n\n\n\n\n\n\nShow code\n\noptions(dplyr.summarise.inform = FALSE)\ntibDatLong %>%\n  mutate(id = (1 + row_number()) %/% 2 ,\n         var = str_sub(name, 1, 1),\n         transform = str_sub(name, 2, 20),\n         transform = if_else(transform == \"\", \"none\", transform),\n         transform = ordered(transform,\n                             levels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"),\n                             labels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"))) %>%\n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = var) -> tibDatLong2\n\ntibDatLong2 %>%\n  group_by(transform) %>%\n  select(x, y) %>%\n  summarise(corrS = list(getCISpearmanList(cur_data())),\n            corrP = list(getCIPearsonList(cur_data()))) %>%\n  unnest_wider(corrS) %>%\n  unnest_wider(corrP) %>% \n  pander(justify = \"lrrrrrr\", split.tables = Inf)\n\n\nAdding missing grouping variables: transform\ntransform\nobsCorrSpear\nLCLSpear\nUCLSpear\nobsCorrPears\nLCLPears\nUCLPears\nnone\n0.666\n0.6485\n0.6833\n0.6906\n0.6755\n0.7049\nLn\n0.666\n0.6496\n0.683\n0.679\n0.6625\n0.6956\nInv\n0.666\n0.6488\n0.6823\n0.279\n0.2516\n0.6728\nSquared\n0.3559\n0.3311\n0.3815\n0.4992\n0.4617\n0.5347\n\nGreat: exactly what we’d expect again.\n\n\n\n",
    "preview": "posts/2021-01-27-bootstrapspearman/bootstrapspearman_files/figure-html5/simulate1-1.png",
    "last_modified": "2021-01-30T18:12:00+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-27-handling-overprinting/",
    "title": "Handling overprinting",
    "description": "This is the first of my blog posts here, about the issue of overprinting and some ways to handle it \nusing R and ggplot().  There's a small spin off topic on the impact on bivariate correlations and on\nlinear regression of discretising continuous variables.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [],
    "contents": "\n\nContents\nOverprinting\nJittering\nUsing transparency\nUsing area to show counts: geom_count()\nHow do those methods work with the original data?\nJittering\nTransparency\nUsing area: geom_count()\n\n\nTangential issue: impact of discretising on relationship between variables\nQuestions and feedback\nTechnical footnote and thanks\nLicensing\n\nOverprinting\nOverprinting is where one point on a graph overlies another. It’s mainly a problem with scattergrams and if you have large numbers of points and few discrete values it can make a plot completely misleading. OK, let’s make up some data.\n\nI am showing the raw R within the Rmarkdown code blocks. I have tried to comment things liberally. Click on “Show code” to see the code.\n\n\nShow code\n\nn <- 5000 # a lot of points means that overprinting is inevitable \nnVals <- 5 # discretising continuous variables to this number of values (below) makes it even more certain\nset.seed(12345) # ensures we get the same result every time \n\n### now generate x and y variables as a tibble\nas_tibble(list(x = rnorm(n),\n               y = rnorm(n))) -> tibDat\n\n### create strong correlation between them by adding x to y (!)\ntibDat %>%\n  mutate(y = x + y) -> tibDat\n\n### now we want to discretise into equiprobable scores so find the empirical quantiles\nvecXcuts <- quantile(tibDat$x, probs = seq(0, 1, 1/nVals))\nvecYcuts <- quantile(tibDat$y, probs = seq(0, 1, 1/nVals))\n\n### now use those to transform the raw variables to equiprobable scores in range 1:5\ntibDat %>%\n  mutate(x5 = cut(x, breaks = vecXcuts, include.lowest = TRUE, labels = FALSE, right = TRUE),\n         y5 = cut(y, breaks = vecYcuts, include.lowest = TRUE, labels = FALSE, right = TRUE)) -> tibDat\n\n\n\nNow let’s have a simple scatterplot.\n\n\nShow code\n\n### use ggplot to generate the simple scattergram for the raw variables\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nThe relationship between the two variables is clear but we don’t know about any overprinting. We can add a loess smoothed regression which clarifies the relationship between the scores but doesn’t resolve the overprinting issue.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth() + # adding the loess smoother\n  theme_bw()\n\n\n\n\nHowever, to really drive home the point about overprinting, if those points are transformed and discretised to five equiprobable scores then things look like this.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) + # use discretised variables instead of raw variables\n  geom_point() +\n  theme_bw()\n\n\n\n\nWhoops: much overprinting as 5000 points have collapsed to 25 visible points on the scattergram but we can’t see how much and no apparent relationship between the variables at all.\nAgain we can add a regression to that plot for amusement and to show that the transform hasn’t removed the relationship. (Has to be a linear regression as the number of distinct points doesn’t allow for loess smoothing.)\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") + # linear regression fit\n  theme_bw()\n\n\n\n\nJittering\nOne way around overprinting it is to jitter the points. Here I have used geom_jitter(width = .2, height = .2) which adds random “jittering” to both x and y values spread across .2 of the “implied bins”. I’ve left the raw data in in blue.\nThere are situations in which you just want jittering on one axis and not the other so you can use geom_jitter(width = .2). Sometimes playing around with width helps get the what seems the best visual fit to the counts.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_jitter(width = .2, height = .2) + # jittered data\n  geom_point(data = tibDat,\n             aes(x = x5, y = y5),\n             colour = \"blue\") +\n  theme_bw()\n\n\n\n\nUsing transparency\nAnother approach is to use transparency. Here you just have the one parameter, alpha and again, sometimes you need to play with different values.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_point(alpha = .01) +\n  theme_bw()\n\n\n\n\nThat’s not working terribly well as we have so many points (n = 5000).\nUsing area to show counts: geom_count()\nAnd another approach, good when values are widely spaced as here, is geom_count().\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_count() +\n  scale_size_area(n.breaks = 10) +\n  theme_bw()\n\n\n\n\nI used a rather excessive number of breaks there but it makes the point.\nHow do those methods work with the original data?\nIn this next set of blocks I’ve applied the same three tricks but to both the raw data and the discretised data.\nJittering\n\n\nShow code\n\n### reshape data to make it easy to get plots side by side using facetting\ntibDat %>%\n  ### first pivot longer \n  pivot_longer(cols = everything()) %>%\n  ### which gets something like this\n  #   # A tibble: 20,000 x 2\n  #    name    value\n  #    <chr>   <dbl>\n  #  1 x      0.586 \n  #  2 y     -0.107 \n  #  3 x5     4     \n  #  4 y5     3     \n  #  5 x      0.709 \n  #  6 y      1.83  \n  #  7 x5     4     \n  #  8 y5     5     \n  #  9 x     -0.109 \n  # 10 y      0.0652\n  # # … with 19,990 more rows\n  ### now get new variables one for x and y\n  mutate(variable = str_sub(name, 1, 1),\n       ### and one for the transform\n       transform = str_sub(name, 2, 2),\n       transform = if_else(transform == \"5\", \"discretised\", \"raw\"),\n       transform = factor(transform,\n                          levels = c(\"raw\", \"discretised\")),\n       ### create an id variable clumping each set of four scores together\n       id = (3 + row_number()) %/% 4) %>%\n  ### so now we can pivot back \n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = variable) -> tibDat2\n### to get this\n# A tibble: 10,000 x 4\n#       id transform        x       y\n#    <dbl> <chr>        <dbl>   <dbl>\n#  1     1 raw          0.586 -0.107 \n#  2     1 discretised  4      3     \n#  3     2 raw          0.709  1.83  \n#  4     2 discretised  4      5     \n#  5     3 raw         -0.109  0.0652\n#  6     3 discretised  3      3     \n#  7     4 raw         -0.453 -2.42  \n#  8     4 discretised  2      1     \n#  9     5 raw          0.606 -1.04  \n# 10     5 discretised  4      2     \n# # … with 9,990 more rows\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = .2)  +\n  facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\n\nAmusing! I’ve put the linear regression fit line on both.\nTransparency\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n  facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_point(alpha = .01) +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\n\nTransparency that works on the right for the discretised data, well works up to a point, is a bit too thin for the raw data. I can’t see that as of now (26.i.21 and ggplot version 3.3.3) that you can map transparency, i.e. alpha to a variable as you can, say, for colour. So to get a side-by-side plot I’m using a different approach. There are various ways of doing this, a useful page seems to be: http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/81-ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page/\n\n\nShow code\n\nggplot(data = filter(tibDat2, transform == \"raw\"), # select just the raw data\n       aes(x = x, y = y)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() -> tmpPlot1\n\n\nggplot(data = filter(tibDat2, transform == \"discretised\"), # select just the raw data\n       aes(x = x, y = y)) +\n  geom_point(alpha = .01) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() -> tmpPlot2\n\n### use ggarrange from the ggpubr package, see the URL for other options\nggpubr::ggarrange(tmpPlot1, tmpPlot2)\n\n\n\n\nUsing area: geom_count()\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n    facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_count() +\n  geom_smooth(method = \"lm\") +\n  scale_size_area(n.breaks = 10) +\n  theme_bw()\n\n\n\n\nI used a rather excessive number of breaks there but it makes the point.\nTangential issue: impact of discretising on relationship between variables\n\n\nShow code\n\nvalRawCorr <- cor(tibDat$x, tibDat$y)\nvalDisc5Corr <- cor(tibDat$x5, tibDat$y5)\n\nvecRawCorrCI <- cor.test(tibDat$x, tibDat$y)$conf.int\nvecDisc5CorrCI <- cor.test(tibDat$x5, tibDat$y5)$conf.int\n\n### or here's another, tidyverse way to do this\n### seems like unnecessary faff except that it makes \n### it so easy to do a micro forest plot (see below)\ngetParmPearsonCI <- function(x, y){\n  ### little function to get parametric 95% CI from two vectors\n  obsCorr <- cor(x, y)\n  tmpCI <- cor.test(x, y)$conf.int\n  return(list(LCL = tmpCI[1],\n              obsCorr = obsCorr,\n              UCL = tmpCI[2]))\n}\ntibDat2 %>%\n  group_by(transform) %>%\n  summarise(pearson = list(getParmPearsonCI(x, y))) %>%\n  unnest_wider(pearson) -> tibCorrs\n### which gives us this\n# tibCorrs\n# # A tibble: 2 x 4\n#   transform     LCL obsCorr   UCL\n#   <fct>       <dbl>   <dbl> <dbl>\n# 1 raw         0.676   0.691 0.705\n# 2 discretised 0.609   0.626 0.643\n\n\n\nThe correlation between the original variables is 0.691 with parametric 95% confidence interval (CI) from 0.676 to 0.705 whereas that between the discretised variables is 0.626 with 95% CI from 0.609 to 0.643 so some clear attenuation there. Micro forest plot of that:\n\n\nShow code\n\nggplot(data = tibCorrs,\n       aes(x = transform, y = obsCorr)) +\n  geom_point() +\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) +\n  ylim(.5, 1) +\n  theme_bw()\n\n\n\n\nYup, that’s a fairly large and clear difference on a y scale from .5 to 1.0. What about the linear regression?\n\n\nShow code\n\n### raw variables\nlm(scale(y) ~ scale(x), data = tibDat)\n\n\n\nCall:\nlm(formula = scale(y) ~ scale(x), data = tibDat)\n\nCoefficients:\n(Intercept)     scale(x)  \n  1.066e-17    6.906e-01  \n\nShow code\n\n### discretised variables\nlm(scale(y5) ~ scale(x5), data = tibDat)\n\n\n\nCall:\nlm(formula = scale(y5) ~ scale(x5), data = tibDat)\n\nCoefficients:\n(Intercept)    scale(x5)  \n -1.726e-17    6.265e-01  \n\nShow code\n\n### or tidyverse way\n### I confess I haven't really got my head aound the broomverse but this is powerful\ntibDat2 %>% \n  group_by(transform) %>%\n  do(broom::tidy(lm(scale(y) ~ scale(x), data = .))) %>%\n  pander(justify = \"llrrrr\", split.tables = Inf)\n\n\ntransform\nterm\nestimate\nstd.error\nstatistic\np.value\nraw\n(Intercept)\n1.066e-17\n0.01023\n1.042e-15\n1\nraw\nscale(x)\n0.6906\n0.01023\n67.51\n0\ndiscretised\n(Intercept)\n-1.726e-17\n0.01102\n-1.566e-15\n1\ndiscretised\nscale(x)\n0.6265\n0.01102\n56.83\n0\n\nOh dear, oh dear! I think I should have known that the standardised regression (slope) coefficients of a simple, two variable linear regression are the Pearson correlations!\nQuestions and feedback\nhttps://www.psyctc.org/psyctc/ web site. In most browsers I think that will open in a new page and if you close it when you have sent your message I think that and most browsers will bring you back here.\nTechnical footnote and thanks\nThis has been created using the distill package in R (and in Rstudio). Distill is a publication format for scientific and technical writing, native to the web. There is a bit more information about distill at https://rstudio.github.io/distill but I found the youtube (ugh) presentation by Maëlle Salmon at https://www.youtube.com/watch?v=Xyc4-bJjdys much more useful than the very minimal notes at that github page. After watching (the first half of) that presentation the github documentation becomes useful.\nLicensing\nAs with most things I put on the web, I am putting this under the Creative Commons Attribution Share-Alike licence.\n\n\n\n",
    "preview": "posts/2021-01-27-handling-overprinting/handling-overprinting_files/figure-html5/scatter1-1.png",
    "last_modified": "2021-01-27T16:31:46+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-27-nudgingonaxes/",
    "title": "Nudging groupings on plot axes",
    "description": "How to nudge categories on an axis of a ggplot plot.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [],
    "contents": "\n\nContents\nHow to “nudge” plots\n\nHow to “nudge” plots\nI can never remember how to do this and keep looking it up. Emily asked me about it so I thought I should crack it and make a file about it to remind myself.\nI’m going to use a little function to get bootstrap confidence intervals around observed means so here’s the code for that.\n\nI am showing the raw R within the Rmarkdown code blocks. I have tried to comment things liberally. Click on “Show code” to see the code.\n\n\nShow code\n\n### function using boot() and boot.ci() from the the boot package to get bootstrap CIs around observed means\ngetCIbootMean <- function(data, ciInt = .95, bootReps = 1000){\n  getMeanForBoot <- function(dat, ind) {mean(dat[ind])} # ind indexes the particular bootstrap sample of vector dat\n  tmpRes <- boot::boot(data, getMeanForBoot, R = bootReps)  # gets the boostrap results\n  tmpCI <- boot::boot.ci(tmpRes, type =  \"perc\")$percent[1,4:5] # gets the percentile method CI\n  return(list(LCL = tmpCI[1],\n              obsMean = tmpRes$t0,\n              UCL = tmpCI[2]))\n}\n# getCIbootMean(1:30) # testing!\n\n\n\nNow let’s get some demonstation data.\n\n\nShow code\n\nn <- 500 # sample size\nset.seed(1245) # get same result every run\ntibble(genderNum = sample(0:1, n, replace = TRUE), # generate gender\n       ageNum = sample(13:17, n, replace = TRUE), # generate age\n       gender = if_else(genderNum == 1, \"F\", \"M\"),\n       score = rnorm(n) + # get randomness unsystematically related to gender or age\n         genderNum*.1*rnorm(n) + # add a simple gender effect\n         ageNum*.1*rnorm(n) + # add a simple age effect\n         (genderNum*(ageNum - 15)*.5*rnorm(n))^2 + # and an interaction\n         20, # make sure values are positive\n       age = as.factor(ageNum)) %>%\n  group_by(age, gender) %>%\n  summarise(mean = list(getCIbootMean(score))) %>%\n  unnest_wider(mean) -> tibDat\n\n\n\nHere’s a crude way to separate things by nudging them on the x axis.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = interaction(age, gender), y = obsMean, colour = gender)) +\n       geom_point() +\n       geom_linerange(aes(ymin = LCL, ymax = UCL))\n\n\n\n\nBut that’s aesthetically and informatively rubbish as it’s not reflecting the grouping. I think what we want is something like this.\n\n\nShow code\n\nvalXdodge = .25 # setting it here makes it easier to try different values when you have multiple geoms you want to dodge\nggplot(data = tibDat,\n       aes(x = age, y = obsMean, colour = gender, group = gender)) + # key thing is that dodging is by the grouping\n  geom_point(position = position_dodge2(width = valXdodge)) +\n  geom_linerange(aes(ymin = LCL, ymax = UCL),\n                 position = position_dodge(width = valXdodge)) \n\n\n\n\nI think “nudge” would have been a much better term than “dodge” but that may be because dodging has a particular meaning in manual printing of photos (where it’s all about changing the darkness of particular areas of the image) which was something I learned about long, long ago.\nI also think the help for dodge is truly awful and is compounded by the fact that dodging works differently depending on the geom you are using (I’ve been lazy and not gotten to the bottom of that but the basic issue is that it works differently for geom_bar() and geom_histogram() where I think it assumes that the x aesthetic is a grouping whereas with geom_point(), geom_linerange() and geom_errorbar() (and probably geom_line()) it needs to be told the grouping on which you are dodging.\nNotwithstanding my grousing, it’s incredibly useful for depicting things. I guess it has something in common with my previous post here https://www.psyctc.org/Rblog/posts/2021-01-27-handling-overprinting/ as both tricks have in common that they actually distort the literal mappings to create mappings that are far more informative and less misleading than the simply “accurate” mapping.\n\n\n\n",
    "preview": "posts/2021-01-27-nudgingonaxes/nudgingonaxes_files/figure-html5/plot1-1.png",
    "last_modified": "2021-01-27T18:34:40+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to CE blog test",
    "description": "Welcome to my blog which I hope will be useful to people using R to analyse data",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [],
    "contents": "\n\n\n\nWelcome to this first attempt to create a blog that should provide easy access to some Rmarkdown files I’m creating that show R tricks I’ve found useful.\nDo contact me if you find issues with them or have questions or suggestions. To do that, use the contact me form on my https://www.psyctc.org/psyctc/ web site. In most browsers I think that will open in a new page and if you close it when you have sent your message I think that and most browsers will bring you back here.\n\n\n\n",
    "preview": "posts/welcome/welcome_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-02-07T09:14:40+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
