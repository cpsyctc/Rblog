[
  {
    "path": "posts/2023-08-09-confidence-intervals-for-quantiles/",
    "title": "Confidence intervals for quantiles",
    "description": "A general coverage of quantiles, the ECDF (Empirical Cumulative Distribution Function) and confidence intervals (CIs) around quantiles",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-09",
    "categories": [
      "confidence intervals",
      "ECDFs",
      "quantiles (quartiles, [per]centiles, deciles)"
    ],
    "contents": "\n\nContents\nIntroduction and background\nQuick introduction to quantiles (skim through if you know this)\nHistogram of samples from Gaussian distribution\nBoxplots of samples from Gaussian distribution\nECDF of samples from Gaussian distribution\n\nConfidence intervals for quantiles\nPlotting the ECDF with quantiles and their CIs\n\n\n\nIntroduction and background\nThis is one of my “background for practitioners” blog posts not one of my geeky ones. It will lead into another about two specific functions I’ve added to the CECPfuns R package. However, this one will cross-link to entries in the free online OMbook glossary.\nQuick introduction to quantiles (skim through if you know this)\nQuantiles are important and very useful but seriously underused. They are important both because they are useful to describe distributions of data but also because they can help us map from an individual’s data to and from collective data: one of the crucial themes in the mental health and therapy evidence base.\nI recommend that you read my post about ECDFs before going further if you don’t feel familiar with quantiles: the ECDF (Empirical Cumulative Density Function) is a great introduction to quantiles.\nQuick terminological point: quantiles are pretty much the same as percentiles, centiles, deciles and the median and lower and upper quartiles are specific quantiles. I’ll come back to that.\nQuantiles tell us about the location of observed values within a distribution of data. Let’s start with: the Gaussian distribution (often called, but a bit misleadingly, the “Normal”. Note the capital letter: it’s not normal in any normal sense of that word!) This facetted plot shows three simulated samples from the Gaussian distribution. The sample sizes are 100, 1,000 and 10,000.\nHistogram of samples from Gaussian distribution\n\n\nShow code\n\nset.seed(12345) # set seed to get the same data regardless of platform and occasion\n# rnorm(10000) %>%\n#   as_tibble() %>%\n#   mutate(n = 10000) %>%\n#   rename(score = value) -> tibGauss10k\n# \n# rnorm(1000) %>%\n#   as_tibble() %>%\n#   mutate(n = 1000) %>%\n#   rename(score = value) -> tibGauss1k\n# \n# rnorm(100) %>%\n#   as_tibble() %>%\n#   mutate(n = 100) %>%\n#   rename(score = value) -> tibGauss100\n# \n# bind_rows(tibGauss100,\n#           tibGauss1k,\n#           tibGauss10k) -> tibGaussAll\n\n### much more tidyverse way of doing that\nc(100, 1000, 10000) %>% # set your sample sizes\n  as_tibble() %>%\n  rename(n = value) %>%\n  ### now you are going to generate samples per value of n so rowwise()\n  rowwise() %>%\n  mutate(score = list(rnorm(n))) %>%\n  ungroup() %>% # overrided grouping by rowwise() and unnest to get individual values\n  unnest_longer(score) -> tibGauss\n\n### get sample statistics\ntibGauss %>%\n  group_by(n) %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score),\n            ### and bootstrap mean (could have used parametric derivation as this is true Gaussian but I couldn't remember it!)\n            CI = list(getBootCImean(score, verbose = FALSE))) %>%\n  unnest_wider(CI) -> tibGaussStats\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 1) +\n  geom_histogram(aes(y = after_stat(density))) +\n  ylim(c(0, .6)) +\n  ylab(\"Count\") +\n  ggtitle(\"Faceted histogram for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000\")\n\n\n\nBoxplots of samples from Gaussian distribution\nHere, taking us to quantiles, are the same data as boxplots overlaid with jittered points for the actual data. This brings us to the simplest quantiles.\n\n\nShow code\n\ntibGauss %>%\n  filter(n == 1000) -> tmpTibGauss\n\ntibGaussStats %>%\n  filter(n == 1000) -> tmpTib\n\nggplot(data = tibGauss,\n       aes(x = 1, y = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 1) +\n  geom_boxplot(notch = TRUE,\n               varwidth = TRUE,\n               fill = \"grey80\") +\n  geom_jitter(width = .35, height = 0,\n              alpha = .05,\n             colour = \"grey40\") +\n  ylab(\"Scores\") +\n  ggtitle(\"Faceted boxplot with jittered observations for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\")\n\n\n\nThe boxplot uses three quantiles describe the box: the median locates the belt and waist across the box, and the quartiles fix the lower and upper limits of the box. What are these quantiles? The median is the score (not necessarily present in the data) that would bisect the data into two equal sized halves so it’s the value such that half the observed values lie below it and half lie above it. The lower quartile is the value such that a quarter of the observed values lie below it and three quarters above it, the upper quartile is the value such that three quarters of the sample lie below it and one quarter above it.\nSo we can now start to look at these names.\nQuantile\nQuartile\nPercentile (a.k.a. centile)\n.25\nlower\n25%\n.50\nmedian\n50%\n.75\nupper\n75%\nNow we get to the ECDF which, like the histogram, violin plot and boxplot is another way to describe a distribution of observed data. This takes us into the richness of quantiles.\nECDF of samples from Gaussian distribution\n\n\nShow code\n\ntibGaussStats %>%\n  select(n, min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibGaussStatsLong\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             ncol = 1) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibGaussStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            nudge_x = -.04,\n            hjust = 1) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"Faceted ECDF plot for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nI’ve changed to faceting by rows here instead of columns to give a better spread on the plot. The ECDF plots on the y axis the proportion of the sample scoring below the value on the x axis.\nA few pretty obvious comments on the impact of sample size when in the classical model of random sampling from an infinitely large population. These impacts are visible in all those distribution plots above.\nIf the possible scores are genuinely continuous then the distributions are less “lumpy” the larger the sample. (Actually not possible to see this in the boxplot but in histogram it’s very clearly in the shift from a set of vertical bars to a smooth distribution. It’s less obvious in the violin plot as that is a smoothed distribution plot and in the ECDF it shows in the steps in the line that are almost invisible when the sample size is 10,000.)\nAs the sample sizes get bigger, if, as with the Gaussian distribution, the possible scores actually range from -Infinity to +Infinity then the limits, i.e. the minimum (quantile zero roughly) and the maximum (quantile 1.0) move out as the sample size goes up as the larger sample gets more chance of including the rare but not impossible extreme values.\nAs the sample sizes get bigger the observed quantiles get closer to their population values. That can be seen in this next table. This shows\nname = name of the quantile\nproportion = the proportion of that quantile\nthe value in the infinitely large population (know from the maths)\nn100 = the observed value for that quantile in this sample of n = 100\nn1000 = the observed value for that quantile in this sample of n = 1,000\nn10000 = the observed value for that quantile in this sample of n = 10,000\n\n\n\nShow code\n\ntibGaussStats %>% \n  select(n, lquart, median, uquart) %>%\n  pivot_longer(cols = -n) %>%\n  mutate(value = round(value, 4),\n         proportion = case_when(\n                              name == \"lquart\" ~ .25,\n                              name == \"median\" ~ .5,\n                              name == \"uquart\" ~ .75),\n         popVal = qnorm(proportion),\n         popVal = round(popVal, 4)) %>%\n  pivot_wider(names_from = n, names_prefix = \"n\", values_from = value) %>%\n  flextable() %>%\n  autofit()\n\nnameproportionpopValn100n1000n10000lquart0.25-0.6745-0.5901-0.6359-0.6652median0.500.00000.48370.0080-0.0019uquart0.750.67450.90040.64530.6576\n\nIt can be seen there that the observed values for the quantiles get closer to the population values the larger the sample.\nConfidence intervals for quantiles\nSo, as we can see in the above any observed quantile value, like any sample statistic, will have a different value for the next sample assuming any real sampling process, whether truly random (only in simulations in my view) or not. That means that, like any sample statistic, any observed quantile value can be given a confidence interval around it and this CI will be narrower the larger the sample size.\nThis brings us to the fact that there are various ways of computing this confidence interval. The R package quantileCI gives three methods including a bootstrap method. They’re all non-parametric, i.e. not making assumptions about the shape of the distribution of the scores for which you computed the quantiles. From a bit of reading led by Michael Höhle’s R package quantileCI (https://github.com/hoehleatsu/quantileCI) it seems to me that the Nyblom method is probably best (and the differences between the methods unlikely to cause us any headaches with typical MH/therapy score data). As ever the confidence interval gives a range around the observed value for a sample statistic that should include the population value in a given proportion of samples. The proportion usually used is 95%, i.e. a 95% confidence interval. Here they are for our sample data.\n\n\nShow code\n\ntibGauss %>%\n  group_by(n) %>%\n  summarise(lquartCI = list(quantileCI::quantile_confint_nyblom(score, .25)),\n            medianCI = list(quantileCI::quantile_confint_nyblom(score, .5)),\n            uquartCI = list(quantileCI::quantile_confint_nyblom(score, .75))) %>%\n  unnest_wider(lquartCI, names_sep = \":\") %>%\n  unnest_wider(medianCI, names_sep = \":\") %>%\n  unnest_wider(uquartCI, names_sep = \":\") %>%\n  rename(lquartLCL = `lquartCI:1`,\n         lquartUCL = `lquartCI:2`,\n         medianLCL = `medianCI:1`,\n         medianUCL = `medianCI:2`,\n         uquartLCL = `uquartCI:1`,\n         uquartUCL = `uquartCI:2`) %>%\n  mutate(lquartCI = paste0(round(lquartLCL, 2), \" to \", round(lquartUCL, 2)),\n         medianCI = paste0(round(medianLCL, 2), \" to \", round(medianUCL, 2)),\n         uquartCI = paste0(round(uquartLCL, 2), \" to \", round(uquartUCL, 2))) %>%\n  left_join(tibGaussStats, by = \"n\") %>%\n  mutate(lquart = round(lquart, 2),\n         median = round(median, 2),\n         uquart = round(uquart, 2)) %>%\n  select(-c(min, mean, sd, max:UCLmean)) -> tmpTib\n\ntmpTib %>%\n  select(n, lquart, lquartCI, median, medianCI, uquart, uquartCI) %>%\n  flextable() %>%\n  autofit()\n\nnlquartlquartCImedianmedianCIuquartuquartCI100-0.59-0.88 to -0.310.48-0.01 to 0.610.900.71 to 1.441,000-0.64-0.72 to -0.550.01-0.06 to 0.080.650.58 to 0.7410,000-0.67-0.69 to -0.640.00-0.03 to 0.020.660.63 to 0.69\n\nThat shows the observed values for the quartiles and the median for the three samples. It’s very clear that the widths of the intervals get tighter as the sample size increases.\nPlotting the ECDF with quantiles and their CIs\nI’ve added the function plotQuantileCIsfromDat to the CECPfuns R package package. This creates these plots below which I like. They show any requested quantiles, here the quartiles and median, with the ECDF from the data, and plots the confidence intervals for those quantiles. Here are the plots for those three quantiles and for the the three simulated samples that we’ve been using so far.\n\n\nShow code\n\ntibGauss %>% \n  filter(n == 100) %>%\n  select(score) %>% \n  pull() -> tmpVec\n\nplotQuantileCIsfromDat(tmpVec, vecQuantiles = c(.25, .5, .75), addAnnotation = FALSE, printPlot =  FALSE, returnPlot = TRUE) -> tmpPlot100\n\ntibGauss %>% \n  filter(n == 1000) %>%\n  select(score) %>% \n  pull() -> tmpVec\n\nplotQuantileCIsfromDat(tmpVec, vecQuantiles = c(.25, .5, .75), addAnnotation = FALSE, printPlot =  FALSE, returnPlot = TRUE) -> tmpPlot1000\n\ntibGauss %>% \n  filter(n == 10000) %>%\n  select(score) %>% \n  pull() -> tmpVec\n\nplotQuantileCIsfromDat(tmpVec, vecQuantiles = c(.25, .5, .75), addAnnotation = FALSE, printPlot =  FALSE, returnPlot = TRUE) -> tmpPlot10000\n\nlibrary(patchwork)\n### standardise the x axis ranges\ntmpPlot100 + \n  xlim(c(-4, 4)) -> tmpPlot100\ntmpPlot1000 + \n  xlim(c(-4, 4)) -> tmpPlot1000\ntmpPlot10000 + \n  xlim(c(-4, 4)) -> tmpPlot10000\n\ntmpPlot100 /\n  tmpPlot1000 /\n  tmpPlot10000\n\n\n\nUnsurprisingly those plots show that the three quantiles are well separated with non-overlapping confidence intervals even for n = 100 and they show how the confidence intervals tighten as the n increases.\nI hope this was a fairly clear introduction to putting confidence intervals around observed quantiles.\n\n\n\n",
    "preview": "posts/2023-08-09-confidence-intervals-for-quantiles/confidence-intervals-for-quantiles_files/figure-html5/plotGaussian1-1.png",
    "last_modified": "2023-08-25T14:29:12+02:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2023-08-11-mapping-individual-scores-to-referential-data-using-quantiles/",
    "title": "Mapping individual scores to referential data using quantiles",
    "description": "Exploring the use of quantiles and their confidence intervals, and ECDFs to map individuals' scores to referential data.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-09",
    "categories": [
      "confidence intervals",
      "ECDFs",
      "quantiles (quartiles, [per]centiles, deciles)",
      "mapping to referential data"
    ],
    "contents": "\n\nContents\nMapping from individual scores to population distributions\nMapping to non-help-seeking referential scores\nMapping to help-seeking referential scores\n\nWhat has this given us that the CSC paradigm doesn’t?\nCan we ignore sociodemographic variables?\n\n\nThis builds on two earlier posts here:\n* What is an empirical cumulative distribution function? and\n* Confidence intervals for quantiles\nEven if you are very familiar with ECDFs, quantiles and confidence intervals it may be worth at least scan reading those before reading this post.\nMapping from individual scores to population distributions\nOne huge issue in MH/therapy work is that we are very interested in individuals but we also need to be aware of aggregated data: to be able to take a population health and sometimes a health economic viewpoint on what our interventions offer in aggregate. There are no simple and perfect ways to be able to think about both individual and about aggregated data and no perfect ways to map individual data to large dataset data. 1\nOne way of mapping an individual client’s score to referential data is the famous “Clinical Significant Change” model of the late Neil Jacobson and his colleagues. That creates a cutting point score and hence a binary categorisation: whether any individual score is more likely to be in the help-seeking (“clinical”) score distribution or in the non-help-seeking distribution. There are many issues with that mapping, some of which I will explore in future posts here, however it’s rightly been hugely important as one step away from just reporting on the effectiveness of interventions solely in terms of parametric or non-parametric statistical analysis of whether the before/after changes were “statistically significant” or of reporting their effect sizes.\nHow do quantiles offer a more nuanced way of mapping individual scores to referential dataset distributions?\nIf we have a large (enough) non-help-seeking dataset of scores we can use it to give us quantiles for those scores and we can take as a refential mapping. Here’s a real example, n = 1,666.\n\n\nShow code\n\nvecQuantiles <- c(.05, .1, .15, .2, .3, .4, .5, .6, .7, .8, .9, .95)\ntmpVecScores <- na.omit(tmpScores$score_OM_T1)\nplotQuantileCIsfromDat(tmpVecScores, vecQuantiles = vecQuantiles, ci = .95, method = \"N\", type = 8, \n                       addAnnotation = FALSE, printPlot = FALSE, returnPlot = TRUE,\n                       titleText = \"ECDF of CORE-OM scores with 95% CIs around the quantiles\",\n                       subtitleText = paste0(\"Quantiles at \",\n                                             convertVectorToSentence(vecQuantiles))) -> plotCOREOM\nplotCOREOM\n\n\n\n\n\nShow code\n\nplotCOREOM + \n  theme_bw() + \n  ggtitle(\"\", subtitle = \"\") + \n  xlim(c(0, 2.2)) + \n  theme(aspect.ratio = 1) + \n  ylab(\"Prob.\") + \n  xlab(\"Score\") + \n  annotate(geom = \"text\", \n           x = 0, y = .95, \n           size = 75, \n           label = \"OMbk\", \n           colour = \"blue\", \n           hjust = 0, vjust = 1, \n           family = \"DejaVu Sans Mono\")\n\n\nWe can see that even with n = 1,666 the CIs of the quantiles are not very tight, more so for the higher quantiles.\nHere are those quantiles and their 95% CIs, and the widths of the CIs, as a table.\n\n\nShow code\n\nround3 <- function(x){\n  round(x, 3)\n}\ngetCIforQuantiles(tmpVecScores, vecQuantiles) %>%\n  select(-c(n, nOK, nMiss)) %>%\n  mutate(CIwidth = UCL - LCL) %>%\n  mutate(across(quantile:CIwidth, round3)) -> tmpTibQuantiles\n\ntmpTibQuantiles %>%\n  flextable() %>%\n  autofit()\n\nprobquantileLCLUCLCIwidth0.050.2060.1760.2350.0590.100.2650.2650.2940.0290.150.3530.3240.3820.0590.200.4120.3820.4410.0590.300.5290.5000.5590.0590.400.6470.6330.6760.0440.500.7500.7350.7940.0590.600.8820.8530.9170.0640.701.0591.0291.0880.0590.801.2351.2061.2940.0880.901.5291.4711.6180.1470.951.8531.7651.9120.147\n\nThat confirms that the CIs of the .2 and .3 quantiles (20% and 30% percentiles) touch but don’t overlap so we seem to on reasonable grounds to say that we can map any score for, say, a new client asking for help, to that referential data to a these quantiles/percentiles.\n\n\nShow code\n\n(c(0, vecQuantiles, 1)) %>% \n  as_tibble() %>%\n  rename(prob = value) %>%\n  mutate(lwr = 100 * prob,\n         upr = lead(lwr),\n         lwr = paste0(lwr, \"th\"),\n         upr = paste0(upr, \"th\"),\n         slot = paste0(\"Between the \", lwr, \" and the \", upr, \"percentiles\"),\n         slot = if_else(lwr == \"0th\", \"Under the 5th percentile\", slot),\n         slot = if_else(upr == \"100th\", \"Above the 95th percentile\", slot),\n         slotN = row_number()) %>%\n  filter(prob < 1) %>% # trim off the spurious top row created by the lead()\n  left_join(tmpTibQuantiles, by = \"prob\") %>%\n  select(slotN, slot, quantile) %>%\n  mutate(uprQuantile = lead(quantile),\n         ### fix the end points with the minimum and maximum possible scores for the measure\n         quantile = if_else(is.na(quantile), 0, quantile),\n         uprQuantile = if_else(is.na(uprQuantile), 4, uprQuantile)) %>%\n  rename(lwrQuantile = quantile) -> tibQuantileSlots\n\ntibQuantileSlots %>%\n  flextable() %>%\n  autofit()\n\nslotNslotlwrQuantileuprQuantile1Under the 5th percentile0.0000.2062Between the 5th and the 10thpercentiles0.2060.2653Between the 10th and the 15thpercentiles0.2650.3534Between the 15th and the 20thpercentiles0.3530.4125Between the 20th and the 30thpercentiles0.4120.5296Between the 30th and the 40thpercentiles0.5290.6477Between the 40th and the 50thpercentiles0.6470.7508Between the 50th and the 60thpercentiles0.7500.8829Between the 60th and the 70thpercentiles0.8821.05910Between the 70th and the 80thpercentiles1.0591.23511Between the 80th and the 90thpercentiles1.2351.52912Between the 90th and the 95thpercentiles1.5291.85313Above the 95th percentile1.8534.000\n\nMapping to non-help-seeking referential scores\nSo we can say that the size of our referential dataset has given us those 13 discriminable slots into which we can map any score. For example above 1.86 is scoring above the 95% centile from this non-help-seeking referential dataset, a score between 1.53 and 1.85 is scoring above the 90% percentile but not above the 95% percentile. We can do the same for the last scores for the clients and say that someone whose score fell from 1.86 to .7 has moved from above the 95% percentile to between the 40th and 50th.\nMapping to help-seeking referential scores\nIf we wanted to, and had a large referential dataset of initial scores for help-seeking clients we could do the same to map scores to that dataset to get an idea where a client stands in that distribution of initial scores: are they at the upper end (severely affected in the terms of the measure) or low. That enables us to say where a client’s first score lay in that, so a score of 1.9 is above the 95% percentile from this non-help-seeking dataset but might be below the 50% percentile from the help-seeking dataset. Ideally we need quite large datasets of initial scores from services to build that referential data. UK IAPT initial scores?!\nWhat has this given us that the CSC paradigm doesn’t?\nThis is a rather different approach from the CSC: rather than dichotomising the score distribution it allows us to translate any score into a quantile on non-help-seeking referential data, if we have that, and to a quantile on referential help-seeking data.\nLike the CSC this is a translation that is independent, in principle, of the measure used.\nIt allows us to say much more than “above/below the CSC”.\nAssuming we have both non-help-seeking and help-seeking referential data and that we have repeated measures across an intervention we can map change\nCan we ignore sociodemographic variables?\nOne thing to watch, as with any consideration of MH measure scores, is whether sociodemographic variables have sufficient impact on score distributions that we should consider those variables when creating mappings, the class (at this point in our history of dataset creation), is considering gender as a binary variable.\nLet’s go back to plots to explore this.\n\n\nShow code\n\ntmpScores %>%\n  rename(COREOMscore = score_OM_T1,\n         Gender = gender) %>%\n  filter(!is.na(Gender) & !is.na(COREOMscore)) -> tmpScores2\n\ntmpScores2 %>%\n  summarise(median = median(COREOMscore)) %>%\n  pull() -> tmpMedian\n\nggplot(data = tmpScores2,\n       aes(y = COREOMscore, x = Gender, fill = Gender)) +\n  geom_boxplot(notch = TRUE,\n               varwidth = TRUE) +\n  geom_hline(yintercept = tmpMedian) +\n  ylim(c(0, 4)) +\n  ylab(\"CORE-OM scores\")\n\n\nShow code\n\ntmpScores2 %>%\n  filter(Gender == \"women\") %>%\n  select(COREOMscore) %>%\n  pull() -> tmpVecScoresF\n\ntmpScores2 %>%\n  filter(Gender == \"men\") %>%\n  select(COREOMscore) %>%\n  pull() -> tmpVecScoresM\n\n\nSo let’s apply the highly inappropriate between groups t-test to test that strong graphic evidence of a gender effect on the CORE-OM scores.\n\n\nShow code\n\nt.test(tmpVecScoresF, tmpVecScoresM)\n\n\n    Welch Two Sample t-test\n\ndata:  tmpVecScoresF and tmpVecScoresM\nt = 3.9475, df = 1517.5, p-value = 8.258e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.04939864 0.14697956\nsample estimates:\nmean of x mean of y \n0.8928289 0.7946398 \n\nA more appropriate non-parametric Mann-Whitney (a.k.a. Wilcoxon) test.\n\n\nShow code\n\nwilcox.test(tmpVecScoresF, tmpVecScoresM)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  tmpVecScoresF and tmpVecScoresM\nW = 373569, p-value = 4.675e-05\nalternative hypothesis: true location shift is not equal to 0\n\nAnd now go back to the means but instead of using the t-test with its assumption that the population distributions are Gaussian, let’s use the robust, non-parametric bootstrap CIs around the observed means.\n\n\nShow code\n\ntmpScores2 %>%\n  mutate(Gender = \"all\") -> tmpTib\n\n\nbind_rows(tmpScores2,\n          tmpTib) -> tmpTibGenderAll\n\nset.seed(12345)\ntmpTibGenderAll %>%\n  group_by(Gender) %>%\n  summarise(CI = list(getBootCImean(COREOMscore))) %>%\n  unnest_wider(CI) %>%\n  mutate(Gender = ordered(Gender,\n                          levels = c(\"men\", \"all\", \"women\"))) %>%\n  arrange(Gender) -> tmpTibMeanByGender\n\ntmpTibMeanByGender %>%\n  mutate(across(obsmean:UCLmean, round3)) %>%\n  flextable() %>%\n  autofit()\n\nGenderobsmeanLCLmeanUCLmeanmen0.7950.7620.832all0.8530.8280.877women0.8930.8600.925\n\nOK, that’s clear again. (With this sample size it would be very bizarre if it weren’t just confirming the tests and giving us confidence intervals: indicators of the likely imprecision from the sample size.)\nGoing to plots as I always like to, here’s a forest type plot of the means and 95% CIs. Suggests a very impressive gender effect.\n\n\nShow code\n\ntmpTibMeanByGender %>%\n  filter(Gender == \"all\") %>%\n  select(obsmean) %>%\n  pull() -> tmpMeanAll\n\nggplot(data = tmpTibMeanByGender,\n       aes(x = Gender, y = obsmean)) +\n  geom_point() +\n  geom_linerange(aes(ymin = LCLmean, ymax = UCLmean)) +\n  geom_hline(yintercept = tmpMeanAll)\n\n\nShow code\n\n  # ylim(c(0, 4)) +\n  ylab(\"CORE-OM score\")\n\n$y\n[1] \"CORE-OM score\"\n\nattr(,\"class\")\n[1] \"labels\"\n\nShow code\n\n# getBootCIgrpMeanDiff(COREOMscore ~ Gender, tmpScores2)\n\n\nOf course when we put it in the context of the full range of possible CORE-OM scores and jitter the actual scores underneath the means and their CIs it’s less impressive.\n\n\nShow code\n\nggplot(data = tmpTibGenderAll,\n       aes(x = Gender, y = COREOMscore, colour = Gender)) +\n  geom_jitter(width = .25,\n              alpha = .1) +\n  geom_point(data = tmpTibMeanByGender,\n             aes(x = Gender, y = obsmean),\n             size = 2) +\n  geom_linerange(data = tmpTibMeanByGender,\n                 inherit.aes = FALSE,\n                 aes(x = Gender, ymin = LCLmean, ymax = UCLmean, colour = Gender),\n                 linewidth = 1.5) +\n  geom_hline(yintercept = tmpMeanAll) +\n  \n  ylim(c(0, 4)) +\n  ylab(\"CORE-OM score\")\n\n\n\nOK, so we have a definite systematic effect of gender on the central location of the scores and it is incredibly unlikely that it arose by sampling vagaries (assuming random sample, i.e. no biasing effects of gender in the recruitment). However, that’s all about the central location of the scores by gender whether using the median or the mean. Now we come to how the ECDF helps tell us more than these simple central location analyses.\n\n\nShow code\n\nggplot(data = tmpScores2,\n       aes(x = COREOMscore, colour = Gender)) +\n  stat_ecdf() +\n  ylab(\"Probability\") +\n  xlab(\"CORE-OM score\")\n\n\n\nI always have to remind myself that the fact that ECDF line for the women is under that for the men is because the women are tending to score higher generally than the men so the quantiles for the women tend to be higher (to the right of) those for the men. A non-parametric test with the glorious name of the Kolmogorov-Smirnov test is a formal test of whether the largest absolute vertical distance between the lines is larger than you would expect to have happened had Gender had no relationship with score in the population and this just a chance sampling vagary.\n\n\nShow code\n\nks.test(tmpVecScoresF, tmpVecScoresM)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  tmpVecScoresF and tmpVecScoresM\nD = 0.11024, p-value = 0.0001138\nalternative hypothesis: two-sided\n\nGiven that we’ve seen the effects of gender on central location we’re not surprised to see that this is highly statistically significant.\nNow we can finally come to the question of whether this matters in terms of mapping scores to quantiles now taking gender into account.\n\n\nShow code\n\ngetCIforQuantiles(tmpVecScoresF, vecQuantiles = vecQuantiles) %>%\n  mutate(Gender = \"F\") -> tmpQuantilesF\n\ngetCIforQuantiles(tmpVecScoresM, vecQuantiles = vecQuantiles) %>%\n  mutate(Gender = \"M\") -> tmpQuantilesM\n\nplotQuantileCIsfromDat(tmpVecScoresF, vecQuantiles = vecQuantiles, addAnnotation = FALSE, printPlot = FALSE, returnPlot = TRUE) -> tmpPlotF\nplotQuantileCIsfromDat(tmpVecScoresM, vecQuantiles = vecQuantiles, addAnnotation = FALSE, printPlot = FALSE, returnPlot = TRUE) -> tmpPlotM\n\ntmpPlotF / tmpPlotM\n\n\nShow code\n\ntmpPlotM +\n  geom_text(aes(x = quantile, y = prob, label = prob),\n            nudge_y = .05,\n            angle = 85,\n            vjust = 0) +\n  geom_linerange(data = tmpQuantilesF,\n             aes(x = quantile, ymin = 0, ymax = prob),\n             colour = \"red\") +\n  stat_ecdf(data = filter(tmpScores2, Gender == \"women\"),\n            aes(x = COREOMscore), \n            colour = \"red\")\n\n\n\n\n\nShow code\n\nby <- join_by(between(firstScore, lwrQuantile, uprQuantile))\n\nleft_join(tibChangeScores, tibQuantileSlots, by) %>%\n  select(-ends_with(\"Quantile\")) %>%\n  rename(firstSlotN = slotN,\n         firstSlot = slot) -> tmpTib\n\nby <- join_by(between(lastScore, lwrQuantile, uprQuantile))\n\nleft_join(tmpTib, tibQuantileSlots, by) %>%\n  select(-ends_with(\"Quantile\")) %>%\n  rename(lastSlotN = slotN,\n         lastSlot = slot) -> tibQuantileChanges\n\nggplot(data = tibQuantileChanges,\n       aes(x = firstSlotN, y = lastSlotN)) +\n  geom_count() +\n  geom_abline(intercept = 0, slope = 1) +\n  ylim(c(6, 13)) +\n  scale_x_continuous(name = \"First quantile\",\n                     breaks = tibQuantileSlots$slotN,\n                     labels = tibQuantileSlots$slot,\n                     limits = c(6, 13)) +\n  scale_y_continuous(name = \"Last quantile\",\n                     breaks = tibQuantileSlots$slotN,\n                     labels = tibQuantileSlots$slot,\n                     limits = c(6, 13)) +\n  theme(axis.text.x = element_text(angle = 70,\n                                   hjust = 1),\n        aspect.ratio = 1)\n\n\n\n\nI’m avoiding the word “population” here and using “large dataset” as I think we’re pretty much never in possession of true random samples from defined populations in our work so I’m trying to step away that the whole “random-sample-from-population” model doing. The maths of the “random-sample-from-population” is the best we have when we generalise from small datasets, I’m not trying to replace that, just to stop us overvaluing what we get from the model. More on that below.↩︎\n",
    "preview": "posts/2023-08-11-mapping-individual-scores-to-referential-data-using-quantiles/mapping-individual-scores-to-referential-data-using-quantiles_files/figure-html5/realDat1-1.png",
    "last_modified": "2023-08-25T14:29:34+02:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2023-08-25-making-a-working-shiny-server/",
    "title": "Making a working shiny server",
    "description": "This is very much work in progress and will probably always be that way.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-09",
    "categories": [
      "Geeky stuff",
      "shiny",
      "R packages",
      "R tricks",
      "R programming"
    ],
    "contents": "\n\nContents\nBackground\nSo what is shiny?\nWhy am I doing this?\nCreating a server\nSome configuring the server\nBackup of the apps\nLogging the use of the server\n\nMy workflow for writing shiny apps\nMy learning curve about programming shiny apps\nStructure of the shiny project\nHow I now think about each app\nReactivity\nKey parts of shiny app code that enable reactivity\nInputs\nReactive data/objects\nOutputs\n\n\n\nLogging server use\nUsing the shiny default logging\nLogging by building that into the apps\n\n\n\n\n\nBackground\nThere is a huge amount of information about shiny on the internet. The place to start is definitely https://shiny.posit.co/. I’m not trying to replace any of the official information at posit.co (formerly Rstudio.com, much still in transition from that site to the new one), I’m just trying to document what I am learning as I struggle to get my head around shiny. This is something that I started trying several years ago and I did create a usable shiny app back then but realised that if I was going to provide something useful I needed to run my own shiny server. That I finally managed in early August 2023 and I am proving a slow learner though I suspect I am not alone in finding the shiny learning curve quite challenging. I hope this will help.\nSo what is shiny?\nWell it’s actually “just” an R package with the crucial power that it enables you to embed R inside a server so that you can provide interactive apps. See https://shiny.psyctc.org/ to see my expanding list of such apps. Clearly that means that shiny is a pretty impressive and no doubt huge lump of clever code! There are two versions: an open source one and a commercial one (or it may be various commercial versions, they’re all completely beyond my budget!) However, the open source version is true open source software: anyone can download and use it without paying any licence fees (as long as you comply with the licence, which is easy for me!)\nWhy am I doing this?\nFrom time to time over the last few weeks I’ve asked myself that question as I’ve probe depths of my stupidity failing to get things working! My simple hope is that the apps I create will be useful, in principle to anyone but particularly for practitioners and researchers working in mental health (MH) and psychological therapies. That will probably mean that the apps will fall into three groups:\nexplanatory/teaching ones illustrating methodological, mostly statistical or psychometric, issues, my first example is onemodelling screening\nones probably more useful to researchers than practitioners, perhaps requiring a bit of methodological savvy to know what they even do or mean! An early example is one to give a confidence interval around an observed coefficient alpha. (That’s a descendent of the first shiny app I ever created!)\nones that I hope will help practitioners explore their own and colleagues’ data. It’s not a good example but one computing and plotting quantiles for a distribution of, say, starting questionnaire scores is a step in that direction. These are going to be the most challenging to create so will take time to grow.\nCreating a server\nIf you only want to create shiny apps to use on your own machine you don’t need a server, you can just write your apps and run them on your machine. The easiest way is probably to do it in Rstudio. However, I can’t see any reason anyone would do that: you’d just write the app as an ordinary R program!\nSo you want to offer your apps to others, in theory you could do that by leaving your shiny apps running on one machine, it might be one that was doing other things too, as long as the machine is visible in your local network anyone on the network can use your apps there. However, what I want is for my apps to be usable for anyone on the internet. For that I needed a server sitting on a publicly visible IP address (the numeric address system that identifies machines on the internet) and it would also need a human readable address. So my server is shiny.psyctc.org currently on the IPv4 address 46.235.229.183 and the IPv6 address 2a00:1098:a6::1. This is actually a “virtual machine” running on a shared machine hosted by my excellent ISP, Mythic Beasts. I’ve been with them for 15 years now for my web servers (CORE, my non-CORE work and my personal site). They also run Jo-anne’s site and our Email. For a while in the last few years I had shiny running alongside my web servers on another VM but I worried that it is probably easy for a malevolent person to overload a shiny server and bring the machine hosting it to a halt so I gave that up in favour of the shiny server sitting on its own VM.\nFor now this server is Mythic Beasts’ “VPS 4” VMs: 2 CPU cores, 4Gb RAM and a 1Gb SSD drive. Looking at /proc/cpuinfo tells me that the cores are “Intel Xeon E312xx (Sandy Bridge)” running 2099.99 MHz (where does that number come from?!) and 16384Kb of onboard cache. It’s not super responsive but some of that may be because my broadband up in the Alps is pretty slow. I’ll watch the responsiveness as, I hope, it gets used more.\nSome configuring the server\nLike me Mythic Beasts try to keep to open source software so the server is running Debian “bullseye” currently the “oldstable” release. Mythic Beasts saved me a lot of work setting it up with R and shiny but those were from the Debian release version of R so I had to tweak the /etc/apt/sources.list.d to add a file, chris.list, saying:\ndeb http://cloud.r-project.org/bin/linux/debian bullseye-cran40/\nto get R up to 4.3.1 (now).\nTo save myself updating the packages daily I created a little bash script /home/chris/updatePackages:\n#!/bin/bash\nnowDate=$(date +%F)\nR CMD BATCH /home/chris/updatePackages.R $nowDate.Rout\nmail -s \"R update on shiny server $nowDate\" shiny@psyctc.org < $nowDate.Rout\nand this file, /home/chris/updatePackages.R\nlocal({\n  r <- getOption(\"repos\")\n  r[\"CRAN\"] <- \"https://cloud.r-project.org/\"\n  options(repos = r)\n})\n\ndate()\nupdate.packages(ask=FALSE,checkBuilt=TRUE)\nset permissions on those files to 700 (i.e. only usable in any way at all to the owner, a bit of security). Then I added this line to crontab:\n0 4 * * * /home/chris/updatePackages\nso that cron (i.e. automatic) script gets R running, updates all the R packages that have new versions and Emails me the transcript. That crontab line gets that done every day at 04.00.\nBackup of the apps\nMythic Beasts back up the entire VM daily so if, heaven forbid, the server dies or gets killed, they can always restore it to a state it was at at most 23 hours and 59 minutes (and some seconds!) earlier. My workflow creating my apps (next) ensures that there are always two mirror copies of all the apps actually before the third copy gets onto the server. This means that the only thing I’d ever lose would be the last day’s activity logs. I can’t see I’ll ever get so fascinated by the server usage that occasionally losing up to a day’s activity would bother me.\nLogging the use of the server\nHm, with Mythic Beasts I have shiny using its default capability to keep a simple log of usage. There appear to be a number of other ways to log shiny usage but for now I’m just letting the default log file get built and I’ll write a bit of R to parse it soon. Details here when I do.\nMy workflow for writing shiny apps\nOne beauty of using shiny from within Rstudio is that I have all my shiny apps as a “project” in R and use git to do what git does (if you’re sensible): to track the changes you made to the whole project every time you “commit”, i.e. tell git to take that image. It’s then pretty easy to couple that git repository to github and then, after ever commit, I can just tell git from within Rstudio to “push” the entire project/repository to github. That’s at https://github.com/cpsyctc/shiny-server meaning that anyone who wants to can see and copy any of the code should they want. (I am using the MIT open source licence, details are in the github repository). Then the final link is that I can “pull” that repository from github to the shiny server thus ensuring that by the time changes appear on the server I will already have the copy on my laptop and the copy on github. (And yes, the copy on the laptop will also get duplicated both locally and to the cloud within minutes of changes to that.)\nThe pulling of the apps from github to the server is automated with another little bash script, /home/chris/cron_pull.sh:\n#!/bin/bash\ncd /srv/shiny-server\ngit pull\nand the line:\n*/15 * * * * /home/chris/cron_pull.sh\nin crontab. That crontab line (aren’t they cryptic?!) runs the pull every fifteen minutes which is overkill really.\nMy learning curve about programming shiny apps\nI’ve put all that up there mostly just to remind myself what had to be done just to get a working shiny server and sensible workflow. Perhaps it will be useful to others. However, now we come to the crunch: writing shiny apps.\nStructure of the shiny project\nThere are at least two ways of organising apps, both involve having a directory “apps” off the root of the shiny server. The apps each have their own directory inside the apps directory and, for now at least, I am using the way of creating the apps that puts all the code into one file always called app.R (which can make it easy to lose which you have open so I put the name of the app, which is the name of the directory, as a comment at the top of all my app.R files).\nHow I now think about each app\n[I suspect this will evolve a lot. This is as of 26.viii.23!]\nThe crucial thing to understand that seemed to take me a while to really understand is that though you are using R and lots of the code is exactly as it would be in an R or even Rmarkdown file it’s probably best to keep telling yourself that because you are constructing something quite different from the linearity of a R or Rmd, you have to let go of some of your habits. The key thing for me has been to learn to “think reactivity”.\nReactivity\nKey parts of shiny app code that enable reactivity\nAt the moment it is helping me to think of my apps having three parts that aren’t in my usual R code.\nInputs: at the moment I’m working with very basic inputs, the big challenge for me is going to be how to input and use files of data\nReactive data/objects: this is fundamental. Reactive data will have been computed from inputs but it is “reactive”: it changes as any of the inputs that contributed to its construction change.\nOutputs: what it says but to me it’s still a bit confusing as it’s declared in one place, where the whole responsive page is defined, and then the specific bits of it that are constructed out of the inputs and whatever you’ve done with them.\nI am starting to think of an app as having those shiny specific bits and these “not-shiny” parts:\nThe usual loading of packages.\nDeclaring/defining of variables and other objects that aren’t reactive.\nDeclaring/defining of functions that aren’t being pulled in from packages.\nThe key thing that I’m still not adjusting to well is that I can’t make my usual assumption that I can read from the top of the file downwards to understand the order in which things happen. I am also finding it hard to adjust to the fact that the structure of any app.R is like this:\n### this is my convention of putting the name here\n### CIcorrelation\n### \n### load packages\nlibrary(shiny)\nlibrary(shinyWidgets)\n\n# Define UI for application that does the work\nui <- fluidPage(\n  setBackgroundColor(\"#ffff99\"),\n  ### this is from\n  ### https://stackoverflow.com/questions/51298177/how-to-centre-the-titlepanel-in-shiny\n  ### and centers the first title across the whole page by tweaking the css for head blocks\n  tags$head(\n    tags$style(\n      \".title {margin: auto; align: center}\"\n    )\n  ),\n  ### to me it's a bit ugly that I end up putting the title here\n  tags$div(class=\"title\", titlePanel(\"Confidence interval for a Pearson or Spearman correlation\\n\\n\")),\n  \n  # Get input values\n  sidebarLayout(\n    sidebarPanel(\n      p(\"This shiny app is one of a growing number in ...\"),\n      ### more of the text cut from here for this Rblog post\n      ###\n      ### now we get set up the interface bit of the inputs\n    numericInput(\"n\",\n                 \"Total n, (zero or positive integer)\",\n                 value = 100,\n                 min = 0,\n                 max = 10^9,\n                 width=\"100%\"),\n   ### others cut to make Rblog post shorter\n  ),\n  \n  ###\n  ### again, I am struggling to remember that the outputs, really the output placeholders go here\n  mainPanel(\n    h3(\"Your input and results\",align=\"center\"),\n    verbatimTextOutput(\"res\"),\n    ### more snipped and you often have a number of outputs, text, tables, plots ...\n  )\n)\n)\n\n\n# Define server logic required\n### this is the standard shiny server constructor\n### the input and output arguments are vital, the session argument is optional but I think always wise\nserver <- function(input, output, session) {\n  ### \n  ### start with validation functions\n  ### I dropped the ones I had because I could use numericInput() to set the ranges\n  ### but you might need functions to check relationships between inputs (say)\n  \n  ### \n  ### now the functions adapted from CECPfuns plotCIcorrelation\n  ### I think I would do this differently now\n  getCI <- function(R, n, ci = 0.95, dp = 2) {\n    z <- atanh(R)\n    norm <- qnorm((1 - ci)/2)\n    den <- sqrt(n - 3)\n    zl <- z + norm/den\n    zu <- z - norm/den\n    rl <- tanh(zl)\n    ru <- tanh(zu)\n    ci.perc <- round(100 * ci)\n    retText <- paste0(\"Given:\\n\",\n                      \"   R = \", R,\"\\n\",\n                      \"   n = \", n,\"\\n\",\n                      \"   observed correlation = \", round(R, dp),\n                      \"\\n\",\n                      \"   \", ci.perc, \"% confidence interval from \", round(rl, dp),\n                      \" to \", round(ru, dp),\"\\n\\n\")\n    return(retText)\n  }\n  \n  output$res <- renderText({\n    validate(\n      ### I have just left this in to demonstrate how validate works which seems OK\n      ### need(checkForPosInt(input$n, minInt = 0), \n      ###     \"n must be a positive integer > 10 and < 10^9\"),\n    )\n    ###\n    ### this is just passing the input variables to the function above\n    ### I could just as well have put the arguments directly into that function\n    getCI(input$R,\n          input$n,\n          input$ci,\n          input$dp)\n  })\n}\n\n### this bit is at the end of all shiny apps and puts together the two objects constructed earlier\n# Run the application (ends all shiny apps in the one file, app.R format)\nshinyApp(ui = ui, server = server)\nSo the structure is always this.\n### name\n### setup stuff\n### define the user interface (doesn't have to be fluidPage but mine are so far)\nui <- fluidPage(\n   ### general text and aesthetics\n   ### input stuff\n   ### output framework/placeholders\n)\n\nserver <- function(input, output, session) {\n   ### any validate functions to do validation not done by the input settings\n   ### functions (I think they could go outside the server definition, not sure)\n   ###    the ones that use inputs (e.g. input$n) actually build something active into the server\n   ###\n   ### define any reactive objects (none in example above, it's too simple to need them)\n   ###\n   ### named outputs like, here the \"res\" links to the quoted \"res\" in the ui\n   output$res <- renderText(\n      ### validation if there is some goes here so no output construction is attempted \n      ### if the input isn't OK.  Seems a late place to put it but that's because I'm \n      ### still thinking in a linear, top to bottom way\n      validate(need(test),\n         \"error message for failed validation\")\n   )\n}\n\n### do the business\nshinyApp(ui = ui, server = server)\nI guess I am getting to understand this and I think it’s helped me to do that sort of anatomising of a very simple app.R down sort of in two stages: first a simplified state but as it were with the muscles and main organs exposed, then stripped to just the skeleton.\nInputs\nAs I am learning, these are defined in the construction of the ui object and there are nice functions for simple inputs with very useful direct validation such as min and max for numeric input. They all have the names input$name1, input$name2 (except that you want more sensible names than “name1” and “name2”, here they are “input$n”, “input$alpha” etc.) It helps me to think of “input” essentially as a named list and it’s a reactive list in the sense that if the user changes any input, the content of that slot in “input” changes instantly.\nReactive data/objects\nYou may or may not have reactive objects. These are like any object in R but crucially they change value the moment that any of their components are changed in the input. To that extent it’s sensible to think of them more as functions than as objects and they are constructed like functions:\nreactiveSausage <- reactive({\n   ### a sausage is made by stuffing stuffing into skin\n   sausage <- makeSausage(input$stuffing, input$skin)\n   sausage\n})\nOK, I know it’s a silly example and this assumes that makeSausage() is an ordinary function that has been defined or supplied from a loaded package somewhere in app.R. Technically I don’t have to construct the sausage object there and then return it by having it as that last line I could just have had\nreactiveSausage <- reactive({\n   ### a sausage is made by stuffing stuffing into skin\n   makeSausage(input$stuffing, input$skin)\n})\nReactive objects are used like functions with no arguments so if I want to print that reactiveSausage it’s:\nprint(reactiveSausage())\n(But of course, we don’t just print() things in shiny apps. (Well, sometimes we do just for debugging to get that on the console of the machine running the app.) That brings us to outputs.\nOutputs\nThe key thing about outputs of a shiny app is that they are always the consummation of two things: the placeholder of the correct type put in the ui by something like\nverbatimTextOutput(\"res\"),\nand the output constructer in the server constructed with something like:\noutput$res <- renderText(),\nThe “$res” in output maps to the quoted “res” in verbatimTextOutput(\"res\") in the ui. I am starting to find it helpful to think of output\\$, rather as I now think of input\\$ as a sort of reactive list whose named members are accessed and used in the ui. I now think that creating outputs that work needs three things really:\ndefine the correct sort of space for the object as a named placeholder in the ui\ndefine the correct sort of output object to match that placeholder by type (e.g. text, table, datatable, plot) and by name within server\nsomewhere in app.R have the appropriate R code to construct the output used in the server output$object, might be paste() to create text, some tidverse code to create a table or some ggplot() to create a plot.\nLogging server use\nUsing the shiny default logging\nShiny comes with the option to log accesses. The configuration is stored in the very simple config file /etc/shiny-server/shiny-server.conf (on Linux):\n# Instruct Shiny Server to run applications as the user \"shiny\"\nrun_as shiny;\naccess_log /var/log/shiny-server/access.log combined;\n\n# Define a server that listens on port 3838\nserver {\n  listen 3838;\n\n  # Define a location at the base URL\n  location / {\n\n    # Host the directory of Shiny Apps stored in this directory\n    site_dir /srv/shiny-server;\n\n    # Log all Shiny output to files in this directory\n    log_dir /var/log/shiny-server;\n\n    # When a user visits the base URL rather than a particular application,\n    # an index of the applications available in this directory will be shown.\n    directory_index on;\n  }\n}\nI have pushed my log format to “combined” from “default” to try to get more information and I’ve written an R script to parse the log and that should get a bit more interesting as things go forward in which case I’ll expand this bit here or create a new post for it.\nLogging by building that into the apps\nI’m glad I’ve started parsing the simple access.log. However, it’s clear that to get more information about use you have to build things into your apps to get that and there appear to be a number of different options, probably at least some of them incompatible with each other and of course each will slow the app and server down a little and some look to be pretty thin on documentation usable for someone like me and at least some look not to have been updated for some time. (Of course, in the open source world, sometimes that’s absolutely fine: the code is simple and sound, pretty much independent of other things (e.g. any changes to shiny) and hasn’t needed any changing for ages … but too often it’s a clue that the developer doesn’t have time to look after it or that it has been orphaned.)\n\n\n\n",
    "preview": "posts/2023-08-25-making-a-working-shiny-server/shiny1.png",
    "last_modified": "2023-08-27T13:02:33+02:00",
    "input_file": {},
    "preview_width": 1235,
    "preview_height": 973
  },
  {
    "path": "posts/2023-08-07-ecdfs/",
    "title": "What is an empirical cumulative distribution function?",
    "description": "A general introductioni to the ECDF and quantiles and why they're useful.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-07",
    "categories": [
      "distributions",
      "ECDFs"
    ],
    "contents": "\n\nContents\nIntroduction and background\nDistributions and quantiles\nECDF of a sample from the Gaussian distribution\nECDF of samples from Gaussian distribution\n\n\nThe ECDF is only one way plotting distributions …\nHistogram of samples from Gaussian distribution\nViolin plots of samples from Gaussian distribution\nBoxplots of samples from Gaussian distribution\n\nQuantiles, quartiles, centiles and percentiles\nSuperimposing individuals’ data on the ECDF\nNot losing site of individuals’ data when aggregating data\nTerminology: using “dataset” in preference to “sample”\n\nIntroduction and background\nThis is one of my “background for practitioners” blog posts, not one of my geeky ones. It will lead into another about two specific functions I’ve added to the CECPfuns R package. However, this one will cross-link to entries in the free online OMbook glossary. It’s also going to link to more posts here on quantiles and mapping individuals’ scores across measures. But first the ECDF and quantiles.\nThe ECDF is pretty much what it says: usually it’s a plot of the the cumulative proportion of a dataset scoring at or below a score. The proportion is plotted on the y axis and the score on the x axis. This means that the line must always start just below the minimum observed score on the x axis and go across the the maximum observed score and the line must be “monotonic positive”: always rising from left to right with no maximum points until the maximum score is reached.\nOne nice thing about the ecdf is that for any value on the y axis, the value on the x axis that maps to this is the corresponding quantile for the observed (empirical) distribution. So if we map from .5 on the y axis, the value on the x axis is the .5 quantile, also known as the median: the score value such that 50% (.5 in proportions) of the scores are below that, and 50% above it.\nQuantiles are important and very useful but seriously underused. They are important both because they are useful to describe distributions of data but also because they can help us map from an individual’s data to and from collective data: one of the crucial themes in the mental health and therapy evidence base.\nQuick terminological point: quantiles are pretty much the same as percentiles, centiles, deciles and the median and lower and upper quartiles are specific quantiles. I’ll come back to that shortly.\nDistributions and quantiles\nLet’s start with datasets not individual scores and with a hugely important theoretical distribution: the Gaussian (often called, but a bit misleadingly, the “Normal” (note the capital letter: it’s not normal in any normal sense of that word!))\nECDF of a sample from the Gaussian distribution\n\n\nShow code\n\nset.seed(12345) # set seed to get the same data regardless of platform and occasion\n# rnorm(10000) %>%\n#   as_tibble() %>%\n#   mutate(n = 10000) %>%\n#   rename(score = value) -> tibGauss10k\n# \n# rnorm(1000) %>%\n#   as_tibble() %>%\n#   mutate(n = 1000) %>%\n#   rename(score = value) -> tibGauss1k\n# \n# rnorm(100) %>%\n#   as_tibble() %>%\n#   mutate(n = 100) %>%\n#   rename(score = value) -> tibGauss100\n# \n# bind_rows(tibGauss100,\n#           tibGauss1k,\n#           tibGauss10k) -> tibGaussAll\n\n### much more tidyverse way of doing that\nc(100, 1000, 10000) %>% # set your sample sizes\n  as_tibble() %>%\n  rename(n = value) %>%\n  ### now you are going to generate samples per value of n so rowwise()\n  rowwise() %>%\n  mutate(score = list(rnorm(n))) %>%\n  ungroup() %>% # overrided grouping by rowwise() and unnest to get individual values\n  unnest_longer(score) -> tibGauss\n\n### get sample statistics\ntibGauss %>%\n  filter(n == 10000) -> tibGauss10000\n\ntibGauss10000 %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score)) -> tibGaussStats10000\n\ntibGaussStats10000 %>%\n  select(min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibGaussStatsLong10000\n\nggplot(data = tibGauss10000,\n       aes(x = score)) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong10000,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibGaussStatsLong10000,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  xlim(c(-4.4, 4.4)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot for a sample from a Gaussian distribution\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nThat shows the typical S shaped (“sigmoid”) shape of the ECDF for a sample from a Gaussian distribution. As it’s a very large (by therapy standards!) sample of n = 10,000 the curve is pretty smooth and the observed median was 0.00. I have marked that with a vertical line and the observed value as I have for some other quantiles, those for 0 (the minimum), .25 (the lower quartile), .75 (the upper quartile) and the maximum.\nDistributions have very different shapes. Here’s another large sample, this time from a “rectangular” distribution, i.e. one where all possible values are equally probable. Here’s an example for a dataset in which scores between 0 and 4 are equiprobable.\n\n\nShow code\n\nrunif(10000, 0, (34*4)) %>%\n  as_tibble() %>%\n  rename(score = value) %>%\n  mutate(score = round(score),\n         score = score / 34) -> tibEquiProb\n\ntibEquiProb %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score)) -> tibEquiProbStats\n\ntibEquiProbStats %>%\n  select(min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibEquiProbStatsLong\n\nggplot(data = tibEquiProb,\n       aes(x = score)) +\n  stat_ecdf() +\n  geom_vline(data = tibEquiProbStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibEquiProbStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  xlim(c(-0.1, 4.1)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot for a sample from rectangular distribution\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nAgain, because the sample size is large, the line is very smooth but now very straight, reflecting the equiprobable scores. The scores there are actually discrete (as of course all values in digital computers are). These were created using the model of CORE-OM scores assuming no omitted items and actually have values from 0, through 1/34 = 0.02941176, 2/34 = 0.05882353, and so on through to 4 - 2/34 = 3.941176, 4 - 1/34 = 3.970588, 4. As you can see, the discrete values are just visible as small steps on the ECDF. (With a dataset of 10,000 all 137 possible scores appeared in the data, that of course wouldn’t be true for smaller datasets and couldn’t be true for n < 137).\nECDFs can be computed for counts but then the curves, as with the discrete CORE-OM scores above, become steps as the numbers must go up as integers. I couldn’t think of a sensible example from MH interventions but this is assuming a service runs median/large groups of size 48 and has a 22% non-attendance rate at any group and shows what the number of clients absent at any one group would look like.\n\n\nShow code\n\nrbinom(10000, 48, .22) %>%\n  as_tibble() %>%\n  rename(nAbsent = value) -> tibCount\n\ntibCount %>%\n  summarise(min = min(nAbsent),\n            median = median(nAbsent),\n            mean = mean(nAbsent),\n            sd = sd(nAbsent),\n            lquart = quantile(nAbsent, .25),\n            uquart = quantile(nAbsent, .75),\n            max = max(nAbsent)) -> tibCountStats\n\ntibCountStats %>%\n  select(min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibCountStatsLong\n\nggplot(data = tibCount,\n       aes(x = nAbsent)) +\n  stat_ecdf() +\n  geom_vline(data = tibCountStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibCountStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  # xlim(c(-0.1, 4.1)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot of number of people not attending any one group (group size 48, probability absent .22)\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nYou can see how that distribution (a binomial) has the quartiles close either side of the median.\nApart from steps and lost smoothness in the ECDF caused by the scores being counts or scores on a fairly limited range of possible scores, e.g. for short questionnaires with binary or short ordinal responses, the other thing that affects the smoothness of ECDF curves is dataset size. This goes back to the samples from the Gaussian distribution but shows three different dataset sizes.\nECDF of samples from Gaussian distribution\n\n\nShow code\n\ntibGauss %>%\n  group_by(n) %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score)) -> tibGaussStats\ntibGaussStats %>%\n  select(n, min, lquart, median, uquart, max) %>%\n  pivot_longer(cols = min:max) %>%\n  rename(Quantile = name) %>%\n  mutate(Quantile = ordered(Quantile,\n                            levels = c(\"min\", \"lquart\", \"median\", \"uquart\", \"max\"),\n                            labels = c(\"Min\", \"Lower quartile\", \"Median\", \"Upper quartile\", \"Max\"))) -> tibGaussStatsLong\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             ncol = 1) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_text(data = tibGaussStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            nudge_x = -.04,\n            hjust = 1) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"Faceted ECDF plot for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nI’ve facetted there (by rows) for three dataset sizes: 100, 1,000 and 10,000.\nA few pretty obvious comments on the impact of sample size when in the classical model of random sampling from an infinitely large population. These impacts are visible in all those distribution plots above.\nIf the possible scores are genuinely continuous or the number of possible scores higher than the datasete size then the distributions are less “lumpy” the larger the sample.\nAs the dataset sizes get bigger, if, as with the Gaussian distribution, the possible scores actually range from -Infinity to +Infinity then the limits, i.e. the minimum (quantile zero) and the maximum (quantile 1.0) move out as the sample size goes up as the larger sample gets more chance of including the rare but not impossible extreme values.\nAs the sample sizes get bigger the observed quantiles get closer to their population values. That can be seen in this next table. This shows\nname = name of the quantile\nproportion = the proportion of that quantile\nthe value in the infinitely large population (know from the maths)\nn100 = the observed value for that quantile in this sample of n = 100\nn1000 = the observed value for that quantile in this sample of n = 1,000\nn10000 = the observed value for that quantile in this sample of n = 10,000\n\n\n\nShow code\n\ntibGaussStats %>% \n  select(n, lquart, median, uquart) %>%\n  pivot_longer(cols = -n) %>%\n  mutate(value = round(value, 4),\n         proportion = case_when(\n                              name == \"lquart\" ~ .25,\n                              name == \"median\" ~ .5,\n                              name == \"uquart\" ~ .75),\n         popVal = qnorm(proportion),\n         popVal = round(popVal, 4)) %>%\n  pivot_wider(names_from = n, names_prefix = \"n\", values_from = value) %>%\n  flextable() %>%\n  autofit()\n\nnameproportionpopValn100n1000n10000lquart0.25-0.6745-0.5901-0.6359-0.6652median0.500.00000.48370.0080-0.0019uquart0.750.67450.90040.64530.6576\n\nIt can be seen there that the observed values for the quantiles get closer to the population values (popVal) the larger the sample.\nThe ECDF is only one way plotting distributions …\n… each has advantages and disadvantages. Let’s look at that.\nHistogram of samples from Gaussian distribution\nThe histogram is probably the plot most commonly used to show the shape of a distribution. Here it is for our Gaussian samples.\n\n\nShow code\n\ntibGauss %>%\n  group_by(n) %>%\n  summarise(min = min(score),\n            median = median(score),\n            mean = mean(score),\n            sd = sd(score),\n            lquart = quantile(score, .25),\n            uquart = quantile(score, .75),\n            max = max(score),\n            ### and bootstrap mean (could have used parametric derivation as this is true Gaussian but I couldn't remember it!)\n            CI = list(getBootCImean(score, verbose = FALSE))) %>%\n  unnest_wider(CI) -> tibGaussStats\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 3) +\n  geom_histogram(aes(y = after_stat(density))) +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  ylim(c(0, .6)) +\n  ylab(\"Count\") +\n  ggtitle(\"Faceted histogram for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000\")\n\n\n\nHence the famous name: “bell-shaped distribution” for the Gaussian distribution. One nice thing about the Gaussian distribution is that it is completely defined by two “parameters” (values in the population): the mean and standard deviation (SD). That is to say that from those two statistics (the mean and SD values observed in the sample) you can fit the distribution you believe the population has given those sample values. Like this!\n\n\nShow code\n\ntibGaussStats %>%\n  mutate(label1 = paste0(\"mean = \", round(mean, 3), \"\\n\",\n                         \"sd = \", round(sd, 3))) -> tmpTib\n\n### I thought I could use geom_function() to map the inplied population density curves to the faceted plots but geom_function() isn't facet aware so\n\ntibGaussStats %>%\n  select(n, mean, sd) %>%\n  rowwise() %>%\n  mutate(x = list(seq(-4, 4, .05))) %>%\n  ungroup() %>%\n  unnest_longer(x) %>%\n  mutate(fitted = dnorm(x, mean, sd)) -> tibFitted\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 3) +\n  geom_histogram(aes(y = after_stat(density))) +\n  geom_vline(data = tmpTib,\n             aes(xintercept = mean),\n             colour = \"red\") +\n  geom_line(data = tibFitted,\n            aes(x = x, y = fitted),\n            colour = \"blue\",\n            linewidth = 1.5) +\n  geom_text(data = tmpTib,\n            aes(label = label1),\n            x = -4,\n            y = .59,\n            hjust = 0) +\n  ylim(c(0, .6)) +\n  ylab(\"Density\") +\n  ggtitle(\"Faceted histogram for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\\nFitted (implied) population distribution in blue\")\n\n\n\nThat shows how the observed distributions get closer to the population distribution as the dataset size increases.\nViolin plots of samples from Gaussian distribution\nViolin plots are increasingly used in place of histograms. Probably some of that is “look how modern we are!” but they have the great advantage over histograms that it is easy to plot a number of distributions side by side and eyeball if they look similar or different in shape. Violin plots use a sort of smoothed histogram rotated through ninety degrees and mirrored to give a nice way of comparing different distributions, here the three different samples.\n\n\nShow code\n\ntibGauss %>%\n  mutate(x = 1) -> tmpTibGauss\n\nggplot(data = tmpTibGauss,\n       aes(x = 1, y = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 3) +\n  geom_violin(fill = \"grey80\") +\n  # geom_jitter(width = .35, height = 0,\n  #             alpha = .1,\n  #            colour = \"grey40\") +\n  geom_hline(data = tmpTib,\n             aes(yintercept = mean),\n             colour = \"red\") +\n  geom_text(data = tmpTib,\n            aes(label = label1),\n            x = .6,\n            y = 3.75,\n            hjust = 0) +\n  ylab(\"Scores\") +\n  ggtitle(\"Faceted violin plot with jittered observations for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\")\n\n\n\nAnother thing that is easier to do with violin plots than with histograms (though perfectly possible with histograms too with some trickery) is to put individual scores onto the plot as I have done here.\n\n\nShow code\n\ntibGauss %>%\n  mutate(x = 1) -> tmpTibGauss\n\nggplot(data = tmpTibGauss,\n       aes(x = 1, y = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 3) +\n  geom_violin(fill = \"grey80\") +\n  geom_jitter(width = .35, height = 0,\n              alpha = .2,\n             colour = \"grey40\") +\n  geom_hline(data = tmpTib,\n             aes(yintercept = mean),\n             colour = \"red\") +\n  geom_text(data = tmpTib,\n            aes(label = label1),\n            x = .6,\n            y = 3.75,\n            hjust = 0) +\n  ylab(\"Scores\") +\n  ggtitle(\"Faceted violin plot with jittered observations for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\")\n\n\n\nThat used vertical “jittering” of the points to spread them out vertically (as a point can’t have a value on the y axis but does have a score value on the x-axis.) More on jittering in the Rblog at handling overprinting.\nBoxplots of samples from Gaussian distribution\nOne more way of plotting a distribution: the boxplot, here again I’ve added jittered points for the individual data points. The boxplot brings us back to the simplest quantiles OK, as the box in the typical boxplot is defined by three quantiles: the median for the belt in the box, and the quartiles for the lower and upper limits of the box. The median is the score (not necessarily present in the data) that would bisect the data into two equal sized halves so it’s the value such that half the observed values lie below it and half lie above it. The lower quartile is the value such that a quarter of the observed values lie below it and three quarters above it, the upper quartile is the value such that three quarters of the sample lie below it and one quarter above it.\n\n\nShow code\n\nggplot(data = tmpTibGauss,\n       aes(x = 1, y = score)) +\n  facet_wrap(facets = vars(n),\n             nrow = 1) +\n  geom_boxplot(notch = TRUE,\n               varwidth = TRUE,\n               fill = \"grey80\") +\n  geom_jitter(width = .35, height = 0,\n              alpha = .05,\n             colour = \"grey40\") +\n  geom_hline(data = tmpTib,\n             aes(yintercept = mean),\n             colour = \"red\") +\n  geom_text(data = tmpTib,\n            aes(label = label1),\n            x = .6,\n            y = 3.75,\n            hjust = 0) +\n  ylab(\"Scores\") +\n  ggtitle(\"Faceted boxplot with jittered observations for three samples from Gaussian distribution\",\n          subtitle = \"Sample sizes 100, 1,000 and 10,000, sample mean in red\")\n\n\n\nQuantiles, quartiles, centiles and percentiles\nThese are all really names for the same things but quantiles are generally mapped to probabilities (from zero to one) and percentiles to probabilities as percentages (from 0% to 100%). There are also deciles: i.e. 0%, 10%, 20% … 80%, 90% and 100%.\nQuantile\nQuartile\nPercentile (a.k.a. centile)\n.25\nlower\n25%\n.50\nmedian\n50%\n.75\nupper\n75%\nSuperimposing individuals’ data on the ECDF\nI don’t think I’ve ever seen this done but one way to put the individual data onto an ECDF is to add a “rug” to the x (score) axis. A rug adds a mark, like the threads at the edge of a rug on the floor to mark the individual points, like this.\n\n\nShow code\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             ncol = 1) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_rug(alpha = .3) +\n  geom_text(data = tibGaussStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  xlim(c(-4.4, 4.4)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot for a sample from a Gaussian distribution\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nThat’s fine for smaller n but you can see that when the n gets up to 10,000 or even 1,000 the overprinting pretty much removes the mapping to the individual data. (That’s true even adding transparency to the rug marks as I have there. See here for a bit more on using transparency with most R ggplot plots.)\nI think this next plot, which uses a bit of R to create a sort of “histogram rug” (“historug” or “histogrug”?) might be a way to handle the challenge of reminding us of the individual scores when creating ECDF plots though I think purists will say, rightly, that it’s starting to muddle interpretation of the y axis as the labels on the y axis are correct for the ECDF but meaningless for the histogrug marks. Perhaps I have to accept that I go too far trying to reveal multiple aspects of a dataset in one plot!\n\n\nShow code\n\ntibGauss %>%\n  mutate(score = round(score, 1)) %>%\n  group_by(n, score) %>%\n  summarise(count = n()) %>%\n  ungroup() %>%\n  mutate(perc = 2 * count / n) %>%\n  ungroup() -> tmpTib\n\nggplot(data = tibGauss,\n       aes(x = score)) +\n  facet_wrap(facets = vars(n),\n             ncol = 1) +\n  stat_ecdf() +\n  geom_vline(data = tibGaussStatsLong,\n             aes(xintercept = value, colour = Quantile)) +\n  geom_bar(data = tmpTib,\n           aes(x = score, y = perc),\n           stat = \"identity\") +\n  geom_text(data = tibGaussStatsLong,\n            aes(label = round(value, 2),\n                x = value,\n                y = .28),\n            size = 6,\n            nudge_x = -.04,\n            hjust = 1) +\n  xlim(c(-4.4, 4.4)) +\n  ylab(\"Proportion of the sample scoring below the value\") +\n  ggtitle(\"ECDF plot for a sample from a Gaussian distribution\",\n          subtitle = \"Sample size 10,000, quantiles shown as coloured lines with their values.\")\n\n\n\nHowever, that brings me back to a key issue.\nNot losing site of individuals’ data when aggregating data\nOne great thing about all of these plots describing the shapes of distributions is that they move us away from oversimplifying summary statistics, typically just the mean and all of the ECDF, violin plot and boxplot can alert us that the distribution is not Gaussian and so can’t just be summarised by the mean and SD.\nThe issue of superimposing individual scores on these plots is about not just moving from simplifications to distributions’ shapes but also trying to keep individual data in mind.\nOne huge issue in MH/therapy work is that we have to be interested both individuals but also to be aware of aggregated data: to be able to take a population health and sometimes a health economic viewpoint on what our interventions offer in aggregate. However, I am sure that quantitative methods too often lose sight of individuals’ data. There are no simple and perfect ways to be able to think about both individual and about aggregated data and no perfect ways to map individual data to large dataset data.\nTerminology: using “dataset” in preference to “sample”\nI have tried to be pedantic and use the words “population” and “sample” when I’m talking about simulation in which maths and computer power make it easy to create “genuine” samples from infinitely large populations (actually to simpulate them). Otherwise I’ve used “dataset” instead of “sample” as I think that with MH/therapy data we’re pretty much never in possession of truly random samples from defined populations in our work. The “random-sample-from-population” model is a great way to help us understand aggregated data and often it’s the best we have when trying to generalise from small datasets. By using “dataset” not “sample” and by emphasizing the importance of looking at distributions and of trying to keep individual data in mind I’m not trying to overthrow group aggregate summary statistical methods, just trying to stop us overvaluing what we get from those models.\n\n\n\n",
    "preview": "posts/2023-08-07-ecdfs/ecdfs_files/figure-html5/plotECDF1-1.png",
    "last_modified": "2023-08-25T14:28:22+02:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2023-08-07-wisdom-of-years/",
    "title": "Wisdom of years!",
    "description": "I've learned a lot about data analysis from my errors, here's what I wish I'd known earlier!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-08-06",
    "categories": [
      "R style",
      "wise practices"
    ],
    "contents": "\n\n\n\nThis is just a little post to point to a new developing page “Wisdom1”(https://www.psyctc.org/Rblog/wisdom.html) in my little Rblog site. It’s a compilation of principles and rules to myself all of which I wish I’d learned earlier and which, I believe, save me weeks of time even though, sometimes, they can add minutes, occasionally hours and, once per project (writing DAPs & and DMPs: Data Analysis Plans and Data Management Plans) they may even take days. Very occasionally, when trying to simulate a project, they may take even longer but those, like long DAPs, may turn into papers in their own rights.\nThis will accumulate and I welcome comments and suggestions contact me, so I’ve made it a page not a post and I’m just using this to flag it up.\n\n\n\n",
    "preview": "posts/2023-08-07-wisdom-of-years/wisdom.png",
    "last_modified": "2023-08-25T14:28:55+02:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4800
  },
  {
    "path": "posts/2023-06-15-r-things-i-and-perhaps-others-forget/",
    "title": "R things I, and perhaps others, forget",
    "description": "This is a developing miscellany of the things that I seem to keep forgetting about R.  What I used to call in my student days \"teflon coated facts\"!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-06-16",
    "categories": [
      "Distill package",
      "R graphics",
      "R tricks"
    ],
    "contents": "\n\nContents\nClipping ggplot plot axes\nPutting citations and references into Rmarkdown/Distill documents\nChanging plot with in Distill\nUpdate history of this post\n\nClipping ggplot plot axes\nI always forget how to do this, I guess it’s not something I need very often, but when I do it seems very hard to find the answer by searching as the obvious words to search on seem to take me to controlling the axis itself, not these little expansions/extensions. So here’s how to do it.\nThe default is that ggplot adds a small extension to the axes. So here is the default for a silly little plot. With rather crude annotation to show what I mean.\n\n\nShow code\n\nseq(1, 4, length = 500) %>%\n  as_tibble() %>%\n  rename(x = value) %>%\n  mutate(y = x) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  annotate(geom = \"label\",\n           x = 1.3, y = 1.8,\n           label = \"Left hand x margin here\",\n           size = 8) +\n  geom_segment(x = 1.1, xend = .92,\n               y = 1.74, yend = 1,\n               arrow = arrow(angle = 30, length = unit(0.02, \"npc\"),\n                     ends = \"last\", type = \"open\")) +\n  annotate(geom = \"label\",\n           x = 1.85, y = 1.1,\n           label = \"Left hand y margin here\",\n           size = 8) +\n  geom_segment(x = 1.4, xend = 1,\n               y = 1.1, yend = .92,\n               arrow = arrow(angle = 30, length = unit(0.02, \"npc\"),\n                     ends = \"last\", type = \"open\")) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nHere I use xlim(c(1.5, 3.5)) and ylim(c(1.5, 3.5)) to shorten the axes…\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  xlim(c(1.5, 3.5)) +\n  ylim(c(1.5, 3.5)) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nOf course a lot of points have been dropped to pull the plot back to these limits. But there is still this margin on the axes. The trick is to add\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0))\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nThis is not the same as expand_limits()\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  expand_limits(x = c(1, 4)) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nBut you can use expand_limits() to create space on the plot. Here I’ve used scale_x_continuous(breaks = 1:4) to stop the x axis getting labelled up to 6.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  expand_limits(x = c(1, 6)) +\n  annotate(geom = \"label\",\n           x = 4.3, y = 3,\n           label = \"This allowed me to put\\na label in here\\nwhere I can put in\\nlots of drivel\\nand other nonsense\",\n           hjust = 0,\n           size = 10) +\n  scale_x_continuous(breaks = 1:4) +\n  theme(axis.text.x = element_text(size = 28),\n        axis.text.y = element_text(size = 28),  \n        axis.title.x = element_text(size = 28),\n        axis.title.y = element_text(size = 28))\n\n\n\nPutting citations and references into Rmarkdown/Distill documents\nThis is something I do from time to time and I’m not claiming this is the best way to do it but it worked for me doing the post Jacobson #1 which, as is my usual, I did in Rstudio. This may not, perhaps probably won’t work if you using a different editor/environment. This also assumes you are using Zotero as your bibliographic database manager … which I do because it’s open source and excellent. You need the Better Bibtex for Zotero plugin for Zotero. That’s what the rbbt package connects to when it’s finding references.\nI installed the rbbt library. It’s not in CRAN so:\nremotes::install_github(\"paleolimbot/rbbt\")\nThat adds addins to the Rstudio addin menu and if you use the “Insert Zotero Citation” you get into the usual Zotero plugin lookup to select the reference(s) you want. The “Insert Zotero bibliography from Zotero Selection” puts the bibliography in at the end of the document so you generally want to end the document with a top level heading “References”\nThe really neat bits are that the plugin will collect just the references you cited into a bib format bibliography for you. To get that you put this\nbibliography: tmpBib.bib\nin the yaml header of the Rmarkdown document. You can call the bib file anything you like of course but it has to be in the same directory as the source file. (So for a distill blog post it goes in the directory for the post, not in the project root. Don’t worry if you don’t use distill as life’s simple then: just keep the bib file with the Rmd file.)\nTo get this to work, first make sure you have saved your Rmarkdown document since you last added a citation and then use the third (and last) addin call created by the rbbt package: “Update bibliography for current document from Zotero” which does what that says and tells you in the console tab how many references it has added.\nWorking like this allows you to use all the Rmarkdown citation tricks like ommitting the author(s) names, adding a reference so it will appear in the reference list despite not being cited in the document. See 4.5 Bibliographies and citations in bookdown.org for more on that.\nThe final neat bit is that one of these strengths is that you can set the formatting for the citations and reference list. I tend to use, despite a bit of grinding of teeth at the rigidity of it all, the APA rules for that but you can get a wide range of csl format files for different journals and rules. That is set by having this in the yaml header.\ncsl: apa.csl\nYou can get csl files from https://github.com/citation-style-language/styles.\nChanging plot with in Distill\nUpdate history of this post\n* 16.vi.23: Started this just with the first two headings for clipping ggplot axes and referencing in Rmarkdown. Updated with Distill bit 9.viii.23\n\n\n\n",
    "preview": "posts/2023-06-15-r-things-i-and-perhaps-others-forget/r-things-i-and-perhaps-others-forget_files/figure-html5/plot1-1.png",
    "last_modified": "2023-08-25T14:27:59+02:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2023-06-10-jacobson1/",
    "title": "Jacobson #1",
    "description": "The Jacobson plot and RCSC (Reliable and Clinical Change) methods for those who have never met them before or don't feel confident they understand them.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2023-06-10",
    "categories": [
      "RCSC paradigm",
      "R graphics",
      "Jacobson plot",
      "granularity of scores"
    ],
    "contents": "\n\nContents\nBackground\nIntroduction\nHistory\nStarting high and finishing high\nStarted high but finished low\nLow to high\nStayed low\n\nFrom CSC dichotomisation and the quadrants to add reliable change: from CSC to RCSC\nDual plot: trajectory plot and Jacobson\nTypical Jacobson summary table\nFinal Jacobson plot\n\nSummary\nNotes on the code for users of R\nUpdate history of this post\n\n\n\n\nBackground\nThis is my first post here for a long time so it’s serving a lot of purposes:\nReminding me how to use the R distill package to add things here!\nIt’s been triggered by excellent peer reviews to a paper of ours so it uses real data and may be a “supplementary” to that paper.\nMore importantly I hope it will take people through the construction of the Jacobson plot of start/finish therapy change scores showing the logic.\nThat expands on, but links to, what Jo-anne and I had about the RCSC and Jacobson plot in the OMbook and its slowly expanding online glossary. If you don’t know about the book and glossary, I recommend that you look at the pages about the book at some point as it could help go beyond the particulars here to wider issues about therapy change data.\nI am putting some cautions in about the assumptions in the RCSC model and about some of the intentions behind it, i.e. to make therapy research change data more meaningful to clinicians and some of the value of the plot to contextualise individual client change data can get lost.\nI hope that writing the code for the plots and tables will be a major step to putting a set of RCSC/Jacobson functions to generate the plots and tables into the CECPfuns R package.\nTechnicalities\nI don’t think this presentation is going to work on a mobile ’phone and you may need to play around resizing you browser window to get the best visibility for you. The other technical point is that you will see buttons saying “Show code”. If you’re not interested in R code, just ignore those; if you are interested in the R code then just clicking on those will show you the code which you are welcome to copy and amend as much as you like but please if you are publishing something that was helped by the code, then please put a link back to this post acknowledging this.\nIntroduction\nOK. Here is a simple Jacobson plot of our data from the paper.\n\n\n\nThat shows data for 182 clients from our paper. Let’s go into the construction of the plot starting without the clients’ data. The Jacobson plot creates a map with the x axis (horizontal axis) being the clients’ first assessment score and the y, the vertical, axis being the finishing score.\nHistory\nThe plot was first described in (Jacobson et al., 1984). Jacobson and colleagues had a mistake in one of the calculations that was pointed out (Christensen & Mendoza, 1986) and accepted (Jacobson et al., 1986) (more that below). Although it’s not the historically canonical reference for the plot, (Jacobson & Truax, 1991) is a nice summary of the method that is often cited for it. I’ve contributed to the literature on it with our attempt to make it easier to follow in (Evans et al., 1998) which a lot of people have told me they found helpful! There is also a shorter explanation of the plot than this one here in Chapter 5 of (Evans & Carlyle, 2021).\nThat’s the beginning of the plot in history but this blog post is about building the its beginnings as blank graph. So this is the canvas onto which we put our change scores.\n\n\nShow code\n\n### set the score limits (implicit in previous plot from the polygon vertices there)\nvalMinPoss <- 0\nvalMaxPoss <- 4\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            hjust = 1,\n            size = 6,\n            vjust = 0) +  \n  ### set limits (this way of setting the axis limits doesn't clip the plotting area)\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"The vertical and horizontal reference lines mark the CSC criterion\")\n\n\n\nThose lines are used to dichotomise the scores, starting and finishing scores into “clinical” and “non-clinical” (>= CSC and < CSC respectively). That means the area is split into four quadrants by those two lines which mark the CSC (Clinically Significant Change) criterion for the measure (here it was the CORE-OM but the principles apply to any measure of change.) There are many ways to split scores into two levels: “clinical” and “non-clinical” and many, many reasons to be cautious about such dichotomisation. Having said that, it seems that there is a huge and diverse wish to have such categories and Jacobson and his colleagues based their “RCSC” (Reliable and Clinically Significant Change) on such dichotomisation. (And they proposed three ways to determine the CSC score for any measure, one of which, their method c, has pretty overwhelming advantages on their other two and has become very widely used.) Here’s how those lines dichotomise the field.\n\n\nShow code\n\n### more polygon vertices\ndatPolyStartedHigh <- data.frame(x = c(csc, csc, valMaxPoss, valMaxPoss),\n                                 y = c(valMinPoss, valMaxPoss, valMaxPoss, valMinPoss))\ndatPolyStartedLow <- data.frame(x = c(valMinPoss, valMinPoss, csc, csc),\n                                y = c(valMinPoss, valMaxPoss, valMaxPoss, valMinPoss))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### starting scores high\n  geom_polygon(data = datPolyStartedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +\n  ### label that \n  annotate(\"label\",\n           x = csc + ((valMaxPoss - csc) / 2),\n           y = ((valMinPoss + valMaxPoss) / 2),\n           size = 6,\n           label = \"Points in here mark clients who\\n started above the CSC\") +\n  ### starting scores low\n  geom_polygon(data = datPolyStartedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +\n  ### label that \n  annotate(\"label\",\n           x = csc / 2,\n           y = ((valMinPoss + valMaxPoss) / 2),\n           size = 6,\n           label = \"Points in here mark clients who\\n started below the CSC\") +  \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  # geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Clients starting scores against the CSC\")\n\n\n\nThe same applies for the finishing scores.\n\n\nShow code\n\n### more vertices for geom_poly()\ndatPolyFinishedHigh <- data.frame(x = c(valMinPoss, valMinPoss, valMaxPoss, valMaxPoss),\n                                  y = c(csc, valMaxPoss, valMaxPoss, csc))\ndatPolyFinishedLow <- data.frame(x = c(valMinPoss, valMinPoss, valMaxPoss, valMaxPoss),\n                                 y = c(valMinPoss, csc, csc, valMinPoss))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### starting scores high\n  geom_polygon(data = datPolyFinishedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = (valMinPoss + valMaxPoss) / 2,\n           y = (csc + valMaxPoss) / 2,\n           size = 6,\n           label = \"Points in here mark clients who\\n finished above the CSC\") +\n  ### starting scores low\n  geom_polygon(data = datPolyFinishedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = (valMinPoss + valMaxPoss) / 2,\n           y = csc / 2,\n           size = 6,\n           label = \"Points in here mark clients who\\n finished below the CSC\") +  \n  ### put in CSC line\n  geom_hline(yintercept = csc) +\n  ### label that\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45),\n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +\n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Clients finishing scores against the CSC\")\n\n\n\nOf course the actual scores remain the actual scores! One thing to watch with all dichotomisation is not to lose sight of that. That will be a theme through this post. If we think of the scores as continuous this shows the starting scores as a colour gradient\n\n\nShow code\n\ndatPolyAll <- data.frame(x = c(valMinPoss, valMinPoss, valMaxPoss, valMaxPoss),\n                         y = c(valMinPoss, csc, csc, valMinPoss))\n\n### The CORE-OM has 41 possible score levels\nvalNlevels <- 41\n### rather crude way to create a full range of possible first/last score pairs\nas_tibble(data.frame(x = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), each = valNlevels),\n                     y = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), times = valNlevels))) -> tibFill\n\nggplot(tibFill,\n       aes(x = x,\n           y = y)) +\n  ### starting score gradient\n  geom_raster(aes(fill = x)) +\n  scale_fill_gradient(low = \"green\", high = \"red\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Continuous starting scores\")\n\n\n\nAnother thing to remember is that our scores aren’t truly continuous. Here are the possible scores for the CORE-10 with no prorating.\n\n\nShow code\n\nggplot(tibFill,\n       aes(x = x,\n           y = y)) +\n  geom_point(aes(colour = y),\n             size = 3) +\n  scale_colour_gradient(low = \"green\", high = \"red\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = 1.05 * valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Discrete starting scores (model of CORE-10 with no prorating)\")\n\n\n\nThis next shows the same but for the CORE-OM with the full 34 items completed, no prorating again. The granularity is clearly much greater. The issue about our scores not being truly continuous does start to be an issue to hold in mind but only when the number of possible scores gets quite low. Here are the possible scores with no pro-rating for the GAD-7 with the UK IAPT cutting score of 8.\n\n\nShow code\n\n### compute the number of possible scores on the GAD-7\nvalNlevels <- 4 * 7 + 1\n### reset limits\nvalMinPoss <- 0\nvalMaxPoss <- 21\ncsc <- 8\n### and now create the full set of possible first/last scores for the GAD-7\nas_tibble(data.frame(x = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), each = valNlevels),\n                     y = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), times = valNlevels))) -> tibFill\n\nggplot(tibFill,\n       aes(x = x,\n           y = y)) +\n  geom_point(aes(colour = y),\n             size = 3) +\n  scale_colour_gradient(low = \"green\", high = \"red\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = 1.05 * valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Discrete starting scores (model of GAD-7 with no prorating)\")\n\n\n\nEven with only seven items and four response levels we have 22 possible scores, 14 about that cutting point and eight below it.\n\n\nShow code\n\n### reset things to the CORE-OM\nvalNlevels <- 4 * 34 + 1\n### reset limits\nvalMinPoss <- 0\nvalMaxPoss <- 4\ncsc <- 1.26\n\nas_tibble(data.frame(x = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), each = valNlevels),\n                     y = rep(seq(valMinPoss, valMaxPoss, length = valNlevels), times = valNlevels))) -> tibFill\n\nggplot(tibFill,\n       aes(x = x,\n           y = y)) +\n  geom_point(aes(colour = y),\n             size = 1) +\n  scale_colour_gradient(low = \"green\", high = \"red\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, \n            y = 1.03 * valMaxPoss, \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Discrete starting scores (model of CORE-OM with no prorating)\")\n\n\n\nSo that’s how the plot relates to the starting and finishing scores. When we look at both scores we have four quadrants. These next four blocks show each quadrant.\nStarting high and finishing high\n\n\nShow code\n\n### another polygon\ndatPolyStayedHigh <- data.frame(x = c(csc, csc, valMaxPoss, valMaxPoss),\n                                y = c(csc, valMaxPoss, valMaxPoss, csc))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### quadrants\n  geom_polygon(data = datPolyStayedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"orange\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = csc + ((valMaxPoss - csc) / 2),\n           y = csc + ((valMaxPoss - csc) / 2),\n           size = 6,\n           label = \"Points in here mark clients who\\n started above CSC and ended above CSC\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"'Stayed high' quadrant\")\n\n\n\nStarted high but finished low\n\n\nShow code\n\ndatPolyHighToLow <- data.frame(x = c(csc, csc, valMaxPoss, valMaxPoss),\n                               y = c(valMinPoss, csc, csc, valMinPoss))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### quadrants\n  geom_polygon(data = datPolyHighToLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = csc + ((valMaxPoss - csc) / 2),\n           y = (csc / 2),\n           size = 6,\n           label = \"Points in here mark clients who\\n started above CSC and ended below CSC\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"'High to low' quadrant\")\n\n\n\nLow to high\n\n\nShow code\n\ndatPolyLowToHigh <- data.frame(x = c(valMinPoss, valMinPoss, csc, csc),\n                               y = c(csc, valMaxPoss, valMaxPoss, csc))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### quadrants\n  geom_polygon(data = datPolyLowToHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = csc / 2,\n           y = csc + (valMaxPoss - csc) / 2,\n           size = 6,\n           label = \"Points in here mark clients\\n who started below CSC\\nand ended above CSC\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"'Low to high' quadrant\")\n\n\n\nStayed low\n\n\nShow code\n\ndatPolyStayedLow <- data.frame(x = c(valMinPoss, valMinPoss, csc, csc),\n                               y = c(valMinPoss, csc, csc, valMinPoss))\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### quadrants\n  geom_polygon(data = datPolyStayedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"yellow\") +\n  ### label that \n  annotate(geom = \"label\",\n           x = csc / 2,\n           y = csc / 2,\n           size = 6,\n           label = \"Points in here mark clients\\n who started below CSC\\nand ended below CSC\") +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Stayed low quadrant\")\n\n\n\nHere’s what that looks like for our real data.\n\n\nShow code\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### quadrants\n  geom_polygon(data = datPolyStayedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"yellow\") +\n  geom_polygon(data = datPolyStayedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"orange\") +\n  geom_polygon(data = datPolyHighToLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +  \n  geom_polygon(data = datPolyLowToHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  ### put in the points\n  geom_point(alpha = .5) +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Real data\")\n\n\n\nDichotomising the two scores using the CSC gives us those four quadrants and what one sometimes sees is the data being tabulated by those quadrants either as a first/last crosstabulation like this.\n\n\nShow code\n\ntibData %>%\n  filter(occasion == 1) %>%\n  select(id, firstScore, lastScore) %>%\n  ### categorise change\n  mutate(firstCSCcategory = if_else(firstScore >= csc, \"startHigh\", \"startLow\"),\n         lastCSCcategory = if_else(lastScore >= csc, \"endHigh\", \"endLow\"),\n         CSCchangeCategory = case_when(\n           firstCSCcategory == \"startHigh\" & lastCSCcategory == \"endHigh\" ~ \"Stayed high\",\n           firstCSCcategory == \"startHigh\" & lastCSCcategory == \"endLow\" ~ \"Clinically improved\",\n           firstCSCcategory == \"startLow\" & lastCSCcategory == \"endHigh\" ~ \"Clinically deteriorated\",\n           firstCSCcategory == \"startLow\" & lastCSCcategory == \"endLow\" ~ \"Stayed low\")) -> tmpTibCSC\n\ntmpTibCSC %>%\n  tabyl(firstCSCcategory, lastCSCcategory) %>%\n  adorn_percentages() %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns() %>%\n  flextable() %>%\n  ### flextable::flextable() uses bg() to set background colour i are rows, j are columns\n  bg(i = 1, j = 2, bg = \"orange\") %>%\n  bg(i = 1, j = 3, bg = \"green\") %>%\n  bg(i = 2, j = 2, bg = \"red\") %>%\n  bg(i = 2, j = 3, bg = \"yellow\")\n\nfirstCSCcategoryendHighendLowstartHigh66.9% (113)33.1% (56)startLow7.7%   (1)92.3% (12)\n\nOr just listing the categories.\n\n\nShow code\n\ntmpTibCSC %>%\n  mutate(CSCchangeCategory = ordered(CSCchangeCategory,\n                                     levels = c(\"Stayed high\",\n                                                \"Stayed low\",\n                                                \"Clinically improved\",\n                                                \"Clinically deteriorated\"),\n                                     labels = c(\"Stayed high\",\n                                                \"Stayed low\",\n                                                \"Clinically improved\",\n                                                \"Clinically deteriorated\"))) %>%\n  tabyl(CSCchangeCategory) %>%\n  adorn_pct_formatting(digits = 1) %>%\n  flextable() %>%\n  bg(i = 1, j = 1:3, bg = \"orange\") %>%\n  bg(i = 2, j = 1:3, bg = \"yellow\") %>%\n  bg(i = 3, j = 1:3, bg = \"green\") %>%\n  bg(i = 4, j = 1:3, bg = \"red\")\n\nCSCchangeCategorynpercentStayed high11362.1%Stayed low126.6%Clinically improved5630.8%Clinically deteriorated10.5%\n\nFrom CSC dichotomisation and the quadrants to add reliable change: from CSC to RCSC\nHowever, that clearly reduces the complexity of the scores perhaps a bit too far even for a quadrant classification based on dichotomising the first and last scores. The key thing that fails to consider is whether the changes, whatever quadrant they put the client into, are large enough that we should be interested!\nThe first step really is just add the no change line to the plot.\n\n\nShow code\n\nas_tibble(data.frame(x = seq(0, 4, length = 41),\n                     y = seq(0, 4, length = 41))) -> tibNoChange\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### quadrants\n  geom_polygon(data = datPolyStayedLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"yellow\") +\n  geom_polygon(data = datPolyStayedHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"orange\") +\n  geom_polygon(data = datPolyHighToLow,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"green\") +  \n  geom_polygon(data = datPolyLowToHigh,\n               inherit.aes = FALSE,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  ### put in the points\n  geom_point(alpha = .5) +\n  ### put in no change line\n  geom_line(data = tibNoChange,\n            aes(x = x, y = y)) +\n  ### label that\n  ggtext::geom_richtext(inherit.aes = FALSE,\n                       x = .9 * valMaxPoss, \n                       y = .9 * valMaxPoss,\n                       label = \"Line of no change\",\n                       angle = 45,\n                       hjust = 1,\n                       vjust = .5) +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### label those\n  geom_text(inherit.aes = FALSE,\n            x = csc, y = valMaxPoss - ((valMaxPoss - valMinPoss) / 45) , \n            label = paste0(\"CSC = \", csc, \"   \"),\n            size = 6,\n            hjust = 1) +\n  geom_text(inherit.aes = FALSE,\n            x = valMaxPoss, y = csc - ((valMaxPoss - valMinPoss) / 45), \n            label = paste0(\"  CSC = \", csc),\n            size = 6,\n            hjust = 1,\n            vjust = 0) +  \n  ### set limits\n  xlim(c(valMinPoss, valMaxPoss)) +\n  ylim(c(valMinPoss, valMaxPoss)) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) +\n  theme(plot.title = element_text(hjust = .5), \n        plot.subtitle = element_text(hjust = .5)) +\n  ggtitle(\"Skeleton of the Jacobson plot\",\n          subtitle = \"Real data with no change line\")\n\n\nShow code\n\ntibData %>% \n  filter(firstLastChange == 0) %>% \n  select(id) %>% \n  distinct() %>%\n  nrow() -> valNnoChange\n\n\nOK so we can now see that the emerging Jacobson plot contextualises each client’s start and finish scores into the quadrants and adding the no change line clarifies which people showed no change (here there are\nfive) with the same starting and ending scores and lying on that no change line.\nHowever, just being reminded that points lying exactly on that line had exactly the same first and last scores would add little to our understanding of our data. Fortunately, there is more to the Jacobson plot. The next important aspect of the Jacobson plot addresses the question of how much change is meaningful. There are no perfect answers to this, just as there are no perfect ways to set the CSC cutting point, but the Jacobson plot uses a method called the Reliable Change Index (RCI). This was where there was the error in the original paper, leaving out the square root of two, sqrt(2) in R code, or 1.414 to three decimal places so making the criterion quite a bit easier to exceed than it is. The beauty of the method is that it allows us to add “tramlines” either side of the no change line like this.\n\n\nShow code\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### add leading diagonal of no change\n  geom_abline(slope = 1, intercept = 0) +\n  ### add RCI tramlines\n  geom_abline(slope = 1, intercept = -rci) +\n  geom_abline(slope = 1, intercept = rci) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  ### need to change or remove these if doing monochrome version\n  scale_color_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_fill_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_shape_manual(values = vecShapesRCI, name = \"Reliable Change Index\") +\n  scale_x_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_y_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1)\n\n\n\nThose tramlines are where the change was less than the RCI, here a change of less than\n0.398 and it tells us that this amount of change could very possibly have arisen simply from the fact that all our measures are imperfect: “unreliable” in psychometric jargon. Strictly the RCI says that given the unreliability of the particular measure used and the scatter of the starting scores you would expect 95% of the changes to lie within those tramlines *had nothing else been impinging” … including had therapy had no impact.\nSo now we can colour areas in terms of the level of change.\n\n\nShow code\n\ndata.frame(x = c(0, 0, valMaxPoss - rci, valMaxPoss - rci),\n           y = c(rci, valMaxPoss, valMaxPoss, valMaxPoss)) -> datRelDetVertices\ndata.frame(x = c(rci, valMaxPoss, valMaxPoss),\n           y = c(0, valMaxPoss - rci, 0)) -> datRelImpVertices\n\n### create data frame for the RCI tramlines\ndatTramlineVertices <- data.frame(x = c(0, 0, rci, 4, 4, 4 - rci),\n                           y = c(rci, 0, 0, 4 - rci, 4, 4))\n### create data frame for the recovered area of the plot\ndatRecoveredVertices <- data.frame(x =c(csc, csc, 4, 4, csc + rci),\n                            y = c(csc - rci, 0, 0, csc, csc))\nc(\"Reliable deterioration\" = 24, \n  \"No reliable change\" = 22, \n  \"Reliable improvement\" = 25) -> vecShapesRCI\n\nc(\"Reliable deterioration\" = \"black\", \n  \"No reliable change\" = \"grey70\", \n  \"Reliable improvement\" = \"grey45\") -> vecColoursRCI\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### add reliable change polygons\n  geom_polygon(inherit.aes = FALSE,\n               data = datRelDetVertices,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  geom_polygon(inherit.aes = FALSE,\n               data = datRelImpVertices,\n               aes(x = x, y = y),\n               fill = \"green\") +  \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### add leading diagonal of no change\n  geom_abline(slope = 1, intercept = 0) +\n  ### add RCI tramlines\n  geom_abline(slope = 1, intercept = -rci) +\n  geom_abline(slope = 1, intercept = rci) +  \n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  ### need to change or remove these if doing monochrome version\n  scale_color_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_fill_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_shape_manual(values = vecShapesRCI, name = \"Reliable Change Index\") +\n  scale_x_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_y_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1)\n\n\n\nSo here is the same with the real data.\n\n\nShow code\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### add reliable change polygons\n  geom_polygon(inherit.aes = FALSE,\n               data = datRelDetVertices,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  geom_polygon(inherit.aes = FALSE,\n               data = datRelImpVertices,\n               aes(x = x, y = y),\n               fill = \"green\") +    \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### add leading diagonal of no change\n  geom_abline(slope = 1, intercept = 0) +\n  ### add RCI tramlines\n  geom_abline(slope = 1, intercept = -rci) +\n  geom_abline(slope = 1, intercept = rci) +  \n  geom_point(alpha = .5) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  ### need to change or remove these if doing monochrome version\n  scale_color_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_fill_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_shape_manual(values = vecShapesRCI, name = \"Reliable Change Index\") +\n  scale_x_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_y_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1)\n\n\n\nNot infrequently the three reliable change categories are tabulated.\n\n\nShow code\n\ntibData %>%\n  filter(occasion == 1) %>%\n  select(id, RCIchange) %>%\n  tabyl(RCIchange) %>%\n  adorn_pct_formatting(digits = 1) %>%\n  flextable() %>%\n  bg(i = 1, j = 1:3, bg = \"red\") %>%\n  bg(i = 2, j = 1:3, bg = \"grey\") %>%\n  bg(i = 3, j = 1:3, bg = \"green\") \n\nRCIchangenpercentReliable deterioration21.1%No reliable change8345.6%Reliable improvement9753.3%\n\nDual plot: trajectory plot and Jacobson\nThis next plot recaps on how the Jacobson plot is formed from the first and last scores. I have taken a few clients from different areas of the Jacobson plot. The left hand plot shows their start and finish scores as a very simple “cat’s cradle plot” and the same clients’ scores are shown on the Jacobson plot on the right so you can map between the two plots.\n\n\nShow code\n\n### To create a spurious ID code (probably excessive anonymisation)\n### create tibble of the numbers from 1 to the number of clients in the data\nvalNtot <- n_distinct(tibData$id)\n1:valNtot %>%\n  as_tibble() %>%\n  ### randomise those\n  mutate(id2 = sample(value, valNtot)) -> tmpIDs ### checking ### count(id2) %>% count(n)\n\n### I want an example from within each change category\ntibData %>%\n  filter(occasion == 1) %>%\n  ### now merge in the CSC categories\n  select(-c(firstScore, lastScore)) %>% # they get reinserted by the left_join()\n  left_join(tmpTibCSC, by = \"id\") %>%\n  ### create a spurious ID code (probably excessive anonymisation)\n  mutate(id2 = tmpIDs$id2) %>% ### checking ### count(id2) %>% count(n)\n  ### get rid of old ID codes\n  select(-id) %>%\n  mutate(absChange = abs(firstLastChange)) %>%\n  select(id2, firstScore, lastScore, firstLastChange, absChange, occasion, RCIchange, CSCchangeCategory) %>%\n  ### get for each RCSC category\n  group_by(RCIchange, CSCchangeCategory) %>% \n  mutate(minChange = min(firstLastChange), \n         maxChange = max(firstLastChange),\n         maxAbsChange = max(absChange)) %>%\n  ungroup() %>%\n  filter(absChange == maxAbsChange) -> tmpTib2\n\n### obsessionall, purge tmpIDs\nrm(tmpIDs)\n\n### pivot longer to get a simple cat's cradle plot\ntmpTib2 %>%\n  select(id2, firstScore, lastScore) %>%\n  pivot_longer(cols = -id2, names_to = \"whichOcc\", values_to = \"score\") %>%\n  ### clean up occasion name and get numeric code for it\n  mutate(whichOcc = str_to_sentence(whichOcc),\n         whichOcc = str_replace(whichOcc, fixed(\"score\"), \"\"),\n         whichOccN = if_else(whichOcc == \"First\", 1, 2)) -> tmpTib2long\n\n### using tribble() to create polygon vertices, nicer than my earlier method\ntribble(~x, ~y,\n        .9, 0,\n        .9, csc,\n        2.1, csc,\n        2.1, 0) -> tmpTibLowVertices\n\ntribble(~x, ~y,\n        .9, csc,\n        .9, valMaxPoss,\n        2.1, valMaxPoss,\n        2.1, csc) -> tmpTibHighVertices\n\nggplot(data = tmpTib2long,\n       aes(x = whichOccN, y = score, \n           group = id2)) +\n  ### colour the plot area\n  geom_hline(yintercept = csc) +\n  geom_polygon(inherit.aes = FALSE,\n               data = tmpTibLowVertices,\n               aes(x = x, y = y),\n                fill = \"green\") +\n  geom_polygon(inherit.aes = FALSE,\n               data = tmpTibHighVertices,\n               aes(x = x, y = y),\n                fill = \"red\") +\n  geom_point() +\n  ### now label the points with their id2 values\n  ### rather clumsy to get justification different for first and last points\n  geom_text(data = filter(tmpTib2long, whichOcc == \"First\"),\n            aes(label = id2),\n            colour = \"black\",\n            size = 6,\n            hjust = 1,\n            nudge_x = -.02,\n            size = 4) +\n  ### but actually I dropped these labels on the last scores\n  # geom_text(data = filter(tmpTib2long, whichOcc == \"Last\"),\n  #           aes(label = id2),\n  #           colour = \"black\",\n  #           size = 6,\n  #           hjust = 0,\n  #           nudge_x = .02,\n  #           size = 4) +\n  geom_line() +\n  ylim(c(0, 4)) +\n  ylab(\"Score\") +\n  xlab(\"Occasion\") +\n  ### colour RCI categories of improvement\n  scale_color_manual(values = vecColoursRCI) +\n  scale_x_continuous(breaks = 1:2,\n                     limits = c(.90, 2.1), \n                     labels = c(\"First\", \"Last\")) +\n  theme(legend.position = \"none\") +\n  theme(aspect.ratio = 1) -> ggplot1\n\ntribble(~x, ~y,\n        rci, 0,\n        csc, csc - rci,\n        csc, 0) -> tibRelImpStayedLow\n\ntribble(~x, ~y,\n        csc + rci,csc,\n        valMaxPoss, valMaxPoss - rci,\n        valMaxPoss, csc) -> tibRelImpStayedHigh\n\ntribble(~x, ~y,\n       csc, 0,\n       csc, csc - rci,\n       csc + rci, csc,\n       valMaxPoss, csc,\n       valMaxPoss, 0) -> tibRelClinSig \n\nggplot(tmpTib2,\n       aes(x = firstScore,\n           y = lastScore)) +\n  ### add reliable change polygons\n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelImpStayedLow,\n               aes(x = x, y = y),\n               fill = \"yellow\") +  \n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelClinSig,\n               aes(x = x, y = y),\n               fill = \"green\") +    \n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelImpStayedHigh,\n               aes(x = x, y = y),\n               fill = \"#8BC34A\") +    \n  geom_polygon(inherit.aes = FALSE,\n               data = datRelDetVertices,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### no change line\n  geom_segment(inherit.aes = FALSE,\n               x = valMinPoss, xend = valMaxPoss, y = valMinPoss, yend = valMaxPoss) +\n  ### upper tramline\n  geom_segment(inherit.aes = FALSE,\n               x = valMinPoss, xend = valMaxPoss - rci, y = valMinPoss + rci, yend = valMaxPoss) +\n  ### lower tramline\n  geom_segment(inherit.aes = FALSE,\n               x = valMinPoss + rci, xend = valMaxPoss, y = valMinPoss, yend = valMaxPoss - rci) +\n  geom_point() +\n  geom_text(aes(label = id2),\n            colour = \"black\",\n            size = 6,\n            hjust = 0,\n            nudge_x = .03,\n            size = 3) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  scale_x_continuous(limits = c(0, 4)) +\n  scale_y_continuous(limits = c(0, 4)) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1) -> ggplot2\n\n### patchwork is a package in the tidyverse that allows you to combine ggplot grobs\n### with hindsight I could have done this with cowplot or ggextra::grid()\nlibrary(patchwork)\nggplot1 + ggplot2 -> patchwork1\n\npatchwork1 +\n  plot_annotation(title = \"Selected data in a cat's cradle plot (left) and in a Jacobson plot (right)\",\n                  theme = theme(plot.title = element_text(size = 20)))\n\n\n\nReading from the top left in the cat’s cradle plot, that client showed a dramatic improvement in score, from above the CSC to just below it and you can see how that maps into the Jacobson plot from the ID code (I can’t reference the ID codes here as I have, ultra obsessionally, randomised them). The next from the top again shows a large score drop but stays above the CSC so mapping to a different quadrant … and so on.\nTypical Jacobson summary table\nAnd the tabulation most often give is the full Jacobson table of clinical change and reliable change.\n\n\nShow code\n\ntibData %>%\n  filter(occasion == 1) %>%\n  select(id, RCIchange) %>%\n  left_join(tmpTibCSC, by = \"id\") %>%\n  mutate(CSCchangeCategory = ordered(CSCchangeCategory,\n                                     levels = c(\"Clinically deteriorated\",\n                                                \"Stayed low\",\n                                                \"Stayed high\",\n                                                \"Clinically improved\"),\n                                     labels = c(\"Clinically deteriorated\",\n                                                \"Stayed low\",\n                                                \"Stayed high\",\n                                                \"Clinically improved\"))) %>%\n  tabyl(CSCchangeCategory, RCIchange) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"all\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns() %>%\n  flextable() %>%\n  ### this is a way that flextable allows you to reset the contents of individual cells\n  flextable::compose(i = 1, j = 4, as_paragraph(as_chunk(''))) %>%\n  flextable::compose(i = 4, j = 2, as_paragraph(as_chunk(''))) %>%\n  bg(i = 1, j = 2, bg = \"red\") %>%\n  bg(i = 4, j = 4, bg = \"green\") %>%\n  bg(i = 1:4, j = 3, bg = \"grey\") %>%\n  bg(i = 2:3, j = 4, bg = \"#8BC34A\") %>%\n  bg(i = 2:3, j = 2, bg = \"#EF6C00\")\n\nCSCchangeCategoryReliable deteriorationNo reliable changeReliable improvementTotalClinically deteriorated0.0% (0)0.5%  (1)0.5%   (1)Stayed low0.0% (0)6.6% (12)0.0%  (0)6.6%  (12)Stayed high1.1% (2)34.1% (62)26.9% (49)62.1% (113)Clinically improved4.4%  (8)26.4% (48)30.8%  (56)Total1.1% (2)45.6% (83)53.3% (97)100.0% (182)\n\nThe blank cells are logically impossible: no-one can show reliable improvement and clinical deterioration nor vice versa.\nFinal Jacobson plot\nThis next plot shows a five area (five polygon if you’re feeling geometrical) summary of our data.\n\n\nShow code\n\nggplot(tibData,\n       aes(x = firstScore,\n           y = lastScore,\n           shape = RCIchange,\n           colour = RCIchange,\n           fill = RCIchange)) +\n  ### add reliable change polygons\n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelImpStayedLow,\n               aes(x = x, y = y),\n               fill = \"yellow\") +  \n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelClinSig,\n               aes(x = x, y = y),\n               fill = \"green\") +    \n  geom_polygon(inherit.aes = FALSE,\n               data = tibRelImpStayedHigh,\n               aes(x = x, y = y),\n               fill = \"#8BC34A\") +    \n  geom_polygon(inherit.aes = FALSE,\n               data = datRelDetVertices,\n               aes(x = x, y = y),\n               fill = \"red\") +  \n  ### put in CSC lines\n  geom_vline(xintercept = csc) +\n  geom_hline(yintercept = csc) +\n  ### add leading diagonal of no change\n  geom_abline(slope = 1, intercept = 0) +\n  ### add RCI tramlines\n  geom_abline(slope = 1, intercept = -rci) +\n  geom_abline(slope = 1, intercept = rci) +  \n  geom_point(alpha = .5) +\n  ### axis labels\n  xlab(\"First score\") +\n  ylab(\"Last score\") +\n  ### scales\n  ### need to change or remove these if doing monochrome version\n  scale_color_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_fill_manual(values = vecColoursRCI, name = \"Reliable Change Index\") +\n  scale_shape_manual(values = vecShapesRCI, name = \"Reliable Change Index\") +\n  scale_x_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_y_continuous(limits = c(0, 4), expand = c(0,0) ) +\n  scale_size(guide=\"none\") +\n  ### theme\n  theme_bw() +\n  ### crucial setting to get square plot\n  theme(aspect.ratio = 1)\n\n\n\nThat shows these areas:\n[White] No reliable change. The level of change fell within the tramlines, so the absolute change (i.e. ignoring the sign/direction of change) fell below the RCI, here 0.398.\n[Red] Reliable deterioration. The scores got worse and by more than the RCI. Regardless of where the client started and finished on the measure, a therapist or service might want to think hard about these.\nThe next three all shows reliable improvement but fall into three groups:\n[Pale green] Reliable improvement but stayed above the CSC. Clearly there can be many reasons for this but again, these bear some thought.\n[Yellow] Reliable improvement but stayed below CSC. We didn’t have any of these but they certainly do occur, at least in services that don’t, utterly wrongly in my view, simply refuse to offer therapies to clients starting below the CSC. Clearly one important question is about why the starting score was below the CSC, were the client’s problems ones not covered well or at all by the measure used?\n[Bright green] Reliable improvement and score moved from above the CSC to below it. “Reliable and clinical improvement” or “Clinical and reliable improvement”. These are now, e.g. in the UK IAPT programme, called “reliably recovered” a term I dislike for many reasons. Certainly these are good outcomes in terms of the measure used and that’s not nothing but even here it is probably wise for therapists/services to consider these clients. One way to conduct a service/therapist audit can be to look at the red reliable deterioration cases and an equal number of those in this reliably and clinically improved group, perhaps the ones who showed the greatest score improvement. That can help a case review audit becoming persecutory.\n\nSummary\nI hope this is useful for anyone puzzled by the RCSC framework and tabulations and the Jacobson plot. If you feel that there could be improvements do contact me and I’d be happy to discuss the issues and very happy to improve this.\nI also hope that it may help people who understand the framework and plot but want code to implement it and find that that code is not readily available in statistics packages and not that easy to implement in spreadsheets (heaven forbid!) If you are in this category you will want to look at the next section about the code and I hope that better things, like functions that just do all this, will appear in CECPfuns and that online interactive shiny apps will follow those.\nMore generally I was struck going back to the basics myself that I think there are two viewpoints on the RCSC framework, or really, on the Jacobson plot:\nIt can be used simply to summarise bodies of first/last change data and generally that is just done by presenting the final table, or even by reducing this to counts of “reliably recovered” (ugh, horrible term).\nNot to discourage a full RCSC table, there is much more that a service or therapist can get from looking at their Jacobson plot: it puts individual clients’ change into a very simple but actually very useful 2D map and allows people to think more about their clients in the context of all the other clients in the plot and the referential lines.\n\nThat could take us into some serious thinking about the CSC and even more about the RCI but I will keep that for other posts and, I hope, papers. I have touched on the issue of our scores being discrete not continuous not to open up the statistical issues that creates (which are not severe even for short measures) but to remind us of the realities of our data, which I think are often hidden in tabulations and plots.\nWe should always remember that these are just questionnaire scores: we should neither undervalue, nor overvalue them, striking that balance is an ongoing process for our field.\nNotes on the code for users of R\nI’m absolutely not a professional statistician nor a good programmer. As with all of my code I provide zero guarantees about it. If you find errors please tell me and I will fix it (and credit you somehow).\nI now use R in the tidyverse way so the code may seem very strange if you don’t. Sorry, but it works for me.\nGenerally the style I use conforms to the typical tidyverse but there are few deviations, for instance I tend to prefix the names of objects with “tmp” if they are not intended to persist across code blocks and then I prefix with three letters describing the object: “val” for a value (i.e. a vector of length 1), “vec” for longer vectors and “tib” for a tibble.\nI have peppered the code with comments, too many by the rules of formal style, but I think people likely to look at this code will be, like me, not trained R coders and they will probably appreciate the comments.\nLooking forward things to add to the final code here for the tables and the plot include:\nThere were few overprinting clients with the same first and last scores in this dataset so I chose not to handle those. Clearly options are to jitter, use transparency or to use geom_count() to scale the points. No one of those is perfect for all datasets so best to allow for all three (and combinations of jittering and transparency?)\nI have ignored gender but there can be situations in which gender, and age, change the CSC (or, in principle the RCI). Tables can be aggregate or separated by gender/age etc. but plots are a bit more complex.\nIf we only have referential data or binary gender can handle this with colour for points and lines, or shape and line type for monochrome. Can also facet by gender.\nThis gets more complicated if we ever have larger enough referential datasets to have three or more gender categories and it gets quite complicated for adolescents where the CSC and the RCI may vary quite markedly with both age, down to year, and gender. That needs a lookup table for the CSC and RCI and I think facetting becomes the only realistic way to plot things.\n\n\nUpdate history of this post\n* 14/6/23: Updated to improve labelling in plots.\n* 15/6/23: Updated to add citations and references.\n* 13/8/23: Updated to add categories to the post.\n\n\n\nChristensen, L., & Mendoza, J. L. (1986). A method of assessing change in a single subject: An alteration of the RC index. Behavior Therapy, 17, 305–308.\n\n\nEvans, C., & Carlyle, J. (2021). Outcome measures and evaluation in counselling and psychotherapy (1st ed.). SAGE Publishing. https://ombook.psyctc.org/book/\n\n\nEvans, C., Margison, F., & Barkham, M. (1998). The contribution of reliable and clinically significant change methods to evidence-based mental health. Evidence Based Mental Health, 1, 70–72. https://doi.org/0.1136/ebmh.1.3.70\n\n\nJacobson, N. S., Follette, W. C., & Revenstorf, D. (1984). Psychotherapy outcome research: Methods for reporting variability and evaluating clinical significance. Behavior Therapy, 15, 336–352.\n\n\nJacobson, N. S., Follette, W. C., & Revenstorf, D. (1986). Towards a standard definition of clinically significant change. Behavior Therapy, 17, 308–311.\n\n\nJacobson, N. S., & Truax, P. (1991). Clinical significance: A statistical approach to defining meaningful change in psychotherapy research. Journal of Consulting and Clinical Psychology, 59(1), 12–19.\n\n\n\n\n",
    "preview": "posts/2023-06-10-jacobson1/jacobson1_files/figure-html5/startHere-1.png",
    "last_modified": "2023-08-25T14:22:35+02:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 2880
  },
  {
    "path": "posts/2022-09-09-drawing-psyctcorg-logo-and-other-circulargroup-things/",
    "title": "Drawing a PSYCTC.org logo (and other circular things)",
    "description": "As title says: very simple stuff about circles in ggplot, does open up somne ideas about drawing therapy groups.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-09-11",
    "categories": [
      "R graphics",
      "R tricks"
    ],
    "contents": "\n\nContents\nArguments to the function that enable you to change the pictures\nNumber of points\nSizes of points\nFill colour\nLine colour\n\n\nWays this might evolve\n\nI have used a rather horrible graphic for some PSYCTC things since the dark ages but I wanted a logo to use for the Zenodo community/repository I have created and this …\nancient gif… ancient 248x248 gif just wasn’t nice enough! To be fair, that’s the Distill/Rmarkdown blowing it up having inserted it just with\n![ancient gif](https://www.psyctc.org/psyctc/wp-content/uploads/2019/01/cropped-g2_256-1.gif)\nWhat happens if I insert it using simple html code and supplying dimensions with this inline html code?\n<img src=\"https://www.psyctc.org/psyctc/wp-content/uploads/2019/01/cropped-g2_256-1.gif\" width=\"248\" height=\"248\"/>\n\nHm. I don’t understand that, clearly I could do some more reading about embedding graphics files into distill output but that’s not the focus here. Enough playing around with that now!\nWhat I really want is a function that will draw a nicer version of that but could also be used by people writing about groups and wanting simple graphic depictions of the group.\nHere’s what I got in place on zenodo, and here it is here. More on it, and where I might got to extend this to something more generally useful, below.\n\n\nShow code\n\nmakeLogo <- function(nPoints = 8,\n                     centre = c(0, 0), # might want to change \n                     diameter = 1,\n                     nCircumfPoints = 500,\n                     circumfThickness = 1,\n                     circumfColour = \"black\",\n                     shape = 21,\n                     pointSize = 3,\n                     pointColour = \"black\",\n                     pointFill = \"white\",\n                     lineThickness = 2,\n                     lineColour = \"black\") {\n  \n  ### start by making tibble of points that make the circumference\n  makeCircumfTib <- function(centre = centre,\n                             diameter = diameter,\n                             npoints = nCircumfPoints){\n    r <- diameter / 2\n    ### sequence of points (in radians)\n    tt <- seq(0,2*pi, length.out = npoints)\n    ### get the coordinates on Cartesian plot using \n    ### elemenary trigonometry!\n    xx <- centre[1] + r * cos(tt)\n    yy <- centre[2] + r * sin(tt)\n    ### make a data frame of those values\n    tmp <- data.frame(x = xx, \n                      y = yy)\n    ### return it as a tibble\n    return(as_tibble(tmp))\n  }\n  ### use that to make the points making up the circumference\n  makeCircumfTib(centre = centre,\n                             diameter = diameter,\n                             npoints = nCircumfPoints) -> tibCircle\n  \n  ### similar to create the locations of the required numbers of points\n  makePointsTib <- function(nPoints = 8, \n                            centre = c(0,0),\n                            diameter = 1) {\n    r <- diameter / 2\n    ### this is the only difference from the circumference (which actually plots the first and last points\n    ###     at essentially the same point (except for rounding inaccuracies))\n    tt <- seq(0, 2*pi, length.out = nPoints + 1) # to get nPoints points need nPoints + 1 steps\n    xx <- centre[1] + r * cos(tt)\n    yy <- centre[2] + r * sin(tt)\n    tmp <- data.frame(x = xx, \n                      y = yy)\n    ### create a tibble of these but with an index for the points, i\n    tmp %>%\n      as_tibble() %>% \n      mutate(i = row_number()) %>%\n      select(i, x, y) -> tmpTib\n    return(tmpTib) \n  }\n  ### use that to make the tibble\n  makePointsTib(nPoints = nPoints,\n                centre = centre,\n                diameter = diameter) -> tibPoints\n  \n  ### now make the tibble for the lines starting from that last tibble\n  makeLinesTib <- function(tibPoints){\n    tibPoints %>%\n      ### rename to create a row for each line termination on the points\n      rename(j = i,\n             x1 = x,\n             y1 = y) %>%\n      mutate(nPoints = max(j)) %>% # get the number of points\n      ### and use that to replicate each row above that number of times\n      uncount(nPoints, .remove = FALSE) %>% \n      ### create a new index for each origin point for each line\n      mutate(i = row_number() %% nPoints, # modulo arithmetic to get that\n             i = if_else(j == 0, nPoints, i)) %>% # sort out first of those\n      ### now we have a row for all n^2 combinations of points\n      ### this next line avoids having each line twice and \n      ### removes the rows that create a line from the point to itself (i == j)\n      filter(i < j) %>% \n      ### now merge that tibble of n*(n-1)/2 rows with the original n point locations\n      left_join(tibPoints, by = \"i\") %>%\n      ### rename to get the second points for each line\n      rename(x2 = x,\n             y2 = y) -> tibLines\n    return(tibLines)\n  }\n  ### use that to make the tibble  of the lines\n  makeLinesTib(tibPoints) -> tibLines\n  \n  ### finally draw the logo/map\n  ggplot(tibCircle,\n         aes(x = x, y = y)) + \n    ### draw the out circle\n    geom_path(colour = circumfColour,\n              size = circumfThickness) +\n    ### draw the lines connecting the points \n    ### do this before drawing the points so the points overlay the lines\n    geom_segment(data = tibLines,\n                 aes(x = x1, y = y1, xend = x2, yend = y2),\n                 size = lineThickness,\n                 colour = lineColour) +\n    ### draw the points\n    geom_point(data = tibPoints,\n               aes(x = x, y = y),\n               shape = 21,\n               size = pointSize,\n               colour = pointColour,\n               fill = pointFill) +\n    ### a bit of fixing of the canvas\n    coord_fixed() + # get square geometry\n    theme_no_axes() + # what it says and get rid of border ...\n    theme(panel.border = element_blank()) -> gLogo\n  print(gLogo)\n}\n\n### so now we use this for nine points\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .5,\n         circumfColour = \"black\",\n         pointSize = 1,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .5,\n         lineColour = \"black\")\n\n\n\nI wanted that as a file that I could upload to Zenodo. The ggsave() function is the way I save ggplot output. I thought I should generate different sizes of file and that actually means tweaking the line widths for the graphic to look OK at the different file sizes. In the next blockI’ve put the code I used to got reasonable looking images at 256x256px, 512x512px, 1024x1024px and 2048x2048px. For each size the png file was smaller and seemed to look no worse than the jpeg. I’ve marked the block not to be run as this is just about exporting files, we don’t need the output, but I have left them here in case they might be useful to others.\n\n\nShow code\n\n### save png files\n### I have kept these lines here so people who want can copy them\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .2,\n         circumfColour = \"black\",\n         pointSize = .6,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .2,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_256x256.png\",\n       device = \"png\",\n       scale = 1,\n       units = \"px\",\n       width = 256,\n       height = 256)\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .4,\n         circumfColour = \"black\",\n         pointSize = .9,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .4,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_512x512.png\",\n       device = \"png\",\n       scale = 1,\n       units = \"px\",\n       width = 512,\n       height = 512)\n\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .5,\n         circumfColour = \"black\",\n         pointSize = 1,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .5,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_1024x1024.png\",\n       device = \"png\",\n       scale = 1,\n       units = \"px\",\n       width = 1024,\n       height = 1024)\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 2,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = 1,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_2048x2048.png\",\n       device = \"png\",\n       scale = 1,\n       units = \"px\",\n       width = 2048,\n       height = 2048)\n\n\n### save jpeg files\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .2,\n         circumfColour = \"black\",\n         pointSize = .6,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .2,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_256x256.jpeg\",\n       device = \"jpeg\",\n       scale = 1,\n       units = \"px\",\n       width = 256,\n       height = 256)\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .4,\n         circumfColour = \"black\",\n         pointSize = .9,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .4,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_512x512.jpeg\",\n       device = \"jpeg\",\n       scale = 1,\n       units = \"px\",\n       width = 512,\n       height = 512)\n\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .5,\n         circumfColour = \"black\",\n         pointSize = 1,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = .5,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_1024x1024.jpeg\",\n       device = \"jpeg\",\n       scale = 1,\n       units = \"px\",\n       width = 1024,\n       height = 1024)\n\nmakeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 2,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = 1,\n         lineColour = \"black\")\nggsave(\"PSYCTC_logo_2048x2048.jpeg\",\n       device = \"jpeg\",\n       scale = 1,\n       units = \"px\",\n       width = 2048,\n       height = 2048)\n\n\nArguments to the function that enable you to change the pictures\nThis is very simple stuff. The function prints the plot, it might be better to return the ggplot object rather than printing it, or to offer the choice between the two as a parameter or two (“print” TRUE/FALSE, or “printOrReturn”) with options “p” or “r” or two arguments, “print” TRUE/FALSE and “return” TRUE/FALSE). The arguments the function has at the moment are:\nmakeLogo(nPoints = 9, # number of points/bodies/group members\n         centre = c(0, 0),        # where to put the centre of the circle, only useful if overlaying a plot on another\n         diameter = 1,            # diameter, again only useful if overlaying or combining plots\n         nCircumfPoints = 500,    # use enough to get a smooth enough circumference at the size of your plot\n         circumfThickness = .5,   # what it says: thickness of the circumference line\n         circumfColour = \"black\", # what it says\n         pointSize = 1,           # allows you to vary this\n         pointColour = \"black\",   # what it says\n         pointFill = \"white\",     # the points default to filled circles\n         lineThickness = .5,      # thickness of the connecting lines between points (same scale as for circumference)\n         lineColour = \"black\")    # colour of the connecting lines\nHere’s some playing around with some of these.\nNumber of points\n\n\nShow code\n\nfor (nPoints in 3:15) {\n  print(paste0(\"nPoints = \", nPoints))\n  makeLogo(nPoints,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 2,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = 1,\n         lineColour = \"black\")\n}\n\n[1] \"nPoints = 3\"\n\n[1] \"nPoints = 4\"\n\n[1] \"nPoints = 5\"\n\n[1] \"nPoints = 6\"\n\n[1] \"nPoints = 7\"\n\n[1] \"nPoints = 8\"\n\n[1] \"nPoints = 9\"\n\n[1] \"nPoints = 10\"\n\n[1] \"nPoints = 11\"\n\n[1] \"nPoints = 12\"\n\n[1] \"nPoints = 13\"\n\n[1] \"nPoints = 14\"\n\n[1] \"nPoints = 15\"\n\n\nQuite pretty, I like the way that the centre is empty for odd numbers of points and a star for even numbers of points.\nSizes of points\n\n\nShow code\n\nfor (pointSize in seq(.5, 7.5, 1)) {\n  print(paste0(\"pointSize = \", pointSize))\n  makeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = pointSize,\n         pointColour = \"black\",\n         pointFill = \"white\",\n         lineThickness = 1,\n         lineColour = \"black\")\n}\n\n[1] \"pointSize = 0.5\"\n\n[1] \"pointSize = 1.5\"\n\n[1] \"pointSize = 2.5\"\n\n[1] \"pointSize = 3.5\"\n\n[1] \"pointSize = 4.5\"\n\n[1] \"pointSize = 5.5\"\n\n[1] \"pointSize = 6.5\"\n\n[1] \"pointSize = 7.5\"\n\n\nFill colour\n\n\nShow code\n\nfor (pointFill in c(\"black\", \"white\", \"red\", \"green\", \"blue\")) {\n  print(paste0(\"pointFill = \", pointFill))\n  makeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 8,\n         pointColour = \"black\",\n         pointFill = pointFill,\n         lineThickness = .3,\n         lineColour = \"black\")\n}\n\n[1] \"pointFill = black\"\n\n[1] \"pointFill = white\"\n\n[1] \"pointFill = red\"\n\n[1] \"pointFill = green\"\n\n[1] \"pointFill = blue\"\n\n\nLine colour\n\n\nShow code\n\nfor (lineColour in c(\"black\", \"white\", \"red\", \"green\", \"blue\")) {\n  print(paste0(\"lineColour = \", lineColour))\n  makeLogo(nPoints = 9,\n         centre = c(0, 0),\n         diameter = 1,\n         nCircumfPoints = 500,\n         circumfThickness = .8,\n         circumfColour = \"black\",\n         pointSize = 2,\n         pointColour = \"black\",\n         pointFill = \"black\",\n         lineThickness = 2,\n         lineColour = lineColour)\n}\n\n[1] \"lineColour = black\"\n\n[1] \"lineColour = white\"\n\n[1] \"lineColour = red\"\n\n[1] \"lineColour = green\"\n\n[1] \"lineColour = blue\"\n\n\nWays this might evolve\nI don’t think this is of enough general utility to do the necessary additions to roll it into CECPfuns now but I may.\nHowever, I do think this creates a base from which I might build something of more general use to group therapists. Sensible additions might be:\nallow different shapes (it’s typical to mark gender usually in binary as square and circle, use square, circle and diamond?), supply as a vector or list\nallow different shapes for therapists and clients (using fill colour?)\nallow addition of labels to the points (probably best to put these outside the circle)\nallow omission of lines or different colours of lines (omissiong can be done by setting colour to background colour). If so, best to supply as a tibble of the lines and sensible to add an auxiliary function making it easy to stipulate colours of bunches of lines: needs some thought.\nallow addition of arrowheads to particular lines (same issues as last point)\nallow setting of thickness of lines (same issues, watch for sensible thicknesses)\nallow omission of points (i.e. keep number of placeholders/chairs, but blank out missing person). Might make omission of lines from/to absent people automatic … or not, sometimes a lot is said about a missing person!) Could just make omission of arrowheads from the missing person/people automatic.\nallow addition of notes/labels to the whole, e.g. “Group 17, 11/9/22”. Perhaps allow four sets of such annotations: top left, top right …\nallow addition of notes/lables outside the naming/identity labels on points that would allow comments like “arrived 30 minutes late” or “very unhappy this week”\nanimate these\nmight be possible to create a standard spreadsheet that would make it easy for therapists to feed these things in for all the members for all the group occasions\nHm. All of that would be a lot of work but perhaps of very real utility. It would probably be evolving beyond just some R to something that might need some wider collaboration on the data to store on groups’ composition and activity (in the sense of those linking lines) and perhaps beyond that to an open data standard for group data more widely (e.g. adding changes in self-appraisals or group appraisals). My experience of the last decade tells me it would then really need an open standard, open source data entry system. Hm, that’s getting beyond my time and skills.\nDo contact me if you might be interested in collaborating to develop things for group therapies along the lines above.\n\n\n\n",
    "preview": "posts/2022-09-09-drawing-psyctcorg-logo-and-other-circulargroup-things/PSYCTC_logo_512x512.png",
    "last_modified": "2023-08-25T14:22:13+02:00",
    "input_file": {},
    "preview_width": 512,
    "preview_height": 512
  },
  {
    "path": "posts/2022-07-23-derangements-2/",
    "title": "Derangements #2",
    "description": "Follows on from 'Scores from matching things'",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-07-23",
    "categories": [
      "rigorous idiography",
      "method of derangements"
    ],
    "contents": "\n\n\nShow code\n\nas_tibble(list(x = 1,\n               y = 1)) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"Derangements #2\",\n            size = 20,\n            colour = \"red\",\n            angle = 30,\n            lineheight = 1) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\n\nUpdated to add contact me 11.ix.22\nIn my last post here, Scores from matching things I gave the background to the probabilities of achieving certain scores on matching tasks by chance alone to help explain the perhaps counter-intuitive finding that matching four or more things correctly is unlikely by chance alone at p < .05 regardless of the number of objects to be matched.\nThis just adds a bit more to that, mostly as plots and complements both that Rblog post and an “ordinary” blog post, Sometimes n=4 is enough.\nWhat I wanted was to show how rapidly the probabilities of achieving any particular score stabilise to an asymptotic value as n increases. Here we are for n from 4 to 15 and scores from 4 to 10.\n\n\nShow code\n\n### create some functions (as in previous post)\nall.derangements <- function(n){\n  cumprob <- prob <- number <- term <- score <- rev(0:n)\n  for (m in 1:n) {\n    i <- m+1\n    s <- n-m\n    term[i] <- ((-1)^(m))/(factorial(m))\n  }  \n  term[1] <- 1\n  for (i in 0:n) {\n    s <- i+1\n    prob[s] <- (sum(term[1:s]))/factorial(n-i)\n  }\n  number <- factorial(n)*prob\n  for (s in 0:n) {\n    m <- n-s\n    i <- m+1\n    cumprob[i] <- sum(prob[1:i])\n  }\n  tmp <- cbind(n, score,number,prob,cumprob)\n  tmp\n}\n\np.derange.score <- function(score,n){\n  if (score > n) stop(\"Score cannot be greater than n\")\n  if (score == (n-1)) stop (\"Score cannot be n-1\")\n  cumprob <- prob <- term <- rev(0:n)\n  for (m in 1:n) {\n    i <- m+1\n    s <- n-m\n    term[i] <- ((-1)^(m))/(factorial(m))\n  }  \n  term[1] <- 1\n  for (i in 0:n) {\n    s <- i+1\n    prob[s] <- (sum(term[1:s]))/factorial(n-i)\n  }\n  for (s in 0:n) {\n    m <- n-s\n    i <- m+1    \n    cumprob[i] <- sum(prob[1:i])\n  }\n  cumprob[n+1-score]\n}\n\n### now let's go a bit further\n### get all the possible scores for n from 4 to 30\nlapply(4:30, FUN = all.derangements) -> tmpList\n### I always forget this nice little bit of base R and I'm a bit surprised that there doesn't seem to be a nice tidyverse alternative\ndo.call(rbind.data.frame, tmpList) %>%\n  as_tibble() -> tmpTib\n### this was just to produce some tables for my blog post at https://www.psyctc.org/psyctc/2022/07/23/sometimes-n4-is-enough/\n# tmpTib %>% \n#   write_csv(file = \"derangements.csv\")\n\n### ditto\n# 1:14 %>% \n#   as_tibble() %>%\n#   rename(n = value) %>%\n#   mutate(PossibleWays = factorial(n),\n#          PossibleWays = prettyNum(PossibleWays, big.mark = \",\")) %>%\n#   write_csv(file = \"numbers.csv\")\n\n### but Vectorizing the function seemed cleaner so ...\nVectorize(FUN = all.derangements) -> All.derangements\nAll.derangements(4:14) -> tmpList\n### back to the do.call() just for the tables \n# do.call(rbind, tmpList) %>%\n#   as_tibble() %>%\n#   filter(score == 4) %>%\n#   select(n, number) %>%\n#   mutate(number = prettyNum(number, big.mark = \",\")) %>%\n#   write_csv(\"correct.csv\")\n\n# do.call(rbind, tmpList) %>%\n#   as_tibble() %>%\n#   filter(score == 4) %>%\n#   mutate(totalPerms = factorial(n)) %>%\n#   select(-prob) %>%\n#   select(n, totalPerms, everything()) %>%\n#   write_csv(\"final.csv\")\n  \n\n### OK, now for this Rblog post!\nAll.derangements(4:15) -> tmpList\ndo.call(rbind, tmpList) %>%\n  as_tibble() %>%\n  filter(score > 3 & score < 11) %>%\n  mutate(score = ordered(score,\n                         levels = 4:10)) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = n, y = cumprob, colour = score, group = score)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 3:15,\n                     minor_breaks = 3:15,\n                     limits = c(3, 15)) +\n  ylab(\"p\") +\n  theme_bw() \n\n\n\nHere’s the same on a log10 y axis to separate the p values for the higher scores.\n\n\nShow code\n\nggplot(data = tmpTib,\n       aes(x = n, y = cumprob, colour = score, group = score)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 3:15,\n                     minor_breaks = 3:15,\n                     limits = c(3, 15)) +\n  scale_y_continuous(trans = \"log10\") +\n  ylab(\"p\") +\n  theme_bw() \n\n\n\nThis next table shows how rapidly p values of real interest stabilise. The table is ordered by number of objects (n) within score. The column p is the probability of getting that score or better by chance alone, diffProb is the absolute change in that p value from the one for the previous n, diffPerc is the difference as a percentage of the previous p value. diffProbLT001 flags when the change in absolute p vaue is below .001 at which point I think in my realm any further precision is spurious. However, diffLT1pct flags when the change in p value is below 1% of the previous p value just in case someone wants that sort precise convergence.\n\n\nShow code\n\nAll.derangements(4:20) -> tmpList\ndo.call(rbind, tmpList) %>%\n  as_tibble() %>%\n  filter(score > 3 & score < 11)  -> tmpTib\n\n\n### just working out how stable the p values get how soon\ntmpTib %>%\n  arrange(score) %>%\n  group_by(score) %>%\n  mutate(diffProb = abs(cumprob - lag(cumprob)),\n         diffProbLT001 = if_else(diffProb < .001, \"Y\", \"N\"),\n         diffPerc = 100 * diffProb /lag(cumprob),\n         diffLT1pct = if_else(diffPerc < 1, \"Y\", \"N\")) %>% \n  ungroup() %>%\n  select(-c(number, prob)) %>%\n  rename(p = cumprob) %>%\n  select(score, everything()) %>%\n  as_hux() %>%\n  set_position(\"left\") %>% # left align the whole table\n  set_bold(row = everywhere, col = everywhere) %>% # everything into bold\n  set_align(everywhere, everywhere, \"center\") %>% # everything centred\n  set_align(everywhere, 1:2, \"right\") %>% # but now right justify the first two columns\n  map_text_color(by_values(\"Y\" = \"green\")) %>% # colour matches by text recognition\n  map_text_color(by_values(\"N\" = \"red\"))\n\nscorenpdiffProbdiffProbLT001diffPercdiffLT1pct440.0417450.008330.0333N80N460.02220.0139N167N470.01830.00397N17.9N480.01910.000868Y4.76N490.0190.000154Y0.807Y4100.0192.31e-05Y0.122Y4110.0193.01e-06Y0.0158Y4120.0193.44e-07Y0.00181Y4130.0193.53e-08Y0.000186Y4140.0193.28e-09Y1.73e-05Y4150.0192.78e-10Y1.47e-06Y4160.0192.17e-11Y1.15e-07Y4170.0191.57e-12Y8.29e-09Y4180.0191.06e-13Y5.59e-10Y4190.0196.71e-15Y3.53e-11Y4200.0193.99e-16Y2.1e-12Y550.00833560.001390.00694N83.3N570.004370.00298N214N580.00350.000868Y19.9N590.003690.000193Y5.52N5100.003663.47e-05Y0.941Y5110.003665.26e-06Y0.144Y5120.003666.89e-07Y0.0188Y5130.003667.95e-08Y0.00217Y5140.003668.2e-09Y0.000224Y5150.003667.65e-10Y2.09e-05Y5160.003666.52e-11Y1.78e-06Y5170.003665.12e-12Y1.4e-07Y5180.003663.72e-13Y1.02e-08Y5190.003662.52e-14Y6.87e-10Y5200.003661.59e-15Y4.35e-11Y660.00139670.0001980.00119N85.7N680.0007190.000521Y262N690.0005650.000154Y21.5N6100.00063.47e-05Y6.15N6110.0005936.31e-06Y1.05N6120.0005949.65e-07Y0.163Y6130.0005941.27e-07Y0.0214Y6140.0005941.48e-08Y0.00248Y6150.0005941.53e-09Y0.000258Y6160.0005941.44e-10Y2.42e-05Y6170.0005941.23e-11Y2.07e-06Y6180.0005949.67e-13Y1.63e-07Y6190.0005947.04e-14Y1.19e-08Y6200.0005944.78e-15Y8.04e-10Y770.000198782.48e-050.000174Y87.5N790.0001027.72e-05Y311N7107.88e-052.31e-05Y22.7N7118.41e-055.26e-06Y6.68N7128.31e-059.65e-07Y1.15N7138.33e-051.48e-07Y0.179Y7148.32e-051.97e-08Y0.0236Y7158.32e-052.3e-09Y0.00276Y7168.32e-052.39e-10Y0.000287Y7178.32e-052.25e-11Y2.7e-05Y7188.32e-051.93e-12Y2.32e-06Y7198.32e-051.53e-13Y1.83e-07Y7208.32e-051.12e-14Y1.34e-08Y882.48e-05892.76e-062.2e-05Y88.9N8101.27e-059.92e-06Y360N8119.67e-063.01e-06Y23.7N8121.04e-056.89e-07Y7.12N8131.02e-051.27e-07Y1.23N8141.03e-051.97e-08Y0.192Y8151.02e-052.62e-09Y0.0256Y8161.02e-053.08e-10Y0.003Y8171.02e-053.22e-11Y0.000314Y8181.02e-053.04e-12Y2.96e-05Y8191.02e-052.62e-13Y2.55e-06Y8201.02e-052.07e-14Y2.02e-07Y992.76e-069102.76e-072.48e-06Y90N9111.4e-061.13e-06Y409N9121.06e-063.44e-07Y24.6N9131.14e-067.95e-08Y7.51N9141.12e-061.48e-08Y1.3N9151.13e-062.3e-09Y0.204Y9161.13e-063.08e-10Y0.0273Y9171.13e-063.62e-11Y0.00322Y9181.13e-063.8e-12Y0.000337Y9191.13e-063.6e-13Y3.2e-05Y9201.13e-063.11e-14Y2.76e-06Y10102.76e-0710112.51e-082.51e-07Y90.9N10121.4e-071.15e-07Y458N10131.05e-073.53e-08Y25.3N10141.13e-078.2e-09Y7.85N10151.11e-071.53e-09Y1.36N10161.11e-072.39e-10Y0.215Y10171.11e-073.22e-11Y0.0289Y10181.11e-073.8e-12Y0.00341Y10191.11e-074e-13Y0.000359Y10201.11e-073.8e-14Y3.41e-05Y\nDo contact me if this interests you and if you might want to use the method with real data.\n\n\n\n",
    "preview": "posts/2022-07-23-derangements-2/derangements-2_files/figure-html5/createGraphic-1.png",
    "last_modified": "2023-08-25T14:21:42+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-07-15-matching-scores/",
    "title": "Scores from matching things",
    "description": "Mostly about the method of derangements but some huxtable!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-07-15",
    "categories": [
      "rigorous idiography",
      "method of derangements"
    ],
    "contents": "\n\nContents\nn(objects) = 3\nn(objects) = 4\nn(objects) = 5\nSummary\nContact me if you are interested in using this and want help\nHistorical footnote\n\n\n\nShow code\n\nas_tibble(list(x = 1,\n               y = 1)) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"Derangements #1\",\n            size = 20,\n            colour = \"red\",\n            angle = 30,\n            lineheight = 1) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\n\n[Created 15.vii.22, tweaked 23.vii.22 and 11.ix.22, neither changing code or outputs.]\nThe theory behind this is fully described in:\nEvans, C., Hughes, J., & Houston, J. (2002). Significance testing the validity of ideographic methods: A little derangement goes a long way.\nBritish Journal of Mathematical and Statistical Psychology, 55(2), 385–390. https://doi.org/10.1348/000711002760554525\nDo contact me through my work site if you would like a copy of that.\nThe idea is of matching things which might be purely idiographic. For example in that original paper the matching task presented\nto therapists from a prison therapy group was to see if they could match the two dimensional principal component plots from person\nrepertory grids created with elicited contructs and varying elements by each of the six members of the group. Both therapists matched\nfour of the six pre-therapy grids successfully; one therapist matched all six post-therapy grids and the other matched three of the six.\nThe paper showed that the probability of matching four or more objects correctly is always unlikely to happen by chance alone with\np < .05 regardless of the number of objects.\nAll I am doing here is using a bit of R, specifically the function permutations() from the admisc package to get all the possible permutations (i.e. ways of chosing) n objects and using a bit of tidyverse to feed this into a huxtable … i.e. into one of R’s various ways of prettifying and managing tables.\nn(objects) = 3\nLet’s start with the situation where you only have three objects (as it makes things small and simple). There are six ways of rearranging three objects, three ways to pick the first, two ways to pick the second and then of course the third one is picked for you.\nThis table shows the six possible permutations of three objects in columns 2 to 4. Then in columns 5 to 7 it shows the matching scores as “Y” or “N” depending on whether each chosen object has been put in the correct place. (Imagine that you had been given three repertory grid plots created from grids from people you knew well and you are trying to match each grid to the person who created it with no other clues.) Finally it shows the matching score.\n\n\nShow code\n\noptions(width = 160)\noptions(huxtable.knitr_output_format = \"html\") \ngetMatches <- function(vec) {\n  ### litle function that returns a vector of zero or one\n  ### depending whether the number in the vector matches its\n  ### position in the vector\n  ### I could have put some input error trapping \n  ### but no need given that I'm only using this here\n  return(as.numeric(vec == 1:length(vec)))\n}\n# getMatches(1:3)\n# getMatches(1:59)\n# getMatches(c(3, 2, 1))\n# getMatches(c(3, 1, 2))\n\n\nmatchScore <- function(vec) {\n  ### similar function to getMatches but this time returns\n  ### total score of matches\n  return(sum(vec == 1:length(vec)))\n}\n# matchScore(1:3)\n# matchScore(1:59)\n# matchScore(c(3, 2, 1))\n# matchScore(c(3, 1, 2))\n\n### I've wrapped this in suppressMessages to get rid of the irritating renaming messages from dplyr\nsuppressMessages(admisc::permutations(1:3) %>%\n                   ### that got me all the permtations of 1:3 \n                   ### but as a matrix\n                   as.data.frame() %>% # go to df (avoids warning from dplyr)\n                   as_tibble() %>% # and then to tibble!\n                   rowwise() %>% # go to rowwise mode\n                   ### and compute the matches as a list/vector\n                   mutate(matches = list(getMatches(across(everything())))) %>%\n                   ungroup() %>% # come out of rowwise (not strictly necessary)\n                   ### unnest that to separate columns\n                   unnest_wider(matches, names_sep = \"_\") %>%\n                   ### do some renaming to make things clearer\n                   rename_with( ~ gsub(\"V\", \"Choice\", .x, fixed = TRUE)) %>%\n                   rename_with( ~ gsub(\"...\", \"Match\", .x, fixed = TRUE)) %>%\n                   mutate(across(starts_with(\"Match\"), ~ if_else(.x == 1, \"Y\", \"N\"))) %>%\n                   ### back into rowwise mode\n                   rowwise() %>%\n                   ### to get the score\n                   mutate(score = matchScore(c_across(starts_with(\"Choice\")))) %>%\n                   ungroup() %>% \n                   ### create permutation number\n                   mutate(permutationN = row_number()) %>%\n                   ### rearrange order of columns\n                   select(permutationN, everything()) -> tmpTib3)\n\n\ntmpTib3 %>%\n  as_hux() %>%\n  set_position(\"left\") %>% # left align the whole table\n  set_bold(row = everywhere, col = everywhere) %>% # everything into bold\n  set_align(everywhere, everywhere, \"center\") %>% # everything centred\n  set_align(everywhere, 1, \"right\") %>% # but now right justify the first column\n  map_text_color(by_values(\"Y\" = \"green\")) %>% # colour matches by text recognition\n  map_text_color(by_values(\"N\" = \"red\"))\n\npermutationNChoice1Choice2Choice3matches_1matches_2matches_3score1123YYY32132YNN13213NNY14231NNN05312NNN06321NYN1\n(Sorry: the colour scheme isn’t great on the yellow I’ve used for this blog/site.) We can see that there is, as there will be for any number of objects, only one way of getting all of them matched correctly. There are three ways to get one matched correctly and that leaves two ways of scoring zero correct matches. There are no ways of scoring two correct matches: if you match the first two correctly then you are left with the last one which you then have to put in the correct place.\nSo nothing very impressive even about getting all three correct: you had a one in six probability of doing that by chance. Let’s go up to n = 4.\nn(objects) = 4\n\n\nShow code\n\nsuppressMessages(admisc::permutations(1:4) %>%\n                   as.data.frame() %>%\n                   as_tibble() %>%\n                   rowwise() %>%\n                   mutate(matches = list(getMatches(across(everything())))) %>%\n                   unnest_wider(matches, names_sep = \"_\") %>%\n                   rename_with( ~ gsub(\"V\", \"Choice\", .x, fixed = TRUE)) %>%\n                   rename_with( ~ gsub(\"...\", \"Match\", .x, fixed = TRUE)) %>%\n                   mutate(across(starts_with(\"Match\"), ~ if_else(.x == 1, \"Y\", \"N\"))) %>%\n                   rowwise() %>%\n                   mutate(score = matchScore(c_across(starts_with(\"Choice\")))) %>%\n                   ungroup() %>% \n                   mutate(permutationN = row_number()) %>%\n                   select(permutationN, everything()) -> tmpTib4)\n\n\ntmpTib4 %>%\n  as_hux() %>%\n  set_position(\"left\") %>%\n  set_bold(row = everywhere, col = everywhere) %>%\n  set_align(everywhere, everywhere, \"center\") %>%\n  set_align(everywhere, 1, \"right\") %>%\n  map_text_color(by_values(\"Y\" = \"green\")) %>%\n  map_text_color(by_values(\"N\" = \"red\"))\n\npermutationNChoice1Choice2Choice3Choice4matches_1matches_2matches_3matches_4score11234YYYY421243YYNN231324YNNY241342YNNN151423YNNN161432YNYN272134NNYY282143NNNN092314NNNY1102341NNNN0112413NNNN0122431NNYN1133124NNNY1143142NNNN0153214NYNY2163241NYNN1173412NNNN0183421NNNN0194123NNNN0204132NNYN1214213NYNN1224231NYYN2234312NNNN0244321NNNN0\nNow we have 24 ways of permuting the objects and still just the one correct matching of all four. As ever it’s impossible to score n - 1, i.e. three here. There are six ways of scoring two correct matches and eight ways of scoring one correct match leaving nine ways of scoring zero correct matches.\nHere’s that score breakdown.\n\n\nShow code\n\ntmpTib4 %>%\n  tabyl(score) %>%\n  adorn_pct_formatting(digits = 2) %>%\n  arrange(desc(score))\n\nscorenpercent414.17%2625.00%1833.33%0937.50%\nSo the chances of getting all four correct by chance alone was p = 1/24 = 0.04, below the conventional p < .05 criterion.\nn(objects) = 5\n\n\nShow code\n\nsuppressMessages(admisc::permutations(1:5) %>%\n                   as.data.frame() %>%\n                   as_tibble() %>%\n                   rowwise() %>%\n                   mutate(matches = list(getMatches(across(everything())))) %>%\n                   unnest_wider(matches, names_sep = \"_\") %>%\n                   rename_with( ~ gsub(\"V\", \"Choice\", .x, fixed = TRUE)) %>%\n                   rename_with( ~ gsub(\"...\", \"Match\", .x, fixed = TRUE)) %>%\n                   mutate(across(starts_with(\"Match\"), ~ if_else(.x == 1, \"Y\", \"N\"))) %>%\n                   rowwise() %>%\n                   mutate(score = matchScore(c_across(starts_with(\"Choice\")))) %>%\n                   ungroup() %>% \n                   mutate(permutationN = row_number()) %>%\n                   select(permutationN, everything()) -> tmpTib5)\n\n\ntmpTib5 %>%\n  as_hux() %>%\n  set_position(\"left\") %>%\n  set_bold(row = everywhere, col = everywhere) %>%\n  set_align(everywhere, everywhere, \"center\") %>%\n  set_align(everywhere, 1, \"right\") %>%\n  map_text_color(by_values(\"Y\" = \"green\")) %>%\n  map_text_color(by_values(\"N\" = \"red\"))\n\npermutationNChoice1Choice2Choice3Choice4Choice5matches_1matches_2matches_3matches_4matches_5score112345YYYYY5212354YYYNN3312435YYNNY3412453YYNNN2512534YYNNN2612543YYNYN3713245YNNYY3813254YNNNN1913425YNNNY21013452YNNNN11113524YNNNN11213542YNNYN21314235YNNNY21414253YNNNN11514325YNYNY31614352YNYNN21714523YNNNN11814532YNNNN11915234YNNNN12015243YNNYN22115324YNYNN22215342YNYYN32315423YNNNN12415432YNNNN12521345NNYYY32621354NNYNN12721435NNNNY12821453NNNNN02921534NNNNN03021543NNNYN13123145NNNYY23223154NNNNN03323415NNNNY13423451NNNNN03523514NNNNN03623541NNNYN13724135NNNNY13824153NNNNN03924315NNYNY24024351NNYNN14124513NNNNN04224531NNNNN04325134NNNNN04425143NNNYN14525314NNYNN14625341NNYYN24725413NNNNN04825431NNNNN04931245NNNYY25031254NNNNN05131425NNNNY15231452NNNNN05331524NNNNN05431542NNNYN15532145NYNYY35632154NYNNN15732415NYNNY25832451NYNNN15932514NYNNN16032541NYNYN26134125NNNNY16234152NNNNN06334215NNNNY16434251NNNNN06534512NNNNN06634521NNNNN06735124NNNNN06835142NNNYN16935214NNNNN07035241NNNYN17135412NNNNN07235421NNNNN07341235NNNNY17441253NNNNN07541325NNYNY27641352NNYNN17741523NNNNN07841532NNNNN07942135NYNNY28042153NYNNN18142315NYYNY38242351NYYNN28342513NYNNN18442531NYNNN18543125NNNNY18643152NNNNN08743215NNNNY18843251NNNNN08943512NNNNN09043521NNNNN09145123NNNNN09245132NNNNN09345213NNNNN09445231NNNNN09545312NNYNN19645321NNYNN19751234NNNNN09851243NNNYN19951324NNYNN110051342NNYYN210151423NNNNN010251432NNNNN010352134NYNNN110452143NYNYN210552314NYYNN210652341NYYYN310752413NYNNN110852431NYNNN110953124NNNNN011053142NNNYN111153214NNNNN011253241NNNYN111353412NNNNN011453421NNNNN011554123NNNNN011654132NNNNN011754213NNNNN011854231NNNNN011954312NNYNN112054321NNYNN1\nSo now we have 120 ways of permuting the objects and still just the one correct matching of all of them. Here’s the score breakdown.\n\n\nShow code\n\ntmpTib5 %>%\n  tabyl(score) %>%\n  adorn_pct_formatting(digits = 2) %>%\n  arrange(desc(score))\n\nscorenpercent510.83%3108.33%22016.67%14537.50%04436.67%\nIt was impossible to score four matches but getting all five correct was unlikely by chance alone at p = 1/120 = 0.008\nSummary\nIt can be seen that the number of possible ways to permute n objects goes up rapidly as n increases. That increasing number of ways of permuting things means that getting four or more correctly matched is always unlikely at p < .05 regardless of n. There’s a lookup table at https://link.psyctc.org/derangements where you can look up the scores and their probabilities for n <= 30.\nContact me if you are interested in using this and want help\nContact me here\nHistorical footnote\nThis was in my ancient derangements.R file that I clearly created while I still had access to S+:\nThis program differs from a program for S+ only in having to declare a function, factorial() which comes with S+ but not\nthe version of R on which I’m testing this (1.7.1) and in explicitly declaring tmp at the end of all.derangements() since\nR won’t return it to the console (does return it for assignment) if you just end the function with the assignment to tmp\n# factorial <- function(n) {\n#   gamma(n+1)\n# }\nI’ve often wondered which was my first R release, so it was 1.7.1 or earlier. R has long since acquired a factorial()\nfunction in the base functions.\n\n\n\n",
    "preview": "posts/2022-07-15-matching-scores/matching-scores_files/figure-html5/createGraphic-1.png",
    "last_modified": "2023-08-25T14:20:48+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-06-19-reading-data-into-r-from-msoft-access-mdb/",
    "title": "Reading data into R from M$oft Access mdb",
    "description": "Work in progress about reading data from M$ mdb file into R",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-06-19",
    "categories": [
      "R tricks",
      "data import in R",
      "database handling"
    ],
    "contents": "\n\nContents\nOS dependent issues\nWindoze\nWindoze 10\nWindoze 11\n\nOpening the mdb file from within R (windoze version)\nLinux\nOpening the mdb file from within R (Linux version)\n\nMore on using the database in R\n\n\n\nShow code\n\n# as_tibble(list(x = 1,\n#                y = 1)) -> tibDat\n# \n# ggplot(data = tibDat,\n#        aes(x = x, y = y)) +\n#   geom_text(label = \"M$oft Access mdb\",\n#             size = 20,\n#             colour = \"red\",\n#             angle = 30) +\n#   xlab(\"\") +\n#   ylab(\"\") +\n#   theme_bw() +\n#   theme(panel.grid.major = element_blank(),\n#         panel.grid.minor = element_blank(),\n#         panel.border = element_blank(),\n#         panel.background = element_blank(),\n#         axis.line = element_blank(),\n#         axis.text = element_blank(),\n#         axis.ticks = element_blank()) \n\n\nHm, second time in a row. Warning: this is pretty geeky stuff but this time it’s more general IT than statistical\nThe situation is that I have received a large lump of fascinating routine service data in M$ Access mdb format and I want to yank it into R to use it.\nThis is a “Work in progress page” as I am not yet where I want to be for this. At the moment (19.vi.22) I am going to split the issues into three:\nOS dependent issues about setting up what R needs to be done in the OS to allow it to find the data.\nWindoze (10 and 11).\nLinux (Unbuntu 22.04 LTS but probably fairly generic)\n\nIssues when you get through those issues and have R accessing the data.\nFor all this I am using the RODBC package as I’ve used it for this sort of task in the past. I think there is at least one alternative package and if someone thinks others are better, do [contact me] (https://www.coresystemtrust.org.uk/contact-form/).\nThe generic issue is whatever OS you use you need something outside R, in the OS, that creates a pipeline that can open the mdb file and offer the data there for use by R, strictly by the RODBC package. As I understand this the pipeline can be called an “ODBC” driver/connection. ODBC means Open DataBase Connectivity, see a typically good Wikipedia article at https://en.wikipedia.org/wiki/Open_Database_Connectivity to get some of the history.\nThe connections look like this.\n\n\nShow code\n\nlibrary(DiagrammeR)\n# png(filename = \"/media/chris/Clevo_SSD2/Data/MyR/R/distill_blog/test2/pipeline.png\", height = 800, width = 600, units = \"px\", bg = \"white\")\ngrViz(\"digraph flowchart {\n      # node definitions with substituted label text\n      node [fontname = Helvetica, shape = rectangle]   \n      \n      # I confess that I hate creating diagrams this way.  I can see that DiagrammeR and the tools \n      # it leans on are powerful but the syntax strikes me as archaic and doesn't lend itself to \n      # reproducability\n\n      ## Level 0 (total who opened links)\n      R [label = '@@1']\n      # edge definitions with the node IDs\n      R -> RODBC\n      RODBC -> R\n      RODBC [label = '@@2']\n      RODBC -> Connector\n      Connector -> RODBC\n      Connector [label = '@@3']\n      Connector -> mdbfile\n      mdbfile -> Connector\n      mdbfile [label = '@@4']\n      }\n\n      ### these are the names mapping to the @@@# above\n      [1]: 'R'\n      [2]: 'RODBC package'\n      [3]: 'ODBC connector (OS specific)'\n      [4]: 'Access mdb file'\n      \")\n\n\n\nShow code\n\n      # dev.off()\n\n\nThe downward arrows pass commands and the upward arrows return information. The commands include the crucial ones to open a connection and to close it but mainly will be requests for content from the data or comamands to change it. I don’t change the mdb data at all preferring to keep any data manipulation I need for execution in R when I have pulled the information in. Information coming back back up that pipeline can be purely contextually informative, e.g. that the connection has been successfully opened/closed, or may include warnings or errors, but when everything is fine, is usually just data. You can (I did) hit the challenge that some of the data may be scored in ways that are understood by Access and SQL but not liked by R. I actually only hit that when I tried to pull tables (the name for rectangular lumps of data in databases) through to tibbles. Tibbles don’t like “binary” data which is a legitimate column content in Access. That pulls through to R dataframes but as_tibble() spits at it saying it will only accept vector data. So brace yourself if you like using tibbles as I do, to pull the data into dataframes and then work out what to do with that little challenge.\nSo what is “SQL”? To a greater or lesser extent the command side of things may or may not be using SQL. I’m not clear whether it is always formally true that it is or not but I think it’s safe to behave as though it is. SQL is Structured Query Language (https://en.wikipedia.org/wiki/SQL). You can ignore this if working in Windoze but I think that you have to use a tiny bit of SQL if working in Linux to make sure the Linux database system you use as part of the connector there is set up to work for you.\nOS dependent issues\nWindoze\nI hate to admit it but this is much easier in Windoze than I have found it so far in Ubuntu. The connector is specific the mdb file you want to access and is called a DSN (Data Source Name) and is pretty much that, actually it gives a name to the file and, vitally, it tells Windoze which particular ODBC driver Windoze will need to access the file and create the link between client program (which doesn’t have to be R with RODBC but could be anything that understands Windoze DSNs). There are various routes to create a DSN but this works for me.\nWindoze 10\nStart menu >> Windoze >> Administrative tools >> ODBC Data Sources (64 bit) then that gets you something like this. (If you have a 32 bit version of Windoze 10, you will need ODBC Data Sources (32 bit)) I think but presumably that’s the only version it will be offering you. (Scratch the interweb to see how to find out if your version of Windoze is 64 bit or 32 bit.)\nODBC Data Sources Windoze 10, screenshot 1The great thing is that you can ignore almost everything in that rather packed little bit of screen space! The main thing is that listing in the middle. In that one there are four rows now and the one that starts “EOS” is the one I created for the M$ file I want to access which is called EOS.mdb (you can call the DSN anything you want). If you have never created any you won’t see that one but you should see the other lines. To create a new DSN you hit the Add button and get to this.\nODBC Data Sources Windoze 10, screenshotChoose the Access driver (selected by default, as you see there, I think). Then hit the “Finish” button (which is a very silly label as you’re not done yet!)\nODBC Data Sources Windoze 10, screenshot 2Now you fill in the name you want for the DSN (you will need this to access the data from R), the optional description and then you hit the Select button to get here.\nODBC Data Sources Windoze 10, screenshot 3Navigate to the mdb file you want to access and select it and hit the “OK” button which gets you back to this.\nODBC Data Sources Windoze 10, screenshot 4You can see that the path to the mdb file is now in there. Now hit that “OK” button and you are done: you have created your DSN and will be able to access it using RODBC from within R.\nWindoze 11\nYou find ODBC Data Sources (64 bit) from the search icon on the task bar, after that everthing is the same as for Windoze 10.\nOpening the mdb file from within R (windoze version)\nThis gets you the idea but I’m running out of time for this bit of “work in progress”. so I will have to come back to it to explain it and go into some wrinkles there. They may be unnecessary for you.\nlibrary(RODBC) # has the vital functions to access the database\nlibrary(tidyverse) # optional but I like working in the tidyverse manner if I can\n# rm(list = ls()) # you might want to do this if you aren't using a new session and are 100% certain you won't delete anything you'll miss\n\n\n### this opens the mdb file pointed to in the DSN created in Windoze\nconnEOS <- odbcConnect(\"EOS\")\n\nallTables3 <- sqlTables(connEOS) # get a list of all the tables in the database\nallTables3\n\n### pull one table through\nsqlFetch(connEOS, \"vw_Prodotti\") %>% \n  as_tibble()\n\n### close the connection\nodbcClose(connEOS)\nLinux\nThere are clearly many ways of doing this and I found much information on the interweb. Some I could ignore as it was for commercial options and I wanted a fully open source way of doing things. In the end I gave up after a few hours failing to get it to work but I will summarise what I did in the hope that someone will tell me how to get all the way there. If I find a way that works for me I will document it fully here as I really think a clear summary is not out there. In what folllows I am mostly following https://gist.github.com/amirkdv/9672857. I think everything needs libodbc1 but I think that’s installed by default in Ubuntu.\nchris@Clevo1:sudo apt-get install libodbc1\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nlibodbc1 is already the newest version (2.3.9-5).\n0 to upgrade, 0 to newly install, 0 to remove and 0 not to upgrade.\nI think the first thing we need that isn’t installed by default is the unixodbc package.\nsudo apt-get instal unixodbc\nshould get it.\nOne thing that is useful is the mdbtools package. You can (of course) install that with:\nsudo apt-get install mdbtools\nand it gives you the command mdb-tables which lists the tables in an mdb file:\n### find the tables in your mdb file\nchris@Clevo1:mdb-tables EOS.mdb \ndbo_Richieste dbo_TabAreaScala dbo_TabCollocazioneAmbientale dbo_TabInviante dbo_TabOperatori dbo_TabPrestazioni dbo_TabProdotti dbo_TabProfessioni dbo_TabQualifiche dbo_TabSettoriEconomici dbo_TabStatiCivili dbo_TabTipoDimissione dbo_TabTipoScala dbo_Valutazione dbo_ValutazioneRisposte dbo_vw_AnamnesiQuestionario vw_CartelleLista vw_ChiaviMultiple vw_Diagnosi vw_PazientiLista vw_Prestazioni vw_Prodotti vw_ProdottiPrestazioni vw_ScaleValutazione dbo_TabDistretto dbo_TabScolarita dbo_vw_AnamnesiRisposte\n\n### find the number of rows in any particular table\nchris@Clevo1:mdb-count EOS.mdb dbo_Richieste\n49600\n\n### export any table to CSV\nchris@Clevo1:mdb-export EOS.mdb dbo_Richieste > dbo_Richieste.csv\nThat was reassuring not least in confirming that my Ubuntu 22.04 had no problem opening and reading the mdb file. In principle I believe it should be possible to create a connector a bit like a DSN in Windoze using the odbc-mdbtools package so I install that in command terminal:\nsudo apt-get odbc-mdbtools\nIn principle what you need now is to add two blocks, one to /etc/obcinst.ini which defines the driver:\n[MDBTools]\nDescription = MDBTools Driver\nDriver      = libmdbodbc.so.1\nSetup       = libmdbodbc.so.1\nFileUsage   = 1\nUsageCount  = 1\nand then one in /etc/odbc.ini which defines the source file (EOS.mdb):\n[EOS]\nDescription=EOS database\nDriver=MDBTools\nDatabase=/media/chris/Clevo_SSD2/Data/CORE/translations/Italian/Modenaplus_CORE-OM/EOS.mdb\nIn principle that should work and the interactive sql command isql from the unixodbc package ought to open the file but for me it doesn’t:\nisql -v EOS\n[IM002][unixODBC][Driver Manager]Data source name not found and no default driver specified\n[ISQL]ERROR: Could not SQLConnect\nThe RDBOC route from R gives the same message. I have tried copying those ini files to my home directory ~, i.e. /home/chris/ but that doesn’t change anything. Permissions look fine to me:\nchris@Clevo1:ls -lsart /etc/odbc*\n4 -rw-r--r-- 1 root root 145 Jun 19 15:18 /etc/odbc.bak\n4 -rw-r--r-- 1 root root 530 Jun 19 15:18 /etc/odbcinst.bak\n4 -rw-r--r-- 1 root root 139 Jun 19 16:05 /etc/odbcinst.ini~\n4 -rw-r--r-- 1 root root 132 Jun 19 16:05 /etc/odbc.ini~\n4 -rw-r--r-- 1 root root 138 Jun 19 16:21 /etc/odbc.ini\n4 -rw-r--r-- 1 root root 134 Jun 19 16:21 /etc/odbcinst.ini\nchris@Clevo1:ls -lsart ~/odbc*\n4 -rw-r--r-- 1 chris chris 139 Jun 19 16:11 /home/chris/odbcinst.ini~\n4 -rw-r--r-- 1 chris chris 133 Jun 19 16:11 /home/chris/odbcinst.ini\n4 -rw-r--r-- 1 chris chris 530 Jun 19 16:11 /home/chris/odbcinst.bak\n4 -rw-r--r-- 1 chris chris 132 Jun 19 16:11 /home/chris/odbc.ini~\n4 -rw-r--r-- 1 chris chris 326 Jun 19 16:11 /home/chris/odbc.ini\n4 -rw-r--r-- 1 chris chris 145 Jun 19 16:11 /home/chris/odbc.bak\nIf anyone can put me straight about what I’m doing wrong, I’d really appreciate it, do [contact me] (https://www.coresystemtrust.org.uk/contact-form/).\nOpening the mdb file from within R (Linux version)\nSame as in Windoze, see above.\nMore on using the database in R\nTo come.\n\n\n\n",
    "preview": "posts/2022-06-19-reading-data-into-r-from-msoft-access-mdb/pipeline_exported.png",
    "last_modified": "2023-08-25T14:20:26+02:00",
    "input_file": {},
    "preview_width": 851,
    "preview_height": 734
  },
  {
    "path": "posts/2022-04-18-exact-confidence-intervals-for-difference-between-proportions/",
    "title": "Exact confidence intervals for difference between proportions",
    "description": "This describes the ExactCIdiff package, unpacks the arguments to the functions, looks at timings and notes an oddity, a typo I think, in the original paper about the package.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-04-18",
    "categories": [
      "confidence intervals",
      "proportions",
      "exact confidence intervals"
    ],
    "contents": "\n\nContents\nBack to the story: background\nDifference between proportions in two separate samples\nThe exact CI of the difference\n\nPaired difference of proportions\nDigression about tables in R and in Rmarkdown\nExact CI for a difference between paired proportions\n\n\nComputation times\nUnpaired example: CIs and timings\n\nSummary\n\nWarning: this is pretty geeky statistical stuff\nWarning2: the package is no longer on CRAN\n## Installing package from source\nUpdate added 26.vi.22\nThe ExactCIdiff package has dropped off CRAN because the authors/maintainers aren’t responding to the CRAN team and a URL in the package is not openning for CRAN. This doesn’t affect the code in the package.\nYou can download the source package from https://cran.r-project.org/src/contrib/Archive/ExactCIdiff/. It’s the last version, v1.3 you want which is https://cran.r-project.org/src/contrib/Archive/ExactCIdiff/ExactCIdiff_1.3.tar.gz\n### you can download it before launching R:\n### install.packages(\"~/Downloads/ExactCIdiff_1.3.tar.gz\", repos = NULL, type = \"source\")\n### replace \"~/Downloads/\" with the directory into which you downloaded it of course!\n\n### or, as I should have remembered, you can omit downloading the package beforehand and just grab it withing the \n### install.packages() call:\ninstall.packages(\"https://cran.r-project.org/src/contrib/Archive/ExactCIdiff/ExactCIdiff_1.3.tar.gz\", repos = NULL, type = \"source\")\n\nThat’s that solved!\nBack to the story: background\nI came on this as Clara and I need to look at the differences in proportions between two arms of our “elephant” study (we call it that because it’s BIG!) I knew that I wasn’t confident about the best way to get 95% confidence intervals (CIs) for a difference between two proportions. I have been using Hmisc::binconf() for certainly over ten years for CIs around a single proportion but knew this was a bit different. A bit of searching led to the ExactCIdiff package and to the paper about it: Shan, G., & Wang, W. (2013). ExactCIdiff: An R Package for Computing Exact Confidence Intervals for the Difference of Two Proportions. The R Journal, 5(2), 62. https://doi.org/10.32614/RJ-2013-026.\nI can’t follow the maths of the method but I do follow the evidence that it does better in terms of coverage probability (the actual probability that it will include the population value) than other methods. It’s a clean package and a nice paper and I say it’s a clean package as it appears to do just two things and the two things it sets out to do, and to do them well.\nThe two things are to give you a CI around an observed difference in proportions for a paired sample (e.g. proportion above a cut-off at baseline and after therapy) or for the same but for unconnected samples (what we have: students versus non-student people of the same age group).\nI will start with the latter.\nDifference between proportions in two separate samples\nI’ll quote from their paper:\n\nThe second data set is from a two-arm randomized clinical trial for testing the effect of tobacco smoking on mice (Essenberg, 1952). In the treatment (smoking) group, the number of mice is n 1 = 23, and the number of mice which developed tumor is x = 21; in the control group, n 2 = 32 and y = 19.\n\nUgh, not my world but it’s their paper. So here’s their data, rows as smoking/non-smoking groups and columns as whether the poor things developed tumours.\n\n\nShow code\n\ntribble(~smoking, ~tumour, ~n,\n        1, 1, 21,\n        1, 0, 2,\n        0, 1, 19,\n        0, 0, 13) %>%\n  uncount(n) -> tibPoorMice\n\ntibPoorMice %>%\n  tabyl(smoking, tumour) %>%\n  adorn_totals(where = c(\"row\", \"col\")) \n\n smoking  0  1 Total\n       0 13 19    32\n       1  2 21    23\n   Total 15 40    55\n\nAnd with percentages.\n\n\nShow code\n\ntibPoorMice %>%\n  tabyl(smoking, tumour) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"row\") %>%\n  adorn_pct_formatting(digits = 1)\n\n smoking     0     1  Total\n       0 40.6% 59.4% 100.0%\n       1  8.7% 91.3% 100.0%\n   Total 27.3% 72.7% 100.0%\n\nPretty clear that’s going to be statistically significant … and it is.\n\n\nShow code\n\ntibPoorMice %>%\n  tabyl(smoking, tumour) %>%\n  chisq.test()\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  .\nX-squared = 5.3625, df = 1, p-value = 0.02057\n\nBut what we want is the confidence interval. Here are the separate intervals.\n\n\nShow code\n\ntibPoorMice %>% \n  group_by(smoking) %>%\n  ### bit back to square one given I stared with the raw numbers but \n  ### this is how it would be with real data\n  summarise(n = n(),\n            nTumour = sum(tumour == 1),\n            ### this is a bit messy, Hmisc::binconf returns a matrix\n            ### but I just want the first row, hence the \"[1,]\"\n            ### and of course I need the list as that's how dplyr\n            ### has to be told that what it's getting is a list or\n            ### a vector (as here)\n            binconf = list(Hmisc::binconf(nTumour, n)[1,])) %>%\n  ### OK, unnest that\n  unnest_wider(binconf) %>%\n  ### recode smoking to a factor\n  mutate(smoking = ordered(smoking,\n                           levels = 0:1,\n                           labels = c(\"Smoked mice\",\n                                      \"Lucky ones\"))) -> tmpTib \n\n### print that\ntmpTib %>%\n  ### do some rounding across the three values PointEst to Upper\n  mutate(across(PointEst:Upper, round, 2)) %>%\n  pander(justify = \"rrrrrr\")\n\nsmoking\nn\nnTumour\nPointEst\nLower\nUpper\nSmoked mice\n32\n19\n0.59\n0.42\n0.74\nLucky ones\n23\n21\n0.91\n0.73\n0.98\n\nAnd here as a plot as I am such a believer in offering both tabulated and plotted data where possible as some people find tables easier to digest (more precision) and others find the plots easier (more, hm, visual impact!)\n\n\nShow code\n\n### get overall proportion for reference line\ntibPoorMice %>%\n  summarise(nTumour = sum(tumour == 1),\n            n = n(),\n            prop = nTumour / n) %>%\n  select(prop) %>%\n  pull() -> tmpAllProp\n\nggplot(data = tmpTib,\n       aes(x = smoking, y = PointEst)) +\n  geom_point() +\n  geom_linerange(aes(ymin = Lower, ymax = Upper)) +\n  geom_hline(yintercept = tmpAllProp) +\n  scale_y_continuous(name = \"Proportion\", breaks = seq(0, 1, .1), limits = c(0,1)) +\n  xlab(\"Whether the poor mice were smoked or not!\") +\n  ggtitle(\"95% confidence intervals for proportions\")\n\n\n\nThe exact CI of the difference\nSo now (finally) we come to ExactCIdiff! The function is BinomCI and the syntax is that you give the four numbers from the crosstabulation, as n1, n2, count1, count2, so here 23, 32, 21, 19:\nBinomCI(23, 32, 21, 19, conf.level = ?, CItype = ?)\nI’m going to take things in the order that the authors do in their paper, starting with:\nuci <- BinomCI(23, 32, 21, 19, conf.level = 0.95, CItype = \"Upper\")$ExactCI so the one-sided upper 95% confidence limit.\n\n\nShow code\n\nSys.time() -> time1\nuci <- BinomCI(23, 32, 21, 19, conf.level = 0.95, CItype = \"Upper\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs1 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nuci\n\n[1] -1.00000  0.48595\n\nThat’s a one-sided interval telling me that the upper 95% confidence limit for the difference is 0.48595: big. The computations are CPU intensive, that took 1 minutes on a fairly powerful laptop. I will generally want a two-sided interval and their next three calls to BinomCI demonstrate the relationship between the two one-sided 97.5% confidence limits and the two-sided CI. First the upper 97.5% limit.\n\n\nShow code\n\nSys.time() -> time1\nu975 <- BinomCI(23, 32, 21, 19, conf.level = 0.975, CItype = \"Upper\")$ExactCI\nSys.time() -> time2\nu975\n\n[1] -1.00000  0.51259\n\nShow code\n\nelapsedTimeSecs2 <- as.numeric(difftime(time2, time1, units = \"secs\"))\n\n\nSo upper 97.5% limit 0.51259 (elapsed time 31.7 seconds).\n\n\nShow code\n\nSys.time() -> time1\nl975 <- BinomCI(23, 32, 21, 19, conf.level = 0.975, CItype = \"Lower\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs3 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nl975\n\n[1] 0.09468 1.00000\n\nShow code\n\n# [1] 0.09468 1.00000\n\n\nSo lower 97.5% limit 0.09468 (elapsed time 6.8 seconds).\n\n\nShow code\n\nSys.time() -> time1\nci95 <- BinomCI(23, 32, 21, 19)$ExactCI\nSys.time() -> time2\nelapsedTimeSecs4 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nci95\n\n[1] 0.09468 0.51259\n\nShow code\n\n# [1] 0.09468 0.51259\n\n\nAnd it can be seen there that the two-sided 95% CI is from 0.09468 to 0.51259, i.e. from the lower 97.5% CL to the upper 97.5% CL. (Elapsed time\n38.5 seconds).\nPaired difference of proportions\nHere again I’ll quote from the paper:\n\nWe illustrate the usage of the PairedCI() function to calculate the exact smallest lower one-sided confidence interval [LP , 1] for θP in (1) with the data from Karacan et al. (1976). In this study, 32 marijuana users are compared with 32 matched controls with respect to their sleeping difficulties, with n11 = 16, n12 = 9, n21 = 3, and n22 = 4. The second argument in the function is t = n11 + n22 = 20.\n\nThe “(1)” refers back to the first equation in the paper which I won’t copy in here as it would need some formatting and doesn’t really matter for our purposes.\n\n\nShow code\n\n# ```{r makeTable, results='asis'}\n# tmpVec <- c(\"\", \"Success at t2\", \"Failure at t2\", \"\",\n#             \"Success at t1\", \"N11, p11\", \"N12, p12\", \"p1 = p11 + p12\",\n#             \"Failure at t1\", \"N21, p21\", \"N22, p22\", \"\",\n#             \"\", \"p2 = p11 + p21\", \"\", \"Total, p = 1\")\n\ntmpVec <- c(\"\", \"Success at t2\", \"Failure at t2\", \"\",\n            \"Success at t1\", \"N11, p11\", \"N12, p12\", \"\",\n            \"Failure at t1\", \"N21, p21\", \"N22, p22\", \"\",\n            \"\", \"\", \"\", \"Total, p = 1\")\n\ntmpMat <- matrix(tmpVec, ncol = 4)\n# print(xtable::xtable(tmpMat, type = \"html\"))\n# print(xtable::xtable(tmpMat, getOption(\"xtable.type\", \"html\")))\n# knitr::kable(tmpMat, \"html\")\n\ntmpMat %>%\n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\")) %>%\n  # kable_styling() %>%\n  row_spec(1, align = \"c\", bold = TRUE) %>%\n  row_spec(2:4, align = \"c\") %>%\n  column_spec(1, bold = TRUE, border_right = TRUE) %>%\n  column_spec(2, border_right = TRUE) %>%\n  column_spec(3, border_right = TRUE) \n\n\n\n\nSuccess at t1\n\n\nFailure at t1\n\n\n\n\nSuccess at t2\n\n\nN11, p11\n\n\nN21, p21\n\n\n\n\nFailure at t2\n\n\nN12, p12\n\n\nN22, p22\n\n\n\n\n\n\n\n\n\n\nTotal, p = 1\n\n\nI do find this way of describing a contingency table pretty counterinuitive!\nDigression about tables in R and in Rmarkdown\nGRRrrrr!!! I continue to feel that table handling in R is almost its Achilles heel. I’ve just wasted the better part of an hour finding out a way to get that table in an even halfway, no quarterway, decent form. I think I first commented on this perhaps twenty years ago and the R team position has always been, I think, that nice tables are for packages to fix and so we have multiple packages that try to fix this, mostly incompatible and none of them working reliably in Rmarkdown and with all output formats from Rmarkdown. I think the R afficionados all love knocking up tables in LaTeX and I’m sure that’s fine if you are really familiar with LaTeX and I suspect that direct R to LaTeX is the most robust and general way to do things but many of us don’t know TeX/LaTeX and don’t really want to have to learn it. Aarghhhh! OK, flame over!\nBack to the data here.\n\n\nShow code\n\ntmpVec2 <- c(\"\", \"Sleep OK, no smokes\", \"Sleep poor, no smokes\", \"\",\n            \"Sleep OK, smokes\", \"16\", \"9\", \"\",\n            \"Sleep poor, smokes\", \"3\", \"4\", \"\",\n            \"\", \"\", \"\", \"Total, p = 1\")\n\ntmpMat2 <- matrix(tmpVec2, ncol = 4)\n# print(xtable::xtable(tmpMat, type = \"html\"))\n# print(xtable::xtable(tmpMat, getOption(\"xtable.type\", \"html\")))\n# knitr::kable(tmpMat, \"html\")\n\ntmpMat2 %>%\n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\")) %>%\n  # kable_styling() %>%\n  row_spec(1, align = \"c\", bold = TRUE) %>%\n  row_spec(2:4, align = \"c\") %>%\n  column_spec(1, bold = TRUE, border_right = TRUE) %>%\n  column_spec(2, border_right = TRUE) %>%\n  column_spec(3, border_right = TRUE) \n\n\n\n\nSleep OK, smokes\n\n\nSleep poor, smokes\n\n\n\n\nSleep OK, no smokes\n\n\n16\n\n\n3\n\n\n\n\nSleep poor, no smokes\n\n\n9\n\n\n4\n\n\n\n\n\n\n\n\n\n\nTotal, p = 1\n\n\nExact CI for a difference between paired proportions\nThat means that the code is:\nPairedCI(9, 20, 3, conf.level = 0.95)\nbecause the syntax is\nPairedCI(n12, t, n21, conf.level, CItype, precision, grid.one, grid.two)\nwhere we can ignore grid.one and grid.two for now and leave them at their default values of 30 and 20 and precision is, as the help says:\nPrecision of the confidence interval, default is 0.00001 rounded to 5 decimals.\nOK, so here we go with:\nPairedCI(9, 20, 3, conf.level = 0.95)\n\n\nShow code\n\nSys.time() -> time1\nlciall <- PairedCI(9, 20, 3, conf.level = 0.95) # store relevant quantities\nSys.time() -> time2\nelapsedTimeSecs5 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nlciall\n\n$conf.level\n[1] 0.95\n\n$CItype\n[1] \"Two.sided\"\n\n$estimate\n[1] 0.1875\n\n$ExactCI\n[1] -0.03564  0.39521\n\nShow code\n\n# $conf.level\n# [1] 0.95\n# \n# $CItype\n# [1] \"Two.sided\"\n# \n# $estimate\n# [1] 0.1875\n# \n# $ExactCI\n# [1] -0.03564  0.39521\n\n\n(Elapsed time\n15.8 seconds.)\nThe odd thing here is that this is not what the authors show in the paper:\nlciall  # print lciall  \n$conf.level  \n[1] 0.95    # confidence level  \n$CItype  \n[1] \"Lower\" # lower one-sided interval    \n$estimate  \n[1] 0.1875  # the mle of p1 - p2  \n$ExactCI \n[1] 0.00613 1.00000 # the lower one-sided 95% interval  \nlci <- lciall$ExactCI # extracting the lower one-sided 95% interval  \nlci         # print lci  \n[1] 0.00613 1.00000  \n\nThe use of marijuana helps sleeping because the interval [ 0.00613, 1 ] for θP is positive.\n\nWhich is clearly not what I just got. However, in the paper they go on:\nThe upper one-sided 95% interval and the two-sided 95% interval for θ~P~ are given below for illustration purpose.\nI think that’s a typo. I think what they are showing are the results of\nPairedCI(9, 20, 3, conf.level = 0.95, CItype = \"lower\")\nLet’s see:\n\n\nShow code\n\nSys.time() -> time1\nlciall <- PairedCI(9, 20, 3, conf.level = 0.95, CItype = \"Lower\") # store relevant quantities\nSys.time() -> time2\nelapsedTimeSecs5 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nlciall\n\n$conf.level\n[1] 0.95\n\n$CItype\n[1] \"Lower\"\n\n$estimate\n[1] 0.1875\n\n$ExactCI\n[1] 0.00613 1.00000\n\nYes! (Elapsed time\n3.5 seconds.)\nThey do go on to give us other things in the paper and that I think confirms that the above call was a typo.\nSo here is the upper 95% CL.\n\n\nShow code\n\nSys.time() -> time1\nuci <- PairedCI(9, 20, 3, conf.level = 0.95, CItype = \"Upper\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs7 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nuci\n\n[1] -1.00000  0.36234\n\nShow code\n\n# [1] -1.00000  0.36234\n\n\n(Elapsed time\n12.5 seconds.)\nThe upper 97.5% CL.\n\n\nShow code\n\nSys.time() -> time1\nu975 <- PairedCI(9, 20, 3, conf.level = 0.975, CItype = \"Upper\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs8 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nu975\n\n[1] -1.00000  0.39521\n\nShow code\n\n# [1] -1.00000  0.39521\n\n\n(Elapsed time\n12.2 seconds.)\nThe lower 97.5% CL.\n\n\nShow code\n\nSys.time() -> time1\nl975 <- PairedCI(9, 20, 3, conf.level = 0.975, CItype = \"Lower\")$ExactCI\nSys.time() -> time2\nelapsedTimeSecs9 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nl975\n\n[1] -0.03564  1.00000\n\nShow code\n\n# [1] -0.03564  1.00000\n\n\n(Elapsed time\n3.4 seconds.)\nAnd back to the two-sided 95% CI (and yes, I’m running it again just to be sure I get the same answer as last time!)\n\n\nShow code\n\nSys.time() -> time1\nci95 <- PairedCI(9, 20, 3, conf.level = 0.95)$ExactCI\nSys.time() -> time2\nelapsedTimeSecs10 <- as.numeric(difftime(time2, time1, units = \"secs\"))\nci95\n\n[1] -0.03564  0.39521\n\nShow code\n\n# [1] -0.03564  0.39521\n\n\n(Elapsed time\n15.6 seconds.)\nYup, the same again and fits with what they say in the paper:\n[1] -0.03564 0.39521 # the two-sided 95% interval\n                      # it is equal to the intersection of two one-sided intervals\nComputation times\nClearly one issue here is that these are small total sample sizes in their two examples but the process is computationally expensive (though 40x faster than another approach with the same accuracy/coverage).\n\n\nShow code\n\nvecMultipliers <- 1:4 # check for sample sizes 1 to 8x the example above\nvecTimesPaired <- rep(NA, length(vecMultipliers))\nmatCI95paired <- matrix(rep(NA, length(vecMultipliers) * 2), ncol = 2)\n\nvecTimesPaired[1] <- elapsedTimeSecs10\nmatCI95paired[1, ] <- ci95\n\nfor (mult in vecMultipliers[-1]) {\n  Sys.time() -> time1\n  matCI95paired[mult, ] <- PairedCI(mult * 9, mult * 20, mult * 3, conf.level = 0.95)$ExactCI\n  Sys.time() -> time2\n  vecTimesPaired[mult] <- as.numeric(difftime(time2, time1, units = \"secs\"))\n}\n\n\nHere are those CIs getting tighter as the numbers go up.\n\n\nShow code\n\nmatCI95paired %>%\n  as_tibble() %>%\n  bind_cols(vecMultipliers) %>%\n  rename(nPairs = `...3`,\n         LCL = V1,\n         UCL = V2) %>%\n  mutate(nPairs = 32 * nPairs) %>%\n  select(nPairs, everything()) -> tibCI95paired\n\ntibCI95paired %>%\n  pander::pander()\n\nnPairs\nLCL\nUCL\n32\n-0.03564\n0.3952\n64\n0.03751\n0.3396\n96\n0.06301\n0.3067\n128\n0.07231\n0.2906\n\nShow code\n\nggplot(data = tibCI95paired,\n       aes(x = nPairs)) +\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) +\n  geom_hline(yintercept = 0.1875) +\n  ylab(\"Difference in proportions\") +\n  scale_x_continuous(name = \"Number of pairs\",\n                     breaks = vecMultipliers * 32) +\n  ggtitle(\"Two sided 95% CI tightening with increasing sample size\",\n          subtitle = \"Horizontal reference line is observed difference in proportions\")\n\n\n\nAnd here are the times (in seconds).\n\n\nShow code\n\nlibrary(tidyverse)\nvecTimesPaired %>%\n  as_tibble() %>%\n  rename(timeSecs = value) %>%\n  bind_cols(vecMultipliers) %>%\n  rename(nPairs = `...2`) %>%\n  mutate(nPairs = 32 * nPairs) %>% \n  select(nPairs, timeSecs) -> tibTimesPaired\n\ntibTimesPaired %>%\n  pander(justify = \"rr\")\n\nnPairs\ntimeSecs\n32\n15.63\n64\n167.3\n96\n243.8\n128\n550.7\n\nShow code\n\nggplot(data = tibTimesPaired,\n       aes(x = nPairs, y = timeSecs)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"Elapsed time (seconds)\") +\n  xlab(\"Number of pairs of participants\") +\n  ggtitle(\"Plot of computation time against sample size\")\n\n\n\nHm. That doesn’t look that far off linear which is not what I had expected.\nUnpaired example: CIs and timings\n\n\nShow code\n\nci95 <- BinomCI(23, 32, 21, 19)$ExactCI\n\nvecTimesUnpaired <- rep(NA, length(vecMultipliers))\nmatCI95unpaired <- matrix(rep(NA, length(vecMultipliers) * 2), ncol = 2)\n\nfor (mult in vecMultipliers) {\n  Sys.time() -> time1\n  matCI95unpaired[mult, ] <- PairedCI(mult * 9, mult * 20, mult * 3, conf.level = 0.95)$ExactCI\n  Sys.time() -> time2\n  vecTimesUnpaired[mult] <- as.numeric(difftime(time2, time1, units = \"secs\"))\n}\n\n\n\n\nShow code\n\nmatCI95unpaired %>%\n  as_tibble() %>%\n  bind_cols(vecMultipliers) %>%\n  rename(nTotal = `...3`,\n         LCL = V1,\n         UCL = V2) %>%\n  mutate(nTotal = 55 * nTotal) %>%\n  select(nTotal, everything()) -> tibCI95unpaired\n\ntibCI95unpaired %>%\n  pander::pander()\n\nnTotal\nLCL\nUCL\n55\n-0.03564\n0.3952\n110\n0.03751\n0.3396\n165\n0.06301\n0.3067\n220\n0.07231\n0.2906\n\nShow code\n\nggplot(data = tibCI95unpaired,\n       aes(x = nTotal)) +\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) +\n  geom_hline(yintercept = 0.1875) +\n  ylab(\"Difference in proportions\") +\n  scale_x_continuous(name = \"Total number of mice\",\n                     breaks = vecMultipliers * 32) +\n  ggtitle(\"Two sided 95% CI tightening with increasing sample size\",\n          subtitle = \"Horizontal reference line is observed difference in proportions\")\n\n\n\nAnd, again, the times.\n\n\nShow code\n\nlibrary(tidyverse)\nvecTimesUnpaired %>%\n  as_tibble() %>%\n  rename(timeSecs = value) %>%\n  bind_cols(vecMultipliers) %>%\n  rename(nTotal = `...2`) %>%\n  mutate(nTotal = 32 * nTotal) %>% \n  select(nTotal, timeSecs) -> tibTimesPaired\n\ntibTimesPaired %>%\n  pander(justify = \"rr\")\n\nnTotal\ntimeSecs\n32\n15.46\n64\n170.1\n96\n244.9\n128\n545.6\n\nShow code\n\nggplot(data = tibTimesPaired,\n       aes(x = nTotal, y = timeSecs)) +\n  geom_point() +\n  geom_line() +\n  ylab(\"Elapsed time (seconds)\") +\n  xlab(\"Number of mice\") +\n  ggtitle(\"Plot of computation time against sample size\")\n\n\n\nAgain, looks fairly linear. Interesting.\nSummary\nThe R package ExactCIdiff provides two functions which give what appear to be the best confidence intervals for differences between two proportions, one function, BinomCI() for differences from unpaired samples and the other PairedCI() for paired samples. I think there’s a typo in the paper about the package and the syntax of the arguments isn’t particularly friendly (it’s even case sensitive so CItype = \"upper\" with throw an error, it has to be CItype = \"Upper\"). However, it’s not difficult to work those things out (and that’s partly why I’ve created this post) and it does seem that these really are the best ways to get these CIs. They’re faily computationally intensive but from my tiny simulation it looks as if the timing is linear across simple multiples of sample size. Thanks and kudos to Shan and Wang!\n\n\n\n",
    "preview": "posts/2022-04-18-exact-confidence-intervals-for-difference-between-proportions/exact-confidence-intervals-for-difference-between-proportions_files/figure-html5/separateIntervals2-1.png",
    "last_modified": "2023-08-25T14:20:02+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-05-prepost-analyses/",
    "title": "Pre/post analyses",
    "description": "Teaching myself about pre/post therapy change analyses using R.\nProbably the first of a series of posts.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-02-12",
    "categories": [
      "change scores",
      "pre/post change"
    ],
    "contents": "\n\nContents\nBackground\nPlan\nGeneration of the data\nExploration of the data\nGender\nAge\nNumbers of sessions\nLook at distributions of scores and how these relate to predictors\nGender\nAge\nCheck on change scores I’ve created\nWhat about change scores themselves?\nAge\n\n\n\nTesting for the effects\nStart over from simplest model and build up\n\n\nStarted 5.ii.22, latest update 12.ii.22, still work in progress but worth mounting to illustrate some of the issues\nBackground\nI was being very slow talking with Emily (Dr. Blackshaw now) about some pre/post analyses she is doing so I realised I should take my ageing brain for some gentle walking pace exercise about pre/post change analyses!\nHer dataset is fairly large and has first and last session CORE scores (CORE-10 or YP-CORE, analysing each dataset separately). She has been asked to look at the impacts on change of the numbers of sessions attended, gender and age.\nDoing this has been helpful to me in thinking through the challenges of testing for effects of even a very small set of predictor variables on pre/post change scores. I hope it may be useful to others on that level of exploring the issues. I also hope that the code both for the exploratory/descriptive graphics, and the effect testing, will be useful to others.\nPlan\nThis is an emerging document and I think it will spin off some separate posts, it’s also a large post and at the moment splits into three sections:\nGeneration of the data. This is really just here to make that process transparent and provide the code but unless you are particularly interested in this sort of thing you can ignore the code and get through this section very fast.\nExploration of the data. With real data I like to “see” the data and not just get jumped into ANOVA tables. Here I was also doing it to get a visual sense of the effects I had created and to be sure my simulation had worked. With real data this helps see where data doesn’t fit the distributional models in the analyses (usually assumptions of Gaussian distributions and linear effects). I have modelled in a quadratic effect, a U-shaped relationship between baseline score and age, but otherwise the data are simulated so are all pretty squeaky clean so skim this too if you want but I encourage you to look at your own data carefully with these sorts of graphics before jumping to modelling.\nTesting for the effects. This was what got me into this self-answered reassurance journey. I was hoping that effect plots would be helpful but got into the issue of interactions so this section is really still taking shape.\nGeneration of the data\nThis code block generates the baseline scores. I’m using Gaussian distributions which is just one of the many unrealistic aspects of this. However, I’m really doing this to explore different ways of displaying the effects and not aspiring to verisimilitude!\n\n\nShow code\n\n### these vectors create population proportions from which to sample with sample()\nvecSessions <- c(rep(2, 40), # I have made these up, I have no idea how realistic they are\n                 rep(3, 30),\n                 rep(4, 20),\n                 rep(3, 15),\n                 rep(4, 10),\n                 rep(5, 8),\n                 rep(6, 7),\n                 rep(7, 5),\n                 rep(8, 3),\n                 rep(9, 2),\n                 rep(10, 1))\nvecAge <- c(rep(15, 20), # ditto\n            rep(16, 25),\n            rep(17, 25),\n            rep(18, 30),\n            rep(19, 35),\n            rep(20, 30),\n            rep(21, 30),\n            rep(22, 33),\n            rep(23, 29),\n            rep(24, 20),\n            rep(25, 18))\nvecGender <- c(rep(\"F\", 63), # ditto\n               rep(\"M\", 30),\n               rep(\"Other\", 7))\n\nnGenders <- length(vecGender)\nnAges <- length(15:25) # lazy but does make tweaking the model later easier!\nnSessLengths <- length(2:20) # ditto\nnCells <- nGenders * nAges * nSessLengths\n\navCellSize <- 20 # trying to make things big enough\npopulnSize <- nCells * avCellSize\n\n### build scores from a Gaussian base variable\nlatentMean <- 0\nlatentSD <- 1\n\n### add baseline differences\n### gender has female as reference vale\neffBaseFemaleMean <- 0\neffBaseFemaleSD <- 1\neffBaseMaleMean <- -.25\neffBaseMaleSD <- 1.8\neffBaseOtherMean <- .35\neffBaseOtherSD <- 2\n### model age as a quadratic\nminAge <- min(vecAge)\nmidAge <- mean(vecAge)\neffBaseAgeMeanMult <- .03\neffBaseAgeSD <- 1\n\n### now create model for change effects\n### start with noise to add to baseline score\nchangeFuzzMean <- .1\nchangeFuzzSD <- .2\n### now gender effects on change\neffChangeFemaleMean <- -.8\neffChangeFemaleSD <- 1\neffChangeMaleMean <- effChangeFemaleMean + .2 # smaller improvement for men\neffChangeMaleSD <- 1.5 # more variance in male change\neffChangeOtherMean <- effChangeFemaleMean - .3 # better for \"other\"\neffChangeOtherSD <- 1\n\n### model age as a quadratic again\neffChangeAgeMeanMult <- .1\neffChangeAgeMeanSD <- 1\n\n### model effect of number of sessions as linear\nminSessions = min(vecSessions)\neffChangeSessionsMult <- -.15\neffChangeSessionsSD  <- 1\n\n### build the sample\nset.seed(12345) # reproducible sample\n### get the basics\nas_tibble(list(ID = 1:populnSize,\n               gender = sample(vecGender, populnSize, replace = TRUE),\n               age = sample(vecAge, populnSize, replace = TRUE),\n               nSessions = sample(vecSessions, populnSize, replace = TRUE),\n               ### now build baseline scores\n               baseLatent = rnorm(populnSize, mean = latentMean, sd = latentSD))) -> tibSimulnVars\n\n### now use those to build baseline data\ntibSimulnVars %>%\n  ### create effect of gender\n  mutate(gendEffect = case_when(gender == \"F\" ~ rnorm(populnSize, mean = effBaseFemaleMean, sd = effBaseFemaleSD),\n                                gender == \"M\" ~ rnorm(populnSize, mean = effBaseMaleMean, sd = effBaseMaleSD),\n                                gender == \"Other\" ~ rnorm(populnSize, mean = effBaseOtherMean, sd = effBaseOtherSD)),\n         ### this is just creating factors, useful when plotting\n         Age = factor(age),\n         facSessions = factor(nSessions)) %>%\n  rowwise() %>%\n  ### create effect of age\n  ### I am, a bit unrealistically, assuming that number of sessions doesn't affect baseline score (nor v.v.)\n  mutate(ageEffect = effBaseAgeMeanMult * rnorm(1, \n                                                mean = (age - midAge)^2, # centred quadratic\n                                                sd = effBaseAgeSD),\n         first = baseLatent + gendEffect + ageEffect) %>%\n  ungroup() -> tibBaselineScores\n\n\n\nNow create the change scores to create the final scores. First time around I went straight to create the final scores, easier to follow this way.\n\n\nShow code\n\ntibBaselineScores %>%\n  ### create basic change scores with noise to add to the baseline scores\n  mutate(change =rnorm(populnSize, \n                       mean = changeFuzzMean,\n                       sd = changeFuzzSD)) %>%\n  ### now add effect of gender on change\n  mutate(gendChangeEffect = case_when(gender == \"F\" ~ rnorm(populnSize, \n                                                            mean = effChangeFemaleMean, \n                                                            sd = effChangeFemaleSD),\n                                      gender == \"M\" ~ rnorm(populnSize, \n                                                            mean = effChangeMaleMean, \n                                                            sd = effChangeMaleSD),\n                                      gender == \"Other\" ~ rnorm(populnSize, \n                                                                mean = effChangeOtherMean, \n                                                                sd = effChangeOtherSD))) %>%\n  rowwise() %>%\n  ### add effect of age\n  mutate(ageChangeEffect = effChangeAgeMeanMult * rnorm(1, \n                                                        mean = (age - midAge)^2, \n                                                        sd = effChangeAgeMeanSD),\n         ### add effect of number of sessions\n         sessionChangeEffect = effChangeSessionsMult * rnorm(1,\n                                                             mean = nSessions - minSessions,\n                                                             sd = effChangeSessionsSD),\n         change = change + gendChangeEffect + ageChangeEffect + sessionChangeEffect,\n         last = first + change) %>%\n  ungroup() -> tibDat\n\n\n\nI’ve simulated a very unrealistic dataset of total size 418000 with a three way gender classification and age ranging from 15 to 25 and numbers of sessions from 2 to 10.\nExploration of the data\nThis was to check my simulation but I think it’s good practice with any dataset to explore it graphically and thoroughly yourself and ideally to make some of that exploration available to others, some in a main paper or report, some in supplementary materials anyone can get to.\nFirst check the breakdown by the predictor variables: gender, age and number of sessions. With a real world dataset I’d check for systematic associations between these variables, e.g. is the age distribution, or the numbers of sessions attended different by gender? Does the number of sessions attended relate to age? However, I built in no non-random associations so I haven’t done that exploration here.\nGender\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(n = n()) %>%\n  mutate(nText = str_c(\"n = \", n),\n         yPos = n * .8) -> tmpTibN\n\nggplot(data = tibDat,\n       aes(x = gender, fill = gender))+\n  geom_histogram(stat = \"count\") +\n  geom_text(data = tmpTibN,\n             aes(x = gender, y = yPos, label = nText))\n\n\n\n\nOK.\nAge\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = age, fill = gender))+\n  geom_histogram(stat = \"count\") +\n  scale_x_continuous(breaks = vecAge, labels = as.character(vecAge))\n\n\n\n\nI haven’t created any systematic association between age and gender.\nNumbers of sessions\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = nSessions, fill = gender))+\n  geom_histogram(stat = \"count\") +\n  scale_x_continuous(breaks = vecSessions, labels = as.character(vecSessions))\n\n\n\n\nI haven’t created any systematic association between number of sessions and gender, nor with age.\nLook at distributions of scores and how these relate to predictors\nChecking distributions is a bit silly here as we know I’ve created samples from Gaussian distributions, however with real world data really marked deviations from Gaussian distributions would clarify that caution would be needed for any tests or confidence intervals when we look at effects on change.\nStart with “first”, i.e. baseline score.\n\n\nShow code\n\n### using a tweak to be able to fit a Gaussian density using ggplot::geom_density() while wanting \n### counts on the y axis not density\n### found the answer at\n###   https://stackoverflow.com/questions/27611438/density-curve-overlay-on-histogram-where-vertical-axis-is-frequency-aka-count\n### it involves using a multiplier which is n * binwidth\n\n### this next is actually based on https://stackoverflow.com/questions/6967664/ggplot2-histogram-with-normal-curve\ntmpBinWidth <- .5\ntibDat %>%\n  summarise(n = n(),\n            mean = mean(first),\n            sd = sd(first)) -> tmpTibStats\n\nggplot(data = tibDat,\n       aes(x = first)) +\n  # geom_histogram(binwidth = tmpBinWidth) #+\n  geom_histogram(binwidth = tmpBinWidth) + #-> p\n  stat_function(fun = function(x) dnorm(x, mean = tmpTibStats$mean, sd = tmpTibStats$sd) * tmpTibStats$n * tmpBinWidth,\n    color = \"green\", \n    linewidth = 1) +\n  geom_vline(xintercept = mean(tibDat$first),\n             colour = \"blue\") +\n  xlab(\"Baseline scores\") +\n  ggtitle(\"Histogram of all baseline scores\",\n          subtitle = \"Blue vertical reference line marks mean\")\n\n\n\nI amused myself by adding the best Gaussian distribution fit so I can\nand tests of fit but clearly no major problem there but that’s all overkill here so I haven’t. Now what about the effects of predictors?\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = gender)) +\n  facet_grid(rows = vars(gender),\n             scales = \"free_y\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of baseline scores against gender\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nWe can see the relationship between mean baseline score and gender. I have set the Y axis as free, i.e. can be different in each facet of the plot, as numbers in each gender category vary a lot.\n\n\nShow code\n\ntibDat %>%\n  group_by(Age) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = Age)) +\n  facet_grid(rows = vars(Age),\n             scales = \"free_y\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of baseline scores against age\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nFine! And just to pander to obsessionality here’s a facetted histogram by both age and gender.\n\n\nShow code\n\ntibDat %>%\n  group_by(Age, gender) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = Age)) +\n  facet_grid(rows = vars(Age),\n             cols = vars(gender),\n             scales = \"free\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of baseline scores against age\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nThat’s just silly! (And it’s very small here, would be lovely if distill created large plots that would open if the small plot is clicked. I think that’s beyond my programming skills.)\n\n\nShow code\n\ntibDat %>%\n  group_by(nSessions) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = nSessions)) +\n  facet_grid(rows = vars(nSessions),\n             scales = \"free\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of baseline scores against number of sessions\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nOK. Now same for final scores.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = last)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tibDat$last),\n             colour = \"blue\") +\n  xlab(\"Final scores\") +\n  ggtitle(\"Histogram of all final scores\",\n          subtitle = \"Blue vertical reference line marks mean\")\n\n\n\n\nOK.\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = gender)) +\n  facet_grid(rows = vars(gender),\n             scales = \"free_y\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of final scores against gender\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nOK again.\n\n\nShow code\n\ntibDat %>%\n  group_by(Age) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = Age)) +\n  facet_grid(rows = vars(Age),\n             scales = \"free_y\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of final scores against age\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nAnd again.\n\n\nShow code\n\ntibDat %>%\n  group_by(Age, gender) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = Age)) +\n  facet_grid(rows = vars(Age),\n             cols = vars(gender),\n             scales = \"free\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of final scores against age\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nStill silly!\n\n\nShow code\n\ntibDat %>%\n  group_by(nSessions) %>%\n  summarise(mean = mean(first)) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, fill = nSessions)) +\n  facet_grid(rows = vars(nSessions),\n             scales = \"free\") +\n  geom_histogram() +\n  geom_vline(data = tmpTibMeans,\n             aes(xintercept = mean)) +\n  ggtitle(\"Histogram of final scores against number of sessions\",\n          subtitle = \"Vertical reference lines mark means\")\n\n\n\n\nGetting lumpy where the cell sizes are getting small of course but fine.\nGender\n\n\nShow code\n\n### get means and bootstrap CIs for baseline gender effect\nset.seed(12345) # reproducible bootstrap\nsuppressWarnings(tibBaselineScores %>%\n                   group_by(gender) %>%\n                   summarise(mean = mean(first),\n                             CI = list(getBootCImean(first, \n                                                     nGT10kerr = FALSE,\n                                                     verbose = FALSE))) %>%\n                   unnest_wider(CI) -> tmpTibMeans)\n\n\nggplot(data = tibBaselineScores,\n       aes(x = gender, y = first)) +\n  geom_violin(aes(fill = gender),\n              scale = \"count\") +\n  geom_hline(yintercept = mean(tibBaselineScores$first)) +\n  geom_point(data = tmpTibMeans,\n             aes(y = mean)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = gender,\n                     ymin = LCLmean, \n                     ymax = UCLmean)) +\n  ylab(\"Baseline score\") +\n  xlab(\"Gender\") +\n  ggtitle(\"Violin plot to check baseline gender differences\",\n          subtitle = \"points are means, tiny vertical lines are 95% bootstrap CI of means\")\n\n\n\n\nOK. It’s not very visible but there is a small baseline gender effect and the confidence intervals are so tight that they are just about invisible.\nAge\n\n\nShow code\n\n### get means and bootstrap CIs for baseline age effect\nset.seed(12345) # reproducible bootstrap\nsuppressWarnings(tibBaselineScores %>%\n                   group_by(Age) %>%\n                   summarise(mean = mean(first),\n                             CI = list(getBootCImean(first, \n                                                     nGT10kerr = FALSE,\n                                                     verbose = FALSE))) %>%\n                   unnest_wider(CI) -> tmpTibMeans)\n\n\nggplot(data = tibBaselineScores,\n       aes(x = Age, y = first)) +\n  geom_violin(aes(fill = Age),\n              scale = \"count\") +\n  geom_hline(yintercept = mean(tibBaselineScores$first)) +\n  geom_point(data = tmpTibMeans,\n             aes(y = mean)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = Age,\n                     ymin = LCLmean, \n                     ymax = UCLmean)) +\n  ylab(\"Baseline score\") +\n  xlab(\"Age\") +\n  ggtitle(\"Violin plot to check baseline gender differences\",\n          subtitle = \"points are means, tiny vertical lines are 95% bootstrap CI of means\")\n\n\n\n\nSmall and very unrealistic quadratic (U shaped) effect of age on baseline scores.\nCheck on change scores I’ve created\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(meanFirst = mean(first),\n            meanLast = mean(last),\n            CIfirst = list(getBootCImean(first, \n                                         nGT10kerr = FALSE,\n                                         verbose = FALSE)),\n            CIlast = list(getBootCImean(last, \n                                        nGT10kerr = FALSE,\n                                        verbose = FALSE))) %>%\n  unnest_wider(CIfirst) %>%\n  ### got to rename to avoid name collision\n  rename(obsmeanFirst = obsmean,\n         LCLmeanFirst = LCLmean,\n         UCLmeanFirst = UCLmean) %>%\n  unnest_wider(CIlast) %>%\n  ### renaming now is just for clarity rather than necessity\n  rename(obsmeanLast = obsmean,\n         LCLmeanLast = LCLmean,\n         UCLmeanLast = UCLmean) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = gender, fill = gender)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  # geom_point(data = tmpTibMeans,\n  #            aes(x = meanFirst, y = meanLast),\n  #            size = 3) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = meanFirst,\n                     ymin = LCLmeanLast,\n                     ymax = UCLmeanLast)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(y = meanLast,\n                     xmin = LCLmeanFirst,\n                     xmax = UCLmeanFirst)) +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by gender\",\n          subtitle = \"Lines are linear regression by gender with 95% confidence intervals\\nCrosshairs are 95% confidence intervals of means\")\n\n\n\n\nNot a very informative plot here but it would be important with real data to plot something like this to see whether there are markedly non-linear relationships. Here it’s just about visible that I’ve created slight differences in slope of final session score on first session score by gender. I’ve put in the means (of first and last scores) by gender which helps remind us of the horizontal shift of the baseline score gender differences seen above. (Cross hairs in black as the ones for the men and for the women disappear if coloured by gender.)\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = gender, fill = gender)) +\n  facet_grid(rows = vars(gender)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  # geom_point(data = tmpTibMeans,\n  #            aes(x = meanFirst, y = meanLast),\n  #            size = 3) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = meanFirst,\n                     ymin = LCLmeanLast,\n                     ymax = UCLmeanLast)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(y = meanLast,\n                     xmin = LCLmeanFirst,\n                     xmax = UCLmeanFirst)) +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by gender\",\n          subtitle = \"Lines are linear regression by gender with 95% confidence intervals\\nCrosshairs are 95% confidence intervals of means\")\n\n\n\n\nAs the main objective here is to look for major problems with the relationship between x and y variables, best to complement that with the same but facetted by gender.\nOK, no issues of non-linearities there (of course they’re not, I didn’t model them so!)\nWhat about change scores themselves?\nPlotting final scores against baseline is vital to look for non-linearities in the relationship but we are as interested in change as final scores. (Actually, we’re interested in both and of course they’re mathematically completely linearly related but the give usefully different views on this whole issue of final score and change.)\nSo plot change against first score now we have seen that the relationships between first and last scores are not markedly non-linear.\n\n\nShow code\n\ntibDat %>%\n  group_by(gender) %>%\n  summarise(meanFirst = mean(first),\n            meanChange = mean(change),\n            CIfirst = list(getBootCImean(first, \n                                         nGT10kerr = FALSE,\n                                         verbose = FALSE)),\n            CIchange = list(getBootCImean(change, \n                                          nGT10kerr = FALSE,\n                                          verbose = FALSE))) %>%\n  unnest_wider(CIfirst) %>%\n  ### got to rename to avoid name collision\n  rename(obsmeanFirst = obsmean,\n         LCLmeanFirst = LCLmean,\n         UCLmeanFirst = UCLmean) %>%\n  unnest_wider(CIchange) %>%\n  ### renaming now is just for clarity rather than necessity\n  rename(obsmeanChange = obsmean,\n         LCLmeanChange = LCLmean,\n         UCLmeanChange = UCLmean) -> tmpTibMeans\n\nggplot(data = tibDat,\n       aes(x = first, y = change, colour = gender, fill = gender)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  geom_point(data = tmpTibMeans,\n             aes(x = meanFirst, y = meanChange),\n             size = 3) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(x = meanFirst,\n                     ymin = LCLmeanChange,\n                     ymax = UCLmeanChange)) +\n  geom_linerange(data = tmpTibMeans,\n                 inherit.aes = FALSE,\n                 aes(y = meanChange,\n                     xmin = LCLmeanFirst,\n                     xmax = UCLmeanFirst)) +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of change (last - first) against first scores by gender\",\n          subtitle = \"Lines are linear regression by gender with 95% confidence intervals\")\n\n\n\n\nNow the mean points show clearly both the horizontal shifts of baseline score gender differences, but also that the change scores are different. The CIs for the female subset are so tiny they disappear but it’s clear that the differences are systematic for the change scores as well as for the baseline scores. Very slight but clear linear relationship between baseline score and change, in real life datasets I’d expect more of a relationship and that’d be an important reason for doing this plot.\nAge\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = Age, fill = Age)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by age\",\n          subtitle = \"Lines are linear regression by age with 95% confidence intervals\")\n\n\n\n\nStrong relationships and no obvious non-linearities but not an easy plot to read. Facetted plot better.\n\n\nShow code\n\ntibDat %>%\n  mutate(xmean = mean(first)) %>% # centre on x axis\n  group_by(Age) %>%\n  summarise(xmean = first(xmean), # to retain that constant\n            last = mean(last)) -> tmpTibMeans\n\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = Age, fill = Age)) +\n  facet_grid(rows = vars(Age)) +\n  geom_point(alpha = .3, size = .5) +\n  geom_smooth(method = \"lm\") +\n  geom_hline(yintercept = mean(tibDat$first)) +\n  geom_point(data = tmpTibMeans,\n             inherit.aes = FALSE,\n             aes(x = xmean, y = last)) +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by age\",\n          subtitle = \"Lines are linear regression by age with 95% confidence intervals\\nBlack reference lines are overall mean final score, points are mean by age.\")\n\n\n\n\nMain thing here is that there are no obvious nonlinearities. I have added the overall mean as a horizontal reference and the facet (age) mean as a point so we can still see that the final score mean is related to age.\nNow change scores.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = change, colour = Age, fill = Age)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of change (last - first) against first scores by age\",\n          subtitle = \"Lines are linear regression by age with 95% confidence intervals\")\n\n\n\n\n@@@ put facetted plot here later, when I have time! @@@\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = last, colour = facSessions, fill = facSessions)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of last against first scores by n(sessions)\",\n          subtitle = \"Lines are linear regression by n(sessions) with 95% confidence intervals\")\n\n\n\n\n@@@ put facetted plot here later, when I have time! @@@\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = first, y = change, colour = facSessions, fill = facSessions)) +\n  geom_point(alpha = .1, size = .5) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"First session score\") +\n  ylab(\"Final session score\") +\n  ggtitle(\"Scatterplot of change (last - first) against first scores by n(sessions)\",\n          subtitle = \"Lines are linear regression by n(sessions) with 95% confidence intervals\")\n\n\n\n\n@@@ put facetted plot here later, when I have time! @@@\n\n\nShow code\n\n### get means and bootstrap CIs for effect of n(sessions) on last score\nset.seed(12345) # reproducible bootstrap\nsuppressWarnings(tibDat %>%\n                   group_by(nSessions) %>%\n                   summarise(mean = mean(change),\n                             CI = list(getBootCImean(change, \n                                                     nGT10kerr = FALSE,\n                                                     verbose = FALSE))) %>%\n                   unnest_wider(CI) -> tmpTibMeans)\n\nggplot(data = tibDat,\n       aes(x = nSessions, y = change, colour = facSessions, fill = facSessions)) +\n  geom_violin(scale = \"count\") +\n  geom_point(data = tmpTibMeans,\n             inherit.aes = FALSE,\n             aes(x = nSessions, y = mean)) +\n  geom_linerange(data = tmpTibMeans,\n             inherit.aes = FALSE,\n             aes(x = nSessions, ymin = LCLmean, ymax = UCLmean),\n             size = 1) +\n  geom_smooth(inherit.aes = FALSE,\n    aes(x = nSessions, y = change),\n    method = \"lm\",\n    colour = \"black\") +\n  xlab(\"Number of sessions\") +\n  ylab(\"Score change\") +\n  ggtitle(\"Violin plot of change (last - first) against first scores by n(sessions)\",\n          subtitle = \"Line is linear regression with 95% confidence interval\\nPoints are means with vertical lines for their bootstrap 95% confidence intervals\")\n\n\n\nShow code\n\nggsave(\"prepost1.png\")\n\n\n\nTesting for the effects\nStart with linear regression of final score on baseline score with all predictors and interactions. Age as factor.\n\n\nShow code\n\nlm(last ~ first + gender + Age + nSessions + \n     ### add two way interactions\n     gender * Age + gender * nSessions + gender * Age + Age * nSessions +\n     ### add three way interaction\n     gender * Age * nSessions, data = tibDat) -> lisLMFull\n\nsummary(lisLMFull)\n\n\n\nCall:\nlm(formula = last ~ first + gender + Age + nSessions + gender * \n    Age + gender * nSessions + gender * Age + Age * nSessions + \n    gender * Age * nSessions, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9992 -0.7800 -0.0006  0.7767  7.3645 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  1.998892   0.021443  93.217  < 2e-16 ***\nfirst                        0.997563   0.001098 908.861  < 2e-16 ***\ngenderM                      0.275614   0.037636   7.323 2.43e-13 ***\ngenderOther                 -0.256914   0.065032  -3.951 7.80e-05 ***\nAge16                       -0.843495   0.028766 -29.323  < 2e-16 ***\nAge17                       -1.535725   0.028707 -53.497  < 2e-16 ***\nAge18                       -2.033553   0.027532 -73.862  < 2e-16 ***\nAge19                       -2.298519   0.026781 -85.828  < 2e-16 ***\nAge20                       -2.417854   0.027672 -87.375  < 2e-16 ***\nAge21                       -2.268483   0.027520 -82.431  < 2e-16 ***\nAge22                       -1.980524   0.027107 -73.063  < 2e-16 ***\nAge23                       -1.453611   0.027717 -52.445  < 2e-16 ***\nAge24                       -0.751826   0.030279 -24.830  < 2e-16 ***\nAge25                        0.167747   0.030820   5.443 5.25e-08 ***\nnSessions                   -0.138232   0.005449 -25.369  < 2e-16 ***\ngenderM:Age16               -0.057543   0.050395  -1.142  0.25353    \ngenderOther:Age16           -0.048794   0.088211  -0.553  0.58016    \ngenderM:Age17               -0.095745   0.050319  -1.903  0.05707 .  \ngenderOther:Age17           -0.069093   0.088763  -0.778  0.43634    \ngenderM:Age18               -0.096002   0.048294  -1.988  0.04683 *  \ngenderOther:Age18           -0.062146   0.084629  -0.734  0.46274    \ngenderM:Age19               -0.089492   0.047165  -1.897  0.05777 .  \ngenderOther:Age19           -0.050296   0.082522  -0.609  0.54220    \ngenderM:Age20               -0.045583   0.048587  -0.938  0.34815    \ngenderOther:Age20           -0.021258   0.085097  -0.250  0.80274    \ngenderM:Age21               -0.121299   0.048397  -2.506  0.01220 *  \ngenderOther:Age21           -0.130454   0.085207  -1.531  0.12577    \ngenderM:Age22               -0.144666   0.047561  -3.042  0.00235 ** \ngenderOther:Age22           -0.029070   0.083188  -0.349  0.72675    \ngenderM:Age23               -0.090242   0.048788  -1.850  0.06436 .  \ngenderOther:Age23           -0.044951   0.085015  -0.529  0.59699    \ngenderM:Age24               -0.056617   0.052857  -1.071  0.28411    \ngenderOther:Age24           -0.094066   0.093598  -1.005  0.31490    \ngenderM:Age25               -0.098099   0.054572  -1.798  0.07224 .  \ngenderOther:Age25           -0.208275   0.095924  -2.171  0.02991 *  \ngenderM:nSessions           -0.014893   0.009499  -1.568  0.11693    \ngenderOther:nSessions       -0.015892   0.016102  -0.987  0.32367    \nAge16:nSessions             -0.011043   0.007308  -1.511  0.13076    \nAge17:nSessions             -0.015418   0.007298  -2.113  0.03463 *  \nAge18:nSessions             -0.007741   0.006985  -1.108  0.26778    \nAge19:nSessions             -0.013239   0.006790  -1.950  0.05120 .  \nAge20:nSessions             -0.008232   0.007035  -1.170  0.24194    \nAge21:nSessions             -0.015335   0.006980  -2.197  0.02801 *  \nAge22:nSessions             -0.008215   0.006883  -1.193  0.23271    \nAge23:nSessions             -0.012965   0.007048  -1.839  0.06585 .  \nAge24:nSessions             -0.012906   0.007693  -1.678  0.09341 .  \nAge25:nSessions             -0.011482   0.007812  -1.470  0.14162    \ngenderM:Age16:nSessions      0.010376   0.012757   0.813  0.41600    \ngenderOther:Age16:nSessions  0.016470   0.022154   0.743  0.45724    \ngenderM:Age17:nSessions      0.022974   0.012738   1.804  0.07129 .  \ngenderOther:Age17:nSessions  0.029906   0.022300   1.341  0.17989    \ngenderM:Age18:nSessions      0.017580   0.012198   1.441  0.14954    \ngenderOther:Age18:nSessions  0.015737   0.021265   0.740  0.45927    \ngenderM:Age19:nSessions      0.016186   0.011926   1.357  0.17470    \ngenderOther:Age19:nSessions  0.012372   0.020598   0.601  0.54807    \ngenderM:Age20:nSessions      0.009190   0.012317   0.746  0.45563    \ngenderOther:Age20:nSessions  0.008813   0.021323   0.413  0.67939    \ngenderM:Age21:nSessions      0.029381   0.012220   2.404  0.01620 *  \ngenderOther:Age21:nSessions  0.036737   0.021375   1.719  0.08567 .  \ngenderM:Age22:nSessions      0.033377   0.012006   2.780  0.00543 ** \ngenderOther:Age22:nSessions  0.007113   0.020690   0.344  0.73100    \ngenderM:Age23:nSessions      0.020473   0.012334   1.660  0.09695 .  \ngenderOther:Age23:nSessions  0.020048   0.021269   0.943  0.34590    \ngenderM:Age24:nSessions      0.012999   0.013351   0.974  0.33025    \ngenderOther:Age24:nSessions  0.026838   0.023344   1.150  0.25027    \ngenderM:Age25:nSessions      0.021564   0.013813   1.561  0.11851    \ngenderOther:Age25:nSessions  0.046793   0.023952   1.954  0.05075 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.203 on 417933 degrees of freedom\nMultiple R-squared:  0.7375,    Adjusted R-squared:  0.7375 \nF-statistic: 1.779e+04 on 66 and 417933 DF,  p-value: < 2.2e-16\n\nShow code\n\n# lisLMFull$coefficients %>%\n#   as_tibble() %>% # that ignores the names so ...\n#   mutate(effect = names(lisLMFull$coefficients)) %>% # get them!\n#   select(effect, value) %>% # more sensible order\n#   rename(coefficient = value )\n\n### hm, that's done for me in broom\nbroom::tidy(lisLMFull) %>%\n  ### identify the order of the terms, i.e. two-way interaction has order 2 etc.\n  mutate(order = 1 + str_count(term, fixed(\":\")),\n         sig = if_else(p.value < .05, 1, 0)) -> tibLMFull\n\nvalNinteractions <- sum(tibLMFull$order > 1)\n\n\n\nThat’s not very digestible but it is, arguably, a sensible place to start. We can ignore the intercept really but it’s not zero!\nMore usefully, we have a very strong effect of initial score on final score, a statistically significant effect of male gender against the reference gender (female) and no statistically significant effect of gender “other” in this saturated model. The reference category for age is the lowest, age 15 and all the other ages show a statistically significantly different final score from that for age 15 except age 25. Finally, in the simple effects, we have a statistically significant effect of number of sessions on final score with coefficient estimate -0.138, i.e. a drop of about that in mean final score for every one more session attended. (Remember the final scores here distribute between -12.34 and 12.31 with SD 2.35 so I appear to have modelled in a pretty small effect of nSessions.\nThe complication is all those statistically significant interactions in this saturated model. We have 67 terms, including the intercept, 14 simple effects (ignoring the intercept) and 52 interactions, 32 two-way interactions and 20 three-way interactions. Here’s the breakdown of the numbers significant.\n\n\nShow code\n\ntibLMFull %>%\n  group_by(order) %>%\n  summarise(n = n(),\n            nSignif = sum(sig),\n            propn = round(nSignif / n, 3)) %>%\n  pander::pander(justify = \"lrrr\")\n\n\norder\nn\nnSignif\npropn\n1\n15\n15\n1\n2\n32\n6\n0.188\n3\n20\n2\n0.1\n\nWith 52 the probability that none of them would come out statistically significant at p < .05 given a true null population model would be .95^52, i.e. 0.069, pretty unlikely but the challenge is to know what to do about this. If we could treat age as linear we wouldn’t have all those effects for each age other than 15 and things would be much simpler, but we know I’ve modelled age as having a quadratic effect.\nCheat a bit and just fit the quadratic for age by centring and then squaring age.\n\n\nShow code\n\n# lm(last ~ first + gender + poly(Age, 2) + nSessions + \n#      gender * poly(Age, 2) + gender * nSessions + gender * poly(Age, 2) + poly(Age, 2) * nSessions +\n#      gender * poly(Age, 2) * nSessions, \n#    data = tibDat) -> lisLMAge2\n\ncentreVec <- function(x){\n  x - mean(x)\n}\ntibDat %>%\n  mutate(ageSquared = centreVec(age),\n         ageSquared = ageSquared^2,\n         ### recentre to get mean zero\n         ageSquared = centreVec(ageSquared)) -> tibDat\n\nlm(last ~ first + gender + ageSquared + nSessions + \n     gender * ageSquared + gender * nSessions + gender * ageSquared + ageSquared * nSessions +\n     gender * ageSquared * nSessions, \n   data = tibDat) -> lisLMAge2\n\nsummary(lisLMAge2)\n\n\n\nCall:\nlm(formula = last ~ first + gender + ageSquared + nSessions + \n    gender * ageSquared + gender * nSessions + gender * ageSquared + \n    ageSquared * nSessions + gender * ageSquared * nSessions, \n    data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9852 -0.7795 -0.0006  0.7767  7.3727 \n\nCoefficients:\n                                   Estimate Std. Error  t value\n(Intercept)                       0.4355888  0.0055454   78.549\nfirst                             0.9975685  0.0010975  908.949\ngenderM                           0.1900903  0.0097396   19.517\ngenderOther                      -0.3206512  0.0173832  -18.446\nageSquared                        0.0994679  0.0006739  147.600\nnSessions                        -0.1490117  0.0014044 -106.106\ngenderM:ageSquared                0.0021081  0.0011869    1.776\ngenderOther:ageSquared           -0.0014332  0.0021039   -0.681\ngenderM:nSessions                 0.0035952  0.0024671    1.457\ngenderOther:nSessions             0.0029519  0.0044030    0.670\nageSquared:nSessions              0.0001480  0.0001706    0.867\ngenderM:ageSquared:nSessions     -0.0004417  0.0003007   -1.469\ngenderOther:ageSquared:nSessions  0.0002704  0.0005293    0.511\n                                 Pr(>|t|)    \n(Intercept)                        <2e-16 ***\nfirst                              <2e-16 ***\ngenderM                            <2e-16 ***\ngenderOther                        <2e-16 ***\nageSquared                         <2e-16 ***\nnSessions                          <2e-16 ***\ngenderM:ageSquared                 0.0757 .  \ngenderOther:ageSquared             0.4958    \ngenderM:nSessions                  0.1450    \ngenderOther:nSessions              0.5026    \nageSquared:nSessions               0.3857    \ngenderM:ageSquared:nSessions       0.1418    \ngenderOther:ageSquared:nSessions   0.6095    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.203 on 417987 degrees of freedom\nMultiple R-squared:  0.7375,    Adjusted R-squared:  0.7375 \nF-statistic: 9.786e+04 on 12 and 417987 DF,  p-value: < 2.2e-16\n\nHm, better but no cigar!\nStart over from simplest model and build up\nBaseline of regression model.\n\n\nShow code\n\nlm(last ~ first, data = tibDat) -> lisLM1\nsummary(lisLM1)\n\n\n\nCall:\nlm(formula = last ~ first, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6682 -1.0108 -0.0606  0.9579  7.5304 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.070443   0.002304  -30.57   <2e-16 ***\nfirst        1.059520   0.001331  796.23   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.48 on 417998 degrees of freedom\nMultiple R-squared:  0.6027,    Adjusted R-squared:  0.6027 \nF-statistic: 6.34e+05 on 1 and 417998 DF,  p-value: < 2.2e-16\n\nOf course, highly significant.\nStart by adding nSessions.\n\n\nShow code\n\nlm(last ~ first + nSessions, data = tibDat) -> lisLMsessions\nsummary(lisLMsessions)\n\n\n\nCall:\nlm(formula = last ~ first + nSessions, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6054 -0.9964 -0.0671  0.9413  7.7406 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.456206   0.005332   85.55   <2e-16 ***\nfirst        1.059541   0.001312  807.51   <2e-16 ***\nnSessions   -0.147375   0.001350 -109.17   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.459 on 417997 degrees of freedom\nMultiple R-squared:  0.6137,    Adjusted R-squared:  0.6137 \nF-statistic: 3.32e+05 on 2 and 417997 DF,  p-value: < 2.2e-16\n\nShow code\n\nanova(lisLM1, lisLMsessions)\n\n\nAnalysis of Variance Table\n\nModel 1: last ~ first\nModel 2: last ~ first + nSessions\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n1 417998 915044                                 \n2 417997 889678  1     25366 11918 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nMarked effect, add gender.\n\n\nShow code\n\nlm(last ~ first + nSessions + gender + \n     first * gender + nSessions * gender, data = tibDat) -> lisLMsessionsGend\nsummary(lisLMsessionsGend)\n\n\n\nCall:\nlm(formula = last ~ first + nSessions + gender + first * gender + \n    nSessions * gender, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7645 -0.9796 -0.0548  0.9426  7.5640 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            0.412336   0.006702  61.524   <2e-16 ***\nfirst                  1.097156   0.001968 557.374   <2e-16 ***\nnSessions             -0.149574   0.001695 -88.260   <2e-16 ***\ngenderM                0.209051   0.011759  17.778   <2e-16 ***\ngenderOther           -0.340363   0.021093 -16.136   <2e-16 ***\nfirst:genderM         -0.054331   0.002792 -19.461   <2e-16 ***\nfirst:genderOther     -0.053663   0.004284 -12.528   <2e-16 ***\nnSessions:genderM      0.005255   0.002977   1.765   0.0776 .  \nnSessions:genderOther  0.008217   0.005312   1.547   0.1219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.451 on 417991 degrees of freedom\nMultiple R-squared:  0.6177,    Adjusted R-squared:  0.6177 \nF-statistic: 8.443e+04 on 8 and 417991 DF,  p-value: < 2.2e-16\n\nShow code\n\nanova(lisLMsessions, lisLMsessionsGend)\n\n\nAnalysis of Variance Table\n\nModel 1: last ~ first + nSessions\nModel 2: last ~ first + nSessions + gender + first * gender + nSessions * \n    gender\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1 417997 889678                                  \n2 417991 880304  6      9374 741.84 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHighly significant effect of session count remains but odd effects of gender and an interaction!\n\n\nShow code\n\n### age effect treating age as continuous\n### short cut syntax for all interactions\nlm(last ~ first * nSessions * gender * age,\n   data = tibDat) -> lisLMAge\nsummary(lisLMAge)\n\n\n\nCall:\nlm(formula = last ~ first * nSessions * gender * age, data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7660 -0.9798 -0.0545  0.9428  7.5527 \n\nCoefficients:\n                                  Estimate Std. Error t value\n(Intercept)                      3.774e-01  4.933e-02   7.650\nfirst                            1.093e+00  3.225e-02  33.906\nnSessions                       -1.436e-01  1.252e-02 -11.466\ngenderM                          2.416e-01  8.381e-02   2.883\ngenderOther                     -2.194e-01  1.547e-01  -1.418\nage                              1.818e-03  2.448e-03   0.743\nfirst:nSessions                 -1.848e-03  8.174e-03  -0.226\nfirst:genderM                   -5.082e-02  4.585e-02  -1.108\nfirst:genderOther               -5.180e-02  6.938e-02  -0.747\nnSessions:genderM               -9.518e-03  2.124e-02  -0.448\nnSessions:genderOther           -2.571e-02  3.898e-02  -0.660\nfirst:age                       -6.619e-05  1.597e-03  -0.041\nnSessions:age                   -3.190e-04  6.213e-04  -0.514\ngenderM:age                     -1.697e-03  4.159e-03  -0.408\ngenderOther:age                 -6.040e-03  7.680e-03  -0.786\nfirst:nSessions:genderM          1.961e-03  1.163e-02   0.169\nfirst:nSessions:genderOther      3.718e-03  1.726e-02   0.215\nfirst:nSessions:age              1.649e-04  4.048e-04   0.407\nfirst:genderM:age                2.636e-04  2.274e-03   0.116\nfirst:genderOther:age            6.470e-06  3.453e-03   0.002\nnSessions:genderM:age            7.593e-04  1.054e-03   0.721\nnSessions:genderOther:age        1.694e-03  1.934e-03   0.876\nfirst:nSessions:genderM:age     -2.211e-04  5.767e-04  -0.383\nfirst:nSessions:genderOther:age -2.149e-04  8.590e-04  -0.250\n                                Pr(>|t|)    \n(Intercept)                     2.02e-14 ***\nfirst                            < 2e-16 ***\nnSessions                        < 2e-16 ***\ngenderM                          0.00394 ** \ngenderOther                      0.15616    \nage                              0.45777    \nfirst:nSessions                  0.82114    \nfirst:genderM                    0.26767    \nfirst:genderOther                0.45530    \nnSessions:genderM                0.65401    \nnSessions:genderOther            0.50957    \nfirst:age                        0.96694    \nnSessions:age                    0.60758    \ngenderM:age                      0.68325    \ngenderOther:age                  0.43160    \nfirst:nSessions:genderM          0.86613    \nfirst:nSessions:genderOther      0.82942    \nfirst:nSessions:age              0.68369    \nfirst:genderM:age                0.90772    \nfirst:genderOther:age            0.99851    \nnSessions:genderM:age            0.47120    \nnSessions:genderOther:age        0.38117    \nfirst:nSessions:genderM:age      0.70141    \nfirst:nSessions:genderOther:age  0.80247    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.451 on 417976 degrees of freedom\nMultiple R-squared:  0.6177,    Adjusted R-squared:  0.6177 \nF-statistic: 2.937e+04 on 23 and 417976 DF,  p-value: < 2.2e-16\n\nShow code\n\nanova(lisLM1, lisLMAge)\n\n\nAnalysis of Variance Table\n\nModel 1: last ~ first\nModel 2: last ~ first * nSessions * gender * age\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1 417998 915044                                  \n2 417976 880290 22     34754 750.09 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNo effect of age as it’s got a quadratic effect in my model!\n\n\nShow code\n\n### age effect treating age as continuous but adding quadratic term\n### short cut syntax for all interactions again\nlm(last ~ first * nSessions * gender * ageSquared,\n   data = tibDat) -> lisLMAge\nsummary(lisLMAge2)\n\n\n\nCall:\nlm(formula = last ~ first + gender + ageSquared + nSessions + \n    gender * ageSquared + gender * nSessions + gender * ageSquared + \n    ageSquared * nSessions + gender * ageSquared * nSessions, \n    data = tibDat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9852 -0.7795 -0.0006  0.7767  7.3727 \n\nCoefficients:\n                                   Estimate Std. Error  t value\n(Intercept)                       0.4355888  0.0055454   78.549\nfirst                             0.9975685  0.0010975  908.949\ngenderM                           0.1900903  0.0097396   19.517\ngenderOther                      -0.3206512  0.0173832  -18.446\nageSquared                        0.0994679  0.0006739  147.600\nnSessions                        -0.1490117  0.0014044 -106.106\ngenderM:ageSquared                0.0021081  0.0011869    1.776\ngenderOther:ageSquared           -0.0014332  0.0021039   -0.681\ngenderM:nSessions                 0.0035952  0.0024671    1.457\ngenderOther:nSessions             0.0029519  0.0044030    0.670\nageSquared:nSessions              0.0001480  0.0001706    0.867\ngenderM:ageSquared:nSessions     -0.0004417  0.0003007   -1.469\ngenderOther:ageSquared:nSessions  0.0002704  0.0005293    0.511\n                                 Pr(>|t|)    \n(Intercept)                        <2e-16 ***\nfirst                              <2e-16 ***\ngenderM                            <2e-16 ***\ngenderOther                        <2e-16 ***\nageSquared                         <2e-16 ***\nnSessions                          <2e-16 ***\ngenderM:ageSquared                 0.0757 .  \ngenderOther:ageSquared             0.4958    \ngenderM:nSessions                  0.1450    \ngenderOther:nSessions              0.5026    \nageSquared:nSessions               0.3857    \ngenderM:ageSquared:nSessions       0.1418    \ngenderOther:ageSquared:nSessions   0.6095    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.203 on 417987 degrees of freedom\nMultiple R-squared:  0.7375,    Adjusted R-squared:  0.7375 \nF-statistic: 9.786e+04 on 12 and 417987 DF,  p-value: < 2.2e-16\n\nShow code\n\nanova(lisLM1, lisLMAge2)\n\n\nAnalysis of Variance Table\n\nModel 1: last ~ first\nModel 2: last ~ first + gender + ageSquared + nSessions + gender * ageSquared + \n    gender * nSessions + gender * ageSquared + ageSquared * nSessions + \n    gender * ageSquared * nSessions\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n1 417998 915044                                 \n2 417987 604514 11    310531 19519 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nShow code\n\njtools::effect_plot(lisLMAge2, pred = ageSquared, interval = TRUE, rug = TRUE)\n\n\n\n\nI’ve added an effect plot with “rugs” for the y and x variables. Shows clear quadratic effect of age (looks linear because we’re plotting against squared age).\nBasically, this is a surprisingly real world mess! I will stop here as I want to check my hunch that I’ve created these (as I say, very real world) interactions by the way I created the final scores using a multiplier rather than a simple addition. However, this does demonstrate the complexities of disentangline effects with even a few predictors particularly when gender is treated not as binary and when age cannot be treated as a linear variable as it clearly has a quadratic effect.\n\n\n\n",
    "preview": "https://www.psyctc.org/psyctc/wp-content/uploads/2022/02/prepost1.png",
    "last_modified": "2023-08-25T14:15:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-24-chance-corrected-agreement/",
    "title": "Chance corrected agreement",
    "description": "Simple plotting of raw agreement and Cohen's kappa for various prevalences of the rated quality\nand only chance agreement",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-01-24",
    "categories": [
      "agreement",
      "Cohen's kappa"
    ],
    "contents": "\n\nContents\nThe issue about raw agreement and prevalence of what is rated\nAdding confidence interval around the observed kappa\nWhat happens with better than chance agreement?\nBut those are fixed agreement rates\n\nCreated 22.i.22, extended 25-26.i.22\nThis post is a bit different from most of my posts here which are mostly about R itself. This is perhaps the first of another theme I wanted to have here about using to R to illustrate general statistical or psychometric issues. This one was created to illustrate points I was making in a blog post Why kappa? or How simple agreement rates are deceptive on my psyctc.org/psyctc/ blog.\nThis starts with some trivial R to illustrate how the prevalence of the quality rated affects a raw agreement rate if agreement is truly random. Then it got into somewhat more challenging R (for me at least) as I explored better than chance agreement.\nThe issue about raw agreement and prevalence of what is rated\nThe code just computes raw agreement and kappa for chance agreement and prevalences from 1 in 1,000 to 999 in 1,000. It shows that the agreement rate rises to very near 1, i.e. 100% as the prevalence gets very high (or low) whereas Cohen’s kappa remains zero at all prevalences because it is a “chance corrected” agreement coefficient.\n\n\nShow code\n\nvalN <- 1000\n1:(valN - 1) %>%\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  mutate(prevalence = prevalence / valN) %>%\n  rowwise() %>%\n  mutate(valPosPos = round(valN * prevalence^2), # just the product of the prevalences and get as a number, not a rate\n         valNegNeg = round(valN * (1 - prevalence)^2), # product of the rate of the negatives\n         valPosNeg = (valN - valPosPos - valNegNeg) / 2, # must be half the difference\n         valNegPos = valPosNeg, # must be the same\n         checkSum = valPosPos + valNegNeg + valPosNeg + valNegPos, # just checking!\n         rawAgreement = (valPosPos + valNegNeg) / valN,\n         kappa = list(DescTools::CohenKappa(matrix(c(valPosPos,\n                                                     valPosNeg,\n                                                     valNegPos,\n                                                     valNegNeg),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = prevalence, y = rawAgreement)) +\n  geom_line(colour = \"red\") +\n  geom_line(aes(y = kappa), colour = \"green\") +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\nShow code\n\nggsave(\"ggsave1.png\")\nvalN2 <- 10^6\n\n\nI think that shows pretty clearly why raw agreement should never be used as a coefficient of agreement and why, despite some real arguments for other coefficients and known weaknesses (see good wikipedia entry), kappa is pretty good and likely to remain the most used such coefficient.\nOne perhaps suprising thing is that the kappa values aren’t all exactly zero: see the zigzag of the values towards the ends of the x axis. The biggest value is 0.0209603 and the smallest is -0.0224949. These non-zero values arise because counts are integers and I have plotted for values of prevalence between 0.001 and 0.999 and a sample size of 1000. Towards the ends of that prevalence range rounding to get integer counts means that kappa cannot be exactly zero.\nIf I don’t round the cell sizes to integers, in effect staying with probabilities, or simulating an infinitely large sample, the issue goes away as shown here.\n\n\nShow code\n\n### valN2 pulled through from block above\n1:(valN - 1) %>%\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  mutate(prevalence = prevalence / valN) %>%\n  rowwise() %>%\n  mutate(valPosPos = valN2 * prevalence^2, # just the product of the prevalences and get as a number, not a rate\n         valNegNeg = valN2 * (1 - prevalence)^2, # product of the rate of the negatives\n         valPosNeg = (valN2 - valPosPos - valNegNeg) / 2, # must be half the difference\n         valNegPos = valPosNeg, # must be the same\n         checkSum = valPosPos + valNegNeg + valPosNeg + valNegPos, # just checking!\n         rawAgreement = (valPosPos + valNegNeg) / valN2,\n         kappa = list(DescTools::CohenKappa(matrix(c(valPosPos,\n                                                     valPosNeg,\n                                                     valNegPos,\n                                                     valNegNeg),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tibDat2\n\nggplot(data = tibDat2,\n       aes(x = prevalence, y = rawAgreement)) +\n  geom_line(colour = \"red\") +\n  geom_line(aes(y = kappa), colour = \"green\") +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\n\nAdding confidence interval around the observed kappa\nConfidence intervals (CIs) are of course informative about imprecision of estimation, here of kappa and I love them for that. However, sometimes they can also alert you that something is being stretched to implausibilty in what you are trying to learn from your data. Here they are for a sample size of 1000.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = prevalence, y = rawAgreement)) +\n  geom_line(colour = \"red\") +\n  geom_linerange(aes(ymax = upr.ci, ymin = lwr.ci), colour = \"palegreen\") +\n  geom_line(aes(y = kappa), colour = \"green\") +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\n\nBetween say a prevalence of .1 and .9 things are sensible there: the confidence interval around the observed kappa widens as the smallest cell sizes in the 2x2 crosstabulation get smaller. That’s because, as with most statistics, it’s the smallest cell size, rather than the total sample size (which is of course constant here), that determine precision of estimation.\nHowever, going out towards a prevalence of .01 or of .99 something is very clearly wrong there as we have confidence limits on kappa that go above 1 and below -1: values that are impossible for a “real” kappa. Here the CI is telling us that it can’t give us real world answers for the CI: one or more cell sizes are simply too small. These impossible kappa confidence limits actually occur when one of the cell sizes is zero.\nHere are the confidence intervals if push the sample size up to 10^{6}.\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = prevalence, y = rawAgreement)) +\n  geom_line(colour = \"red\") +\n  geom_linerange(aes(ymax = upr.ci, ymin = lwr.ci), colour = \"palegreen\") +\n  geom_line(aes(y = kappa), colour = \"green\") +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\n\nVery tight and no confidence limits impossible.\nWhat happens with better than chance agreement?\nHere I am looking at agreement rates from .6 up to .90 with the agreement imposed on the sample and the cell sizes worked out to the nearest integer, all given the sample size of 1,000.\n\n\nShow code\n\nvalN <- 1000\nvecAgreeRate <- c(seq(60, 90, 10)) / 100\n1:valN %>%\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  mutate(posA = prevalence, # so change number of positives for rater A\n         negA = valN - posA, # and negative\n         prevalence = prevalence / valN, # get prevalence as a rate not a count\n         ### now put in the agreement rates from vecAgreeRate\n         agreeRate = list(vecAgreeRate)) %>%\n  unnest_longer(agreeRate) %>%\n  ### now create the rater B counts using those agreement rates\n  mutate(posAposB = round(posA * agreeRate),\n         posAnegB = round(posA * (1 - agreeRate)),\n         negAposB = round(negA * (1 - agreeRate)),\n         negAnegB = round(negA * agreeRate),\n         checkSum = posAposB + posAnegB + negAposB + negAnegB,\n         rawAgreement = (posAposB + negAnegB) / valN) %>%\n  rowwise() %>%\n  mutate(kappa = list(DescTools::CohenKappa(matrix(c(posAposB,\n                                                     negAposB,\n                                                     posAnegB,\n                                                     negAnegB),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tibDat3\n\ntibDat3 %>% \n  mutate(txtAgree = str_c(\"Sample agreement: \", agreeRate)) -> tibDat3\n\nggplot(data = tibDat3,\n       aes(x = prevalence, y = rawAgreement)) +\n  facet_wrap(facets = vars(txtAgree),\n             ncol = 2) +\n  geom_line(colour = \"red\") +\n  geom_linerange(aes(ymin = lwr.ci, ymax = upr.ci),\n                 colour = \"palegreen\") +\n  geom_line(aes(y = kappa),\n            colour = \"green\") +  \n  geom_hline(yintercept = 0)\n\n\n\nOf course agreement wouldn’t be exactly the same for every sample, this is a slightly more realistic simulation treating the actually sample agreement as a binomial variable with population value\n\n\nShow code\n\nvalN <- 1000\nvecAgreeRate <- c(seq(50, 90, 10)) / 100\n1:valN %>%\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  mutate(agreeRate = list(vecAgreeRate)) %>%\n  unnest_longer(agreeRate) %>%\n  rowwise() %>%\n  mutate(prevalence = prevalence / valN,\n         posA = rbinom(1, valN, prevalence),\n         negA = valN - posA,\n         posAposB = rbinom(1, posA, agreeRate),\n         posAnegB = posA - posAposB,\n         negAnegB = rbinom(1, negA, agreeRate),\n         negAposB = negA - negAnegB,\n         checkSum = posAposB + posAnegB + negAnegB + negAposB,\n         rawAgreement = (posAposB + negAnegB) / valN,\n         kappa = list(DescTools::CohenKappa(matrix(c(posAposB,\n                                                     negAposB,\n                                                     posAnegB,\n                                                     negAnegB),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tibDat4\n\ntibDat4 %>% \n  mutate(txtAgree = str_c(\"Population agreement: \", agreeRate)) -> tibDat4\n  \n\nggplot(data = tibDat4,\n       aes(x = prevalence, y = rawAgreement)) +\n  facet_wrap(facets = vars(txtAgree),\n             ncol = 2) +\n  geom_line(colour = \"red\") +\n  geom_linerange(aes(ymin = lwr.ci, ymax = upr.ci),\n                 colour = \"palegreen\") +\n  geom_line(aes(y = kappa),\n            colour = \"green\") +  \n  geom_hline(yintercept = 0)\n\n\n\nBut those are fixed agreement rates\nYou may have been wondering why the raw agreement rates don’t show the U shaped relationship with prevalence as they do, must do, when I modelled random agreement earlier. That’s because this was modelling a agreement rate in the sample so, even when I treated the agreement as a binomial distribution rather than a fixed rate, the relationship with prevalence was removed. It’s really a completely artificial representation of raw agreement.\nSo let’s have a population model. This was a bit more challenging to program. What I have done is first to simulate samples with bivariate Gaussian distributions from populations with fixed correlations between those Gaussian variables. I have set the population correlations at 0, .3, .6 and .9 (Pearson correlations). Then I created the binary data for different prevalences simply by dichotomising the Gaussian variables at the appropriate cuttings points on the Gaussian cumulative density curve setting prevalences of .01 to .99. The sample size is set at 10,000.\nThat gets us this.\n\n\nShow code\n\nmakeCorrMat <- function(corr) {\n  matrix(c(1, corr, corr, 1), ncol = 2)\n}\n# makeCorrMat(0)\n# makeCorrMat(.5)\n\n# valN <- 1000\nvalN <- 10000\n# vecCorr <- seq(0, .9, .1)\nvecCorr <- c(0, .3, .6, .9)\nvecMu <- c(0, 0) # set means for mvrnorm\nvecPrevalence <- 1:99 / 100\n\n### test\n# cor(MASS::mvrnorm(100, mu = vecMu, Sigma = makeCorrMat(.9)))\n\nset.seed(12345)\nvecPrevalence %>% # start from the prevalences to build tibble\n  as_tibble() %>%\n  rename(prevalence = value) %>%\n  ### get the cutting points on the cumulative Gaussian distribution per prevalence\n  mutate(cutPoint = qnorm(prevalence),\n         ### input the vector of correlations\n         corr = list(vecCorr)) %>%\n  ### unnest to create a row for each correlation\n  unnest_longer(corr) %>%\n  rowwise() %>%\n  ### now create a bivariate Gaussian distribution sample from those population correlations\n  mutate(rawDat = list(MASS::mvrnorm(valN, mu = vecMu, Sigma = makeCorrMat(corr))),\n         obsCorr = cor(rawDat)[1, 2]) %>%\n  ungroup() %>% \n  unnest(rawDat, names_repair = \"universal\") %>%\n  rowwise() %>%\n  ### I'm sure I ought to be able to do this more elegantly but this gets from the embedded dataframe to two column vectors\n  mutate(rawA = rawDat[[1]],\n         rawB = rawDat[[2]]) %>%\n  select(-rawDat) %>%\n  ### end of that mess!\n  mutate(binaryA = if_else(rawA > cutPoint, 1, 0),\n         binaryB = if_else(rawB > cutPoint, 1, 0),\n         sumBinaries = binaryA + binaryB,\n         posAposB = if_else(sumBinaries == 2, 1, 0),\n         negAnegB = if_else(sumBinaries == 0, 1, 0),\n         negAposB = if_else(binaryA == 0 & binaryB == 1, 1, 0),\n         posAnegB = if_else(binaryA == 1 & binaryB == 0, 1, 0),\n         checkSum = sum(posAposB:posAnegB)) %>%\n  ungroup() -> tibBigDat\n\ntibBigDat %>%\n  group_by(prevalence, corr) %>%\n  summarise(obsCorr = first(obsCorr),\n            across(posAposB:posAnegB, sum)) %>% \n  ungroup() %>%\n  rowwise() %>% \n  mutate(rawAgreement = (posAposB + negAnegB) / valN,\n         kappa = list(DescTools::CohenKappa(matrix(c(posAposB, posAnegB, \n                                                   negAposB, negAnegB),\n                                                   ncol = 2),\n                                            conf.level = .95))) %>%\n  ungroup() %>%\n  unnest_wider(kappa) -> tmpTib\n\n### improve labelling of corr for facets\ntmpTib %>%\n  mutate(txtCorr = str_c(\"Population correlation: \", corr)) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = prevalence, y = rawAgreement)) +\n  facet_wrap(facets = vars(txtCorr)) +\n  geom_point(colour = \"red\",\n             size = 1) +\n  geom_linerange(aes(ymin = lwr.ci, ymax = upr.ci),\n                 colour = \"palegreen\") +\n  geom_point(aes(y = kappa),\n             colour = \"green\",\n             size = 1) +\n  geom_hline(yintercept = c(0, 1)) +\n  ylab(\"Agreement\") +\n  ggtitle(\"Chance agreement against prevalence of quality rated\",\n          subtitle = \"Raw agreement in red, Cohen's kappa in green\")\n\n\n\nThat shows correctly that the U shaped and misleading relationship between raw agreement and prevalence is not only true for random agreement, but is there as some real agreement is there, though the more the real agreement, the shallower the U curve as you’d expect.\nThis last is just me checking how tightly sample correlations approximate the population correlations (for n = 10,000).\n\n\nShow code\n\n### how well did the correlations work?\nset.seed(12345) # fix the jittering\nggplot(data = tmpTib,\n       aes(x= corr, y = obsCorr)) +\n  geom_jitter(height = 0, width = .05, alpha = .4) +\n  geom_smooth(method = \"lm\") +\n  xlab(\"Population correlation\") +\n  ylab(\"Observed correlation\") +\n  ggtitle(\"Scatterplot: observed correlations against the population correlations\",\n          subtitle = \"Horizontal jittering and transparency used to handle overprinting.  Blue line is linear fit\")\n\n\n\nHere’s the raw linear of the observed correlations on the population ones.\n\n\nShow code\n\nlm(obsCorr ~ corr, data = tmpTib)\n\n\nCall:\nlm(formula = obsCorr ~ corr, data = tmpTib)\n\nCoefficients:\n(Intercept)         corr  \n  0.0007104    0.9991870  \n\nFine!\nOK. I hope all this is useful in explaining these issues. Do contact me if you have questions or suggestions for improvements.\n\n\n\n",
    "preview": "posts/2022-01-24-chance-corrected-agreement/chance-corrected-agreement_files/figure-html5/simulate-1.png",
    "last_modified": "2023-08-25T14:15:00+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-15-data-ellipses-and-confidence-ellipses/",
    "title": "Data ellipses and confidence ellipses",
    "description": "This just clarifies the distinction between a data ellipse and a confidence ellipse, \ni.e. an ellipse describing the joint confidence intervals on two parameters of a model",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "R graphics",
      "distributions",
      "correlation",
      "regression"
    ],
    "contents": "\n\nContents\nHistory\nggExtra::ggMarginal() adds marginal histograms\n“Densigram”\nBoxplot\nViolin plot\n\nData ellipses\n“Robust” data ellipse\nEllipsoid hulls (or ellipsoidhulls)\n\nConfidence ellipses\n\n\nHistory\nCreated 15.i.22\nUpdated 16.i.22 adding ggExtra::ggMarginal() plots\nFollows the typically generous and helpful post from John Fox on the R-help list:\nDear Paul,\n\nOn 2022-01-14 1:17 p.m., Paul Bernal wrote:\n> Dear John and R community friends,\n>\n> To be a little bit more specific, what I need to accomplish is the\n> creation of a confidence interval ellipse over a scatterplot at\n> different percentiles. The confidence interval ellipses should be drawn\n> over the scatterplot.\n\nI'm not sure what you mean. Confidence ellipses are for regression\ncoefficients and so are on the scale of the coefficients; data\n(concentration) ellipses are for and on the scale of the explanatory\nvariables. As it turns out, for a linear model, the former is the\nrescaled 90 degree rotation of the latter.\n\nBecause the scatterplot of the (two) variables has the variables on the\naxes, a data ellipse but not a confidence ellipse makes sense (i.e., is\nin the proper units). Data ellipses are drawn by car::dataEllipse() and\n(as explained by Martin Maechler) cluster::ellipsoidPoints(); confidence\nellipses are drawn by car::confidenceEllipse() and the various methods\nof ellipse::ellipse().\n\nI hope this helps,\n  John\nThat made me realise that I was only “sort of” sure I understood that and reminded me that I have so far never used ellipses either as a way to describe 2D data or to map the confidence intervals of two parameters from a model. I decided to get to grips with this, starting by creating some correlated data.\n\n\nShow code\n\nset.seed(12345) # get replicability\nvalN <- 300 # sample size (doh!)\nx <- rnorm(valN) # Gaussian distribution\ny <- x + rnorm(valN, sd = .3) # create correlated y variable\nbind_cols(x = x, y = y) -> tibDat # build into a tibble\n\n\nHere’s the head of that dataset.\n\n\nShow code\n\n### show the data\ntibDat\n\n# A tibble: 300 × 2\n        x       y\n    <dbl>   <dbl>\n 1  0.586  0.742 \n 2  0.709  0.712 \n 3 -0.109 -0.241 \n 4 -0.453 -0.0937\n 5  0.606  0.571 \n 6 -1.82  -1.81  \n 7  0.630  0.989 \n 8 -0.276 -0.173 \n 9 -0.284 -0.383 \n10 -0.919 -0.418 \n# ℹ 290 more rows\n\nAnd here is a simple ggplot scattergram of that using transparency to handle overprinting.\n\n\nShow code\n\n### simple scattergram\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point(alpha = .4) +\n  geom_smooth(method = \"lm\") +\n  xlim(c(-3, 3)) +\n  ylim(c(-3, 3)) +\n  coord_fixed(1) -> p\np\n\n\n\nggExtra::ggMarginal() adds marginal histograms\nThis is just so I remember where to find this and for the fun of it: ggExtra::ggMarginal() can add marginal histograms, density plots, boxplots, violin plots or “densigrams”, a combination of a histogram and a density plot, to the sides of a scattergram. I like this!\n“Densigram”\n\n\nShow code\n\nggExtra::ggMarginal(p, type = \"densigram\")\n\n\n\nBoxplot\n\n\nShow code\n\nggExtra::ggMarginal(p, type = \"boxplot\")\n\n\n\nViolin plot\n\n\nShow code\n\nggExtra::ggMarginal(p, type = \"violin\")\n\n\n\nOK, back to the main issue.\nData ellipses\nA 95% data ellipse is an ellipse expected to contain 95% of the joint population distributions of x and y based on the observed data and the assumption of bivariate Gaussian distributions. The area contained can be what you like really (within the logical restrictions of it being a positive proportion/percentage and lower than 100%!) Here are data ellipses for that dataset created with car::dataEllipse(). I’ve used its default confidence intervals of 50% and 95%.\n\n\nShow code\n\ncar::dataEllipse(tibDat$x, tibDat$y) -> retDat # collect up data for the lines\n\n\nShow code\n\n# str(retDat)\n### retDat is a list containing a mapping for the ellipses\n### 50% ellipse points\nretDat$`0.5` %>%\n  as_tibble() -> tib50\n### 95% ellipse points\nretDat$`0.95` %>%\n  as_tibble() -> tib95\n\n\nHere’s the same but using cluster::ellipsoidPoints(). A bit more work than car::dataEllipse().\n\n\nShow code\n\ntibDat %>%\n  as.data.frame() %>%\n  as.matrix() -> matDat\n\nmatCovLS <- cov(matDat)\nvecMeans <- colMeans(matDat)\nvecMeans <- colMeans(matDat)\n### get 95% CI ellipse\nd2.95 <- qchisq(0.95, df = 2)\ncluster::ellipsoidPoints(matCovLS, d2.95, loc = vecMeans) -> matEllipseHull95\n### and now 50%\nd2.50 <- qchisq(0.5, df = 2)\ncluster::ellipsoidPoints(matCovLS, d2.50, loc = vecMeans) -> matEllipseHull50\n\nplot(matDat, asp = 1, xlim = c(-3, 3))\nlines(matEllipseHull95, col=\"blue\")\nlines(matEllipseHull50, col=\"blue\")\n\n\n\nThat really is the same as the other, well, minus the 50% interval but it looks different because of the changed scales.\n“Robust” data ellipse\nJust to extend things, the help for cluster::ellipsoidPoints() shows that you can use it with a “robust covariance” estimate rather than the least squares lm() or cov() one. Turns out that this uses cov.rob() from the MASS package which essentially does some censoring off of perceived or potential outliers to get an covariance matrix that would be less sensitive to outliers. Here we go.\n\n\nShow code\n\nCxy <- MASS::cov.rob(cbind(x,y))\ncluster::ellipsoidPoints(Cxy$cov, d2 = d2.95, loc=Cxy$center) -> matEllipseHullRob\n\nplot(matDat, asp = 1, xlim = c(-3, 3))\nlines(matEllipseHull95, col=\"blue\")\nlines(matEllipseHullRob, col=\"green\")\n\n\n\nThat has the 95% ellipse from the robust covariance matrix in green and the simple least squares ellipse in blue. As you would expect the difference is negligible as these are bivariate Gaussian data so there are few real outliers.\nThese plots are reminding me that all that learning curve to understand ggplot was worth it! However, the corollary is that I have forgotten most of what I ever knew about improving base R graphic output. Fortunately, I can take the output from car::dataEllipse() and feed it into ggplot where I use geom_path() to plot it.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point(alpha = .4) +\n  geom_smooth(method = \"lm\") +\n  xlim(c(-3, 3)) +\n  ylim(c(-3, 3)) +\n  coord_fixed(1) +\n  geom_path(data = tib50,\n            aes(x = x, y = y), colour = \"red\") +\n  geom_path(data = tib95,\n            aes(x = x, y = y), colour = \"orange\") \n\n\n\nSo that’s same again, now just feeding the points created by car::dataEllipse() for each CI into tibbles and those into ggplot and overlaying them on the scattergram.\nEllipsoid hulls (or ellipsoidhulls)\nThis was an interesting extension of my learning. An ellipsoid hull is different from a data ellipse: it’s the ellipse that contains all the observed points (with some on the boundary of the ellipse). Here using cluster::ellipsoidhull() and base graphics.\n\n\nShow code\n\ntibDat %>%\n  as.data.frame() %>%\n  as.matrix() -> matDat\n\ncluster::ellipsoidhull(matDat) -> ellipseHull\n\nplot(matDat, asp = 1, xlim = c(-3, 3))\nlines(predict(ellipseHull), col=\"blue\")\n\n\n\nAnd the same spitting the data into ggplot.\n\n\nShow code\n\npredict(ellipseHull) %>%\n  as_tibble(.name_repair = \"universal\") %>%\n  rename(x = `...1`) -> tibEllipseHullPath\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point(alpha = .4) +\n  xlim(c(-4, 4)) +\n  ylim(c(-4, 4)) +\n  coord_fixed(1) +\n  geom_path(data = tibEllipseHullPath,\n            aes(x = x, y = y), colour = \"blue\")\n\n\n\nConfidence ellipses\nSo what are confidence ellipses? These are not about estimation the distribution of the population data but confidence ellipses for model parameters estimated from the data. Here the model is linear regression of y on x and assuming Gaussian distributions and here are the model parameters estimated using lm().\n\n\nShow code\n\nlm(y ~ x, data = tibDat)\n\n\nCall:\nlm(formula = y ~ x, data = tibDat)\n\nCoefficients:\n(Intercept)            x  \n    0.02265      1.00701  \n\nThe two parameters are the intercept and the slope and the confidence ellipse shows the area containing the desired joint CIs. The default interval is 95% and here it is constructed using car::confidenceEllipse(). The point in the middle marks the point estimates of intercept and slope and the ellipse the CI around that.\n\n\nShow code\n\ncar::confidenceEllipse(lm(y ~ x, data = tibDat))\n\n\n\nHere is the same ellipse created using ellipse::ellipse().\n\n\nShow code\n\nellipse::ellipse(lm(y ~ x, data = tibDat)) -> matEllipseEllipse\nplot(ellipse::ellipse(lm(y ~ x, data = tibDat)), type = \"l\")\n\n\n\nJust for completeness, it’s easy to get the ellipse path using ellipse::ellipse() and spit that into ggplot.\n\n\nShow code\n\nellipse::ellipse(lm(y ~ x, data = tibDat)) -> matEllipseEllipse\n\n### rather clumsy creation of tibble of parameters for ggplot\nlm(y ~ x, data = tibDat)$coefficients -> vecLM\nbind_cols(Intercept = vecLM[1], Slope = vecLM[2]) -> tibParms\n\n### slightly nicer creation of tibble of the points on the ellipse\nmatEllipseEllipse %>%\n  as_tibble() %>%\n  rename(Intercept = `(Intercept)`,\n         Slope = x) -> tmpTib\n\n### plot those\nggplot(data = tmpTib,\n       aes(x = Intercept, y = Slope)) +\n  geom_path() +\n  geom_point(data = tibParms,\n             colour = \"blue\", \n             size = 3)\n\n\n\nThat shows a joint distribution suggesting that the two estimated parameters are pretty much uncorrelated. I think that doesn’t have to be the case. Let’s try the very non-Gaussian joint distribution we get if we square both x and y. Here’s the scattergram and 50% and 95% data ellipses for that.\n\n\nShow code\n\ntibDat %>%\n  mutate(xSqrd = x^2,\n         ySqrd = y^2) -> tibDat\n\ncar::dataEllipse(tibDat$xSqrd, tibDat$ySqrd) -> retDat2 # collect up data for the lines\n\n\nShow code\n\n# str(retDat)\nretDat2$`0.5` %>%\n  as_tibble() -> tibSqrd50\n\nretDat2$`0.95` %>%\n  as_tibble() -> tibSqrd95\n\nggplot(data = tibDat,\n       aes(x = xSqrd, y = ySqrd)) +\n  geom_point(alpha = .4) +\n  geom_smooth(method = \"lm\") +\n  xlim(c(0, 9)) +\n  ylim(c(0, 9)) +\n  coord_fixed(1) +\n  geom_path(data = tib50,\n            aes(x = x, y = y), colour = \"red\") +\n  geom_path(data = tib95,\n            aes(x = x, y = y), colour = \"orange\")\n\n\n\nAnd here is the confidence ellipse from car::confidenceEllipse().\n\n\nShow code\n\ncar::confidenceEllipse(lm(ySqrd ~ xSqrd, data = tibDat))\n\n\n\nSame by ggplot.\n\n\nShow code\n\nellipse::ellipse(lm(ySqrd ~ xSqrd, data = tibDat)) -> matEllipseEllipse2\n\n### rather clumsy creation of tibble of parameters for ggplot\nlm(ySqrd ~ xSqrd, data = tibDat)$coefficients -> vecLM2\nbind_cols(Intercept = vecLM2[1], Slope = vecLM2[2]) -> tibParms2\n\n### slightly nicer creation of tibble of the points on the ellipse\nmatEllipseEllipse2 %>%\n  as_tibble() %>%\n  rename(Intercept = `(Intercept)`,\n         Slope = xSqrd) -> tmpTib2\n\n### plot those\nggplot(data = tmpTib2,\n       aes(x = Intercept, y = Slope)) +\n  geom_path() +\n  geom_point(data = tibParms2,\n             colour = \"blue\", \n             size = 3)\n\n\n\nOK. I think that’s enough on this!\n\n\n\n",
    "preview": "https://www.psyctc.org/psyctc/wp-content/uploads/2022/01/dataEllipse-scaled.jpg",
    "last_modified": "2023-08-25T13:41:17+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-12-compiling-packages-reporting-missing-headers-in-windows/",
    "title": "Compiling packages reporting missing headers in windows",
    "description": "For anyone else who hits this and doesn't want to wait for someone to put\nthe compiled package into CRAN",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-11-12",
    "categories": [
      "R geeky",
      "R packages",
      "reminder (for CE!)"
    ],
    "contents": "\n\nContents\nGeeky background\nThe issue\n“SOLVED”, “howto”, solution!\nMy unpacking of what I had to do\n\n\n\nShow code\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nas_tibble(list(x = 1,\n               y = 1)) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"Rtools, Bash and pacman:\\npackages for packages!\",\n            size = 12,\n            colour = \"red\",\n            angle = 30) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\n\nLanguage in the text tweaked a bit 9.viii.23\nThis really is pretty geeky and probably mainly for my benefit when, as I suspect I will, I hit it in the future and have forgotten these details.\nGeeky background\nIn case you don’t know, most software is written in “source code” and then compiled to create an executable: the program itself that can be run within an operating system if it compiles correctly. (Actually, some code is “interpreted” and run directly, line by line; R and most forms of BASIC on which many people learned to program a few decades ago are such interpreted languages.)\nR packages for Windoze exist in two forms: executable/binary packages which run directly within R if installed with\ninstall.packages(\"packagename\"))\nand packages which need to be compiled before they can be run within R.\nThe issue\nRecently I found that a number of R packages were reporting that there were upgrades available but only available as source packages, i.e. requiring me to compile them on my own machine if I wanted the latest upgrade. That happens regularly for me but I think that in the default R setup in Windoze R won’t waste your time telling you about the upgrade being available because the default R setup assumes you’re not going to want to do that and doesn’t install all the extra software necessary to compile source packages.\nTo compile R packages from source in Windoze you need software which is provided by the Rtools toolset. See https://cran.r-project.org/bin/windows/Rtools/. Rtools is not an R package, it’s a collection of programs that run under Windoze and a lot of other support materials. Because it’s not an R package you don’t install it with\ninstall.packages()\nbut you install it into Windoze like any other Windoze program: that confused me some years ago. As well as installing Rtools, for it to work from R to compile source packages you must also add the location of Rtools to the Windoze path so R knows where to find the tools: https://github.com/r-windows/docs/blob/master/rtools40.md explains this. (In my VM Windoze I found that I had to put the location of Rtools in the system path not the user path for the Rtools shell to find Rtools, we’ll come to that below.)\nRtools, as the name suggests, gives you all the tools necessary to compile many packages. All the tools are open source so there is no charge for Rtools. Furthermore, it ensures that compiling R packages is entirely automated: usually all you have to do if you have installed Rtools is to say “yes” when asked if you want local compiling of an R package where the source package is more up to date than the compiled version. Then R crunches through the compiling with a fascinating cascade of messages from the various stages and then you get the usual messages that installation has worked (in my experience it’s extremely rare that it doesn’t and probably means that your version of Rtools has drifted out of synch with the version of R you are using).\nSo if you have installed Rtools then if you use the menu option to update packages R will will give you the option to compile locally (i.e. on your own machine) if the source package is more up to date than the executable package. As I say, in my experience it’s very rare that compiling will fail if you have the correct version of Rtools for your version of R. When that happens I find that usually I only have to wait a few days and the compiled package, or a new source package that has fixed whatever failed, appears on CRAN and your package updating works for that package again. Even more occasionally you wait some days and still the issue doesn’t go away and you start to wonder if there is something wrong with your system!\nThis happened for me with three packages: gsl, igraph and nloptr. This is where I discovered that sometimes you don’t just need Rtools to compile source packages locally but you may also need some packages for packages.\nWhat was happening was that instead of the cascade of compilation messages (and the occasional warning) scooting past and ending up with a message that the package had been installed each was giving the message:\n   **********************************************\n   WARNING: this package has a configure script\n         It probably needs manual configuration\n   **********************************************\nAnd things like this:\n*** arch - i386\n\"C:/rtools40/mingw32/bin/\"gcc  -I\"C:/PROGRA~1/R/R-41~1.2/include\" -DNDEBUG -I/include         -O2 -Wall  -std=gnu99 -mfpmath=sse -msse2 -mstackrealign  -c airy.c -o airy.o\nairy.c:1:10: fatal error: gsl/gsl_sf_airy.h: No such file or directory\n #include <gsl/gsl_sf_airy.h>\n          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake: *** [C:/PROGRA~1/R/R-41~1.2/etc/i386/Makeconf:238: airy.o] Error 1\nERROR: compilation failed for package 'gsl'\nI know enough about computers and compiling source code to know that message is telling me that the compiler couldn’t find a “header” file, in this case gsl_sf_airy.h. (After all, that’s what it says!!) However, searching the interweb for that didn’t come up with anything recent about anyone having this problem under windows (beware things on the interweb with problems and solutions more than a year old: too often they’ve been superceded by subsequent developments).\nI was also puzzled by all three giving the message:\n   **********************************************\n   WARNING: this package has a configure script\n         It probably needs manual configuration\n   **********************************************\nAgain, I wasn’t finding answers about this. After a while I decided that the fact I had been seeing this for a week or so and on two really rather different Windoze systems (one sitting directly on an old laptop, the other running in a VirtualBox virtual machine under Ubuntu) meant I ought to try harder to work out what was wrong. I took a punt and Emailed Jeroen Ooms the maintainer of Rtools. I got a fast response pointing me back into https://github.com/r-windows/docs/blob/master/rtools40.md, specifically to https://github.com/r-windows/docs/blob/master/rtools40.md#example-installing-a-library-with-pacman and this time I persevered trying to understand it and got there in the end. Hence I’m writing this for others who might, like me, not be sufficiently immersed in these things as people likeas Jeroen. I struggled to understand not only the instructions but also that you have to be a bit lateral thinking and search a bit more if things aren’t quite as easy as the example he has there.\n“SOLVED”, “howto”, solution!\nFirstly, I think the “this package has a configure script” is a bit of red herring as I don’t think any of these packages or their supporting facilities actually have a configure script or need manual configuration. What they do need is installation of the necessary header (and no doubt other) files and actually that’s pretty easy and in Jeroen’s page on github. However, I needed to unpack it.\nMy unpacking of what I had to do\nYou have to use two tools that are installed by Rtools.\nOne is bash which is C:.exe (assuming you have installed Rtools in the default place on your Windoze machine). If your version of Rtools is higher than 4.0 beware of these instructions and make sure what is in that github page doesn’t contradict what I am writing here: things change.\nBash is the Bourne again shell (it replaced the Bourne shell) and it’s the command prompt of Linux. (Actually it’s rather more than that and there are other shells in Linux but if you’re a fairly ordinary Windoze user that’s probably a sensible analogy; if you dive a bit deeper into Windoze then the power shell is a better analogy).\nBash allows you to run a collection of Linux utilities that are provided in Rtools including the crucial pacman. But before we get there …\n… first launch bash. I made a shortcut to C:.exe using the Windoze file explorer and put onto the desktop because I like working that way but bash is in the Windoze app menu under Rtools so you can launch it by clicking on it there (and I think getting to it that way will make sure bash finds all the Rtools components, launching from the executable or a short cut to it I found I had to have the C: location in the system path.)\nPacman is a package manager (something in the name?) and it installs packages of software beyond those already in Rtools including the ones I was missing.\n[Note here: these are the “packages for packages”: the packages that pacman manages are packages of software, including header files, that are needed in order to compile the R packages. These are two analogical but completely different uses of “package”.]\nSo now you are in the Bash shell and can use pacman. Start by typing\npacman -Syu\nwhich updates pacman’s repositories of information.\nNow let’s get what we need for the gsl package:\npacman -S mingw-w64-{i686,x86_64}-gsl\nwhich pulls down (-S = synchronise) the package gsl where we need it and the “{i686,x86_64}” ensures that both the versions for 32 bit (i686) and for 64 bit R (x86_64 doh!) are pulled down.\nNow if you relaunch R and type\n\ninstall.packages(“gsl”, type = “source”)\n\nor if you use the “packages, Update packages …” menu entry, you should find that gsl compiles nicely.\nFor the nloptr R package it’s slightly less obvious: you need\npacman -S mingw-w64-{i686,x86_64}-gsl\n(not the crucial absence of the “r” on the end!) I had to do a bit of searching on the interweb to find that.\nFor igraph it’s even less obvious, what you need is\npacman -S mingw-w64-{i686,x86_64}-glpk\npacman -S mingw-w64-{i686,x86_64}-libxml2\nThat one took a bit more searching from the error message to get there but it wasn’t very difficult.\nThat’s it! Problem cracked. Huge thanks to Jeoem Ooms for a nearly instant response, for pushing me in the right direction but above all for his work on Rtools and the other open source projects he supports … and thanks to the package maintainers for the three packages and really everyone contributing to R.\n\n\n\n",
    "preview": "posts/2021-11-12-compiling-packages-reporting-missing-headers-in-windows/compiling-packages-reporting-missing-headers-in-windows_files/figure-html5/createGraphic-1.png",
    "last_modified": "2023-08-25T13:40:50+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-09-ombookglossary/",
    "title": "For OMbook glossary",
    "description": {},
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-11-09",
    "categories": [
      "OMbook & glossary"
    ],
    "contents": "\n\nContents\nExplanation\nNotes to self\nEntries\nIllustrating mean and median from modified (truncated) Gaussian: pseudo CORE-OM “clinical” scores.\nNow illustrate mean and median on positively skew data\n\nIllustrating quartiles and IQR\nMore on quartiles\nUniform distribution\nGaussian distribution\nThrowing dice and central limit theorem\nTossing coins and the central limit theorem\nVariance: Gaussian\nVariance: real, from CORE-OM items\nHistograms and barplots\nBoxplots: Gaussian\nBoxplots: real, age and CORE-OM scores\nNotched boxplots: Gaussian\nNotched boxplots: real, age & CORE-OM scores\nViolin plot\n\n\nExplanation\nThis file is simply a cumulative collection of code I wrote to create graphics (or occasionally raw numbers) for entries in the glossary for our book Outcome measures and evaluation in counselling and psychotherapy.\nInformation about the book is at https://ombook.psyctc.org/SAGE/\nSupplementary information is at https://ombook.psyctc.org/book/ and …\n… the glossary itself is at https://ombook.psyctc.org/glossary/\nI’m not claiming the code is good, some of it certainly isn’t, but it works. Some of it uses real data which for now I am not making available until I am sure that is safe and OK with others involved in collecting the data.\nNotes to self\nBest to store theme\nAlways set xlab and ylab explicitly\nUsing ggsave helps create standard size\nProbably correct not to bother with plot titles for simple plots\n\n\nShow code\n\nsetwd(\"/media/chris/Clevo_SSD2/Data/MyR/R/distill_blog/test2/_posts/2021-11-09-ombookglossary\")\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(pander)\nlibrary(CECPfuns)\n\n### set theme here\ntheme_set(theme_bw())\ntheme_update(plot.title = element_text(hjust = .5),\n             plot.subtitle = element_text(hjust = .5),\n             axis.title = element_text(size = 15))\n\n\nEntries\nIllustrating mean and median from modified (truncated) Gaussian: pseudo CORE-OM “clinical” scores.\n\n\nShow code\n\nset.seed(12345)\nvalN <- 350\nvalMean <- 14\nvalMin <- 0\nvalMax <- 40\nvalSD <- 7\n\nrnorm(valN, valMean, valSD) %>%\n  as_tibble() %>%\n  mutate(score = round(value),\n         ### reset out of range scores\n         score = if_else(score < 0, 0, score),\n         score = if_else(score > 40, 40, score)) -> tibDat\n\ntibDat %>%\n  summarise(min = min(score),\n            lQuart = quantile(score, .25),\n            mean = mean(score),\n            median = median(score),\n            uQuart = quantile(score, .75),\n            max = max(score),\n            SD = sd(score)) -> tibSummary\n\n\nggplot(data = tibDat,\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = tibSummary$mean, \n             colour = \"blue\",\n             size = 2) +\n    geom_vline(xintercept = tibSummary$median, \n             colour = \"green\",\n             size = 2) +\n  scale_x_continuous(breaks = seq(0, 40, 2)) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"mean.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nNow illustrate mean and median on positively skew data\n\n\nShow code\n\ntibDat %>%\n  ### skew by exponentiation\n  mutate(score2 = score^1.9) -> tibDat\n\nmax(tibDat$score2)\n\n[1] 767.6734\n\nShow code\n\nmean(tibDat$score2)\n\n[1] 200.9804\n\nShow code\n\nmedian(tibDat$score2)\n\n[1] 171.6222\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = score2)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = mean(tibDat$score2), \n             colour = \"blue\",\n             size = 2) +\n  geom_vline(xintercept = median(tibDat$score2), \n             colour = \"green\",\n             size = 2) +\n  scale_x_continuous(breaks = seq(0, 800, 50)) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"mean2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\n\n\nShow code\n\nset.seed(12345)\nvalN <- 1750\nvalMean <- 14\n\nrnorm(valN) %>%\n  as_tibble() %>%\n  rename(score = value) -> tibSymm\n\n\nggplot(data = tibSymm,\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"symm.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nstandardiseVar <- function(x){\n  x <- x - mean(x, na.rm = TRUE)\n  x / sd(x, na.rm = TRUE)\n}\ntibSymm %>%\n  mutate(score2 = (5 + score)^3,\n         score2 = standardiseVar(score2)) -> tibPosSkew\n\nggplot(data = tibPosSkew,\n       aes(x = score2)) + \n  geom_histogram(center = TRUE) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"posSkew.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### now negative skew\ntibSymm %>%\n  mutate(score3 = log(5 + score),\n         score3 = standardiseVar(score3)) -> tibNegSkew\n\nggplot(data = tibNegSkew,\n       aes(x = score3)) + \n  geom_histogram(center = TRUE) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"negSkew.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nIllustrating quartiles and IQR\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = tibSummary$mean, \n             colour = \"blue\",\n             size = 2) +\n    geom_vline(xintercept = tibSummary$median, \n             colour = \"green\",\n             size = 2) +\n  scale_x_continuous(breaks = seq(0, 40, 2)) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\ntibDat %>%\n  rename(symm = score,\n         skew = score2) %>%\n  pivot_longer(cols = symm:skew, names_to = \"dist\", values_to = \"score\") -> tibDatLong\n\ntibDatLong %>%\n  group_by(dist) %>%\n  summarise(min = min(score),\n            lQuart = quantile(score, .25),\n            mean = mean(score),\n            median = median(score),\n            uQuart = quantile(score, .75),\n            max = max(score),\n            SD = sd(score)) -> tibSummary2\n\ntibSummary2 %>%\n  select(dist, ends_with(\"Quart\")) %>%\n  pivot_longer(cols = ends_with(\"Quart\")) -> tibIQR\n\n\nggplot(data = tibDatLong,\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(data = tibSummary2,\n             aes(xintercept = median),\n             colour = \"green\",\n             size = 2) +\n  geom_vline(data = tibSummary2,\n             aes(xintercept = mean),\n             colour = \"red\",\n             size = 1) +\n  geom_vline(data = tibSummary2,\n             aes(xintercept = lQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(data = tibSummary2,\n             aes(xintercept = uQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_line(data = tibIQR,\n            aes(y = 5, x = value),\n            arrow = arrow(ends = \"both\"),\n            colour = \"blue\",\n            size = 1.2) +\n  facet_wrap(facets = vars(dist),\n             nrow = 2,\n             scales = \"free\") +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"IQR.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibSummary2 %>%\n  filter(dist == \"symm\") -> tibSummary2symm\ntibIQR %>%\n  filter(dist == \"symm\") -> tibIQRsymm\n\nggplot(data = filter(tibDatLong, dist == \"symm\"),\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(data = tibSummary2symm,\n             aes(xintercept = median),\n             colour = \"green\",\n             size = 2) +\n  geom_vline(data = tibSummary2symm,\n             aes(xintercept = mean),\n             colour = \"red\",\n             size = 1) +\n  geom_vline(data = tibSummary2symm,\n             aes(xintercept = lQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(data = tibSummary2symm,\n             aes(xintercept = uQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_line(data = tibIQRsymm,\n            aes(y = 5, x = value),\n            arrow = arrow(ends = \"both\"),\n            colour = \"blue\",\n            size = 1.2) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"IQRsymm.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\ntibSummary2 %>%\n  filter(dist == \"skew\") -> tibSummary2skew\ntibIQR %>%\n  filter(dist == \"skew\") -> tibIQRskew\n\nggplot(data = filter(tibDatLong, dist == \"skew\"),\n       aes(x = score)) + \n  geom_histogram(center = TRUE) +\n  geom_vline(data = tibSummary2skew,\n             aes(xintercept = median),\n             colour = \"green\",\n             size = 2) +\n  geom_vline(data = tibSummary2skew,\n             aes(xintercept = mean),\n             colour = \"red\",\n             size = 1) +\n  geom_vline(data = tibSummary2skew,\n             aes(xintercept = lQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(data = tibSummary2skew,\n             aes(xintercept = uQuart),\n             colour = \"blue\",\n             size = 2) +\n  geom_line(data = tibIQRskew,\n            aes(y = 5, x = value),\n            arrow = arrow(ends = \"both\"),\n            colour = \"blue\",\n            size = 1.2) +\n  xlab(\"Score\") +\n  ylab(\"Count\") \n\n\nShow code\n\nggsave(filename = \"IQRskew.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nMore on quartiles\n\n\nShow code\n\nvalN <- 300\nset.seed(12345)\n\nrnorm(valN, 40, 5) %>%\n  as_tibble() %>%\n  rename(score = value) -> tibDat1\n\nggplot(data = tibDat1,\n       aes(x = score)) +\n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = mean(tibDat1$score),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(xintercept = quantile(tibDat1$score, c(.25, .75)),\n             colour = \"green\",\n             size = 2) +\n  xlab(\"Score\") +\n  ylab(\"Count\") +\n  scale_x_continuous(breaks = seq(0, 100, 10),\n                     limits = c(0, 100))\n\n\nShow code\n\nggsave(filename = \"quartiles1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nmean(tibDat1$score)\n\n[1] 40.40703\n\nShow code\n\nquantile(tibDat1$score, c(.25, .75))\n\n     25%      75% \n36.87730 43.78248 \n\nShow code\n\nrnorm(valN, 40, 15) %>%\n  as_tibble() %>%\n  rename(score = value) -> tibDat1\n\nggplot(data = tibDat1,\n       aes(x = score)) +\n  geom_histogram(center = TRUE) +\n  geom_vline(xintercept = mean(tibDat1$score),\n             colour = \"blue\",\n             size = 2) +\n  geom_vline(xintercept = quantile(tibDat1$score, c(.25, .75)),\n             colour = \"green\",\n             size = 2) +\n  xlab(\"Score\") +\n  ylab(\"Count\") +\n  scale_x_continuous(breaks = seq(0, 100, 10),\n                     limits = c(0, 100))\n\n\nShow code\n\nggsave(filename = \"quartiles2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nmean(tibDat1$score)\n\n[1] 41.16123\n\nShow code\n\nquantile(tibDat1$score, c(.25, .75))\n\n     25%      75% \n32.48149 50.16902 \n\nUniform distribution\n\n\nShow code\n\nset.seed(12345)\nc(6, 6, 6, 6, 12, 18, 24, 50, 100, 200, 500, 1000, 1000, 5000, 10000) %>%\n  as_tibble() %>%\n  rename(sampSize = value) %>%\n  mutate(sampID = row_number()) %>%\n  # group_by(sampID) %>%\n  # uncount(weights = sampSize, .id  = \"indID\")\n  rowwise() %>%\n  mutate(values = list(sample(1:6, sampSize, replace = TRUE))) %>%\n  ungroup() %>%\n  unnest_longer(values) -> tibUnif\n\n# tibUnif\n\nggplot(data = filter(tibUnif, sampSize == 6),\n       aes(x = values)) +\n  # geom_histogram(binwidth = 1,\n  #                boundary = 0) +\n  geom_bar() +\n  facet_grid(rows = vars(sampID),\n             cols = NULL,\n             scales = \"free_y\") +\n  scale_y_continuous(breaks = 0:2) +\n  scale_x_continuous(name = \"Observed scores on each die\", breaks = 1:6)\n\n\nShow code\n\nggsave(filename = \"dice1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibUnif %>%\n  filter(sampSize > 6 & sampSize < 200) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = values)) +\n  geom_bar(aes(y = ..prop..)) +\n  facet_grid(rows = vars(sampSize),\n             cols = NULL,\n             scales = \"fixed\") +\n  scale_x_continuous(name = \"Observed scores on each die\", breaks = 1:6) +\n  scale_y_continuous(name = \"Proportion\", breaks = (0:5)/10)\n\n\nShow code\n\nggsave(filename = \"dice2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibUnif %>%\n  filter(sampSize > 200) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = values)) +\n  geom_bar(aes(y = ..prop..)) +\n  facet_grid(rows = vars(sampSize),\n             cols = NULL,\n             scales = \"fixed\") +\n  scale_x_continuous(name = \"Observed scores on each die\", breaks = 1:6) +\n  scale_y_continuous(name = \"Proportion\", breaks = (0:5)/10)\n\n\nShow code\n\nggsave(filename = \"dice3.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nGaussian distribution\n\n\nShow code\n\nseq(-5, 5, length = 5001) %>%\n  as_tibble() %>%\n  mutate(p = dnorm(value)) -> tibGauss1\n\nggplot(data = tibGauss1,\n       aes(x = value, y = p)) +\n  geom_line() +\n  xlab(\"Observed value\") +\n  ylab(\"Probability\")\n\n\nShow code\n\nggsave(filename = \"Gauss1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nThrowing dice and central limit theorem\n\n\nShow code\n\nthrowDice <- function(nThrows, nSides = 6, scoreSides = 1:6){\n  ### function to simulate throwing dice (or anything really)\n  ### defaults to six sided die with scores 1:6 but you can override that\n  if(nThrows <= 0) {\n    stop(\"nThrows must be positive\")\n  }\n  if(nThrows > 8) {\n    stop(\"nThrows must be under 9 (to keep things easy!)\")\n  }\n  if(nThrows - round(nThrows) > .Machine$double.eps) {\n    warning(\"nThrows wasn't integer, rounded to integer\")\n    nThrows <- round(nThrows)\n  }\n  newScores <- scoreSides\n  while(nThrows > 1) {\n    newScores <- as.vector(outer(newScores, scoreSides, FUN = \"+\"))\n    nThrows <- nThrows - 1\n  }\n  newScores\n}\n# throwDice(0)\n# throwDice(1)\n# throwDice(1.1)\n# throwDice(11)\n# throwDice(2)\n# length(throwDice(2))\n# min(throwDice(2))\n# max(throwDice(2))\n# nThrows <- 3\n# throwDice(nThrows)\n# length(throwDice(nThrows))\n# min(throwDice(nThrows))\n# max(throwDice(nThrows))\n# nThrows <- 4\n# throwDice(nThrows)\n# length(throwDice(nThrows))\n# min(throwDice(nThrows))\n# max(throwDice(nThrows))\n\n\n1:8 %>%\n  as_tibble() %>%\n  rename(nThrows = value) %>%\n  rowwise() %>%\n  mutate(score = list(throwDice(nThrows)),\n         nThrowsFac = factor(nThrows)) %>%\n  ungroup() %>%\n  unnest_longer(score) %>%\n  group_by(nThrowsFac) %>%\n  mutate(nScores = n()) %>%\n  ungroup() %>%\n  group_by(nThrowsFac, score) %>%\n  summarise(nThrows = first(nThrows),\n            nScores = first(nScores),\n            n = n(),\n            p = n / nScores) %>%\n  ungroup() -> tibDiceThrows\n\nggplot(data = tibDiceThrows,\n       aes(x = score, y = p, colour = nThrowsFac)) +\n  geom_point()  +\n  geom_line() +\n  ylab(\"Probability\") +\n  scale_x_continuous(name = paste0(\"Total score from n dice (with n from 1 to \",\n                                   max(tibDiceThrows$nThrows),\n                                   \")\"),\n                     breaks = seq(2, max(tibDiceThrows$score), 2)) +\n  scale_colour_discrete(name = \"n(throws)\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"throwingDice.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibDiceThrows %>% \n  filter(nThrows == 6) -> tmpTib\n\ntmpTib %>%\n  summarise(n = n(),\n            minScore = min(score),\n            meanScore = Hmisc::wtd.mean(score, weights = nScores),\n            maxScore = max(score),\n            SDScore = sqrt(Hmisc::wtd.var(score, weights = nScores)),\n            totP = sum(p)) -> tmpTibSumm\n\ntmpTib %>%\n  mutate(normScore = score - tmpTibSumm$meanScore,\n         normScore = normScore / 3,\n         normProb = dnorm(normScore) / 3) -> tmpTib\ntmpTib %>%\n  summarise(meanNormScore = mean(normScore),\n            SDNormScore = sd(normScore),\n            sumNormProb = sum(normProb))\n\n# A tibble: 1 × 3\n  meanNormScore SDNormScore sumNormProb\n          <dbl>       <dbl>       <dbl>\n1             0        3.03        1.00\n\nShow code\n\nggplot(data = tmpTib,\n       aes(x = score, y = p)) +\n  geom_point()  +\n  geom_line() +\n  ylab(\"Probability\") +\n  scale_x_continuous(name = \"Total score from 6 dice\",\n                     breaks = 6:36)\n\n\nShow code\n\nggsave(filename = \"throwing6Dice.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\nggplot(data = tmpTib,\n       aes(x = score, y = p)) +\n  geom_point()  +\n  geom_line() +\n  geom_point(aes(y = normProb), \n            colour = \"green\") +\n  geom_line(aes(y = normProb), \n            colour = \"green\") +    \n  ylab(\"Probability\") +\n  scale_x_continuous(name = \"Total score from 6 dice\",\n                     breaks = 2:max(tmpTib$score)) +\n  scale_colour_discrete(name = \"n(throws)\")\n\n\nShow code\n\nggsave(filename = \"throwing6DiceWithGaussian.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\nggplot(data = tmpTib,\n       aes(x = score, y = p)) +\n  geom_point()  +\n  geom_line() +\n  ylab(\"Probability\") +\n  scale_x_continuous(name = \"Total score from 6 dice\",\n                     breaks = 2:max(tmpTib$score)) +\n  scale_colour_discrete(name = \"n(throws)\")\n\n\nShow code\n\nggsave(filename = \"throwing6Dice.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nTossing coins and the central limit theorem\n\n\nShow code\n\nset.seed(12345)\nnReps <- 50000\nc(1:5, seq(10, 50, 10), 100, 200, 300, 400, 500) %>%\n  as_tibble() %>%\n  rename(nThrows = value) %>%\n  rowwise() %>%\n  mutate(score = list(rbinom(nReps, nThrows, .5))) %>%\n  ungroup() %>%\n  unnest_longer(score) %>%\n  group_by(nThrows, score) %>%\n  summarise(n = n(),\n            p = n / nReps,\n            nThrowsFac = factor(nThrows)) %>%\n  ungroup() %>%\n  group_by(nThrows) %>%\n  mutate(scaledScore = scale(score)) -> tibBinom\n\n\nggplot(data = tibBinom,\n       aes(x = score, y = p, colour = nThrowsFac)) +\n  geom_point() +\n  geom_line()\n\n\nShow code\n\nggsave(filename = \"tossingCoins1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nggplot(data = tibBinom,\n       aes(x = scaledScore, y = p, colour = nThrowsFac)) +\n  geom_point() +\n  geom_line()\n\n\nShow code\n\nggsave(filename = \"tossingCoinsScaled.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\npbinom(0:2, 2, .5)\n\n[1] 0.25 0.75 1.00\n\nShow code\n\nc(1:5, 10, 20, 50, 100) %>%\n  as_tibble() %>%\n  rename(nThrows = value) %>%\n  rowwise() %>%\n  mutate(pVals = list(pbinom(0:nThrows, nThrows, .5))) %>%\n  ungroup() %>%\n  unnest_longer(pVals) %>%\n  group_by(nThrows) %>%\n  mutate(score = row_number() - 1,\n         nThrowsFac = factor(first(nThrows)),\n         p = if_else(row_number() == 1, pVals, pVals - lag(pVals)),\n         meanScore = Hmisc::wtd.mean(score, weights = p),\n         sdScore = sqrt(Hmisc::wtd.var(score, weights = p, normwt = TRUE)),\n         stdScore = (score - meanScore) / sdScore) -> tibpBinom\n\n\n# tibGauss1 %>%\n#   summarise(max(p)) # .399\n# \n# tibpBinom %>%\n#   filter(nThrows == 100) %>%\n#   summarise(max(p)) # .0796\n\nggplot(data = tibpBinom,\n       aes(x = stdScore, y = p, colour = nThrowsFac)) +\n  geom_point() +\n  geom_line() +\n  geom_line(inherit.aes = FALSE,\n            data = tibGauss1,\n            aes(x = value, y = p * .0796 / .399)) +\n  ylab(\"Probability\") +\n  xlab(\"Standardised score across tossing n fair coins, heads = 1, tails = 0\") +\n  scale_colour_discrete(name = \"n(throws)\")\n\n\nShow code\n\nggsave(filename = \"pbinomWithGauss.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nVariance: Gaussian\n\n\nShow code\n\nseq(0, 20, length = 5001) %>%\n  as_tibble() %>%\n  mutate(NHS = dnorm(value, 8, 2),\n         HS = dnorm(value, 12, 1)) %>% \n  pivot_longer(cols = ends_with(\"HS\"), names_to = \"Population\", values_to = \"p\") -> tibGauss2\n\nggplot(data = tibGauss2,\n       aes(x = value, y = p, colour = Population)) +\n  geom_line(size = 2, alpha = .8) +\n  xlab(\"Observed score\") +\n  ylab(\"Probability\")\n\n\nShow code\n\nggsave(filename = \"Gauss2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nVariance: real, from CORE-OM items\n\n\nShow code\n\n# tmpDatDir <- \"~/internalHDD/Data/CORE/translations/SPA_other/Clara/Our_papers/NClinPaper\"\ntmpDatDir <- \"/media/chris/Clevo_SSD2/Data/CORE/translations/SPA_other/Clara/Our_papers/2020_NClinPaper\"\nreadxl::read_excel(paste0(tmpDatDir, \"/\", \"Muestra_NoClinica_CP180520.xlsx\")) %>%\n  mutate(Gender = recode(Genero,\n                         Masculino = \"Male\",\n                         Femenino = \"Female\")) -> tibRawDat\n\n### this is a silly way to get around having to up the na.rm = TRUE clause in the mutate\nmeanNotNA <- function(x){\n  mean(x, na.rm = TRUE)\n}\nmedianNotNA <- function(x){\n  median(x, na.rm = TRUE)\n}\nvarNotNA <- function(x){\n  var(x, na.rm = TRUE)\n}\nsdNotNA <- function(x){\n  sd(x, na.rm = TRUE)\n}\nminNotNA <- function(x){\n  min(x, na.rm = TRUE)\n}\nmaxNotNA <- function(x){\n  max(x, na.rm = TRUE)\n}\n\n# tibRawDat %>% \n#   select(starts_with(\"COREOM01\")) %>%\n#   corrr::correlate(diagonal = 1) %>%\n#   pivot_longer(cols = -term) %>%\n#   filter(term != name) %>%\n#   arrange(desc(value))\n### OK, all recoded\n\ntibRawDat %>%\n  filter(Excluded == \"NO\" & !is.na(Genero)) -> tibUseDat\n\ntibUseDat %>%\n  select(Gender, Edad, starts_with(\"COREOM01\")) %>%\n  select(-COREOM01_35) %>%\n  pivot_longer(cols = starts_with(\"COREOM01\"), names_to = \"Item\", values_to = \"Score\") %>%\n  # group_by(Gender, Item) %>%\n  group_by(Item) %>%\n  summarise(totN = n(),\n            nOK = getNOK(Score),\n            min = minNotNA(Score),\n            mean = meanNotNA(Score),\n            median = medianNotNA(Score),\n            max = maxNotNA(Score),                                      \n            var = varNotNA(Score),\n            sd = sdNotNA(Score)) %>%\n  ungroup() %>%\n  # filter(min != 0 | max != 4) ## OK full range on all items\n  arrange(var) %>%\n  filter(row_number() %in% c(1,2,3, 32, 33, 34)) -> tmpTibSummary\n\ntmpTibSummary %>%\n  select(Item) %>% \n  pull() -> tmpVecItems\n\nc(\"R: plans to end my life\",\n  \"R: hurt myself physically...\",\n  \"R: threatened or intimidated\",\n  \"P: difficulty getting to sleep ...\",\n  \"P: thought to blame ...\",\n  \"P: felt unhappy\") -> tmpVecNames\n\ntibUseDat %>%\n  select(all_of(tmpVecItems)) -> tmpTibRawScores\n\ntmpTibRawScores %>%\n  pivot_longer(cols = everything(), names_to = \"Item\", values_to = \"Score\") %>%\n  filter(!is.na(Score)) %>%\n  mutate(Item = ordered(Item,\n                        levels = tmpVecItems,\n                        labels = tmpVecNames)) -> tmpTibLongScores\n\ntmpTibSummary %>%\n  mutate(Item = ordered(Item,\n                        levels = tmpVecItems,\n                        labels = tmpVecNames),\n         meanMinusSD = mean - sd,\n         meanPlusSD = mean + sd) -> tmpTibSummary\n\n\n\nggplot(data = tmpTibLongScores,\n       aes(x = Score)) +\n  ylim(0, 1000) +\n  geom_bar(width = 1) +\n  geom_vline(data = tmpTibSummary,\n             aes(xintercept = mean),\n             colour = \"green\") +\n  facet_wrap(facets = vars(Item), ncol = 3)\n\n\nShow code\n\nggsave(filename = \"Variance1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nggplot(data = tmpTibLongScores,\n       aes(x = Score)) +\n  ylim(0, 1000) +\n  geom_bar(width = 1) +\n  geom_vline(data = tmpTibSummary,\n             aes(xintercept = mean),\n             colour = \"green\") +\n  geom_segment(inherit.aes = FALSE,\n               data = tmpTibSummary,\n               aes(y = 900, yend = 900,\n                   x = meanMinusSD,\n                   xend = meanPlusSD),\n               colour = \"blue\",\n               lineend = \"round\",\n               arrow = arrow(ends = \"both\",\n                             length = unit(2, \"mm\"))) +\n  geom_text(data = tmpTibSummary,\n            aes(y = 960, x = 4.5, label = paste0(\"Mean: \", \n                                                 round(mean, 2),\n                                                 \"\\nVar: \",\n                                                 round(var, 2),\n                                                 \"\\nSD: \",\n                                                 round(sd, 2))),\n            size = 2.5,\n            hjust = 1,\n            vjust = 1) +\n  facet_wrap(facets = vars(Item), ncol = 3)\n\n\nShow code\n\nggsave(filename = \"Variance2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nHistograms and barplots\n\n\nShow code\n\nggplot(data = tibRawDat,\n       aes(x = Gender)) +\n  geom_bar() +\n  xlab(\"Gender\") +\n  theme(axis.text = element_text(size = 40),\n        axis.title = element_text(size = 50)) -> ggGender1\n\nggplot(data = tibRawDat,\n       aes(x = Edad)) +\n  geom_histogram(center = TRUE,\n                 breaks = 18:80) +\n  xlab(\"Age\") +\n  theme(axis.text = element_text(size = 40),\n        axis.title = element_text(size = 50)) -> ggAge1\n\nggpubr::ggarrange(ggGender1, ggAge1) -> ggArrange1\n\nggpubr::ggexport(ggArrange1,\n         filename = \"Histogram1.png\",\n         width = 1700,\n         height = 1470,\n         units = \"px\")\n\n[1] \"Histogram1%03d.png\"\n\nShow code\n\nggplot(data = tibRawDat,\n       aes(x = Edad)) +\n  geom_histogram(center = TRUE,\n                 breaks = c(18, 20, 30, 40, 50, 80),\n                 closed = \"right\") +\n  xlab(\"Age\") +\n  scale_x_continuous(breaks = seq(10, 80, 10))\n\n\nShow code\n\n  # theme(axis.text = element_text(size = 40),\n  #       axis.title = element_text(size = 50))\n\nggsave(filename = \"HistAge.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tibRawDat,\n       aes(x = Edad)) +\n  geom_histogram(aes(y = stat(count) / sum(count)),\n                 center = TRUE,\n                 breaks = c(18, 20, 30, 40, 50, 80),\n                 closed = \"right\") +\n  xlab(\"Age\") +\n  ylab(\"Proportion\") +\n  scale_x_continuous(breaks = seq(10, 80, 10))\n\n\nShow code\n\n  # theme(axis.text = element_text(size = 40),\n  #       axis.title = element_text(size = 50))\n\nggsave(filename = \"HistAge2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tibRawDat,\n       aes(x = Edad, fill = Gender)) +\n  geom_histogram(aes(y = stat(count) / sum(count)),\n                 position = \"dodge2\",\n                 center = TRUE,\n                 breaks = c(18, seq(20, 80, 10)),\n                 closed = \"left\") +\n  xlab(\"Age\") +\n  ylab(\"Proportion\") +\n  scale_x_continuous(breaks = seq(20, 80, 5))\n\n\nShow code\n\nggsave(filename = \"HistAgeGendDodge.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nggplot(data = tibRawDat,\n       aes(x = Edad, fill = Gender)) +\n  geom_histogram(aes(y = stat(count) / sum(count)),\n                 position = \"stack\",\n                 center = TRUE,\n                 breaks = c(18, seq(20, 80, 10)),\n                 closed = \"left\") +\n  xlab(\"Age\") +\n  ylab(\"Proportion\") +\n  scale_x_continuous(breaks = seq(20, 80, 5))\n\n\nShow code\n\nggsave(filename = \"HistAgeGendStack.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\ntibRawDat %>%\n  filter(!is.na(Edad)) %>%\n  mutate(Age = case_when(Edad < 20 ~ \"<20\",\n                         Edad < 30 ~ \"20 to 29\",\n                         Edad < 40 ~ \"30 to 39\",\n                         Edad < 50 ~ \"40 to 49\",\n                         Edad >= 50 ~ \"50 and over\"),\n         Age = ordered(Age, \n                       levels = c(\"<20\",\n                                  \"20 to 29\",\n                                  \"30 to 39\",\n                                  \"40 to 49\",\n                                  \"50 and over\"))) -> tmpTib #%>% select(Edad, Age)\n\ntmpTib %>%\n  tabyl(Age) %>%\n  adorn_pct_formatting(digits = 1) %>%\n  pander(justify = \"lrr\")\n\nAge\nn\npercent\n<20\n148\n14.9%\n20 to 29\n545\n55.1%\n30 to 39\n131\n13.2%\n40 to 49\n80\n8.1%\n50 and over\n86\n8.7%\n\nShow code\n\nggplot(data = tmpTib,\n       aes(x = Age)) +\n  geom_bar() +\n  xlab(\"Age\") \n\n\nShow code\n\nggsave(filename = \"BarAge.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nBoxplots: Gaussian\n\n\nShow code\n\nset.seed(12345)\ntmpValN <- 10000\nrnorm(tmpValN) %>% \n  as_tibble() -> tmpTibGaussian\n\ntmpTibGaussian %>%\n  summarise(min = min(value),\n            lwrQ = quantile(value, .25),\n            median = median(value),\n            uprQ = quantile(value, .75),\n            max = max(value))\n\n# A tibble: 1 × 5\n    min   lwrQ   median  uprQ   max\n  <dbl>  <dbl>    <dbl> <dbl> <dbl>\n1 -3.88 -0.664 0.000494 0.663  3.36\n\nShow code\n\nggplot(data = tmpTibGaussian,\n       aes(y = value)) +\n  geom_boxplot(fill = \"grey\") +\n  ylim(4, 4) +\n  xlab(\"\") -> ggGaussBox1\n\nggGaussBox1\n\n\nShow code\n\nggsave(filename = \"GaussBox1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tmpTibGaussian,\n       aes(y = value)) +\n  geom_boxplot(fill = \"grey\") +\n  ylim(-4, 4) +\n  xlab(\"\") +\n  theme(axis.text = element_text(size = 40),\n        axis.title = element_text(size = 50)) -> ggGaussBox2\n\nggGaussBox2\n\n\nShow code\n\nggplot(data = tmpTibGaussian,\n       aes(x = value)) +\n  geom_histogram(aes(y = ..density..),\n                 fill = \"grey\") +\n  geom_vline(xintercept = median(tmpTibGaussian$value)) +\n  geom_line(data = tibGauss1,\n            aes(x = value, y = p)) +\n  ylab(\"Probability\") -> ggGaussHistOnX\n\nggGaussHistOnX\n\n\nShow code\n\nggsave(filename = \"GaussHistOnX.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tmpTibGaussian,\n       aes(y = value)) +\n  geom_histogram(aes(x = ..density..),\n                 fill = \"grey\") +\n  geom_hline(yintercept = median(tmpTibGaussian$value)) +\n  geom_line(data = tibGauss1,\n            aes(y = value, x = p),\n            orientation = \"y\") +\n  ylim(-4, 4) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(axis.text = element_text(size = 40),\n        axis.title = element_text(size = 50)) -> ggGaussHistOnY\n\nggGaussHistOnY\n\n\nShow code\n\nggpubr::ggarrange(ggGaussBox2, ggGaussHistOnY,\n                  ncol = 2) -> ggArrange2\n\nggArrange2\n\n\nShow code\n\nggpubr::ggexport(ggArrange2,\n         filename = \"BoxAndHist.png\",\n         width = 1700,\n         height = 1470,\n         units = \"px\")\n\n[1] \"BoxAndHist%03d.png\"\n\nBoxplots: real, age and CORE-OM scores\n\n\nShow code\n\nggplot(data = tibRawDat,\n       aes(y = Edad, x = Gender, fill = Gender)) +\n  geom_boxplot(varwidth = TRUE) +\n  ylab(\"Age\")\n\n\nShow code\n\nggsave(filename = \"AgeByGenderBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  mutate(SocState = `Estado civil`,\n         SocState = recode(SocState,\n                           `Casado/a` = \"Coupled\",\n                           `Soltero`  = \"Single\",\n                           `Separado/a` = \"Separated\",\n                           `Divorciado/a` = \"Divorced\",\n                           `Unido/a` = \"Coupled\",\n                           `Unido` = \"Coupled\",\n                           `Viudo/a` = \"Widowed/Widower\")) %>%\n  rowwise() %>%\n  mutate(meanCOREOM = meanNotNA(c_across(COREOM01_01:COREOM01_34))) %>%\n  ungroup() -> tibRawDat\n\ntibRawDat %>%\n  mutate(SocStatNA = if_else(is.na(SocState), \"NA\", SocState)) %>%\n  group_by(SocStatNA) %>%\n  summarise(tmpN = n(),\n            tmpMedianAge = median(Edad, na.rm = TRUE),\n            varAge = var(Edad, na.rm = TRUE),\n            SDAge = sqrt(varAge)) -> tmpTib\n\ntmpTib %>%\n  arrange(desc(tmpN)) %>%\n  select(SocStatNA) %>%\n  pull() -> tmpVecN\n\ntmpTib %>%\n  arrange(tmpMedianAge) %>%\n  select(SocStatNA) %>%\n  pull() -> tmpVecAge\n\n\ntibRawDat %>%\n  mutate(SocStatNA = if_else(is.na(SocState), \"NA\", SocState),\n         SocStatN = ordered(SocStatNA,\n                            levels = tmpVecN,\n                            labels = tmpVecN),\n         SocStateAge = ordered(SocStatNA,\n                               levels = tmpVecAge,\n                               labels = tmpVecAge)) -> tibRawDat\n\nggplot(data = tibRawDat,\n       aes(y = Edad, x = SocStatN, fill = SocStatN)) +\n  geom_boxplot(varwidth = TRUE) +\n  geom_text(data = tmpTib,\n            inherit.aes = FALSE,\n            aes(x = SocStatNA, y = 12, label = tmpN),\n            vjust = .5) +\n  geom_text(x = .5, y = 12, label = \"n: \",\n            vjust = .5,\n            hjust = 0) +\n  ylim(10, 80) +\n  scale_fill_discrete(name = \"Social status\") +\n  ylab(\"Age\") +\n  xlab(\"Social status\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"AgeBySocStatnBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tibRawDat,\n       aes(y = meanCOREOM, x = SocStatN, fill = SocStatN)) +\n  geom_boxplot(varwidth = TRUE) +\n  geom_text(data = tmpTib,\n            inherit.aes = FALSE,\n            aes(x = SocStatNA, y = -.2, label = tmpN),\n            vjust = .5) +\n  geom_text(x = .5, y = -.2, label = \"n: \",\n            vjust = .5,\n            hjust = 0) +\n  ylim(-.5, 4) +\n  scale_fill_discrete(name = \"Social status\") +\n  ylab(\"CORE-OM score (mean item score)\") +\n  xlab(\"Social status\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"COREOMBySocStatnBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  filter(!is.na(`Tiene Hijos`)) %>%\n  mutate(nChildren = `Especifique cuántos`,\n         nChildren = if_else(nChildren > 3, 4, nChildren),\n         nChildren = if_else(`Tiene Hijos` == \"NO\", 0, nChildren),\n         nChildren = ordered(nChildren,\n                             levels = 0:4,\n                             labels = c(as.character(0:3),\n                                        \"4 or more\"))) %>%  \n  filter(!is.na(nChildren)) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(y = Edad, x = nChildren, fill = nChildren)) +\n  geom_boxplot(varwidth = TRUE) \n\n\nShow code\n\ntmpTib %>%\n  rowwise() %>%\n  mutate(meanCOREOM = meanNotNA(c_across(COREOM01_01:COREOM01_34))) %>%\n  ungroup() -> tmpTib\n\nggplot(data = tmpTib,\n       aes(y = meanCOREOM, x = nChildren, fill = nChildren)) +\n  geom_boxplot(varwidth = TRUE) \n\n\n\nNotched boxplots: Gaussian\n\n\nShow code\n\ntmpTibGaussian %>%\n  filter(row_number() <= 500) %>%\n  mutate(samp250 = if_else(row_number() <= 250, value, NA_real_),\n         samp100 = if_else(row_number() <= 100, value, NA_real_),\n         samp50 = if_else(row_number() <= 50, value, NA_real_),\n         samp10 = if_else(row_number() <= 10, value, NA_real_)) %>%\n  rename(samp500 = value) -> tmpTib\n\ntmpTib %>%\n  pivot_longer(cols = everything(), names_to = \"sample\") %>%\n  mutate(sample = ordered(sample,\n                          levels = paste0(\"samp\", c(500, 250, 100, 50, 10)))) -> tmpTibLong\n\nggplot(data = tmpTib,\n       aes(y = samp250)) +\n  geom_boxplot(fill = \"grey\", varwidth = TRUE, notch = TRUE) +\n  ylim(-4, 4) +\n  xlab(\"\") +\n  ylab(\"value\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank()) \n\n\nShow code\n\nggsave(filename = \"notchedGaussianBox1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tmpTib,\n       aes(y = samp250)) +\n  geom_boxplot(fill = \"grey\", varwidth = TRUE) +\n  ylim(-4, 4) +\n  xlab(\"\") +\n  ylab(\"value\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank()) \n\n\nShow code\n\nggsave(filename = \"notNotchedGaussianBox1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\nggplot(data = tmpTibLong,\n       aes(y = value)) +\n  geom_boxplot(fill = \"grey\", varwidth = TRUE, notch = TRUE) +\n  geom_hline(yintercept = 0) +\n  ylim(-4, 4) +\n  facet_grid(cols = vars(sample)) +\n  xlab(\"\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank()) \n\n\nShow code\n\nggsave(filename = \"notchedGaussianBoxes2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntmpTibGaussian %>%\n  mutate(simulN = row_number() %% 20) %>% \n  group_by(simulN) %>%\n  mutate(samp250 = if_else(row_number() <= 250, value, NA_real_),\n         samp100 = if_else(row_number() <= 100, value, NA_real_),\n         samp50 = if_else(row_number() <= 50, value, NA_real_),\n         samp10 = if_else(row_number() <= 10, value, NA_real_)) %>%\n  rename(samp500 = value) %>%\n  ungroup() -> tmpTib\n\ntmpTib %>%\n  filter(simulN > 1 & simulN <= 5) %>%\n  pivot_longer(cols = -simulN, names_to = \"sample\") %>%\n  mutate(sample = ordered(sample,\n                          levels = paste0(\"samp\", c(500, 250, 100, 50, 10)))) -> tmpTibLong\n\nggplot(data = tmpTibLong,\n       aes(y = value)) +\n  geom_boxplot(fill = \"grey\", varwidth = TRUE, notch = TRUE) +\n  geom_hline(yintercept = 0) +\n  ylim(-4, 4) +\n  facet_grid(cols = vars(sample),\n             rows = vars(simulN)) +\n  xlab(\"\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank()) \n\n\nShow code\n\nggsave(filename = \"notchedGaussianBoxes3.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\nNotched boxplots: real, age & CORE-OM scores\n\n\nShow code\n\ntibRawDat %>%\n  group_by(Gender) %>%\n  summarise(n = n(),\n            median = median(Edad, na.rm = TRUE)) -> tmpTibSummary\n\nggplot(data = tibRawDat,\n       aes(y = Edad, x = Gender, fill = Gender)) +\n  geom_boxplot(varwidth = TRUE, notch = TRUE) +\n  geom_text(data = tmpTibSummary,\n            aes(x = Gender, y = 14, label = n),\n            vjust = 0) +\n  geom_text(x = .90, y = 14, label = \"n: \",\n            hjust = 1,\n            vjust = 0) +\n  geom_text(data = tmpTibSummary,\n            aes(x = Gender, y = 11, label = median),\n            vjust = 0,\n            fontface = \"plain\") +\n  geom_text(x = .9, y = 11, label = \"median: \",\n            hjust = 1,\n            vjust = 0,\n            fontface = \"plain\") +\n  geom_hline(yintercept = median(tibRawDat$Edad, na.rm = TRUE)) +\n  ylab(\"Age\")\n\n\nShow code\n\nggsave(filename = \"AgeByGenderNotchedBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  mutate(SocStatNA = if_else(is.na(SocState), \"NA\", SocState)) %>%\n  group_by(SocStatNA) %>%\n  summarise(tmpN = n(),\n            tmpMedianAge = median(Edad, na.rm = TRUE),\n            varAge = var(Edad, na.rm = TRUE),\n            SDAge = sqrt(varAge)) -> tmpTib\n\ntmpTib %>%\n  arrange(desc(tmpN)) %>%\n  select(SocStatNA) %>%\n  pull() -> tmpVecN\n\ntmpTib %>%\n  arrange(tmpMedianAge) %>%\n  select(SocStatNA) %>%\n  pull() -> tmpVecAge\n\n\ntibRawDat %>%\n  mutate(SocStatNA = if_else(is.na(SocState), \"NA\", SocState),\n         SocStatN = ordered(SocStatNA,\n                            levels = tmpVecN,\n                            labels = tmpVecN),\n         SocStateAge = ordered(SocStatNA,\n                               levels = tmpVecAge,\n                               labels = tmpVecAge)) -> tibRawDat\n\nggplot(data = tibRawDat,\n       aes(y = Edad, x = SocStatN, fill = SocStatN)) +\n  geom_boxplot(varwidth = TRUE, notch = TRUE) +\n  geom_hline(yintercept = median(tibRawDat$Edad, na.rm = TRUE)) +\n  geom_text(data = tmpTib,\n            inherit.aes = FALSE,\n            aes(x = SocStatNA, y = 12, label = tmpN),\n            vjust = .5) +\n  geom_text(x = .5, y = 12, label = \"n: \",\n            vjust = .5,\n            hjust = 0) +\n  ylim(10, 80) +\n  scale_fill_discrete(name = \"Social status\") +\n  ylab(\"Age\") +\n  xlab(\"Social status\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"AgeBySocStatnNotchedBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  filter(!is.na(meanCOREOM)) %>%\n  group_by(SocStatN) %>%\n  summarise(n = n(),\n            median = round(median(meanCOREOM), 1)) -> tmpTibSummary\n\nggplot(data = tibRawDat,\n       aes(y = meanCOREOM, x = SocStatN, fill = SocStatN)) +\n  geom_boxplot(varwidth = TRUE, notch = TRUE) +\n  geom_hline(yintercept = median(tibRawDat$meanCOREOM, na.rm = TRUE)) +\n  geom_text(data = tmpTibSummary,\n            inherit.aes = FALSE,\n            aes(x = SocStatN, y = -.3, label = n),\n            vjust = .5) +\n  geom_text(x = .7, y = -.3, label = \"n: \",\n            vjust = .5,\n            hjust = 1) +\n  geom_text(data = tmpTibSummary,\n            inherit.aes = FALSE,\n            aes(x = SocStatN, y = -.5, label = median),\n            vjust = .5) +\n  geom_text(x = .7, y = -.5, label = \"median: \",\n            vjust = .5,\n            hjust = 1) +  \n  ylim(-.5, 4) +\n  expand_limits(x = -.7) +\n  scale_fill_discrete(name = \"Social status\") +\n  ylab(\"CORE-OM score (mean item score)\") +\n  xlab(\"Social status\") +\n  theme(axis.text.x = element_text(angle = 70, hjust = 1))\n\n\nShow code\n\nggsave(filename = \"COREOMBySocStatnBox.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntibRawDat %>%\n  filter(!is.na(`Tiene Hijos`)) %>%\n  mutate(nChildren = `Especifique cuántos`,\n         nChildren = if_else(nChildren > 3, 4, nChildren),\n         nChildren = if_else(`Tiene Hijos` == \"NO\", 0, nChildren),\n         nChildren = ordered(nChildren,\n                             levels = 0:4,\n                             labels = c(as.character(0:3),\n                                        \"4 or more\"))) %>%  \n  filter(!is.na(nChildren)) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(y = Edad, x = nChildren, fill = nChildren)) +\n  geom_boxplot(varwidth = TRUE) \n\n\nShow code\n\ntmpTib %>%\n  rowwise() %>%\n  mutate(meanCOREOM = meanNotNA(c_across(COREOM01_01:COREOM01_34))) %>%\n  ungroup() -> tmpTib\n\nggplot(data = tmpTib,\n       aes(y = meanCOREOM, x = nChildren, fill = nChildren)) +\n  geom_boxplot(varwidth = TRUE) \n\n\n\nViolin plot\n\n\nShow code\n\n### start with simple Gaussian n = 50\nset.seed(12345)\ntmpValSampN <- 50\n1:5 %>%\n  as_tibble() %>%\n  rename(simN = value) %>%\n  group_by(simN) %>%\n  mutate(values = list(rnorm(tmpValSampN))) %>%\n  ungroup() %>%\n  unnest_longer(values) %>%\n  mutate(simN = ordered(simN)) -> tmpTib\n\nggplot(data = filter(tmpTib, simN == 1),\n       aes(x = simN, y = values)) +\n  geom_violin(fill = \"grey\") +\n  xlab(\"\")\n\n\nShow code\n\nggsave(filename = \"geomViolin1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### add jittered points to the plot\nggplot(data = filter(tmpTib, simN == 1),\n       aes(x = simN, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_jitter(height = 0, width = .07, alpha = .7) +\n  xlab(\"\")\n\n\nShow code\n\nggsave(filename = \"geomViolin2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### no jittering, just transparency\nggplot(data = filter(tmpTib, simN == 1),\n       aes(x = simN, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_point(alpha = .3) +\n  xlab(\"\")\n\n\nShow code\n\nggsave(filename = \"geomViolin3.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n### now use all five simulations\nggplot(data = tmpTib,\n       aes(x = simN, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_jitter(height = 0, width = .07, alpha = .7) +\n  geom_hline(yintercept = 0) +\n  xlab(\"Simulations\")\n\n\nShow code\n\nggsave(filename = \"geomViolin4.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### now move to different sample sizes\nc(50, 100, 200, 500, 1000, 5000, 50000, 500000) -> tmpVecN\n### get sample sizes into human readable rather than scientific format\nprettyNum(tmpVecN, big.mark = \",\", scientific = FALSE) -> tmpVecLabels\n\ntmpVecN %>%\n  as_tibble() %>%\n  rename(n = value) %>%\n  ### need rowwise() otherwise dplyr seems to use same seed each time\n  rowwise() %>%\n  mutate(values = list(rnorm(n))) %>%\n  ungroup() %>%\n  ### applying the labels to get discrete and readable variable\n  mutate(nFac = ordered(n,\n                     labels = tmpVecLabels)) %>%\n  unnest_longer(values) -> tmpTib\n\nggplot(data = tmpTib,\n       aes(x = nFac, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_jitter(height = 0, width = .1, alpha = .3) +\n  geom_hline(yintercept = 0) +\n  scale_x_discrete()\n\n\nShow code\n\nggsave(filename = \"geomViolin5.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### but showing the points in the larger samples is mad so ...\nggplot(data = tmpTib,\n       aes(x = nFac, y = values)) +\n  geom_violin(fill = \"grey\") +\n  geom_jitter(data = filter(tmpTib, n < 5000),\n              height = 0, width = .15, alpha = .1) +\n  geom_hline(yintercept = 0) +\n  scale_x_discrete()\n\n\nShow code\n\nggsave(filename = \"geomViolin6.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n### now some real data\ntibRawDat %>% \n  filter(!is.na(SocState)) %>%\n  filter(!is.na(Gender)) %>%\n  mutate(SocStat2 = recode(SocStatNA,\n                           \"NA\" = NA_character_,\n                           \"Divorced\" = \"Single\",\n                           \"Separated\" = \"Single\",\n                           \"Widowed/Widower\" = \"Single\")) -> tmpTib\n\ntmpTib %>%\n  group_by(Gender, SocStat2) %>%\n  summarise(CI = list(getBootCImean(meanCOREOM))) %>%\n  unnest_wider(CI) -> tmpSummary\ntmpSummary\n\n# A tibble: 4 × 5\n# Groups:   Gender [2]\n  Gender SocStat2 obsmean LCLmean UCLmean\n  <chr>  <chr>      <dbl>   <dbl>   <dbl>\n1 Female Coupled    0.778   0.695   0.861\n2 Female Single     1.03    0.978   1.08 \n3 Male   Coupled    0.728   0.670   0.793\n4 Male   Single     1.03    0.967   1.09 \n\nShow code\n\nggplot(data = tmpTib,\n       aes(x = interaction(Gender, SocStat2), y = meanCOREOM, fill = Gender)) +\n  geom_violin() +\n  geom_jitter(height = 0, width = .1, alpha = .2) +\n  geom_hline(yintercept = mean(tmpTib$meanCOREOM)) +\n  ### add summary statistics\n  geom_point(data = tmpSummary,\n             aes(y = obsmean),\n             position = position_nudge(x = -.15),\n             size = 2.5) +\n  geom_linerange(data = tmpSummary,\n                 inherit.aes = FALSE,\n                 aes(x = interaction(Gender, SocStat2),\n                     ymin = LCLmean, ymax = UCLmean),\n                 position = position_nudge(x = -.15),\n                 size = 1.5) +\n  xlab(\"Gender and social status\") +\n  ylab(\"CORE-OM score (mean item score)\")\n\n\nShow code\n\nggsave(filename = \"geomViolin7.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\n\nShow code\n\n### get the data\ntibUseDat %>%\n  mutate(SocState = `Estado civil`,\n         SocState = recode(SocState,\n                           `Casado/a` = \"Coupled\",\n                           `Soltero`  = \"Single\",\n                           `Separado/a` = \"Separated\",\n                           `Divorciado/a` = \"Divorced\",\n                           `Unido/a` = \"Coupled\",\n                           `Unido` = \"Coupled\",\n                           `Viudo/a` = \"Widowed/Widower\")) %>%\n  rowwise() %>%\n  mutate(meanCOREOM = meanNotNA(c_across(COREOM01_01:COREOM01_34))) %>%\n  ungroup() -> tibUseDat\n\n### histogram of the raw data\nggplot(data = tibUseDat,\n       aes(x = meanCOREOM)) +\n  geom_histogram() +\n  xlab(\"CORE-OM total score (item mean)\") +\n  ylab(\"Count\")\n\n\nShow code\n\nggsave(filename = \"jackknife1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### get the parametric CI\nmean(tibUseDat$meanCOREOM)\n\n[1] 0.9312496\n\nShow code\n\nvar(tibUseDat$meanCOREOM)\n\n[1] 0.2700728\n\nShow code\n\nsd(tibUseDat$meanCOREOM)\n\n[1] 0.5196853\n\nShow code\n\nsd(tibUseDat$meanCOREOM)/sqrt(nrow(tmpTib))\n\n[1] 0.01646686\n\nShow code\n\nmean(tibUseDat$meanCOREOM) - 1.96*sd(tibUseDat$meanCOREOM)/sqrt(nrow(tmpTib))\n\n[1] 0.8989745\n\nShow code\n\nmean(tibUseDat$meanCOREOM) + 1.96*sd(tibUseDat$meanCOREOM)/sqrt(nrow(tmpTib))\n\n[1] 0.9635246\n\nShow code\n\n# tibUseDat %>%\n#   select(Gender, SocState, meanCOREOM) %>%\n#   drop_na() -> tmpTib\n# \n# ggplot(data = tmpTib,\n#        aes(x = Gender, y = meanCOREOM)) +\n#   geom_boxplot(varwidth = TRUE, notch = TRUE)\n# \n# ggplot(data = tmpTib,\n#        aes(x = Gender, y = meanCOREOM)) +\n#   geom_violin()\n\n### get a well validated jackknife method\nbootstrap::jackknife(tmpTib$meanCOREOM, mean)\n\n$jack.se\n[1] 0.01688115\n\n$jack.bias\n[1] 0\n\n$jack.values\n  [1] 0.9630860 0.9628791 0.9629973 0.9629418 0.9634407 0.9629678\n  [7] 0.9630564 0.9627608 0.9628809 0.9634998 0.9629086 0.9631747\n [13] 0.9627904 0.9631747 0.9630564 0.9633225 0.9633225 0.9617263\n [19] 0.9632338 0.9634112 0.9633986 0.9629086 0.9632929 0.9632042\n [25] 0.9627608 0.9631156 0.9629382 0.9624948 0.9628200 0.9630269\n [31] 0.9633816 0.9625244 0.9634703 0.9629973 0.9629973 0.9631451\n [37] 0.9623327 0.9631451 0.9628495 0.9631451 0.9628495 0.9635294\n [43] 0.9629382 0.9625244 0.9628791 0.9632634 0.9625835 0.9629678\n [49] 0.9636181 0.9628791 0.9632634 0.9631156 0.9632042 0.9632634\n [55] 0.9633682 0.9625835 0.9623766 0.9635885 0.9625835 0.9628791\n [61] 0.9632929 0.9629418 0.9637954 0.9633520 0.9635885 0.9637067\n [67] 0.9618149 0.9637067 0.9637067 0.9637363 0.9627608 0.9635294\n [73] 0.9626722 0.9629113 0.9631451 0.9630269 0.9632338 0.9627608\n [79] 0.9632634 0.9633225 0.9632042 0.9636181 0.9620810 0.9630636\n [85] 0.9627017 0.9618741 0.9629113 0.9628200 0.9637336 0.9630860\n [91] 0.9627904 0.9627017 0.9633816 0.9637954 0.9628791 0.9632042\n [97] 0.9627608 0.9626130 0.9627904 0.9632929 0.9632929 0.9626068\n[103] 0.9631451 0.9632929 0.9635294 0.9636181 0.9630269 0.9627017\n[109] 0.9629770 0.9636476 0.9633225 0.9634703 0.9630860 0.9626426\n[115] 0.9634998 0.9636476 0.9635590 0.9632042 0.9632634 0.9631451\n[121] 0.9629678 0.9631156 0.9635885 0.9630564 0.9628200 0.9634703\n[127] 0.9629382 0.9634998 0.9611055 0.9627608 0.9629382 0.9632634\n[133] 0.9627017 0.9630564 0.9632338 0.9631451 0.9625835 0.9631451\n[139] 0.9633225 0.9630564 0.9631451 0.9634998 0.9629678 0.9617854\n[145] 0.9622879 0.9626722 0.9627904 0.9624357 0.9625835 0.9627608\n[151] 0.9628791 0.9626426 0.9632634 0.9633816 0.9627904 0.9629973\n[157] 0.9633816 0.9630269 0.9627608 0.9631451 0.9632929 0.9632338\n[163] 0.9623766 0.9634998 0.9635590 0.9633816 0.9625835 0.9629722\n[169] 0.9619036 0.9626426 0.9625539 0.9629678 0.9632929 0.9634112\n[175] 0.9623561 0.9632929 0.9630269 0.9629382 0.9634407 0.9637067\n[181] 0.9623174 0.9618149 0.9625539 0.9629678 0.9630860 0.9636181\n[187] 0.9632929 0.9630860 0.9624948 0.9627608 0.9627017 0.9629973\n[193] 0.9634112 0.9633816 0.9621992 0.9629086 0.9630860 0.9625835\n[199] 0.9615785 0.9629086 0.9632929 0.9637032 0.9624948 0.9631451\n[205] 0.9629973 0.9629678 0.9631156 0.9624061 0.9620219 0.9625835\n[211] 0.9627608 0.9635294 0.9624357 0.9630564 0.9631747 0.9633816\n[217] 0.9628495 0.9623631 0.9629276 0.9628504 0.9623661 0.9625244\n[223] 0.9630860 0.9637067 0.9633225 0.9631451 0.9637954 0.9632929\n[229] 0.9632042 0.9629086 0.9629382 0.9627017 0.9634703 0.9632463\n[235] 0.9634900 0.9636181 0.9622109 0.9627017 0.9629973 0.9633520\n[241] 0.9631451 0.9632042 0.9627608 0.9633225 0.9634703 0.9631156\n[247] 0.9637067 0.9637067 0.9629382 0.9635885 0.9636476 0.9628791\n[253] 0.9633520 0.9621105 0.9615785 0.9628200 0.9634703 0.9632929\n[259] 0.9631747 0.9633225 0.9629382 0.9631451 0.9630564 0.9632634\n[265] 0.9615785 0.9632634 0.9621105 0.9634407 0.9635885 0.9630564\n[271] 0.9624948 0.9634703 0.9635885 0.9636476 0.9635590 0.9632634\n[277] 0.9629973 0.9627017 0.9634703 0.9633520 0.9618741 0.9629418\n[283] 0.9635590 0.9627313 0.9631451 0.9632042 0.9620219 0.9632042\n[289] 0.9631451 0.9636727 0.9634998 0.9630860 0.9630880 0.9624948\n[295] 0.9635813 0.9631156 0.9628809 0.9615713 0.9628809 0.9626130\n[301] 0.9632634 0.9626722 0.9630564 0.9632042 0.9627591 0.9629973\n[307] 0.9637363 0.9626722 0.9635813 0.9631451 0.9632929 0.9634112\n[313] 0.9636181 0.9627608 0.9631747 0.9628200 0.9632634 0.9634112\n[319] 0.9629382 0.9624061 0.9634998 0.9620810 0.9628791 0.9629382\n[325] 0.9632042 0.9634703 0.9633520 0.9630269 0.9624652 0.9634112\n[331] 0.9627608 0.9631156 0.9629678 0.9630860 0.9635590 0.9631156\n[337] 0.9632929 0.9629678 0.9632929 0.9627017 0.9619332 0.9628495\n[343] 0.9623766 0.9635885 0.9629722 0.9633225 0.9627608 0.9632929\n[349] 0.9632042 0.9629086 0.9628495 0.9628791 0.9632634 0.9627904\n[355] 0.9629678 0.9637363 0.9625539 0.9629113 0.9630564 0.9634407\n[361] 0.9626130 0.9628200 0.9630860 0.9628791 0.9629382 0.9629678\n[367] 0.9632042 0.9634703 0.9634998 0.9629973 0.9635294 0.9626722\n[373] 0.9637659 0.9631747 0.9624948 0.9631654 0.9629113 0.9613438\n[379] 0.9635590 0.9622879 0.9632042 0.9615489 0.9629382 0.9637945\n[385] 0.9636118 0.9621105 0.9627608 0.9631451 0.9630269 0.9632042\n[391] 0.9633225 0.9634112 0.9620219 0.9634407 0.9623022 0.9633377\n[397] 0.9624061 0.9630564 0.9634112 0.9633520 0.9620586 0.9626130\n[403] 0.9614307 0.9624061 0.9628791 0.9627904 0.9618741 0.9627904\n[409] 0.9631747 0.9626068 0.9626130 0.9632634 0.9631451 0.9627313\n[415] 0.9627017 0.9626130 0.9632929 0.9620810 0.9633225 0.9620514\n[421] 0.9628495 0.9632634 0.9627608 0.9633816 0.9628200 0.9622457\n[427] 0.9625835 0.9627904 0.9629678 0.9621697 0.9634703 0.9629086\n[433] 0.9631747 0.9616376 0.9624357 0.9629418 0.9633225 0.9612829\n[439] 0.9625835 0.9627904 0.9628495 0.9632042 0.9632042 0.9631747\n[445] 0.9629973 0.9626130 0.9614898 0.9624061 0.9629086 0.9629973\n[451] 0.9633816 0.9626981 0.9629722 0.9629086 0.9627017 0.9632634\n[457] 0.9636181 0.9625539 0.9626130 0.9621401 0.9627904 0.9625539\n[463] 0.9630564 0.9631550 0.9624948 0.9614602 0.9630860 0.9625539\n[469] 0.9627017 0.9617558 0.9630269 0.9624357 0.9628200 0.9620219\n[475] 0.9621992 0.9625539 0.9622879 0.9622583 0.9625835 0.9633520\n[481] 0.9630860 0.9625835 0.9615785 0.9615489 0.9630564 0.9630269\n[487] 0.9624948 0.9625244 0.9621992 0.9629973 0.9624061 0.9624652\n[493] 0.9627313 0.9633816 0.9629086 0.9625539 0.9626722 0.9629678\n[499] 0.9625835 0.9632338 0.9630636 0.9626426 0.9626130 0.9631747\n[505] 0.9632929 0.9623174 0.9619923 0.9633225 0.9630564 0.9627313\n[511] 0.9631245 0.9622879 0.9631451 0.9633225 0.9627608 0.9621992\n[517] 0.9631156 0.9622879 0.9617854 0.9628200 0.9621992 0.9627904\n[523] 0.9623766 0.9627313 0.9623174 0.9623174 0.9616967 0.9626722\n[529] 0.9619036 0.9629678 0.9634703 0.9632929 0.9631451 0.9634703\n[535] 0.9631747 0.9634703 0.9626426 0.9628495 0.9634407 0.9615785\n[541] 0.9619332 0.9624545 0.9632634 0.9626722 0.9629086 0.9631747\n[547] 0.9625835 0.9632338 0.9623174 0.9629973 0.9623470 0.9632042\n[553] 0.9631156 0.9630564 0.9627313 0.9624948 0.9619627 0.9631550\n[559] 0.9623022 0.9627482 0.9624652 0.9632929 0.9622879 0.9621697\n[565] 0.9631156 0.9628791 0.9624061 0.9630860 0.9624357 0.9627608\n[571] 0.9630860 0.9625244 0.9630564 0.9632042 0.9636772 0.9628495\n[577] 0.9622879 0.9630269 0.9631747 0.9627608 0.9625539 0.9619036\n[583] 0.9625244 0.9624652 0.9617854 0.9616931 0.9629973 0.9634703\n[589] 0.9615104 0.9628809 0.9625835 0.9623470 0.9619332 0.9628495\n[595] 0.9618445 0.9632463 0.9616671 0.9626722 0.9624948 0.9620810\n[601] 0.9615489 0.9621992 0.9624745 0.9627904 0.9623766 0.9626722\n[607] 0.9621992 0.9624652 0.9628791 0.9629678 0.9620890 0.9631747\n[613] 0.9617263 0.9635885 0.9631451 0.9629973 0.9632338 0.9623470\n[619] 0.9626426 0.9627904 0.9628200 0.9633520 0.9627608 0.9622288\n[625] 0.9627608 0.9630269 0.9629678 0.9632929 0.9633816 0.9630269\n[631] 0.9634407 0.9632338 0.9623936 0.9620810 0.9635885 0.9632634\n[637] 0.9630564 0.9625244 0.9628495 0.9625835 0.9635885 0.9633816\n[643] 0.9636476 0.9629382 0.9624652 0.9624652 0.9623174 0.9616080\n[649] 0.9634998 0.9634703 0.9629678 0.9615193 0.9627608 0.9632338\n[655] 0.9632338 0.9631156 0.9632042 0.9632929 0.9631156 0.9626426\n[661] 0.9634112 0.9627608 0.9620810 0.9618149 0.9629382 0.9629973\n[667] 0.9623470 0.9630860 0.9636772 0.9627608 0.9635294 0.9625244\n[673] 0.9631156 0.9632338 0.9630860 0.9633520 0.9635885 0.9630564\n[679] 0.9627017 0.9627017 0.9638250 0.9633520 0.9633225 0.9632042\n[685] 0.9623470 0.9633986 0.9633816 0.9633225 0.9628495 0.9628791\n[691] 0.9626130 0.9629973 0.9625244 0.9626722 0.9628200 0.9635885\n[697] 0.9633816 0.9616671 0.9628504 0.9629086 0.9617263 0.9630636\n[703] 0.9629418 0.9633816 0.9623936 0.9624652 0.9633520 0.9634407\n[709] 0.9618741 0.9624948 0.9626426 0.9636181 0.9628791 0.9614898\n[715] 0.9613715 0.9621105 0.9631156 0.9630860 0.9623470 0.9624948\n[721] 0.9619332 0.9631451 0.9619332 0.9634112 0.9634998 0.9631451\n[727] 0.9632338 0.9615785 0.9621401 0.9634407 0.9630564 0.9626426\n[733] 0.9622879 0.9612829 0.9637954 0.9629382 0.9627904 0.9621401\n[739] 0.9636772 0.9634900 0.9636181 0.9633520 0.9630564 0.9627591\n[745] 0.9632634 0.9629382 0.9633816 0.9625835 0.9617558 0.9630564\n[751] 0.9631747 0.9635885 0.9612829 0.9631747 0.9627017 0.9630269\n[757] 0.9627017 0.9634112 0.9622583 0.9634112 0.9631854 0.9629382\n[763] 0.9634407 0.9631451 0.9636181 0.9628200 0.9632042 0.9615489\n[769] 0.9631156 0.9631747 0.9623766 0.9633816 0.9627904 0.9628791\n[775] 0.9630860 0.9627608 0.9626426 0.9622288 0.9632042 0.9630269\n[781] 0.9615489 0.9630860 0.9631747 0.9632929 0.9629382 0.9632929\n[787] 0.9628495 0.9628495 0.9632042 0.9630269 0.9627608 0.9631156\n[793] 0.9630941 0.9626426 0.9619332 0.9628495 0.9624357 0.9633225\n[799] 0.9619627 0.9629086 0.9634703 0.9630269 0.9634703 0.9623766\n[805] 0.9625539 0.9630860 0.9624948 0.9630269 0.9629678 0.9634998\n[811] 0.9628495 0.9619627 0.9630564 0.9622288 0.9625244 0.9618454\n[817] 0.9629973 0.9635294 0.9618741 0.9625539 0.9629086 0.9627313\n[823] 0.9630564 0.9633816 0.9627017 0.9633225 0.9629973 0.9625539\n[829] 0.9629973 0.9628791 0.9610464 0.9623470 0.9609873 0.9624357\n[835] 0.9619923 0.9632634 0.9615713 0.9620219 0.9628791 0.9624061\n[841] 0.9629678 0.9614602 0.9624357 0.9628791 0.9629678 0.9629382\n[847] 0.9630860 0.9623470 0.9619923 0.9629678 0.9622879 0.9631451\n[853] 0.9637363 0.9627904 0.9631747 0.9626068 0.9635590 0.9634998\n[859] 0.9632338 0.9629142 0.9628809 0.9626130 0.9629418 0.9629086\n[865] 0.9627904 0.9632634 0.9621697 0.9625244 0.9629382 0.9627904\n[871] 0.9631451 0.9630564 0.9626130 0.9632042 0.9619672 0.9631747\n[877] 0.9614602 0.9621697 0.9630564 0.9629678 0.9627904 0.9629973\n[883] 0.9633520 0.9608099 0.9629678 0.9627608 0.9634595 0.9628791\n[889] 0.9631451 0.9624357 0.9627904 0.9625835 0.9630860 0.9633520\n[895] 0.9621992 0.9619332 0.9625244 0.9630564 0.9628495 0.9621697\n[901] 0.9636476 0.9621697 0.9630564 0.9625244 0.9629382 0.9625154\n[907] 0.9625835 0.9617558 0.9628495 0.9620810 0.9618741 0.9625244\n[913] 0.9629973 0.9629678 0.9633520 0.9623470 0.9633225 0.9630269\n[919] 0.9633225 0.9630860 0.9628495 0.9628200 0.9631747 0.9626130\n[925] 0.9636476 0.9629086 0.9628200 0.9620219 0.9632929 0.9617263\n[931] 0.9620219 0.9627608 0.9622879 0.9636772 0.9630269 0.9628791\n[937] 0.9623174 0.9609873 0.9626426 0.9628495 0.9631451 0.9634998\n[943] 0.9637363 0.9636181 0.9631156 0.9630564 0.9630269 0.9633816\n[949] 0.9635590 0.9633520 0.9625835 0.9624652 0.9621992 0.9622583\n[955] 0.9624061 0.9632042 0.9631451 0.9629382 0.9624357 0.9633816\n[961] 0.9617854 0.9629086 0.9627904 0.9632929 0.9636476 0.9629456\n[967] 0.9627608 0.9627313 0.9629973 0.9625835 0.9626981 0.9622839\n[973] 0.9627313 0.9629086 0.9634998 0.9618149 0.9609281 0.9629382\n[979] 0.9625835 0.9623470 0.9624061 0.9626722 0.9625835 0.9619923\n[985] 0.9626426 0.9622879 0.9614898 0.9614602 0.9633816 0.9632634\n[991] 0.9630860 0.9620219 0.9632338 0.9631747 0.9630398 0.9612237\n\n$call\nbootstrap::jackknife(x = tmpTib$meanCOREOM, theta = mean)\n\nShow code\n\n### but I'm a masochistic, obsessional idiot so I replicate that!\njackMean <- function(x, confint = .95){\n  x <- na.omit(x)\n  valN <- length(x)\n  valSampMean <- mean(x)\n  vecJackMeans <- rep(NA, valN)\n  vecPseudoVals <- rep(NA, valN)\n  \n  for (i in 1:valN){\n    vecJackMeans[i] <- mean(x[-i])\n    vecPseudoVals[i] <- valN * valSampMean - (valN - 1)*vecJackMeans[i]\n  }\n  valBias <- (valN - 1) * (mean(vecJackMeans) - valSampMean)\n  valSE <- sqrt(((valN - 1) / valN) * sum((vecJackMeans - mean(vecJackMeans))^2))\n  valQt <- qt((1 - (1 - confint)/2), valN - 1)\n  print(valQt)\n  LCL <- valSampMean - (valSE * valQt)\n  UCL <- valSampMean + (valSE * valQt)\n  return(list(n = valN,\n              mean = valSampMean,\n              bias = valBias,\n              SE = valSE,\n              confint = confint,\n              LCL = LCL,\n              UCL = UCL,\n              CI = c(LCL, UCL),\n              vecJackMeans = vecJackMeans,\n              vecPseudoVals = vecPseudoVals))\n}\n\n# jackMean(1:8)\n# bootstrap::jackknife(1:8, mean)\n\njackMean(tmpTib$meanCOREOM) -> listJackknife\n\n[1] 1.962351\n\nShow code\n\nbootstrap::jackknife(tmpTib$meanCOREOM, mean)$jack.bias\n\n[1] 0\n\nShow code\n\nbootstrap::jackknife(tmpTib$meanCOREOM, mean)$jack.se\n\n[1] 0.01688115\n\nShow code\n\nlistJackknife$bias\n\n[1] 0\n\nShow code\n\nlistJackknife$SE\n\n[1] 0.01688115\n\nShow code\n\nlistJackknife$CI\n\n[1] 0.9297305 0.9959840\n\nShow code\n\nrange(listJackknife$vecJackMeans)\n\n[1] 0.9608099 0.9638250\n\nShow code\n\nlistJackknife$vecJackMeans %>%\n  as_tibble() -> tmpTibJackknife\n\nmean(tmpTibJackknife$value)\n\n[1] 0.9628573\n\nShow code\n\nmean(tmpTib$meanCOREOM)\n\n[1] 0.9628573\n\nShow code\n\nggplot(data = tmpTibJackknife,\n       aes(x = value)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tmpTibJackknife$value),\n             colour = \"green\",\n             size = 2) +\n  xlab(\"Jackknife means\") +\n  ylab(\"Count\")\n\n\nShow code\n\nggsave(filename = \"jackknife2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n### create an evenly spread small subsample\ntmpTib %>%\n  arrange(meanCOREOM) %>%\n  mutate(rowN = row_number()) %>%\n  filter(rowN %% 20 == 10) -> tmpTib2\n# max(tmpTib2$rowN)\n# nrow(tmpTib2)\n\nggplot(data = tmpTib2,\n       aes(x = meanCOREOM)) +\n  geom_histogram() +\n  xlab(\"CORE-OM total score (item mean)\") +\n  ylab(\"Count\")\n\n\nShow code\n\nggsave(filename = \"jackknife3.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n### parametric CI\nmean(tmpTib2$meanCOREOM)\n\n[1] 0.9687147\n\nShow code\n\nvar(tmpTib2$meanCOREOM)\n\n[1] 0.296774\n\nShow code\n\nsd(tmpTib2$meanCOREOM)/sqrt(nrow(tmpTib))\n\n[1] 0.01726169\n\nShow code\n\nmean(tmpTib2$meanCOREOM) - 1.96*sd(tmpTib2$meanCOREOM)/sqrt(nrow(tmpTib2))\n\n[1] 0.8177122\n\nShow code\n\nmean(tmpTib2$meanCOREOM) + 1.96*sd(tmpTib2$meanCOREOM)/sqrt(nrow(tmpTib2))\n\n[1] 1.119717\n\nShow code\n\n### jackknifing\njackMean(tmpTib2$meanCOREOM) -> listJackknife2\n\n[1] 2.009575\n\nShow code\n\nbootstrap::jackknife(tmpTib2$meanCOREOM, mean)$jack.bias\n\n[1] 0\n\nShow code\n\nbootstrap::jackknife(tmpTib2$meanCOREOM, mean)$jack.se\n\n[1] 0.07704206\n\nShow code\n\nlistJackknife2$bias\n\n[1] 0\n\nShow code\n\nlistJackknife2$SE\n\n[1] 0.07704206\n\nShow code\n\nlistJackknife2$CI\n\n[1] 0.8138928 1.1235365\n\nShow code\n\nrange(listJackknife2$vecJackMeans)\n\n[1] 0.9356632 0.9866836\n\nShow code\n\nlistJackknife2$vecJackMeans %>%\n  as_tibble() -> tmpTibJackknife2\n\nggplot(data = tmpTibJackknife2,\n       aes(x = value)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tmpTibJackknife$value),\n             colour = \"green\",\n             size = 2) +\n  xlab(\"Jackknife means\") +\n  ylab(\"Count\")\n\n\nShow code\n\nggsave(filename = \"jackknife4.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\n\n\n\nShow code\n\nset.seed(12345)\n### function to use for bootstrapping\nmean4boot <- function(x, i){\n  mean(x[i])\n}\n\n### do the bootstrap (and time it)\nstart.time <- Sys.time()\ntmpRes <- boot::boot(tmpTib$meanCOREOM, mean4boot, 10000)\nend.time <- Sys.time()\nelapsed.time <- end.time - start.time\nelapsed.time\n\nTime difference of 0.3178759 secs\n\nShow code\n\n### get the data ready for plotting\ntmpRes$t[, 1] %>%\n  as_tibble() -> tmpTibBoot\n\nrange(tmpTibBoot$value)\n\n[1] 0.8807916 1.0319069\n\nShow code\n\nmean(tmpTibBoot$value)\n\n[1] 0.9630155\n\nShow code\n\ntmpRes$t0\n\n[1] 0.9628573\n\nShow code\n\nggplot(data = tmpTibBoot,\n       aes(x = value)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tmpTibBoot$value),\n             colour = \"green\",\n             size = 3.5) +\n  geom_vline(xintercept = mean(tmpTib$meanCOREOM),\n             colour = \"blue\")\n\n\nShow code\n\nggsave(filename = \"bootstrap1.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntmpRes$t0\n\n[1] 0.9628573\n\nShow code\n\nboot::boot.ci(tmpRes)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = tmpRes)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.9292,  0.9962 )   ( 0.9293,  0.9954 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.9303,  0.9964 )   ( 0.9304,  0.9965 )  \nCalculations and Intervals on Original Scale\n\nShow code\n\n### now the same on the reduced dataset \n### do the bootstrap (and time it)\nstart.time <- Sys.time()\ntmpRes <- boot::boot(tmpTib2$meanCOREOM, mean4boot, 10000)\nend.time <- Sys.time()\nelapsed.time <- end.time - start.time\nelapsed.time\n\nTime difference of 0.08973837 secs\n\nShow code\n\n### get the data ready for plotting\ntmpRes$t[, 1] %>%\n  as_tibble() -> tmpTibBoot\n\nrange(tmpTibBoot$value)\n\n[1] 0.7056996 1.2611440\n\nShow code\n\nggplot(data = tmpTibBoot,\n       aes(x = value)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean(tmpTibBoot$value),\n             colour = \"green\",\n             size = 3.5) +\n  geom_vline(xintercept = mean(tmpTib$meanCOREOM),\n             colour = \"blue\",\n             size = 2)\n\n\nShow code\n\nggsave(filename = \"bootstrap2.png\",\n       width = 1700,\n       height = 1470,\n       units = \"px\")\n\ntmpRes$t0\n\n[1] 0.9687147\n\nShow code\n\nboot::boot.ci(tmpRes)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = tmpRes)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.8214,  1.1181 )   ( 0.8157,  1.1128 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.8246,  1.1217 )   ( 0.8347,  1.1363 )  \nCalculations and Intervals on Original Scale\n\n\n\nShow code\n\ntribble(~DNA, ~Gender, ~n,\n        \"All\", \"F\", 125,\n        \"All\", \"M\", 125,\n        \"DNA...\", \"F\", 125,\n        \"DNA...\", \"M\", 125) -> tibAssoc125\ntibAssoc125 %>%\n  uncount(n) -> tibAssoc125long\n\n### good old fashioned, well hybrid, way to do this\ntibAssoc125long %>%\n  summarise(chisq = list(unlist(chisq.test(Gender, DNA)))) %>%\n  unnest_wider(chisq)\n\n# A tibble: 1 × 21\n  `statistic.X-squared` parameter.df p.value method          data.name\n  <chr>                 <chr>        <chr>   <chr>           <chr>    \n1 0                     1            1       Pearson's Chi-… Gender a…\n# ℹ 16 more variables: observed1 <chr>, observed2 <chr>,\n#   observed3 <chr>, observed4 <chr>, expected1 <chr>,\n#   expected2 <chr>, expected3 <chr>, expected4 <chr>,\n#   residuals1 <chr>, residuals2 <chr>, residuals3 <chr>,\n#   residuals4 <chr>, stdres1 <chr>, stdres2 <chr>, stdres3 <chr>,\n#   stdres4 <chr>\n\nShow code\n\n### let's see if I can get to understand the purrr and broom approach\ntibAssoc125long %>%\n  ### nest() makes all the data into a row with a dataframe in it of all the data\n  ### have to use \"data = everything\" when not using it after group_by() so it knows to use all columns\n  nest(data = everything()) %>%\n  ### so now we have a single row tibble so mutate() not summarise()\n  ### this says apply chisq.test to the data computing the chisq.test for Gender and DNA\n  ### this seems horrible syntax to me\n  mutate(chisq = purrr::map(data, ~chisq.test(.$Gender, .$DNA)),\n         ### this is similar: apply broom::tidy() to all of chisq\n         ### broom::tidy actually uses tidy.htest() seeing that chisq is an htest list\n         ### so it pulls the elements into a list\n         tidied = purrr::map(chisq, broom::tidy)) %>%\n  ### at this point you've still got all the data in a column data \n  ### and all the output of chisq.test() as a list\n  ### don't need all that!\n  select(tidied) %>%\n  ### now unnest the elements of tidied\n  unnest_wider(tidied)\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      <dbl>   <dbl>     <int> <chr>                     \n1         0       1         1 Pearson's Chi-squared test\n\nShow code\n\n### this was part of my abortive attempt to create tables I could easily \n### paste into WordPress by making them into jpegs or pngs using tools\n### it gridExtra.  I didn't crack it!\n# tt3 <- ttheme_default(core=list(fg_params=list(hjust=0, x=0.1)),\n#                       rowhead=list(fg_params=list(hjust=0, x=0)))\n# tibAssoc125long %>%\n#   tabyl(DNA, Gender) %>%\n#   as.data.frame() -> tmpDF\n# \n# \n# gridExtra::tableGrob(tmpDF, rows = NULL) -> tmpGrob\n# png(\"test.png\", height=1000, width=200)\n# gridExtra::grid.table(tmpDF, rows = NULL)\n# dev.off()\n\n\n\n\nShow code\n\n### trying to shift tables to WordPress using csv and the wpDataTables plugin\n### recode to long labels\ntibAssoc125long %>%\n  mutate(DNA = recode(DNA,\n                      \"All\" = \"Attended all sessions\",\n                      \"DNA...\" = \"DNAed at least one session\")) -> tibAssoc125long\n\n### simplest xtab\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(n = DNA) %>%\n  write_csv(file = \"xtab1.csv\")\n\n### simplest xtab\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(n = DNA) %>%\n  flextable::flextable() %>%\n  flextable::autofit() %>%\n  flextable::save_as_image(path = \"xtab1.png\", zoom = 3)\n\n[1] \"xtab1.png\"\n\nShow code\n\n### row percentages: CSV\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(`n (row %)` = DNA) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"row\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns(position = \"front\") %>%\n  write_csv(file = \"xtab2.csv\")\n\n### row percentages: png\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(`n (row %)` = DNA) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"row\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns(position = \"front\") %>%\n  flextable::flextable() %>%\n  flextable::autofit() %>%\n  flextable::save_as_image(path = \"xtab1rowperc.png\", zoom = 3)\n\n[1] \"xtab1rowperc.png\"\n\nShow code\n\n### col percentages\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(`n (col %)` = DNA) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"col\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns(position = \"front\") %>%\n  write_csv(file = \"xtab3.csv\")\n  \n### total percentages\ntibAssoc125long %>%\n  tabyl(DNA, Gender)  %>%\n  ### get top left cell correct\n  rename(`n (% of total)` = DNA) %>%\n  adorn_totals(where = c(\"row\", \"col\")) %>%\n  adorn_percentages(denominator = \"all\") %>%\n  adorn_pct_formatting(digits = 1) %>%\n  adorn_ns(position = \"front\") %>%\n  write_csv(file = \"xtab4.csv\")\n\n\n\n\nShow code\n\nvecCOREwbItems <- c(4,14,17,31)\nvecCOREprobItems <- c(2,5,8,11,13,15,18,20,23,27,28,30)\nvecCOREfuncItems <- c(1,3,7,10,12,19,21,25,26,29,32,33)\nvecCOREriskItems <- c(6,9,16,22,24,34)\nvecCOREnrItems <- c(vecCOREwbItems, vecCOREprobItems, vecCOREfuncItems)\n\nvecCOREwbItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREwbItems))\nvecCOREprobItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREprobItems))\nvecCOREfuncItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREfuncItems))\nvecCOREriskItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREriskItems))\nvecCOREnrItems <- str_c(\"COREOM01_\", sprintf(\"%02.0f\", vecCOREnrItems))\n\ntibUseDat %>% \n  rowwise() %>%\n  mutate(meanCOREwb = meanNotNA(c_across(all_of(vecCOREwbItems))),\n         meanCOREprob = meanNotNA(c_across(all_of(vecCOREprobItems))),\n         meanCOREfunc = meanNotNA(c_across(all_of(vecCOREfuncItems))),\n         meanCORErisk = meanNotNA(c_across(all_of(vecCOREriskItems))),\n         meanCOREnr = meanNotNA(c_across(all_of(vecCOREnrItems))),) %>%\n  ungroup() -> tibUseDat\n\n\n### had too much overprinting with the full dataset so:\nset.seed(12345)\ntibUseDat %>%\n  ### my first ever use of slice_sample(): nice\n  slice_sample(n = 88) -> tibSmallDat\n\nggplot(data = tibSmallDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_point() +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatterpoint.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nggplot(data = tibSmallDat,\n       aes(x = meanCOREnr, y = meanCORErisk, colour = Gender, fill = Gender)) +\n  geom_point() +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatterpointGender.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\ntibUseDat %>%\n  select(meanCORErisk, meanCOREnr) %>% \n  distinct() %>% \n  summarise(n())\n\n# A tibble: 1 × 1\n  `n()`\n  <int>\n1   346\n\nShow code\n\ntibUseDat %>%\n  select(meanCORErisk, meanCOREnr) %>% \n  group_by(meanCOREnr, meanCORErisk) %>%\n  summarise(n = n()) %>%\n  ungroup() %>%\n  filter(n > 1) %>%\n  arrange(desc(n))\n\n# A tibble: 148 × 3\n   meanCOREnr meanCORErisk     n\n        <dbl>        <dbl> <int>\n 1      0.643        0        19\n 2      0.536        0        16\n 3      0.429        0        14\n 4      0.75         0        14\n 5      0.607        0        13\n 6      0.679        0        13\n 7      1.07         0        13\n 8      0.286        0        12\n 9      0.393        0        12\n10      0.893        0.167    12\n# ℹ 138 more rows\n\nShow code\n\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_point() +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatter1point.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_point(alpha = .1) +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatter1pointAlph.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_count() +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatter2count.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  geom_count(alpha = .3) +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"scatter3count.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\n\n\n\nShow code\n\nset.seed(12345)\nggplot(data = tibUseDat,\n       aes(x = meanCOREnr, y = meanCORErisk)) +\n  # geom_point(colour = \"blue\", alpha = .2) +\n  geom_jitter(width = 0, height = .05, alpha = .2) +\n  xlab(\"CORE-OM non-risk score (mean item score)\") +\n  ylab(\"CORE-OM risk score (mean item score)\") +\n  coord_fixed(ratio = 1) +\n  xlim(c(0, 4)) +\n  ylim(c(0, 4))\n\n\nShow code\n\nggsave(filename = \"jitter1.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\ntibUseDat %>%\n  # select(starts_with(\"COREOM01_\")) %>%\n  # select(-COREOM01_35) %>%\n  select(all_of(vecCOREriskItems)) %>%\n  na.omit() %>%\n  pivot_longer(everything()) %>%\n  group_by(name) %>%\n  summarise(var = var(value)) %>% \n  arrange(desc(var)) %>%\n  slice_head()\n\n# A tibble: 1 × 2\n  name          var\n  <chr>       <dbl>\n1 COREOM01_06 0.591\n\nShow code\n\ntibUseDat %>%\n  # select(starts_with(\"COREOM01_\")) %>%\n  # select(-COREOM01_35) %>%\n  select(all_of(vecCOREnrItems)) %>%\n  na.omit() %>%\n  pivot_longer(everything()) %>%\n  group_by(name) %>%\n  summarise(var = var(value)) %>% \n  arrange(desc(var)) %>%\n  slice_head()\n\n# A tibble: 1 × 2\n  name          var\n  <chr>       <dbl>\n1 COREOM01_27  1.65\n\nShow code\n\nggplot(data = tibUseDat,\n       aes(x =  COREOM01_27, y = COREOM01_06)) +\n  geom_point() +\n  xlab(\"Scores on CORE-OM item 27:\\nI have felt unhappy\") +\n  ylab(\"Scores on CORE-OM item 6:\\nI have been physically violent to others\")\n\n\nShow code\n\nggsave(filename = \"items27and6_plot1.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\nset.seed(12345)\nggplot(data = tibUseDat,\n       aes(x =  COREOM01_27, y = COREOM01_06)) +\n  geom_jitter(width = .4, height = .4) +\n  xlab(\"Scores on CORE-OM item 27:\\nI have felt unhappy\") +\n  ylab(\"Scores on CORE-OM item 6:\\nI have been physically violent to others\")\n\n\nShow code\n\nggsave(filename = \"items27and6_jitter1.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\n\nset.seed(12345)\nggplot(data = tibUseDat,\n       aes(x =  COREOM01_27, y = COREOM01_06)) +\n  geom_jitter(width = .4, height = .4, alpha = .3) +\n  xlab(\"Scores on CORE-OM item 27:\\nI have felt unhappy\") +\n  ylab(\"Scores on CORE-OM item 6:\\nI have been physically violent to others\")\n\n\nShow code\n\nggsave(filename = \"items27and6_jitter2.png\",\n       width = 1700,\n       height = 1700,\n       units = \"px\")\n\n\n\n\nShow code\n\nsave.image(file = \"Glossary.rda\")\n\nload(file = \"Glossary.rda\")\n\n\n\n\n\n",
    "preview": "posts/2021-11-09-ombookglossary/OMbook_glossary_files/figure-html5/mean1-1.png",
    "last_modified": "2023-08-22T13:57:26+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-31-subscaletotal-correlations/",
    "title": "Subscale/total correlations",
    "description": "A look at subscale/total correlations in the null model",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-10-31",
    "categories": [
      "correlation",
      "psychometrics",
      "regression"
    ],
    "contents": "\nThis came about from some work I am doing with colleagues looking at Authenticity Scale (AS; Wood et al., -Wood et al. (2008)). The AS has twelve items and a nicely balanced set of three subscales of four items each. The subscales are named Self-Alienation (SA), Accepting External Influence (AEI) and Authentic Living (AL). I was doing what I have always done before and looking at the simple correlations between the subscales and between them and the total score. As it happened a low correlation between one subscale and the other two took me back to something that has been in my mind a lot this year: when is a correlation structure that is not simply unidimensional/unifactorial, perhaps even fairly cleanly of two factors such that we shouldn’t report total scores but only the subscale (factor) scores?\nThat’s for another day and another blog post or several but I found myself aware that in a true null model in which correlations between the items of a measure are purely random, the correlations between subscale scores and the total score must be higher than zero as there is shared variance between the subscale score and the total score. That got me pondering why tradition has it (and like a slave, I have always followed it) that for subscale/total correlations we report the raw correlation but when looking item/total correlations we report “corrected” item/total correlations (CITCs), i.e. the correlation between the scores on the item and the scores on the whole scale corrected: with that item’s scores omitted.\nIf the items scores are Gaussian and uncorrelated and all have equal variance then it’s not rocket science to work out that the asymptotic Pearson correlation (i.e. the correlation as the sample size tends to \\(\\infty\\)) between the subscale score and the total score will be:\n\\[ \\sqrt{\\frac{k_{subscale}}{k_{total}}} \\]\nWhere \\(k_{subscale}\\) is the number of items in the subscale and \\(k_{total}\\) is the number of items in the entire measure. (Quick reductio ad absurdum checking: if \\(k_{subscale}\\) is zero then the correlation will be zero and if \\(k_{subscale}\\)\nis the same as \\(k_{total}\\)) then the correlation is one.)\nSo for the AS with four items per subscale the asymptotic correlation would be \\(\\sqrt{\\frac{4}{12}}\\), i.e. sqrt(1/3) = 0.577 (to 3 d.p.) were there no systematic covariance across the items.\nHere’s the relationship between the correlation and the fraction of the total number of items in the subscale (always assuming a null model that there is no covariance across the items). I have added reference lines for the proportions of items in the subscales of the CORE-OM and the AS assuming their were zero population item covariance.\n\n\nShow code\n\nlibrary(tidyverse)\nvalK <- 340\n0:340 %>%\n  as_tibble() %>%\n  rename(fraction = value) %>%\n  mutate(fraction = fraction / valK,\n         R = sqrt(fraction)) -> tibRvals\n\ntibble(scale = c(\"CORE-OM WB (4/34)\",\n                 \"CORE-OM Risk (6/34)\",\n                 \"CORE-OM Problems or Functioning (17/34)\",\n                 \"AS any subscale (4/12)\"),\n       fraction = c(4/34, 6/34, 18/34, 4/12)) %>%\n  mutate(R = sqrt(fraction)) -> tibCOREandAS\n\nggplot(data = tibRvals,\n       aes(x = fraction, y = R)) +\n  geom_point() +\n  geom_line() +\n  geom_linerange(data = tibCOREandAS,\n             aes(xmin = 0, xmax = fraction, y = R)) +\n  geom_linerange(data = tibCOREandAS,\n                 aes(x = fraction, ymin = 0, ymax = R)) +\n  geom_text(data = tibCOREandAS,\n             aes(x = 0, y = R + .015, label = scale),\n             hjust = 0,\n             size = 2.2) +\n  xlab(bquote(k[subscale]/k[total])) +\n  ylab(\"Asymptotic correlation\") +\n  ggtitle(\"Plot of asymptotic subscale/total correlation\\nagainst proportion of total items in subscale\") +\n  scale_x_continuous(breaks = (0:10/10)) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = .5))\n\n\n\nI amused myself simulating this for a sample size of 5000.\n\n\nShow code\n\nlibrary(tidyverse)\noptions(dplyr.summarise.inform = FALSE)\n\n### generate Gaussian null model data\nset.seed(12345) # set for reproducible results\nvalN <- 5000 # sample size\nvalK <- 12 # total number of items\n\n### now make up the data in long format, i.e.\n###   an item score\n###   an item label\n###   a person ID\nrnorm(valN * valK) %>% # gets uncorrelated Gaussian data\n  as_tibble() %>%\n  mutate(itemN = ((row_number() - 1) %% 12) + 1, # use modulo arithmetic to get item number\n         item = str_c(\"I\", sprintf(\"%02.0f\", itemN)), # format it nicely\n         ID = ((row_number() - 1) %/% 12) + 1, # use modulo arithmetic to get person ID \n         ID = sprintf(\"%03.0f\", ID)) %>% # and format that, can now dump itemN\n  select(-itemN) -> tibLongItemDat\n\n### now just pivot that to get it into wide format, valK items per row\ntibLongItemDat %>%\n  pivot_wider(id_cols = ID, names_from = item, values_from = value) -> tibWideItemDat\n\n### map items to scales (just sequentially here, that's not the AS mapping)\nvecItemsScale1 <- str_c(\"I\", sprintf(\"%02.0f\", 1:4))\nvecItemsScale2 <- str_c(\"I\", sprintf(\"%02.0f\", 5:8))\nvecItemsScale3 <- str_c(\"I\", sprintf(\"%02.0f\", 9:12))\n\n### now use those maps to get the subscale scores as well as the total score\ntibWideItemDat %>%\n  rowwise() %>%\n  mutate(scoreAll = mean(c_across(-ID)),\n         score1 = mean(c_across(all_of(vecItemsScale1))),\n         score2 = mean(c_across(all_of(vecItemsScale2))),\n         score3 = mean(c_across(all_of(vecItemsScale3)))) %>%\n  ungroup() -> tibWideAllDat\n\ntibWideAllDat %>%\n  select(starts_with(\"score\")) -> tibScores\n\n### corrr::correlate() has a message about the method and handling of missing\n### punches through markdown despite the block header having \"message=FALSE\"\n### I could have wrapped this in suppressMessages() however you can suppress \n### that with \"quiet = TRUE\", see below\ntibScores%>%\n  ### here is the \"quiet = TRUE\" suppression of the message\n  corrr::correlate(diagonal = 1, quiet = TRUE) %>%\n  mutate(across(starts_with(\"score\"), round, 2)) %>%\n  pander::pander(justify = \"lrrrr\", digits = 2)\n\nterm\nscoreAll\nscore1\nscore2\nscore3\nscoreAll\n1\n0.57\n0.58\n0.59\nscore1\n0.57\n1\n-0.01\n0.01\nscore2\n0.58\n-0.01\n1\n0.01\nscore3\n0.59\n0.01\n0.01\n1\n\nAnd here’s the plot of the simulated scores. The blue lines are the linear regression lines.\n\n\nShow code\n\nlm_fn <- function(data, mapping, ...){\n  p <- ggplot(data = data, mapping = mapping) + \n    geom_point(alpha = .05) + \n    geom_smooth(method=lm, fill=\"blue\", color=\"blue\", ...)\n  p\n}\n\n\nGGally::ggpairs(tibScores,\n                lower = list(continuous = lm_fn)) +\n  theme_bw()\n\n\n\nI am still not sure why we report CITCs for item analyses but raw subscale/total correlations for subscales. I keep trying to convince myself there’s a logic to my long entrenched behaviour but I’m not sure there is. I have a suspicion that we have all been doing it following others’ examples and that it started long ago when SPSS made CITCs easy to compute in its RELIABILITY function. I have long felt that RELIABILITY was one of the better parts of SPSS!\n\n\n\nWood, Alex M., P. Alex Linley, John Maltby, Michael Baliousis, and Stephen Joseph. 2008. “The Authentic Personality: A Theoretical and Empirical Conceptualization and the Development of the Authenticity Scale.” Journal of Counseling Psychology 55 (3): 385–99. https://doi.org/10.1037/0022-0167.55.3.385.\n\n\n\n\n",
    "preview": "posts/2021-10-31-subscaletotal-correlations/subscaletotal-correlations_files/figure-html5/plot1-1.png",
    "last_modified": "2023-08-25T13:40:05+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1536
  },
  {
    "path": "posts/2021-04-09-spearman-brown-formula/",
    "title": "Spearman-Brown formula",
    "description": "How does internal reliability relate to number of items?",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-04-09",
    "categories": [
      "correlation",
      "psychometrics",
      "regression"
    ],
    "contents": "\n\nContents\nBackground\nTheory behind the Spearman-Brown formula\nYou can use getRelBySpearmanBrown from the CECPfuns package\n\n[Created 10.iv.21, tweak 15.iv.21 to add “, build_manual = TRUE” to install_github call]\nThe Spearman-Brown formula:\n\\[{\\rho^{*}}=\\frac{n\\rho}{1 + (n-1)\\rho}\\]\ngives us this plot.\n\n\nShow code\n\nlibrary(CECPfuns) # for getRelBySpearmanBrown()\n### to get the CECPfuns package use:\n# remotes::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE)\n### for which you may have needed to do \n# install.packages(\"remotes\")\n### in order to get the remotes package\n### you can also use install_github(), essentially the same as in remotes\n### from the devtools package if you have installed that but if you aren't\n### making R packages then you probably don't want all of devtools\n### see https://www.psyctc.org/Rblog/posts/2021-02-10-making-my-first-usable-package/\nmaxK <- 60\nvecK <- 2:maxK\nvecK %>%\n  as_tibble() %>%\n  rename(k = value) %>%\n  rowwise() %>%\n  ### I have put the explict mapping of getRelBySpearmanBrown to my CECPfuns package here to avoid confusion\n  mutate(rel.1 = CECPfuns::getRelBySpearmanBrown(oldRel = .1, lengthRatio = k / 2,  verbose = FALSE),\n         rel.2 = CECPfuns::getRelBySpearmanBrown(oldRel = .2, lengthRatio = k / 2,  verbose = FALSE),\n         rel.3 = CECPfuns::getRelBySpearmanBrown(oldRel = .3, lengthRatio = k / 2,  verbose = FALSE), \n         rel.4 = CECPfuns::getRelBySpearmanBrown(oldRel = .4, lengthRatio = k / 2,  verbose = FALSE)) %>%\n  ungroup() -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = starts_with(\"rel.\"), names_to = \"IIC\", values_to = \"Reliability\") %>%\n  mutate(IIC = factor(str_sub(IIC, 4, 5))) -> tibDatLong\n  \nggplot(data = tibDatLong,\n       aes(x = k, y = Reliability, group = IIC, colour = IIC)) +\n  geom_point(size = 1) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = c(1, seq(2, 8, 2), seq(0, maxK, 10))) + # and I want the x axis with these tick marks and labels\n  scale_y_continuous(breaks = seq(0, 1, .1)) + # same for the y axis\n  ggtitle(\"Relationship of reliability to k, number of items in a measure\",\n          subtitle = \"for fixed inter-item correlation (IIC)\") +\n  theme_bw() + # I like this simple theme with white plot area\n  theme(plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5), # I like titles and subtitles centered\n        panel.grid.minor = element_blank(), # gets grid lines only where the axis tick marks are not adding minor ones between those\n        axis.text.x = element_text(angle = 80, # trying to get the axis point labels rotated for maximum clarity\n                                   hjust = 1,  # and aligning them, \n                                   vjust = .75)) # is there a bug in the ggplot code failing to handle the number of characters?\n\n\n\nBackground\nI must have discovered this neat little formula back in the very early 1990s thinking about the Body Shape Questionnaire (BSQ; Cooper et al. (1986)). That thinking led to a paper I still like quite a bit: Evans & Dolan -Evans and Dolan (1993). In the formula \\(\\rho\\) is the reliability of a test and the equation is predicting \\(\\rho^{*}\\) the reliability of a new test longer, or shorter, than the first by a ratio \\(n\\).\nIn fact, the title of this page could have been the more accurate: “How does internal reliability/consistency relate to number of items in Classical Test Theory (CTT) assuming that mean inter-item correlation remains the same?” That’s what the Spearman-Brown (prediction) formula tells us.\nThere’s a typically excellent wikipedia entry about the formula. As well as a very thorough explanation of the formula the page also has a fascinating bit of history about the factor that Spearman-Brown was neither a single person with a double barrelled surname, nor a working partnership. I do love the way wikipedia contributors often add these things.\nWhy am I posting about this now, thirty years later? Well, it came in useful recently looking at the psychometrics of the YP-CORE. The YP-CORE has ten items, seven negatively cued, e.g. “My problems have felt too much for me” and three that are positively cued, e.g. “I’ve felt able to cope when things go wrong”. Emily wanted to test whether the reliability of the positively cued items was the same as that of the negatively cued items and had discovered the excellent cocron package (see also http://comparingcronbachalphas.org). The cocron package implements in R formulae for inference testing one Cronbach alpha value, and for testing equality of more than one alpha (both for values from the same sample, i.e. a within participants test, as would have been the case for Emily’s question, and for the probably more common question where values from multiple groups are to be compared, e.g. is the alpha higher when women complete the measure than it is when men complete it. These are based on the parametric model of and developed by Feldt and summarised nicely in Feldt, Woodruff, and Salih (1987).\nThat looked to give the test Emily wanted, however, the truism that unless something is very wrong, a measure of a latent variable with more items will always have higher reliability than one with fewer. I avoid gambling like the plague but I should have offered a bet to Emily that the negative items would have the higher reliability, and given that she had an aggregated dataset with n in the thousands, that the difference would be highly statistically significant.\nTheory behind the Spearman-Brown formula\nWhy should a longer test have higher reliability? Simply because as you get more items each item’s error (unreliability) variance, by definition uncorrelated with any other item’s error variance will tend to cancel out while any systematic variance that each item captures from the latent variable will accumulate. (Why do I say that items’ error variances are uncorrelated with each other: that simply follows from the definition that unreliability is random contamination of scores: if errors were correlated they would be part of invalidity: systematic contamination of scores, not of unreliability.)\nSo it’s logical that longer tests will have higher reliability than shorter assuming the same or similar mean inter-item correlations which reflect the systematic variance across scores is similar or the same.\nThe Spearman-Brown formula tells us how reliability changes with k, the number of items. How does the relationship look? That was the plot above. Here it is again.\n\n\nShow code\n\nmaxK <- 60\nvecK <- 2:maxK\nvecK %>%\n  as_tibble() %>%\n  rename(k = value) %>%\n  rowwise() %>%\n  mutate(rel.1 = getRelBySpearmanBrown(oldRel = .1, lengthRatio = k / 2, verbose = FALSE),\n         rel.2 = getRelBySpearmanBrown(oldRel = .2, lengthRatio = k / 2, verbose = FALSE),\n         rel.3 = getRelBySpearmanBrown(oldRel = .3, lengthRatio = k / 2, verbose = FALSE), \n         rel.4 = getRelBySpearmanBrown(oldRel = .4, lengthRatio = k / 2, verbose = FALSE)) %>%\n  ungroup() -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = starts_with(\"rel.\"), names_to = \"IIC\", values_to = \"Reliability\") %>%\n  mutate(IIC = factor(str_sub(IIC, 4, 5))) -> tibDatLong\n  \n\nggplot(data = tibDatLong,\n       aes(x = k, y = Reliability, group = IIC, colour = IIC)) +\n  geom_point(size = 1) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = c(1, seq(2, 8, 2), seq(0, maxK, 10))) + # and I want the x axis with these tick marks and labels\n  scale_y_continuous(breaks = seq(0, 1, .1)) + # same for the y axis\n  ggtitle(\"Relationship of reliability to k, number of items in a measure\",\n          subtitle = \"for fixed inter-item correlation (IIC)\") +\n  theme_bw() + # I like this simple theme with white plot area\n  theme(plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5), # I like titles and subtitles centered\n        panel.grid.minor = element_blank(), # gets grid lines only where the axis tick marks are not adding minor ones between those\n        axis.text.x = element_text(angle = 80, # trying to get the axis point labels rotated for maximum clarity\n                                   hjust = 1,  # and aligning them, \n                                   vjust = .75)) # is there a bug in the ggplot code failing to handle the number of characters?\n\n\n\nThat shows clearly how reliability climbs very rapidly as you move from having two items (the minimum to have an internal reliability) through single figures and then how the improvement steadily slows and will never reach 1.0 (unless you start with a reliability of 1.0 which isn’t our real world and arguably isn’t any real world). It also shows that the curve depends on the inter-item correlation (IIC). I have plotted starting with a correlation between two items of 0.1, 0.2, 0.3 or 0.4.\nSo knowing that there are seven negatively cued items in the YP-CORE and three positively cued items, how would the ratio of the reliabilities of the two vary with the mean ICC assuming that it was the same in each set of items? Here, in red, is the plot of the Spearman-Brown predicted reliability for the negatively cued items given a range of reliabilty for the three positively cued items from .01 to .35.\n\n\nShow code\n\nnNeg <- 7\nnPos <- 3\nvecRelPos <- seq(.01, .35, .01)\nvecRelPos %>%\n  as_tibble() %>%\n  rename(relPos = value) %>%\n  rowwise() %>%\n  mutate(relNeg = getRelBySpearmanBrown(oldRel = relPos, lengthRatio = 7/3, verbose = FALSE),\n         relRatio = relPos / relNeg) %>%\n  ungroup() -> tibDat2\n\nggplot(data = tibDat2,\n       aes(x = relPos, y = relNeg)) +\n  geom_line(colour = \"red\",\n            size = 2) +\n  geom_abline(intercept = 0, slope = 7 / 3) +\n  geom_abline(intercept = 0, slope = 1,\n              colour = \"blue\") +\n  scale_x_continuous(breaks = seq(0, .55, .05), \n                     limits = c(0, .55)) +\n  scale_y_continuous(breaks = seq(0, .55, .05), \n                     limits = c(0, .55)) +\n  ylab(\"Reliability for negative items\") +\n  xlab(\"Reliability for positive items\") +\n  ggtitle(\"Plot of reliability for seven negatively cued items given reliability of three positively cued items\",\n          subtitle = \"Assumes same mean inter-item correlation, black line marks inaccurate prediction using just y = 7/3 * x not Spearman-Brown formula\\n\n          blue line is y = x\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5),\n        aspect.ratio = 1)\n\n\n\nWe can see that the values are always well above equality to the reliability of the three items (the blue line) and we can see that the relationship isn’t a simple proportion and that the values are always lower than 7/3 times the reliability from the positively cued items.\nYou can use getRelBySpearmanBrown from the CECPfuns package\nAs announced here, there is a developing CECPfuns package (https://cecpfuns.psyctc.org/) which contains the function getRelBySpearmanBrown and which was used in the code above. There is also the function SpearmanBrown in the psychometric package by Thomas D. Fletcher which does the same thing (and gives the same results!)\n\n\nCooper, P. J., M. J. Taylor, Z. Cooper, and C. G. Fairburn. 1986. “The Development and Validation of the Body Shape Questionnaire.” International Journal of Eating Disorders 6: 485–94.\n\n\nEvans, Chris, and Bridget Dolan. 1993. “Body Shape Questionnaire: Derivation of Shortened \"Alternate Forms\".” International Journal of Eating Disorders 13: 315–21.\n\n\nFeldt, Leonard S., David J. Woodruff, and Fathi A. Salih. 1987. “Statistical Inference for Coefficient Alpha.” Applied Psychological Measurement 11: 93–103.\n\n\n\n\n\n",
    "preview": "posts/2021-04-09-spearman-brown-formula/spearman-brown-formula_files/figure-html5/graphic1-1.png",
    "last_modified": "2023-08-25T13:39:35+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-26-compiling-r-on-a-raspberry-pi-4/",
    "title": "Compiling R on a Raspberry Pi 4",
    "description": "I thought I should document this process as it turned out to be fairly easy",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-03-26",
    "categories": [
      "Raspberry pi",
      "Geeky stuff"
    ],
    "contents": "\n\nContents\nGetting started with the machine\nCompiling the latest R from source\nAcknowledgement\n\n\n\nShow code\n\nlibrary(ggplot2)\nlibrary(tidyverse)\nas_tibble(list(x = 1,\n               y = 1)) -> tibDat\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_text(label = \"R 4.0.4 on Pi 4!\",\n            size = 20,\n            colour = \"red\",\n            angle = 30) +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank()) \n\n\n\n[update tweak 15.iv.21 to add “, build_manual = TRUE” to install_github call]\nI have recently spent a (very small) amount of money to have a Raspberry Pi 4 to play with to see if I can run the open source shiny server off it. I am using the lovely service my ISP, Mythic Beasts provide, see https://www.mythic-beasts.com/order/rpi. So this has got me a Pi 4 with 4Gb of RAM and a choice of three operating systems: Ubuntu, Raspbian and, my current choice “Raspberry Pi OS 64 bit”, Debian GNU/Linux 10 (buster) according to lsb_release -a. The nice way that Mythic Beasts do this uses NFS file storage rather than an SD card for the main storage and I have paid for 10Gb at this point. That may matter if someone is trying to follow this but using less storage.\nI am putting this up here in the hope it will help others. The combination of R and the Raspberry Pi, particularly the newer, really impressively powerful iterations of the Pi, strike me as an extremely low cost way to get yourself formidable number crunching power. However, my experience so far is that this is not a well documented path to take and that there can be real messes for you as things are different on ARM hardware from the commoner AMD or Intel processors and as, as always in the open source world, things change and documentation tends to lag behind the changes so that old documentation can create real problems. Like pretty much everyone else in the open source world, I’m not paid to do this so my page here will go out of date too. I will try to update it and please contact me if you find what I have put here doesn’t work for you and I’ll try to update this to reflect whatever has caused the issue.\nGetting started with the machine\nOK, so I started with a raw machine, logged in and ran:\napt-get update\napt-get upgrade\nto get things up to date. Then I ran:\napt-get install apt-file \n# helps finding packages for missing resources\napt-file update \n# initialises and in future will update the cache that apt-file uses\nThat was because\napt-file search missingThing\ncan be a very good way to find the particular package you need to install to find the missingThing you need!\nNext came:\napt-get install emacs #because I prefer it to vi[m]\nI think that got me python 2.7 as a byproduct.\nAnd then:\napt-get install curl\napt-get install wget\nas they are two ways of yanking things down from the internet and I don’t think they’re installed by default.\nThen I did this:\napt-get install r-base\nas I was told that would get some other Debian packages that I would need for R. I suspect that’s true and it was pretty fast, got me R version 3.5.2 and\nhaving that doesn’t seem to have interfered with the next stages.\nCompiling the latest R from source\nThe first thing is to get the latest source from CRAN. You can see the URL here and you should be tweaking these version numbers unless you are copying this in the next few days.\n[Update 13.iv.21 for R 4.0.5 on 32-bit Raspbian: obviously you change “4.0.4” below to “4.0.5”]\nwget https://cran.r-project.org/src/base/R-4/R-4.0.4.tar.gz\ngunzip R-4.0.4.tar.gz\ntar -xvf R-4.0.4.tar\nSo that’s yanked down the gzipped, tar packed, sources and then unzipped and unpacked them into a directory that, for this version, called R-4.0.4. Surprise, surprise!\nNow the key thing is the compiling. That means this but don’t do it yet …\ncd R-4.0.4\n./configure\nThat runs a stunning configuring script that checks out whether you have everything needed for the compilation. I had to keep running this until it stopped terminating with requests for resources. For example, the first error message for me was\nX11 headers/libs are not available\nwhich was satisfied by me doing apt-get install libxt-dev.\nWhen you have sorted all the missing resources that cause full errors there are still warnings. Again, my first was:\nconfigure: WARNING: you cannot build info or HTML versions of the R manuals.\nFinally, when you have got rid of all the warnings by adding things you are left with capabilities that are omitted. I had:\nCapabilities skipped:        TIFF, Cairo\nIt’s tedious and time wasting to keep going through these cycles of ./configure and correcting so to save yourself time I think you can safely do this lot before your first ./configure and then that run should work. Here are the things I pulled in.\napt-get install libxt-dev # supports x11 screen handling\napt-get install libpcre2-dev # gets the PCRE libraries used by grep and its relatives\napt-get install libcurl4-openssl-dev # adds SSL/TLS encrypted downloading\napt-get install libtiff-dev # for tiff graphic output\napt-get install libgtk-3-dev # may not have been necessary\napt-get install libghc-cairo-dev # for Cairo system for graphic output\napt-get install texinfo texlive texlive-fonts-extra # for creating of help/man pages\n### that pulled a huge amount but allows you got get TIFF and Cairo output, then\nfmtutil-sys --missing \n### rebuilds format files for new fonts (I think)\n[Update 13.iv.21 for R 4.0.5 on 32-bit Raspbian]\nInterestingly I had to add:\napt-get install libbz2-dev libreadline-dev\nOn Raspbian 32-bit, a.k.a. (also known as, healthcare slang?) Linux raspberrypi 5.10.17-v7l+ #1403 SMP Mon Feb 22 11:33:35 GMT 2021 armv7l GNU/Linux\nAt that point, i.e. after ./configure ran fine, I could finally go for\nmake -j4\nApparently the “-j4” allows the make process to use four processes which speeds things up. The compilation took less than 30 minutes on my machine.\nOne message I noticed as the compilation proceeded was a familiar one:\nmake[1]: Entering directory '/home/chris/R-4.0.4/doc'\nconfiguring Java ...\n\n*** Cannot find any Java interpreter\n*** Please make sure 'java' is on your PATH or set JAVA_HOME correspondingly\nI’ll come back to that.\nFinally we get to:\nmake install \nputs R into /usr/local/lib. To my surprise I had to copy the ./bin/R executable from the temporary directory to /usr/bin/R:\ncp ./bin/R /usr/bin/R\n\nand then I was away! R 4.0.[4|5] up and running in what I think was less than an hour.\n\nupdate.packages(ask = FALSE, checkBuilt = TRUE)\n\ngot the base and recommended packages installed by default updated. That through up one error:\nERROR: dependency ‘openssl’ is not available \nSo I added these from the OS prompt:\napt-get install libssl-dev\napt-get install libxml2-dev\napt-get install libgit2-dev     \nI use components of the tidyverse a lot so the next step was to go back into R and run the obvious\n> install.packages(\"tidyverse\") \nwhich pulls in the key tidyverse packages was vital for me. That took quite a while to get all the components compiled in. Then I could add my own little package:\nremotes::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE)\n### or \nremotes::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE)\n### to get the PDF manual as well\nThat pulled in some more other packages but all compiled without issues.\nFinally, I could come back to the Java issue. Back out of R and to the OS prompt. This seemed to get me the Java I wanted.\napt-get install default-jdk\nand then I could do\nR CMD javareconf\nwhich found all it wanted and so I could install the rJava package in R and check that it works: it does.\nThat’s it! R 4.0.[4|5] installed on a Raspberry Pi 4 and I’m now much more confident that I compile subsequent releases on the machine too.\nAcknowledgement\nI am very grateful for encouragement and tips from Denis Brion. I think some of his work with R on Raspberry Pi machines can be seen at https://qengineering.eu/deep-learning-examples-on-raspberry-32-64-os.html.\n\n\n\n",
    "preview": "posts/2021-03-26-compiling-r-on-a-raspberry-pi-4/compiling-r-on-a-raspberry-pi-4_files/figure-html5/createGraphic-1.png",
    "last_modified": "2023-08-25T13:39:17+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-28-where-to-store-different-settings-in-rmarkdown-files/",
    "title": "Where to store different settings in Rmarkdown files",
    "description": "This may be of use to others but it's partly for me as I keep forgetting these and searching around for the .Rmd files in which I used the one I want!",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-28",
    "categories": [
      "R tricks",
      "Rmarkdown"
    ],
    "contents": "\n\nContents\nSetttings in the yaml header\nyaml heading settings for distill\n… in index.Rmd\n… in “posts”\n… in articles/pages\n\n\nSettings in css block\nSettings in early/first R code block\nSetting ggplot defaults\n\nUpdated with improved information about ggplot defaults 25.x.21\nRmarkdown is brilliant as a framework in which to create reports using R and it’s often useful to reset various defaults at the start of a file. Increasingly I work from Rmarkdown to html so some of this only applies there. I find there are three places I set things:\nin the yaml header\nin a css block or separate file (only for html output)\nin the first or an early R code block\nsetting defaults for ggplot (usually in that same early block)\nSetttings in the yaml header\nThis is well documented in many places and https://bookdown.org/yihui/rmarkdown/html-document.html is probably the canonical reference but searching will provide much other advice. I often use:\n---\ntitle: \"A title here\"\nauthor: \"Xxxx Yxxx\"\ndate: \"03/01/2021\"\n\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    toc_depth: 4\n    fig_height: 8\n    fig_width: 11    \n# bibliography: references.bib\n---\n\nI think the main things to say about that is that I don’t find that the floating table of contents (toc_float: true) always works, with long documents and complex blocks with graphics and text I find it sometimes mangles the toc so I am using it less than I used to. This can be a useful place to set the figure heading if they might be the same for all your code blocks with graphic output. I am not sure how many other code block header settings you could set here. I must experiment more: could save me a lot of typing. The only other thing there is the bibliography line, commented out. I still haven’t got into regular use of the citation and referencing capacities built into Rmarkdown. Must try harder!\nHere is another\n---\ntitle: \"ICCs from multilevel models analysed with lme4 or nlmer\"\nauthor: \"CE\"\ndate: \"26/02/2021\"\noutput:\n  html_document:\n    # css: \"test.css\"\n    toc: TRUE\n    toc_float: FALSE\n    dpi: 200\n    out.width: \"100%\"\n    fig.height: 40\n---\nThat shows that you can call an external css file (see next section), so far I haven’t found that I have enough css to make that worth doing. More important here, and I’m still working on this, I have found that you can use out.width: \"100%\" to make the html avail itself of more of your screen width which I find useful. The dpi: 200 and huge fig.height: 40 settings were me trying to optimise my graphic output for some complex plots.\nyaml heading settings for distill\n… in index.Rmd\nThis is all I have in my index.Rmd file. As yet I haven’t found any other options that can usefully be added here.\n---\ntitle: \"An R SAFAQ by Chris Evans\"\nsite: distill::distill_website\nlisting: posts\n---\n… in “posts”\nThis where most of the Distill extensions to routine Rmarkdown yaml header blocks go. Here’s an example.\n---\ntitle: \"Making the CECPfuns package: my own usable package\"\ndescription: |\n  This is very much work in progress so look for later posts about CECPfuns as well as this.\nbase_url: https://www.xxxx.org/psyctc/Rblog/  \npreview: https://www.xxxx.org/pelerinage2016/wp-content/uploads/2020/07/P1160474.jpg\nauthor:\n  - name: Xxxx Yyyy\n    url: https://www.xxxx.org/R_blog/\n    affiliation: xxxx.org\n    affiliation_url: https://www.xxxx.org/psyctc/\n    orcid_id: xxxx-xxxx-xxxx-xxxx\n\ndate: 2021-02-10\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 4\n    hightlight_downlit: true\n    self_contained: false\n    code_folding: true\ncreative_commons: CC BY-SA\n---\nI think that’s mostly self-explanatory and I hope I’ve messed up my own data with sufficient “xxxx” insertions that it’s safe for people to copy and paste to create their own extension on the basic yaml that distill:create_post(\"post title\") creates. The code_folding option means that blocks of code are “folded” away by default but have a “Show code” button so the reader can unfold the code and read it.\n… in articles/pages\nHere is one of my yaml headers:\n---\ntitle: \"Welcome to these pages\"\ndescription: |\n  Welcome to these pages which I hope will be useful to people using R to analyse data.\nbase_url: https://www.xxx.org/psyctc/Rblog/  \nauthor:\n  - name: Xxxx Yyyy\n    url: https://www.xxx.org/R_blog/\n    affiliation: Xxxx.org\n    affiliation_url: https://www.xxx.org/psyctc/\n    orcid_id: xxxx-xxxx-xxxx-xxxx\ndate: \"2023-08-25\"\noutput: \n  distill::distill_article:\n    self_contained: false\ncreative_commons: CC BY-SA    \n---\nI think that’s all pretty self-explanatory. I am sure you can see what to if copying and pasting this!\nSettings in css block\nCSS is definitely not my expert area but I have been using a block like this:\nclick to show css_chunk.txt\n(Apologies for this way of putting the code in here: I gave up on trying to work out how to escape characters or otherwise override things being mangled in knitting that!)\nThat is using the principles behind css (cascading style sheet) to set some html defaults. The first two stanzas allow raw R text output (which comes out in the html “pre” format) to come up in a horizontally scrollable window which can be useful where you find you are spitting out wide output and the next stanza I think determines the formatting of raw code (not sure about that!).\nThe body stanza is a recent discovery of mine. The “body” section of html is everything except the header information, i.e. it’s what the human reading an html document sees. That allows my html output to use more of my nice big screen.\nSettings in early/first R code block\nWhen you create a new Rmd file in Rstudio it always has this first R code block.\nclick to show default_setup_chunk.txt\nI often expand that to something like this:\nclick to show big_setup_chunk.txt\nSetting ggplot defaults\n[updated 25.x.21]\nI have my own preferences for some of the “theme” elements in ggplot and discovered that I can set these for a whole Rmarkdown files like this:\n### set ggplot defaults\ntheme_set(theme_bw())\ntheme_update(plot.title = element_text(hjust = .5),\n             plot.subtitle = element_text(hjust = .5))\nThat theme_set() sets the default theme that will be used by ggplot() for the rest of the session, here I have set it to theme_bw() and then the theme_update() updates that. You can also make that a named object\n### save whatever the current theme settings are to an object\ntheme_get() -> CEtheme\nWhich can make it easy to reinstate it with theme_set(CEtheme). And, of course, if you wanted to, you could even save that to a tiny file:\n### set ggplot defaults to file\nsave(CEtheme, \"CEtheme\")\nSo in any other R work you can load() that file and set theme.\nload(file = \"CEtheme\")\noldTheme <- theme_set(CEtheme) # uses invisible return of the pre-existing default theme by theme_set() to save that \nDo contact me if you have advice about setting Rmarkdown options and if have corrections to the above.\n\n\n\n",
    "preview": "posts/2021-02-28-where-to-store-different-settings-in-rmarkdown-files/css.png",
    "last_modified": "2023-08-25T13:38:51+02:00",
    "input_file": {},
    "preview_width": 260,
    "preview_height": 321
  },
  {
    "path": "posts/2021-02-10-making-my-first-usable-package/",
    "title": "Making the CECPfuns package: my own usable package",
    "description": "This is very much work in progress so look for later posts about CECPfuns as well as this.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-10",
    "categories": [
      "R packages",
      "R tricks"
    ],
    "contents": "\n\nContents\nLatest update\nBackground\nWhy do it?\nWarning\n\nCreate your package\nCreate your first function\nStart to insert help/documentation contents\n\nNow check and build your package\nThat’s it! You’re done!\nUsing functions from other packages\n\nHow I am synching my package to machines other than my main machine\nThings that are still work in progress for me!\nCECPfuns is a start\n\nMont BlancLatest update\n[Started 10.ii.21, tweak 15.iv.21 to add “, build_manual = TRUE” to install_github call]\nBackground\nI have been meaning to do this for years but I have still found it one of R’s more tough learning curves even by R’s sometimes challenging standards. The tidyverse is perhaps a higher and better known climb but making this package is K2 to the tidyverse Everest: nastier, colder, more dispiriting and my attempt of the ascent a few years ago hardly got beyond base camp. This time I’m sort at a first support camp level above base camp and I’m trying to document things here.\nWhy do it?\nI would have saved many hours over the last few years had I actually got on top of this when I first started. Why:\nit hugely simplifies keeping track of functions I’ve written and making sure I find the latest version,\nusing the devtools functions and the way these have been integrated into Rstudio actually makes it easy to create new functions, document them (at least minimally) and update them\nintegrating with git it becomes easy to keep track of changes you make\nif you integrate the local git repository to GitHub you can easily share the package, even if that’s only between your own individual machines, for me that’s my main laptop, my backup laptop, my lightweight and ageing Windoze machine and my web server: it’s easy to make sure they’re all looking at the same functions in the same package.\nWarning\nThere are hugely powerful tools to help the creation of R packages and many pages and PDFs on the web to help you. However, for me finding exactly the information I need, getting its context, being sure the advice isn’t outdated and sometimes just understanding what people have written has not always been easy. That’s partly why I’ve created this.\nPlease, I will try to remember to amend any mistakes I find in here, or things I discover change or can be done more easily than whatever I say here, but anything here doesn’t work for you, please:\nlook at the “Latest update” date above;\nuse the search function (in the navigation bar above) and search for “CECPfuns” and look for more recent posts about this;\nuse an advanced search on the web to search for the particular topic looking for things since that “Latest update” date;\ncontact me to tell me, ideally tell me how to fix what didn’t work for you;\nplease realise this is not my job, this, as with everything I put on the web is offered with no warranties, I accept no liabilities, and I probably will have very little time to try to help you explore anything … if I really have time on my hands though, I will try to help. I am doing this using the the Rstudio package building tools, it’s highly unlikely that I will be any help with any other ways of building a package (there are several but I see them as mostly for real R and software experts).\nHm, that I’m writing that probably conveys that this has been a bit tricky.\nOK, not K2, actually my view in the Alps, see (www.psyctc.org/pelerinage2016/Create your package\nOK, the first bit is easy: create a new package using a new directory and the “Create R package” option; give your package a name, e.g. “SillyDemoPackage”. There is the option to include some source (i.e. R code for our purposes) files here but I would recommend starting completely cleanly and creating new source files, one per function, and copying and pasting the code you already have into the new file.\nThat will have created a subdirectory of wherever you were titled named “SillyDemoPackage” and beneath it you have three more subdirectories:\nR (where you are going to put you R source files, one per function)\nman (where you will, using devtools::document(), create Rd files that in turn create the help for the package and functions)\n.Rproj.user (project information: can ignore it)\nCreate your first function\nThat’s your next step: create a new R script file; if your function is myFunction() then save the script into the R subdirectory that creating the project will have created.\nYou now have a single source file with a single function in it. (I think you can put more than one function in a single source file but I think it would be making your life more difficult so don’t).\nPut your cursor inside the function then go to the Code menu above and select “Insert Roxygen Skeleton”. Let’s say I start with this:\nmyFunction <- function(x){\n  return(paste(\"This is a silly function of\", x))\n}\nStart to insert help/documentation contents\nUsing Code, Insert Roxygen Skeleton changes that to this\n#' Title\n#'\n#' @param x \n#'\n#' @return\n#' @export\n#'\n#' @examples\nmyFunction <- function(x){\n  return(paste(\"This is a silly function of\", x))\n}\nAnd you now change that to this:\n#' Title\n#'    This is a very silly function purely for demonstration.\n#' @param x can be any printable object\n#'\n#' @return a string that pastes a silly comment before x\n#' @export\n#'\n#' @examples\n#' x <- \"stunning silliness\"\n#' myFunction(x)\n\nmyFunction <- function(x){\n  return(paste(\"This is a silly function of\", x))\n}\nYou see I have given a short description of the function, I have clarified the one parameter (argument) to the function and what the function returns and I have given a suitably inane example of how it might be used.\nNow put your cursor in the function and type devtools::document(). That will (essentially invisibly) create a new file myFunction.Rd in the /man subdirectory I mentioned above. **Remember to rerun devtools::document() within the function every time you tweak the documentation in those header lines and every time you tweak the function otherwise the help will lag behind what you’ve done (which might or might not be caught at the next stage, but better safe than sorry.)\nNow check and build your package\nNow the exciting bit: under the Build menu, pick “Check” and sit back and watch Rstudio and devtools (and perhaps other things for all I know) whiz through many checks on your package (in the top right hand pane of Rstudio in the usual layout, in the Build tab. I don’t think you can miss it. Those checks can flag up erorrs, warnings and notes and you hope to see an all green summary line at the end saying there were none of any of those. If the checks find issues some messages are very clear and helpful, some are more challenging but I have found that searching on the web usually translates them for me.\nI would then make sure you set up version control on the project using git and I would also recommend then pushing the package to GitHub if you want others to be able to find it easily.\nThat’s it! You’re done!\nOK, I lie. That’s it for the SillyDemoPackage and it’s one function, myFunction(). I think that’s a like a short afternoon stroll out of Kathmandu in summer. When you start to do something remotely useful the gradient goes up a bit and the air gets a little thinner.\nUsing functions from other packages\nThis is a huge issue but actually fairly easy to handle. Most useful functions will call on functions from packages outside of the base functions. Where you do this you need to handle declaring these in a way that means that the package will know what comes from where. There are simple and more sophisticated issues here and the Build, Clean error messages are pretty clear and helpful and there are good guides to the subtleties on the web. So far I have stayed with making the function calls explicit so instead of cor(x, y) I write stats::cor(x, y) in the function and then I add:\nSuggests:\n  stats\nat the bottom of the DESCRIPTION file in the root directory of the package and\nimportFrom(\"stats\", \"cor\")\nat the bottom of the NAMESPACE file, also in the root directory of the package. I think usethis::use_package() helps with this but I have done it manually so far.\nThe other thing you have to do at the head of any such function instead of having a\nlibrary(sausages) # I wouldn't have had this for stats as, of course,\n### the stats package is launched by default when R starts, \n### imagine I am calling sausages::recipe() \n### NO! I made that up!\nyou use:\ninvisible(stopifnot(requireNamespace(\"sausages\")))\n### so a call that doesn't spit out a message but will stop things \n### if you don't have the sausages package on your system\n### requireNamespace() only checks if you have the package\n### it doesn't load the entire package as library() or \n### require() would so if you are only going to call one\n### or a few functions explicitly with sausages::functionName()\n### this is more efficient\nThat’s the lightest way to do things. If you are going to use lots of functions from a package you may be better with other options but this works for me for now.\nHow I am synching my package to machines other than my main machine\nAdded 28.ii.21: dept to Winston Change!\nIf you’re using M$ Windoze I think it’s best to ignore this section. Because Windoze won’t let anything alter a dll on disc that has been loaded into memory, with the really rather complicated way that R (and Rstudio too) pull things into memory as they launch and run .Rprofile this tends to lead to some package upgrading being blocked, e.g. of cachem which Winston maintains.\nI am developing my package on my main Linux laptop. As I can’t really survive without it, I have a near duplicate backup machine and a little, old Windows laptop and Windows in a VM on the main machine and I have R on my web server (serving up this blog, my CORE work https://www.coresystemtrust.org.uk/; my non-CORE work site https://www.psyctc.org/psyctc/; and my personal web site: https://www.psyctc.org/pelerinage2016/. Do go and have a look!)\nI wanted to make sure that every time I (or cron) launched R on any of the those machines it would automatically check for an update to the package on GitHub and install it if there were one. That meant putting a call to install it with devtools::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE) into .Rprofile.\nAdded evening 18.ii.21 with input from Clara\n### Locating your .Rprofile file\nYou should find, or create that in locations that are operating system dependent:\n* on linux machines it is /home/username/.Rprofile\n* on Windows machines it is C:/Users/username/Documents/.Rprofile\n* on Macs I am told it is /Users/username/.Rprofile and I am also told that as it is a hidden file, you will need cmd + shift + [.] in order to show the hidden files.\nAdded evening 10.ii.21, with help from Bill Dunlap via the R-help Email list\nHowever, my original addition to .Rprofile cause R to keep looping when launched. Bill Dunlap confirmed that’s because something, probably invoked by the devtools::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE) call, is restarting the R session and so rerunning the .Rprofile, and so on ad infinitum and Bill gave me the answer so my .Rprofile is now:\nif (Sys.getenv(\"INSTALLING_FROM_GITHUB\", unset = \"no\") == \"no\") {\n  Sys.setenv(INSTALLING_FROM_GITHUB = \"yes\")\n  devtools::install_github(\"cpsyctc/CECPfuns\", build_vignettes = TRUE, build_manual = TRUE)\n}\nAs I understand that code, it checks for an environment variable (i.e. a variable set in the operating system) called “INSTALLING_FROM_GITHUB” and if it finds its value is “no” it runs the the commands inside the brackets, resetting the variable to “yes” and then, the next line, checking if there has been an update of the package on GitHub and installing it if there has been. However, if/when .Rprofile is rerun in that R session the environment variable now has the value “yes” so the looping is prevented. Lovely!\nThings that are still work in progress for me!\nI am slowly learning about all the extras that transform the basic documentation, such as I created above, into really good help for a function.\nI haven’t worked out how to document a whole package yet. The function devtools::build_manual() seems to build at least the typical nice PDF about a package that you see on CRAN, e.g. https://cran.r-project.org/web/packages/boot/boot.pdf but it puts it in the directory above the package directory and the file doesn’t seem to get integrated into the package which seems puzzling and less than entirely helpful to me. I’m sure there must be an answer to that but I haven’t found it yet.\nI haven’t worked out how to create index files like https://cran.r-project.org/web/packages/boot/index.html though that may be because my package is so small that it doesn’t have most of the information that is in there. I can’t really believe that’s the whole reason though.\nCECPfuns is a start\nThis is pretty embarrassing but I will share that this first actual package of mine, probably the only one I’ll ever need to create, is available if you want to see what I’ve managed to create. It will develop into a package mainly of functions I and Clara Paz have found useful (with, I hope, comments and suggestions from Emily) It’s at https://github.com/cpsyctc/CECPfuns and there is a web site for the package at https://cecpfuns.psyctc.org/. You can use git on pretty much any operating system to pull a copy from github if you want to look at the all the raw constituent parts and I think if you do pull that you can see the commit history, i.e. of the changes and updating. (A graph of the commits against date is at https://github.com/cpsyctc/CECPfuns/graphs/commit-activity). I am not opening it to submissions as it’s too early in my learning, I may never reach that place, so, if you have suggestions or corrections and any comments really,contact me through my work site. I hope this helps someone and encourages them to create their own package. I do wish I’d done it earlier!\nMont Blanc from my Alpine balcony\n\n\n",
    "preview": "https://www.psyctc.org/pelerinage2016/wp-content/uploads/2020/07/P1160474.jpg",
    "last_modified": "2023-08-25T13:38:06+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-10-more-piping-introducing-rowwise/",
    "title": "More piping, and rowwise()",
    "description": "This extends https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/ and introduces rowwise()",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-10",
    "categories": [
      "R packages",
      "tidyverse",
      "piping",
      "R tricks"
    ],
    "contents": "\nThis is a slight adaptation of a file I did for Emily (https://www.researchgate.net/profile/Emily_Blackshaw2) back in October 2020 when she and wanted to look at whether Cronbach’s alpha for the YP-CORE varied from session to session across help-seeking clients data: a very basic exploration of longitudinal measurement invariance. I realised it was a good chance for me to pull together what I had been learning back then about piping and to share it with her.\nAs a page here it probably should have come before https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, or been woven into that, but I had managed to lose the file (!). However, I think it complements what I put in there and it does introduce the rowwise() function and c_across().\nAs is my wont, I prefer to explore methods with simulated data so the first step was to make such data. Here I am simulating 500 clients each having ten sessions and just a five item questionnaire (the YP-CORE has ten items but five is quicker and fits output more easily!)\n\n\nShow code\n\n### make some nonsense data \nlibrary(tidyverse)\nnParticipants <- 500\nnSessions <- 10\n### give myself something to start with: the sessions\nsession <- rep(1:nSessions, nParticipants) # 1,2,3 ...10, 1,23 ...10 ...\nsession %>%\n  as_tibble() %>%  # turn from vector to tibble, that means I have rename it back to the vector name!\n  rename(session = value) %>%\n  mutate(baseVar = rnorm(nParticipants*nSessions),  # this creates a new variable in the tibble and sort of reminds me that variables may be vectors\n         item1 = baseVar + 0.7*rnorm(nParticipants*nSessions), # creates a first item\n         item2 = baseVar + 0.7*rnorm(nParticipants*nSessions), # and a second\n         item3 = baseVar + 0.7*rnorm(nParticipants*nSessions), # and a third\n         item4 = baseVar + 0.7*rnorm(nParticipants*nSessions), # and a 4th ...\n         item5 = baseVar + 0.7*rnorm(nParticipants*nSessions)) -> tmpDat\n\n### look at it\ntmpDat\n\n# A tibble: 5,000 × 7\n   session baseVar   item1   item2  item3  item4   item5\n     <int>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n 1       1 -0.0834 -0.0989 -0.544   1.79   0.328  0.0842\n 2       2  2.55    1.75    1.33    2.30   1.27   1.69  \n 3       3 -1.77   -1.47   -1.81   -1.59  -2.37  -1.97  \n 4       4 -0.324  -0.533  -0.0590  0.633 -0.867 -0.453 \n 5       5 -0.413  -0.904  -0.987  -1.56  -0.202  0.756 \n 6       6  1.06    1.37    1.42    0.882 -0.457  1.34  \n 7       7  0.0410 -1.35    0.788   0.624  0.437 -0.0140\n 8       8  0.770  -0.252   0.753   0.865  1.82   0.364 \n 9       9  0.0827  0.285   0.269  -1.33   0.815  0.0147\n10      10 -1.07   -1.47   -0.803  -0.693 -2.50  -1.86  \n# ℹ 4,990 more rows\n\nShow code\n\n### check the simple correlation\ncor(tmpDat[, 3:7])\n\n          item1     item2     item3     item4     item5\nitem1 1.0000000 0.6685281 0.6638093 0.6743247 0.6602781\nitem2 0.6685281 1.0000000 0.6684304 0.6797483 0.6696608\nitem3 0.6638093 0.6684304 1.0000000 0.6730563 0.6645038\nitem4 0.6743247 0.6797483 0.6730563 1.0000000 0.6738977\nitem5 0.6602781 0.6696608 0.6645038 0.6738977 1.0000000\n\nShow code\n\n### OK, I can play with that, here's the overall alpha (meaningless even for the simulation really but just checking)\npsychometric::alpha(tmpDat[, 3:7])\n\n[1] 0.9101857\n\nOK. Now I could start playing with the data in the tidyverse/dplyr/piping way. The key thing to remember is that the default behaviour of mutate() or summarise() within group_by() in dplyr is for a function to act on a vertical vector, i.e. on a variable\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(mean1 = mean(item1))\n\n# A tibble: 10 × 2\n   session    mean1\n     <int>    <dbl>\n 1       1  0.00630\n 2       2  0.00253\n 3       3  0.0293 \n 4       4  0.0727 \n 5       5 -0.0505 \n 6       6 -0.0188 \n 7       7 -0.0348 \n 8       8 -0.0437 \n 9       9 -0.0403 \n10      10  0.0420 \n\nSo that simply got us the mean for item1 across all completions but broken down by session. Trivial dplyr/piping but I still find it satisfying in syntax and in its utility.\nAs introduced in https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, if I have a function that returns more than one value dplyr\nhandles this nicely but I have to tell it the function is creating a list (even if it’s just a vector), as below. The catch to remember is that you then have to unnest() the list to see its values, usually unnest_wider() is what I want but there is unnest_longer().\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(summary1 = list(summary(item1))) %>%\n  unnest_wider(summary1)\n\n# A tibble: 10 × 7\n   session Min.        `1st Qu.`   Median       Mean   `3rd Qu.` Max. \n     <int> <table[1d]> <table[1d]> <table[1d]>  <tabl> <table[1> <tab>\n 1       1 -2.885444   -0.8542505   0.008453520  0.00… 0.7741324 4.33…\n 2       2 -3.714294   -0.8196168  -0.013540736  0.00… 0.7568785 4.40…\n 3       3 -3.397608   -0.7093374   0.048696677  0.02… 0.7308804 3.65…\n 4       4 -3.282683   -0.7507610   0.111586870  0.07… 0.7971904 3.54…\n 5       5 -5.841905   -0.8259870  -0.001774436 -0.05… 0.8063849 3.16…\n 6       6 -3.885132   -0.8912593  -0.001092184 -0.01… 0.7633556 3.57…\n 7       7 -4.745932   -0.8515208   0.005372354 -0.03… 0.8805254 4.01…\n 8       8 -2.739551   -0.8400586  -0.082297507 -0.04… 0.7367041 3.84…\n 9       9 -3.423777   -0.7875004  -0.042422361 -0.04… 0.7737911 3.34…\n10      10 -3.231467   -0.7180346   0.070618081  0.04… 0.8567534 3.19…\n\nShow code\n\n###  names are messy but it is easy to solve that ...\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(summary1 = list(summary(item1))) %>%\n  unnest_wider(summary1) %>%\n  ###  sometimes you have to clean up names that start \n  ###  with numbers or include spaces if you want to avoid backtick quoting\n  rename(Q1 = `1st Qu.`,\n         Q3 = `3rd Qu.`)\n\n# A tibble: 10 × 7\n   session Min.        Q1          Median       Mean       Q3    Max. \n     <int> <table[1d]> <table[1d]> <table[1d]>  <table[1d> <tab> <tab>\n 1       1 -2.885444   -0.8542505   0.008453520  0.006301… 0.77… 4.33…\n 2       2 -3.714294   -0.8196168  -0.013540736  0.002530… 0.75… 4.40…\n 3       3 -3.397608   -0.7093374   0.048696677  0.029293… 0.73… 3.65…\n 4       4 -3.282683   -0.7507610   0.111586870  0.072742… 0.79… 3.54…\n 5       5 -5.841905   -0.8259870  -0.001774436 -0.050492… 0.80… 3.16…\n 6       6 -3.885132   -0.8912593  -0.001092184 -0.018803… 0.76… 3.57…\n 7       7 -4.745932   -0.8515208   0.005372354 -0.034829… 0.88… 4.01…\n 8       8 -2.739551   -0.8400586  -0.082297507 -0.043682… 0.73… 3.84…\n 9       9 -3.423777   -0.7875004  -0.042422361 -0.040280… 0.77… 3.34…\n10      10 -3.231467   -0.7180346   0.070618081  0.041990… 0.85… 3.19…\n\nAgain, as I introduced in https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, I can extend this to handle more than one vector/variable at a time if they’re similar and I’m doing the same to each.\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(across(starts_with(\"item\"), ~mean(.x)))\n\n# A tibble: 10 × 6\n   session    item1   item2   item3     item4     item5\n     <int>    <dbl>   <dbl>   <dbl>     <dbl>     <dbl>\n 1       1  0.00630 -0.0293 -0.0539 -0.0636   -0.0104  \n 2       2  0.00253  0.0218  0.0177 -0.00370   0.0116  \n 3       3  0.0293   0.0246 -0.0194 -0.0278    0.00223 \n 4       4  0.0727   0.0776  0.0686  0.0163    0.0642  \n 5       5 -0.0505  -0.0927 -0.0322 -0.0703   -0.0350  \n 6       6 -0.0188  -0.0989 -0.0497 -0.0347   -0.0573  \n 7       7 -0.0348   0.0310  0.0215  0.0750   -0.000846\n 8       8 -0.0437  -0.0405 -0.0383 -0.0482    0.00199 \n 9       9 -0.0403  -0.0204  0.0198  0.000882 -0.0117  \n10      10  0.0420   0.120   0.0578  0.0288    0.0585  \n\nI can also do that with the following syntax. I have not yet really understood why the help for across() gives that one with function syntax (“~”) and the explicit call of “.x) rather than this and I really ought to get my head around the pros and cons of each.\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(across(starts_with(\"item\"), mean))\n\n# A tibble: 10 × 6\n   session    item1   item2   item3     item4     item5\n     <int>    <dbl>   <dbl>   <dbl>     <dbl>     <dbl>\n 1       1  0.00630 -0.0293 -0.0539 -0.0636   -0.0104  \n 2       2  0.00253  0.0218  0.0177 -0.00370   0.0116  \n 3       3  0.0293   0.0246 -0.0194 -0.0278    0.00223 \n 4       4  0.0727   0.0776  0.0686  0.0163    0.0642  \n 5       5 -0.0505  -0.0927 -0.0322 -0.0703   -0.0350  \n 6       6 -0.0188  -0.0989 -0.0497 -0.0347   -0.0573  \n 7       7 -0.0348   0.0310  0.0215  0.0750   -0.000846\n 8       8 -0.0437  -0.0405 -0.0383 -0.0482    0.00199 \n 9       9 -0.0403  -0.0204  0.0198  0.000882 -0.0117  \n10      10  0.0420   0.120   0.0578  0.0288    0.0585  \n\nAgain, as I introduced in https://www.psyctc.org/Rblog/posts/2021-02-07-why-pipe-why-the-tidyverse/, I can do multiple functions of the same items\n\n\nShow code\n\ntmpDat %>% \n  group_by(session) %>%\n  summarise(across(starts_with(\"item\"), list(mean = mean, sd = sd)))\n\n# A tibble: 10 × 11\n   session item1_mean item1_sd item2_mean item2_sd item3_mean item3_sd\n     <int>      <dbl>    <dbl>      <dbl>    <dbl>      <dbl>    <dbl>\n 1       1    0.00630     1.18    -0.0293     1.21    -0.0539     1.16\n 2       2    0.00253     1.21     0.0218     1.16     0.0177     1.23\n 3       3    0.0293      1.16     0.0246     1.20    -0.0194     1.18\n 4       4    0.0727      1.20     0.0776     1.21     0.0686     1.22\n 5       5   -0.0505      1.22    -0.0927     1.17    -0.0322     1.17\n 6       6   -0.0188      1.28    -0.0989     1.26    -0.0497     1.19\n 7       7   -0.0348      1.26     0.0310     1.24     0.0215     1.24\n 8       8   -0.0437      1.18    -0.0405     1.18    -0.0383     1.17\n 9       9   -0.0403      1.17    -0.0204     1.16     0.0198     1.22\n10      10    0.0420      1.20     0.120      1.29     0.0578     1.26\n# ℹ 4 more variables: item4_mean <dbl>, item4_sd <dbl>,\n#   item5_mean <dbl>, item5_sd <dbl>\n\nI like that that names things sensibly\nI said the default behaviour of mutate() and summarise() is to work on variables, i.e. vectors, whether that is to work on all the values of the variable if there is no group_by(), or within the groups if there is a grouping. If I want to do something on individual values, i.e. by rows, “rowwise”, then I have to use rowwise() which basically treats each row as a group.\nIf, as you often will in that situation, you want to use a function of more than one value, i.e. values from more than one variable, then you have to remember to use c_across() now, not across(): “c_” as it’s by column.\nYou also have to remember to ungroup() after any mutate() as you probably don’t want future functions to handle things one row at a time.\n\n\nShow code\n\ntmpDat %>% \n  filter(row_number() < 6) %>% # just for this example\n  rowwise() %>%\n  mutate(mean = mean(c_across(starts_with(\"item\")))) %>%\n  ungroup() # see above about ungrouping after rowwise() and mutate()\n\n# A tibble: 5 × 8\n  session baseVar   item1   item2  item3  item4   item5   mean\n    <int>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>  <dbl>\n1       1 -0.0834 -0.0989 -0.544   1.79   0.328  0.0842  0.311\n2       2  2.55    1.75    1.33    2.30   1.27   1.69    1.67 \n3       3 -1.77   -1.47   -1.81   -1.59  -2.37  -1.97   -1.84 \n4       4 -0.324  -0.533  -0.0590  0.633 -0.867 -0.453  -0.256\n5       5 -0.413  -0.904  -0.987  -1.56  -0.202  0.756  -0.579\n\nOK, so that’s recapped these things, now what about if I want to look at multiple columns and multiple rows?\nthe trick seems to be cur_data().\nThat gives me a sensible digression from Cronbach’s alpha here as I often find I’m wanting to get correlation matrices when I’m wanting to get alpha (and its CI)\nand I think getting correlation matrices from grouped data ought to be much easier than it is!\n\n\nShow code\n\ntmpDat %>% \n  select(item1:item5) %>%\n  summarise(cor = list(cor(cur_data()))) %>%\n  unnest_wider(cor) \n\n# A tibble: 1 × 5\n  item1[,\"item1\"] item2[,\"item1\"] item3[,\"item1\"] item4[,\"item1\"]\n            <dbl>           <dbl>           <dbl>           <dbl>\n1               1           0.669           0.664           0.674\n# ℹ 5 more variables: item1[2:5] <dbl>, item2[2:5] <dbl>,\n#   item3[2:5] <dbl>, item4[2:5] <dbl>, item5 <dbl[,5]>\n\nThat, as you can see, is a right old mess!\nbut we can use correlate() from the corrr package:\n\n\nShow code\n\ntmpDat %>% \n  select(item1:item5) %>%\n  corrr::correlate()\n\n# A tibble: 5 × 6\n  term   item1  item2  item3  item4  item5\n  <chr>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 item1 NA      0.669  0.664  0.674  0.660\n2 item2  0.669 NA      0.668  0.680  0.670\n3 item3  0.664  0.668 NA      0.673  0.665\n4 item4  0.674  0.680  0.673 NA      0.674\n5 item5  0.660  0.670  0.665  0.674 NA    \n\nAs you see, corrr::correlate() puts NA in the leading diagonal not 1.0. That does make finding the maximum off diagonal correlations easy but I confess it seems wrong to me!\nWhat about using that and group_by()?\n\n\nShow code\n\ntmpDat %>% \n  select(-baseVar) %>%\n  group_by(session) %>%\n  corrr::correlate()\n\n# A tibble: 6 × 7\n  term     session    item1    item2   item3   item4    item5\n  <chr>      <dbl>    <dbl>    <dbl>   <dbl>   <dbl>    <dbl>\n1 session NA       -0.00915  0.00828  0.0111  0.0139  0.00340\n2 item1   -0.00915 NA        0.669    0.664   0.674   0.660  \n3 item2    0.00828  0.669   NA        0.668   0.680   0.670  \n4 item3    0.0111   0.664    0.668   NA       0.673   0.665  \n5 item4    0.0139   0.674    0.680    0.673  NA       0.674  \n6 item5    0.00340  0.660    0.670    0.665   0.674  NA      \n\nHm, that completely ignores the group_by() and includes session variable. That seems plain wrong to me. I feel sure this is something the package will eventually\nchange but for now I need another way to get what I want.\n\n\nShow code\n\ntmpDat %>% \n  select(-baseVar) %>%\n  group_by(session) %>%\n  corrr::correlate(cur_data())\n\n\nI have not evaluated that as it stops with the moderately cryptic error message which I’m putting in here as I quite often forget the summarise(x = ) bit\n# Error: `cur_data()` must only be used inside dplyr verbs.\n# Run `rlang::last_error()` to see where the error occurred.\nSo let’s fix that.\n\n\nShow code\n\ntmpDat %>% \n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(cor = corrr::correlate(cur_data()))\n\n# A tibble: 50 × 2\n# Groups:   session [10]\n   session cor$term $item1 $item2 $item3 $item4 $item5\n     <int> <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1       1 item1    NA      0.702  0.632  0.654  0.634\n 2       1 item2     0.702 NA      0.658  0.704  0.672\n 3       1 item3     0.632  0.658 NA      0.655  0.647\n 4       1 item4     0.654  0.704  0.655 NA      0.684\n 5       1 item5     0.634  0.672  0.647  0.684 NA    \n 6       2 item1    NA      0.661  0.684  0.673  0.642\n 7       2 item2     0.661 NA      0.695  0.682  0.672\n 8       2 item3     0.684  0.695 NA      0.665  0.682\n 9       2 item4     0.673  0.682  0.665 NA      0.671\n10       2 item5     0.642  0.672  0.682  0.671 NA    \n# ℹ 40 more rows\n\nHm. That does get me the analyses I want but in what is, to my mind, a very odd structure.\nOK, after that digression into the corrr package, let’s get to what Emily actually wanted: Cronbach’s alpha across the items but per session.\n\n\nShow code\n\ntmpDat %>%\n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(alpha = psychometric::alpha(cur_data()))\n\n# A tibble: 10 × 2\n   session alpha\n     <int> <dbl>\n 1       1 0.908\n 2       2 0.911\n 3       3 0.908\n 4       4 0.910\n 5       5 0.904\n 6       6 0.924\n 7       7 0.915\n 8       8 0.907\n 9       9 0.899\n10      10 0.913\n\nI get my CI around alpha using the following code.\n\n\nShow code\n\npsychometric::alpha(tmpDat[, 3:7])\n\n[1] 0.9101857\n\nShow code\n\ngetAlphaForBoot <- function(dat, i) {\n  # a little function that happens to use psych::alpha to get alpha\n  # but indexes it with i as boot() will require\n  psychometric::alpha(na.omit(dat[i,]))\n}\ngetAlphaForBoot(tmpDat[, 3:7], 1:nrow(tmpDat)) # just checking that it works\n\n[1] 0.9101857\n\nShow code\n\nbootReps <- 1000\ngetCIAlphaDF3 <- function(dat, ciInt = .95, bootReps = 1000) {\n  tmpRes <- boot::boot(na.omit(dat), getAlphaForBoot, R = bootReps)\n  tmpCI <- boot::boot.ci(tmpRes, conf = ciInt, type = \"perc\")$percent[4:5]\n  return(data.frame(alpha = tmpRes$t0,\n                    LCL = tmpCI[1],\n                    UCL = tmpCI[2]))\n}\ngetCIAlphaDF3(tmpDat[, 3:7])\n\n      alpha       LCL      UCL\n1 0.9101857 0.9055701 0.914153\n\nActually, now I have my CECPfuns package I create\na better, more robust function for this, but later!\nSo that’s the overall Cronbach alpha with bootstrap confidence interval.\nCan also do that within a group_by() grouping.\n\n\nShow code\n\ntmpDat %>%\n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(alpha = list(getCIAlphaDF3(cur_data()))) %>% \n  unnest_wider(alpha)\n\n# A tibble: 10 × 4\n   session alpha   LCL   UCL\n     <int> <dbl> <dbl> <dbl>\n 1       1 0.908 0.892 0.920\n 2       2 0.911 0.896 0.923\n 3       3 0.908 0.890 0.921\n 4       4 0.910 0.897 0.923\n 5       5 0.904 0.890 0.916\n 6       6 0.924 0.912 0.934\n 7       7 0.915 0.902 0.927\n 8       8 0.907 0.891 0.920\n 9       9 0.899 0.884 0.911\n10      10 0.913 0.899 0.924\n\nAnd that was nice and easy to feed into a forest style plot, as follows.\n\n\nShow code\n\ntmpDat %>%\n  select(-baseVar) %>%\n  group_by(session) %>%\n  summarise(alpha = list(getCIAlphaDF3(cur_data()))) %>% \n  unnest_wider(alpha) -> tmpTib\n\npsychometric::alpha(tmpDat[, 3:7]) -> tmpAlphaAll\n\nggplot(data = tmpTib,\n       aes(x = session, y = alpha)) +\n  geom_point() + # get the observed alphas in as points\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) + # add the CIs as lines\n  geom_hline(yintercept = tmpAlphaAll) + # not really very meaningful to have an overall alpha but \n    # perhaps better than not having a reference line\n  xlab(\"Session\") +\n  ylab(\"Cronbach alpha\") +\n  ggtitle(\"Forest plot of observed Cronbach alpha per session\",\n          subtitle = paste0(\"Vertical lines are 95% CIs, \",\n                            bootReps,\n                            \" bootstrap replications, percentile method.\")) +\n  theme_bw() + # nice clean theme\n  theme(plot.title = element_text(hjust = .5), # centre the title\n        plot.subtitle = element_text(hjust = .5)) # and subtitle\n\n\n\nWell, as you’d expect from the simulation method, no evidence of heterogeneity of Cronbach’s alpha across sessions!\nI hope this is a useful further introduction to piping, dplyr and some of the tidyverse approach. I guess it introduced the corrr package, cur_data() and rowwise() … and it finished with a, for me, typical use of ggplot() (from the ggplot2 package.)\nDo contact me if you have any comments, suggestions, corrections, improvements … anything!\n\n\n\n",
    "preview": "posts/2021-02-10-more-piping-introducing-rowwise/more-piping-introducing-rowwise_files/figure-html5/useDat14-1.png",
    "last_modified": "2023-08-25T13:38:29+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-07-why-pipe-why-the-tidyverse/",
    "title": "Why pipe?  Why the tidyverse?",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-07",
    "categories": [
      "R packages",
      "piping",
      "tidyverse"
    ],
    "contents": "\n\nContents\nSo what is piping?\nA worked example of R piping\nSummarising\n\n\nThis was a topic suggested by Emily who is nearing the end of her PhD on the YP-CORE as you can see from her/our RG pages about the PhD here . (More about the YP-CORE here and the CORE system here.) She and I have been on a learning curve moving from base R (https://www.r-project.org/) to increasing use of the tidyverse (https://www.tidyverse.org/) developments of R.\nSo what is piping?\nIt’s this sort of thing.\ndata %>%\n  group_by(gender) %>%\n  summarise(n = n(),\n            minAge = min(age),\n            meanAge = mean(age),\n            maxAge = max(age))\nTo me the idea of piping comes out unix/linux where you can pipe the output of one command into another, for example:\nfind . -name filename.here -printf \"%T@ %Tc %p\\n\" | sort -n\nThe pipe operator in linux is the vertical line “|” and what happens is the text output from the linux find command is fed straight into the sort command to give a sorted list of files matching “filename.here”.\nI think “piping” in R is a bit different (hence some in jokes: piping was introduced into R through a package [magittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html, and also. The “magittr” name was a lovely play on the famous print by Magritte which said firmly Ceci n’est pas un pipe (this is not a pipe) and depicted … a pipe.\nComing back to that R code I showed above …\ndata %>%\n  group_by(gender) %>%\n  summarise(n = n(),\n            minAge = min(age),\n            meanAge = mean(age),\n            maxAge = max(age))\nThat assumes you have a data frame or tibble named “data” and that it has variables gender and age within it. The “%>%” “pipes” those data to the next command, group_by() which in turn pipes the data, now grouped by gender into a four line command, summarise(). That creates a new tibble with four variables each with their one value, for each value of gender.\nThis is so well described by Shannon Pileggi in her page https://www.pipinghotdata.com/posts/2018-11-05-welcome-to-piping-hot-data/ that if you use R but so far haven’t used piping, go and look at her page and then come back here.\nAt first I wasn’t convinced by piping. That was partly because I thought the documentation I found wasn’t terribly clear (it has improved greatly) and didn’t convince me that the new syntax, new way of sequencing instructions, was worth the learning curve. It was also because it was, by the time I found it, very bound in with what has become the tidyverse in R: a lot of other, sometimes really quite radical, changes from base R. To me it felt like having to learn a new language and I’m neither a good linguist nor a good programmer.\nHowever, I have become a convert. I still sometimes go back to what I think of as “classical R”, I still sometimes find there are things I can do more easily with lists and lapply() and its relatives. That’s particularly true when my data isn’t rectangular or a relational data structure of linked rectangular sets of data. If I have a data on the members of families and the data may differ between the members and the families and the relational structures are messy, I will probably use lists and lapply(). However, the vast majority of the data I work with is, or can be, “tidy”. A very common example for me is to have an ID and some demographics per participant, and then answers from each participant on items of one or more questionnaires where every participant may have more than one set of responses. Here the data is a relational rectangular structure one rectangular structure with one row per participant then one or more rows in another rectangular structure for each time they completed the questionnaires with the ID codes enabling us to link the two.\nLong, long ago, when I used SPSS, I was very uncomfortable if I had to handle single rectangular data structures and I would have handled that by having one “wide” file with one row per participant and multiple sets of responses by questionnaire going off to the right and added to for each new completion. That’s doable when you might only have two completions per participant but when you have many per partipant, and numbers perhaps varying from one completion to hundreds, then that becomes a terrible structure.\nOf course, classical R handles structures like this fine. That was one thing that helped me move from SPSS to R (a minor contributor but certainly there in the long list). However, I didn’t shake of some uncertainty with my data handling as handled data linking across data frames.\nNow I have moved to piping and the tidyverse the effect for me has been liberating. I no longer think of data as having to be in a single rectangle, it’s become entirely natural to handle it in a series of linked data sets. I use the tidyverse tibbles: a fundamental building stone in the tidyverse and in many ways a relatively minor extension of the data frame in R. One difference from a data frame is that the default print function for a tibble only prints the first ten rows where the default print function for a data frame would try to print all of it, or until you run out of console lines. At first that seemed an annoyance until I started to use that printing to build what you want iteratively.\nA worked example of R piping\nIn what follows I haven’t folded the code away partly as I wanted to give a bit of the experience of writing code iterative, starting usually with one line and building on that.\nOK, let’s simulate some data\n\n\nlibrary(tidyverse) # get the main packages of the tidyverse\n### that gives us the pipe operator \"%>% and as_tibble and \n### a few other things I use below\n### the piping tools I'm using are in the dplyr package which\n### is within the set of packages called by the tidyverse \"superpackage\"\nset.seed(12345) # get replicable data\nn <- 500 # n(participants)\n\n### create some demographics\n### start by giving each participant a gender ...\nlist(gender = sample(c(\"M\", \"F\"), n, replace = TRUE),\n     age = sample(11:18, n, replace = TRUE), # and an age\n     ### and now a number of questionnaire completions\n     nCompletions = sample(1:30, n, replace = TRUE)\n     ) %>% # and pipe that list forward to convert it to a tibble\n  as_tibble() %>%\n  ### now use a piping trick, mutate() does something to each row\n  ### here a new variable, ID, is created and given the value of \n  ### the row number of each participant, i.e. 1:n\n  mutate(ID = row_number()) -> tibParticipants # give that tibble a name\n\n\nNow if I want to understand what I’ve created I just type its name:\n\n\ntibParticipants\n\n# A tibble: 500 × 4\n   gender   age nCompletions    ID\n   <chr>  <int>        <int> <int>\n 1 F         11            8     1\n 2 M         11           30     2\n 3 F         15            8     3\n 4 F         13            2     4\n 5 F         13           12     5\n 6 F         15           20     6\n 7 F         15            6     7\n 8 M         18            4     8\n 9 M         11           30     9\n10 F         14           15    10\n# ℹ 490 more rows\n\nInstead of showing me all 500 rows, I just get the top ten (like using head(dataFrame) in classical R) but I also get pretty much everything else I need to know about the data. Had there been too many variables to fit on the screen the listing would have ended with a line giving me the names of all the variables that wouldn’t fit on the screen.\n\n\ntibParticipants %>% \n  select(ID, nCompletions) %>% # get just the ID codes\n  ### I confess that I always have to look up uncount(): \n  ### I can never remember the name, let's just stop there\n  ### and see what it did ...\n  uncount(nCompletions) \n\n# A tibble: 7,679 × 1\n      ID\n   <int>\n 1     1\n 2     1\n 3     1\n 4     1\n 5     1\n 6     1\n 7     1\n 8     1\n 9     2\n10     2\n# ℹ 7,669 more rows\n\nOK, it’s replicated each ID value by the value in the variable nCompletions. Good, that’s what I wanted. Imagine I’m doing this interactively at the console …\n\n\ntibParticipants %>% \n  select(ID, nCompletions) %>% # get just the ID codes\n  uncount(nCompletions) %>%\n  group_by(ID) %>%\n  mutate(nCompletions2 = n(), \n         ### that gets me the number of completions per ID \n         ### (which is just getting back to nCompletions in\n         ### tibParticipants)\n         completionN = row_number()) %>%\n  ### that gets an index number for each completion \n  ### of the questionnaire ...\n  ### it's a very short questionnaire and so badly \n  ### designed the item are uncorrelated answers \n  ### between 0 and 5) ...\n  mutate(item1 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item2 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item3 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item4 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item5 = sample(0:5, nCompletions2[1], replace = TRUE))\n\n# A tibble: 7,679 × 8\n# Groups:   ID [500]\n      ID nCompletions2 completionN item1 item2 item3 item4 item5\n   <int>         <int>       <int> <int> <int> <int> <int> <int>\n 1     1             8           1     5     4     1     4     0\n 2     1             8           2     5     1     2     4     2\n 3     1             8           3     5     1     0     3     5\n 4     1             8           4     0     0     0     2     0\n 5     1             8           5     2     1     1     1     2\n 6     1             8           6     5     1     4     3     3\n 7     1             8           7     0     3     3     4     4\n 8     1             8           8     2     5     4     0     1\n 9     2            30           1     4     1     5     1     2\n10     2            30           2     4     0     0     0     3\n# ℹ 7,669 more rows\n\n### little note here, that used to work with just:\n###   item1 = sample(0:5, nCompletions2, replace = TRUE)\n### instead of what I have now:\n###   item1 = sample(0:5, nCompletions2[1], replace = TRUE)\n### i.e. I didn't have to instruct sample() to just use the first value for nCompletions2\n### for each group\n### now R throws an error \"! invalid 'size' argument\" \n### it took me a good few minutes to work out that the error messages was, perhaps very reasonably\n### complaining that it didn't know how to handle a vector for its size argument\n### I assume it used to default silently just to use the first element of the vector if given a\n### vector for size\n### This little historical diversion illustrates a number of things\n###   1. R does change and even very, very old and tested base functions like sample() can become\n###      more demanding about their arguments\n###   2. Sadly, I don't think the error messages R throws are always as helpful as they might be\n###      and this can bite you if you're not a programmer or very used to terse error messages\n###   3. More constructively, this is a reminder that the default behaviour of summarise() or of\n###      mutate() is using variables as vectors, not as single values, one per row\n###      If you want variables to be handled row by row, as single values, you need to use the \n###      particular version of group_by(): rowwise() which groups by row.  I'll come to that in \n###      another post here.\n###      But really it was sloppy code writing by me not to have trusted sample() to have just\n###      used the first value if given a vector (of length > 1) for an argument expecting a single\n###      value!\n\n\nOK. Looking like what I wanted so just put it into a tibble.\n\n\ntibParticipants %>% \n  select(ID, nCompletions) %>% # get just the ID codes\n  uncount(nCompletions) %>%\n  group_by(ID) %>%\n  mutate(nCompletions2 = n(), \n         ### that gets me the number of completions per ID \n         ### (which is just getting back to nCompletions in\n         ### tibParticipants)\n         completionN = row_number()) %>%\n  ### that gets an index number for each completion \n  ### of the questionnaire ...\n  ### it's a very short questionnaire and so badly \n  ### designed the item are uncorrelated answers \n  ### between 0 and 5) ...\n  mutate(item1 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item2 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item3 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item4 = sample(0:5, nCompletions2[1], replace = TRUE),\n         item5 = sample(0:5, nCompletions2[1], replace = TRUE))%>%\n  ### this can catch you out, if you have used group_by() \n  ### before a mutate, the data stay grouped which is \n  ### probably not what you want so ungroup(), \n  ### rowwise() is just a particular group_by() but\n  ### grouping by row so that variables are treated \n  ### by each individual value, not as vectors\n  ungroup() -> tibQuaireData\n\n\nNow I want to join the demographics back into that so …\n\n\ntibQuaireData %>%\n  left_join(tibParticipants, by = \"ID\") \n\n# A tibble: 7,679 × 11\n      ID nCompletions2 completionN item1 item2 item3 item4 item5\n   <int>         <int>       <int> <int> <int> <int> <int> <int>\n 1     1             8           1     4     1     0     5     2\n 2     1             8           2     0     1     1     2     5\n 3     1             8           3     1     3     5     3     5\n 4     1             8           4     5     3     5     4     1\n 5     1             8           5     0     1     2     3     4\n 6     1             8           6     0     1     5     0     2\n 7     1             8           7     2     4     1     1     3\n 8     1             8           8     3     1     4     5     4\n 9     2            30           1     0     0     0     0     5\n10     2            30           2     2     5     1     3     0\n# ℹ 7,669 more rows\n# ℹ 3 more variables: gender <chr>, age <int>, nCompletions <int>\n\nI didn’t actually have to put the by = \"ID\" argument in there as left_join will join every row in tibQuaireData to any row with a matching value in any variable shared between tibQuaireData and tibParticipants and in my little example the only shared variable is ID. OK, that’s looking good.\n\nThere are a full set of join functions: inner_join(), left_join(), right_join() and full_join() that handle the full set of ways you might want to merge to data sets on index variables. They are making three bits of work I’m involved in, each of which involve relational database structures feel very easy.\n\n\ntibQuaireData %>%\n  left_join(tibParticipants, by = \"ID\") %>%\n  ### I will change the order of the variables, \n  ### this order seems better to me\n  ### everything() picks up any variables not already named as we see\n  select(ID, gender, age, nCompletions, nCompletions2, \n         everything()) \n\n# A tibble: 7,679 × 11\n      ID gender   age nCompletions nCompletions2 completionN item1\n   <int> <chr>  <int>        <int>         <int>       <int> <int>\n 1     1 F         11            8             8           1     4\n 2     1 F         11            8             8           2     0\n 3     1 F         11            8             8           3     1\n 4     1 F         11            8             8           4     5\n 5     1 F         11            8             8           5     0\n 6     1 F         11            8             8           6     0\n 7     1 F         11            8             8           7     2\n 8     1 F         11            8             8           8     3\n 9     2 M         11           30            30           1     0\n10     2 M         11           30            30           2     2\n# ℹ 7,669 more rows\n# ℹ 4 more variables: item2 <int>, item3 <int>, item4 <int>,\n#   item5 <int>\n\nOK, I’m working at the console (well actually, in a file and running the lines each time I finish tweaking them) so now assign that.\n\n\ntibQuaireData %>%\n  left_join(tibParticipants, by = \"ID\") %>%\n  ### I will change the order of the variables, \n  ### this order seems better to me\n  ### everything() picks up any variables not already named as we see\n  select(ID, gender, age, nCompletions, nCompletions2, \n         everything()) -> tibQuaireData\n\n\nNow I can do simple things with the data exploring it. I am going to stick to simple things that can be done just using pipes and dplyr.\n\n\n### gender breakdown of age\ntibQuaireData %>%\n  group_by(gender) %>%\n  summarise(n = n(), # gives the number of rows within the gender grouping\n            ### n_distinct() like length(unique()) in classical R, \n            ### gives number of distinct values of ID\n            nParticipants = n_distinct(ID), \n            minAge = min(age), # minimum age within the gender grouping\n            meanAge = mean(age), # ... similarly!\n            sdAge = sd(age),\n            maxAge = max(age))\n\n# A tibble: 2 × 7\n  gender     n nParticipants minAge meanAge sdAge maxAge\n  <chr>  <int>         <int>  <int>   <dbl> <dbl>  <int>\n1 F       3765           249     11    14.7  2.35     18\n2 M       3914           251     11    14.6  2.33     18\n\nNow I want to check the range of responses on the items. This introduces the across() selection and within it the starts_with(). They pretty much do what their names suggests. There is also an ends_with() selector. I could also have used item1:item5 as the colon gives all the variables from the left hand side to the right hand side, i.e. here from item1 to item5.\n\n\ntibQuaireData %>%\n  summarise(across(starts_with(\"item\"), # that was the selector, \n                                        # explained above, now we want done with those variables ...\n                   list(min = min, max = max)))\n\n# A tibble: 1 × 10\n  item1_min item1_max item2_min item2_max item3_min item3_max\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         0         5         0         5         0         5\n# ℹ 4 more variables: item4_min <int>, item4_max <int>,\n#   item5_min <int>, item5_max <int>\n\nOK, so the full range of scores was used for every item (doh!). Not the most obvious way to show that so that introduces pivoting. I think the name came from its use in spreadsheets but pivot_longer() and pivot_wider() give lovely control over converting data from wide to long (pivot_longer() … doh!) and pivot_wider() does the opposite.\n\n[This is called an aside in Rmarkdown]\npivot_longer() and pivot_wider() replaced two earlier components of the dplyr/tidyverse system: gather() and spread() respectively. I found gather() and spread() sometimes hard use and have found pivot_longer() and pivot_wider() much better. If you see examples using gather() and spread() in the web, I would strongly recommend that you ignore them and find more recent work.\n\n\ntibQuaireData %>%\n  pivot_longer(cols = starts_with(\"item\"))\n\n# A tibble: 38,395 × 8\n      ID gender   age nCompletions nCompletions2 completionN name \n   <int> <chr>  <int>        <int>         <int>       <int> <chr>\n 1     1 F         11            8             8           1 item1\n 2     1 F         11            8             8           1 item2\n 3     1 F         11            8             8           1 item3\n 4     1 F         11            8             8           1 item4\n 5     1 F         11            8             8           1 item5\n 6     1 F         11            8             8           2 item1\n 7     1 F         11            8             8           2 item2\n 8     1 F         11            8             8           2 item3\n 9     1 F         11            8             8           2 item4\n10     1 F         11            8             8           2 item5\n# ℹ 38,385 more rows\n# ℹ 1 more variable: value <int>\n\n### \"name\" and \"value\" are the default names for the variables \n### created by pivoting but you can override them ...\ntibQuaireData %>%\n  pivot_longer(cols = starts_with(\"item\"), \n               names_to = \"item\", \n               values_to = \"score\")\n\n# A tibble: 38,395 × 8\n      ID gender   age nCompletions nCompletions2 completionN item \n   <int> <chr>  <int>        <int>         <int>       <int> <chr>\n 1     1 F         11            8             8           1 item1\n 2     1 F         11            8             8           1 item2\n 3     1 F         11            8             8           1 item3\n 4     1 F         11            8             8           1 item4\n 5     1 F         11            8             8           1 item5\n 6     1 F         11            8             8           2 item1\n 7     1 F         11            8             8           2 item2\n 8     1 F         11            8             8           2 item3\n 9     1 F         11            8             8           2 item4\n10     1 F         11            8             8           2 item5\n# ℹ 38,385 more rows\n# ℹ 1 more variable: score <int>\n\n### now just group and get min and max\ntibQuaireData %>%\n  pivot_longer(cols = starts_with(\"item\"), \n               names_to = \"item\", \n               values_to = \"score\") %>%\n  group_by(item) %>%\n  summarise(minScore = min(score),\n            maxScore = max(score))\n\n# A tibble: 5 × 3\n  item  minScore maxScore\n  <chr>    <int>    <int>\n1 item1        0        5\n2 item2        0        5\n3 item3        0        5\n4 item4        0        5\n5 item5        0        5\n\nMuch easier to read that way around.\nSummarising\nThese have been trivial examples but they’ve introduced some of the fundamental powers of piping using the dplyr package in the R tidyverse. They grew on me and now, as I said at the beginning, they are how I do pretty much all my data manipulation and analyses. There was certainly a learning curve for me and I guess my conversion to piping really happened through 2020. The advantages I have found are:\nthe iterative building of code to do what I want, that I’ve tried to illustrate above, feels a very easy way to write code: you see the output of each step and build things step by step\nI am sure this has meant that I am writing better code, faster, with fewer mistakes\nthe method is very similar to creating graphics with the ggplot2 package and ggplot() so my conversion to working with pipes was perhaps helped by a slightly earlier decision to move from classical R graphics to ggplot() and I find the two complement each other … but I do occasionally forget that the piping operator in ggplot() is “+”, not “%>%”!\nI find code I wrote this way far, far easier to read when I come back to it after time has passed\nI think piping, and particularly pivoting, have really helped me break old “SPSS thinking” and made me comfortable with relational data structures\nThese really have been trivial examples, I’ll be making up pages here illustrating more complicated and much more powerful aspects of piping and the tidyverse over the months ahead.\n\n\n\n",
    "preview": "posts/2021-02-07-why-pipe-why-the-tidyverse/redpipe.png",
    "last_modified": "2023-08-25T13:37:52+02:00",
    "input_file": {},
    "preview_width": 6000,
    "preview_height": 4800
  },
  {
    "path": "posts/2021-02-06-how-i-have-done-this/",
    "title": "How I have done this",
    "description": "Just documenting how I have created these pages with the Distill package.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-06",
    "categories": [
      "Distill package",
      "R graphics",
      "R tricks"
    ],
    "contents": "\n\nContents\nDistill\nAutomate transfer to my web server\nHow to get images into the preview\nHow to get wider images\nDefault plot width: l-body\nWider plot width: l-body-outset\nWider plot width still: l-page\nFull screen: l-screen\n\n\n\n\n\n\n\n\nDistill\nThese pages have been created using the distill package in R. To quote its authors: “Distill is a publication format for scientific and technical writing, native to the web. Learn more about using Distill at https://rstudio.github.io/distill.”\nThat’s a pretty good summary and the documentation there covers most of the powers of Distill. However, as with much software documentation, I also felt there were things missing that I needed or that would have speeded things up for me. It’s the usual problem that the people who write the code, and many of the people who use it, are very clever and know what they are doing but don’t always remember that we’re not all that clever or that some things had become so familiar to them that they don’t notice they haven’t put those things in the documentation.\nSo Distill is an R package and I suppose it could be run without Rstudio but it’s clearly designed to dovetail with Rstudio. So I installed the package and followed the instructions to create a site at https://rstudio.github.io/distill/#creating-an-article. The system is as they say “a publication format” and they frame it as a tool with which to make a blog. It actually has what I would call “base pages” as well as pages that it creates as “blog posts”. It took me a while to realise that I had to create pages and posts at the command line withdistill::create_article()\nanddistill::create_post() (with some arguments, pretty much all you need to do withdistill::create_post() is to give the post a title: distill::create_post(\"My latest post\")).\nThe package code creates a template page which is basically Rmarkdown, just as happens when you create a new Rmarkdown page in Rstudio. You have all the usual Rmarkdown capacities: “knitting” together blocks of code and blocks of text, embedded figures, inline code in text blocks, TeX/LaTeX equation formatting inline and in equation blocks, tables, citations and reference insertion, tables of contents etc. The help at https://rstudio.github.io/distill goes through the various very nice things the templates can do for you that go a bit beyond what Rmarkdown does:\ncode folding (which I have used throughout) which “folds” code away but allows the reader of the page to open it by just clicking\nnice syntax highlighting in the code blocks pretty much mimicking the syntax highlighting in Rstudio\nyou can change theme with css (so I have a Rblog.css file where I’ve reset the background colour)\nfootnotes\nThere’s a lot that has been done to make some of the things you want for open scientific/academic/research publishing easy that is set in “yaml” (a recursive acronym for “YAML Ain’t Markup Language”) … it’s a header block above the markdown/markup in many markdown/up files. My _site.yml file (as of 6.ii.21) is this:\n---\nname: \"test2\"\ntitle: \"Chris (Evans) R SAFAQ\"\nbase_url: https://www.psyctc.org/R_blog\ndescription: |\n  CE's pages \"blog posts\" about using R\noutput_dir: \"_site\"\nnavbar:\n  logo:\n    image: images/g2_128.gif\n    href: https://www.psyctc.org/Rblog/\n    icon: images/g2_128.gif\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Welcome\"\n      href: \"Welcome.html\"\n    - text: \"About\"\n      href: about.html\n    - text: \"Copyright/permissions\"\n      href: Copyright_and_permissions.html\noutput: \n  distill::distill_article:\n    theme: Rblog.css\ncitations: true\ncookie_consent:\n  style: simple\n  type: express\n  palette: light\n  lang: en\n  cookies_policy: url\n---\nLet’s break that up and comment it some things that are perhaps not totally obvious. (Hm, not sure if you can comment yaml, hm, yes I think you can.)\nThis first block is defining the whole site.\nname: \"test2\" # this is the directory\ntitle: \"Chris (Evans) R SAFAQ\"\nbase_url: https://www.psyctc.org/R_blog # this makes sure the pages index to that URL\ndescription: |\n  CE's pages \"blog posts\" about using R\noutput_dir: \"_site\" # and this is the directory in which the site is compiled by Distill\nDistill automatically creates a simple site structure with a navigation bar at the top. The next bits define that. This first bit just allows you to put an image and icon in. (I could do with a bigger one!)\nnavbar:\n  logo:\n    image: images/g2_128.gif\n    href: https://www.psyctc.org/Rblog/\n    icon: images/g2_128.gif\nAnd this bit puts in links to pages you may have created with distill::create_article() … you have to put these into the navigation bar manually by putting lines like these next ones.\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Welcome\"\n      href: \"Welcome.html\"\n    - text: \"About\"\n      href: about.html\n    - text: \"Copyright/permissions\"\n      href: Copyright_and_permissions.html\noutput: \n  distill::distill_article: # not sure what this does!\n    theme: Rblog.css # this is where I invoke my theme/style css\nThen some very nice convenience powers of the package.\ncitations: true # automatically inserts a nice footnote about citing things on the site\ncookie_consent: # and a nice cookie consent for you\n  style: simple\n  type: express\n  palette: light\n  lang: en\n  cookies_policy: url\nPages created with distill::create_article(), like all Rmarkdown, start with their own yaml blocks and again these allow some nice things.\ntitle: \"Welcome to CE blog test\"\ndescription: |\n  Welcome to my blog which I hope will be useful to people using R to analyse data.\nauthor:\n  - name: Chris Evans\n    url: https://www.psyctc.org/R_blog/\n    affiliation: PSYCTC.org                              # put your affiliation/organisation in\n    affiliation_url: https://www.psyctc.org/psyctc/      # URL for that\n    orcid_id: 0000-0002-4197-4202                        # put your ORCID ID in\ndate: \"2023-08-25\"\noutput: \n  distill::distill_article:\n    self_contained: false\nAutomate transfer to my web server\nThis took me some hours today to sort out but will save me many hours over the years ahead. I suspect that anyone who is more familiar with git than I was will manage to do this much more quickly than I did. What I’ve done is:\ninstall git on the machine on which I’m running Rstudio and storing the site pages\ntell Rstudio that git is there and is to be used for “version control”, i.e. automatic backing up of all changes that you “commit” keeping a full historical archive of the changes\ncreated a free personal account on gitHub (https://github.com/cpsyctc/) and create a respository in it (https://github.com/cpsyctc/Rblog)\ncreated a personal token which works instead of a password to log into my repository there and makes sure that I’m the only one who can write things to that repository (but anyone can download, “pull” in git terminology, from it) (I have now discovered from https://usethis.r-lib.org/reference/use_github.html that these bits might have bee expedited with a )\nuse that to “push” each new committed update to the pages to that github repository\ninstall git on my web server (pretty sure my ISP had already done this actually)\n[this bit, and the next, are linux specific but could be done, though the terminology is different, in Windoze] create a little shell script on the server that “pulls” a copy of the repository content down to the server from github (git handles the tracking of changes and makes sure that only the minimum necessary material is stored and transferred) and uses rsync to copy things to the web pages (rsync, a bit like git, will only copy changed files)\nput a call into crontab to run that little script every ten minutes\nSo I’ve now got a site/blog developing here as an Rstudio project that I can commit and push to github (where anyone can pull it if they want it) and which then automatically updates my server, at slowest, ten minutes later.\nNow I need to spend a bit more time creating more content but perhaps I’ll browse some other people’s examples first: see https://pkgs.rstudio.com/distill/articles/examples.html.\nHow to get images into the preview\n[Added 7.ii.21] I couldn’t work out how to get an ordinary image into listing of “posts” in the base of the “blog” but, courtesy of Shannon Pileggi of the excellent https://www.pipinghotdata.com/ site she created with Distill, I now have the trick: put the graphic in the directory holding the post and put a line in the yaml pointing to it. So here’s the YAML header of the Rmarkdown file that creates this page:\n---\ntitle: \"How I've done this\"\ndescription: |\n  Just documenting how I have created these pages\nbase_url: https://www.psyctc.org/psyctc/Rblog/\npreview: distill.png\nauthor:\n  - name: Chris Evans\n    url: https://www.psyctc.org/R_blog/\n    affiliation: PSYCTC.org\n    affiliation_url: https://www.psyctc.org/psyctc/\n    orcid_id: 0000-0002-4197-4202\n\ndate: 2021-02-06\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    self_contained: false\n---\nYou see the crucial preview: distill.png (I downloaded the graphic from https://blog.rstudio.com/2020/12/07/distill/distill.png). That’s it: thanks Shannon! Shannon also pointed me to her public github repository at https://github.com/shannonpileggi which has all the code for her blog at https://github.com/shannonpileggi/pipinghotdata_distill … I should have been able to find that without Emailing her.\nHow to get wider images\nThis is from https://rstudio.github.io/distill/figures.html.\nThe default format is l-body, wider layouts are l-body-outset, l-page and l-screen. Let’s see!\nDefault plot width: l-body\n\n\n\nWider plot width: l-body-outset\n\n\n\nWider plot width still: l-page\n\n\n\nFull screen: l-screen\n\n\n\nThose were ECDF plots showing confidence intervals around the observed quartiles (and median) see Rblog post: Confidence intervals for quantiles. Plots created using plotQuantileCIsfromDat() from the CECPfuns package.\n\n\n\n",
    "preview": "posts/2021-02-06-how-i-have-done-this/distill.png",
    "last_modified": "2023-08-25T13:37:35+02:00",
    "input_file": {},
    "preview_width": 2521,
    "preview_height": 2911
  },
  {
    "path": "posts/2021-01-27-bootstrapspearman/",
    "title": "Bootstrap_Spearman",
    "description": "A quick exploration of bootstrapping a Spearman and why you might, or might not, want it.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [
      "confidence intervals",
      "bootstrapping"
    ],
    "contents": "\n\nContents\nBootstrapping Spearman correlation coefficient\n\nBootstrapping Spearman correlation coefficient\nTraditionally people used the Spearman correlation coefficient where the sample observed distributions of the variables being correlated were clearly not Gaussian. The logic is that as the Spearman correlation is a measure of correlation between the ranks of the values, the distribution of the scores, population or sample, was irrelevant to any inferential interpretation of the Spearman correlation. By contrast, inference about the statistical (im)probability of a Pearson correlation, or a confidence interval (CI) around an observed correlation, was based on maths which assumed that population values were Gaussian. This is simply and irrevocably true: so if the distributions of your sample scores are way off Gaussian then the p values and CIs for a Pearson can be very misleading.\nThe logic of doing a test of fit to Gaussian on your sample data (univariate and/or bivariate test of fit) is dodgy as if your sample is small then even a large deviation from Gaussian that may give very poor p values and CIs has a fair risk of not being flagged as statistically significantly different from Gaussian and with a large sample, even trivial deviations from Gaussian that would have no effect on the p values and CIs will show as statistically significant. How severe that problem is should really have simulation exploration and I haven’t searched for that but the theoretical/logical problem is crystal clear.\nKeen supporters of non-parametrical statistical methods sometimes argued, reasonably to my mind, that the simple answer was to use non-parametric tests regardless of sample distributions, their opponents argued that this threw away some statistical power: true but the loss wasn’t huge.\nAll this has been fairly much swept away, again, to my mind, by the arrival of bootstrapping which allows you, for anything above a very small sample and for pretty much all but very, very weird distributions, to get pretty robust CIs around observed Pearson correlations regardless of the distributions, population or sample distributions, of the variables.\nBecause of this I now report Pearson correlations with bootstrap CIs around them for any correlations unless there is something very weird about the data. This has all the advantages of CIs over p values and is robust to most distributions.\nHowever, I often want to compare with historical findings (including my own!) that were reported using Spearman’s correlation so I still often report Spearman correlations. However, there is no simple parametric CI for the Spearman correlation and I’m not sure there should be outside the edge case where you have perfect ranking (i.e. no ties on either variable). Then the Spearman correlation is the Pearson correlation of the ranks and I think the approximation of using the parametric Pearson CI computation for the Spearman is probably sensible. I am not at all sure that once you have ties that you can apply the same logic though probably putting in n as the lower of the number of distinct values of the two variables probably gives a safe but often madly wide CI. (“Safe” in the sense that it will include the true population correlation 95% of the time (assuming that you are computing the usual 95% CI)).\nFortunately, I can see reason why the bootstrap cannot be used to find a CI around an observed Spearman correlation and this is what I do now when I am reporting a Spearman correlation.\n\n\nShow code\n\ngetCISpearmanTxt <- function(x, bootReps = 999, conf = .95, digits = 2) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  in format \"rho (LCL to UCL)\"\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  ### digits sets the rounding\n  require(boot) # need boot package!\n  ### now we need a function that \n  spearmanForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i, 1], \n        x[i, 2],\n        method = \"spearman\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = spearmanForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### get observed Spearman correlation and confidence limits as vector\n  retVal <- (c(tmpBootRes$t0,\n           tmpCI$percent[4],\n           tmpCI$percent[5]))\n  ### round that\n  retVal <- round(retVal, digits)\n  ### return it as a single character variable\n  retVal <- paste0(retVal[1],\n                   \" (\",\n                   retVal[2],\n                   \" to \",\n                   retVal[3],\n                   \")\")\n  retVal\n}\n\ngetCISpearmanList <- function(x, bootReps = 999, conf = .95) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  returns a list with items obsCorr, LCL and UCL\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  require(boot) # need boot package!\n  ### now we need a function that \n  spearmanForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"spearman\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = spearmanForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### return observed Spearman correlation and confidence limits as a list\n  retVal <- list(obsCorrSpear = as.numeric(tmpBootRes$t0),\n                 LCLSpear = tmpCI$percent[4],\n                 UCLSpear = tmpCI$percent[5])\n  retVal\n}\n\ngetCIPearsonTxt <- function(x, bootReps = 999, conf = .95, digits = 2) {\n  ### function to give bootstrap CI around bivariate PearsonR\n  ###  in format \"R (LCL to UCL)\"\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  ### digits sets the rounding\n  require(boot) # need boot package!\n  ### now we need a function that \n  pearsonForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"pearson\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = pearsonForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### get observed Spearman correlation and confidence limits as vector\n  retVal <- (c(tmpBootRes$t0,\n           tmpCI$percent[4],\n           tmpCI$percent[5]))\n  ### round that\n  retVal <- round(retVal, digits)\n  ### return it as a single character variable\n  retVal <- paste0(retVal[1],\n                   \" (\",\n                   retVal[2],\n                   \" to \",\n                   retVal[3],\n                   \")\")\n  retVal\n}\n\ngetCIPearsonList <- function(x, bootReps = 999, conf = .95) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  returns a list with items obsCorr, LCL and UCL\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  require(boot) # need boot package!\n  ### now we need a function that \n  pearsonForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"pearson\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = pearsonForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### return observed Spearman correlation and confidence limits as a list\n  retVal <- list(obsCorrPears = as.numeric(tmpBootRes$t0),\n                 LCLPears = tmpCI$percent[4],\n                 UCLPears = tmpCI$percent[5])\n  retVal\n}\n\n\n\n\nShow code\n\n### generate some Gaussian and some non-Gaussian data\nn <- 5000 # sample size\nset.seed(12345) # get replicable results\n\nas_tibble(list(x = rnorm(n),\n               y = rnorm(n))) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) %>%\n  summarise(absMinVal = abs(min(value))) %>%\n  pull() -> varMinVal\n\ntibDat %>%\n  mutate(xSquared = x^2,\n         ySquared = y^2,\n         xLn = log(x + varMinVal + 0.2),\n         yLn = log(y + varMinVal + 0.2),\n         xInv = 1/(x + varMinVal + 0.1),\n         yInv = 1/(y + varMinVal + 0.1)) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) -> tibDatLong\n\nggplot(data = tibDatLong,\n       aes(x = value)) +\n  facet_wrap(facets = vars(name),\n             ncol = 2,\n             scales = \"free\",\n             dir = \"v\") +\n  geom_histogram(bins = 100) +\n  theme_bw()\n\n\nShow code\n\nggpairs(tibDat)\n\n\n\nGood! Got some weird variables there: x and y are Gaussian random variables and uncorrelated then we have their squares, a natural log (after adding enough to the values to avoid trying to get ln(0)) and their inverses (with the same tweak to avoid getting 1/0).\n\n\nShow code\n\noptions(dplyr.summarise.inform = FALSE)\ntibDatLong %>%\n  mutate(id = (1 + row_number()) %/% 2 ,\n         var = str_sub(name, 1, 1),\n         transform = str_sub(name, 2, 20),\n         transform = if_else(transform == \"\", \"none\", transform),\n         transform = ordered(transform,\n                             levels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"),\n                             labels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"))) %>%\n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = var) -> tibDatLong2\n\ntibDatLong2 %>%\n  group_by(transform) %>%\n  select(x, y) %>%\n  summarise(corrS = list(getCISpearmanList(cur_data())),\n            corrP = list(getCIPearsonList(cur_data()))) %>%\n  unnest_wider(corrS) %>%\n  unnest_wider(corrP) %>% \n  pander(justify = \"lrrrrrr\", split.tables = Inf)\n\ntransform\nobsCorrSpear\nLCLSpear\nUCLSpear\nobsCorrPears\nLCLPears\nUCLPears\nnone\n-0.03547\n-0.06164\n-0.005442\n-0.02368\n-0.0495\n0.003609\nLn\n-0.03547\n-0.06259\n-0.008897\n-0.01938\n-0.04774\n0.01032\nInv\n-0.03547\n-0.06106\n-0.006073\n0.0001815\n-0.0324\n0.0382\nSquared\n-0.00467\n-0.03253\n0.02274\n-0.009833\n-0.03985\n0.02017\n\nThat’s what we would expect to see: the observed Spearman correlations are the same for the raw data, the ln and inv transformed values (as these are transforms that preserve monotonic, i.e. ranked, ordered, relationships between values while changing the values a lot) but the value is different for the squared transform as that’s not monotonic. The values for the Pearson correlations change with ln and inv transforming as they should as the correlations between the transformed values are not the same as between the raw values. The CIs for the Spearman raw and ln and inv transformed values are not quite identical because the bootstrapping will have produced different bootstrapped samples for each. (I think there’s a way I could have got all three in the same call to boot() but that would have needed a different function to bootstrap.)\nReassuring that all the CIs include zero: you’d hope so really with n = 5000 and uncorrelated raw values.\nNow let’s get a moderately correlated pair of variables.\n\n\nShow code\n\n### generate some Gaussian and some non-Gaussian data\nn <- 5000 # sample size\nset.seed(12345) # get replicable results\n\ntibDat %>% \n  mutate(y = x + y) %>%\n  select(x, y) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) %>%\n  summarise(absMinVal = abs(min(value))) %>%\n  pull() -> varMinVal\n\ntibDat %>%\n  mutate(xSquared = x^2,\n         ySquared = y^2,\n         xLn = log(x + varMinVal + 0.2),\n         yLn = log(y + varMinVal + 0.2),\n         xInv = 1/(x + varMinVal + 0.1),\n         yInv = 1/(y + varMinVal + 0.1)) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) -> tibDatLong\n\nggplot(data = tibDatLong,\n       aes(x = value)) +\n  facet_wrap(facets = vars(name),\n             ncol = 2,\n             scales = \"free\",\n             dir = \"v\") +\n  geom_histogram(bins = 100) +\n  theme_bw()\n\n\nShow code\n\nggpairs(tibDat)\n\n\n\n\n\nShow code\n\noptions(dplyr.summarise.inform = FALSE)\ntibDatLong %>%\n  mutate(id = (1 + row_number()) %/% 2 ,\n         var = str_sub(name, 1, 1),\n         transform = str_sub(name, 2, 20),\n         transform = if_else(transform == \"\", \"none\", transform),\n         transform = ordered(transform,\n                             levels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"),\n                             labels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"))) %>%\n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = var) -> tibDatLong2\n\ntibDatLong2 %>%\n  group_by(transform) %>%\n  select(x, y) %>%\n  summarise(corrS = list(getCISpearmanList(cur_data())),\n            corrP = list(getCIPearsonList(cur_data()))) %>%\n  unnest_wider(corrS) %>%\n  unnest_wider(corrP) %>% \n  pander(justify = \"lrrrrrr\", split.tables = Inf)\n\ntransform\nobsCorrSpear\nLCLSpear\nUCLSpear\nobsCorrPears\nLCLPears\nUCLPears\nnone\n0.666\n0.6485\n0.6833\n0.6906\n0.6755\n0.7049\nLn\n0.666\n0.6496\n0.683\n0.679\n0.6625\n0.6956\nInv\n0.666\n0.6488\n0.6823\n0.279\n0.2516\n0.6728\nSquared\n0.3559\n0.3311\n0.3815\n0.4992\n0.4617\n0.5347\n\nGreat: exactly what we’d expect again.\n\n\n\n",
    "preview": "posts/2021-01-27-bootstrapspearman/bootstrapspearman_files/figure-html5/simulate1-1.png",
    "last_modified": "2023-08-25T13:36:50+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-27-handling-overprinting/",
    "title": "Handling overprinting",
    "description": "This is the first of my blog posts here, about the issue of overprinting and some ways to handle it \nusing R and ggplot().  There's a small spin off topic on the impact on bivariate correlations and on\nlinear regression of discretising continuous variables.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [
      "Graphics",
      "Overprinting",
      "Jittering",
      "R tricks"
    ],
    "contents": "\n\nContents\nOverprinting\nJittering\nUsing transparency\nUsing area to show counts: geom_count()\nHow do those methods work with the original data?\nJittering\nTransparency\nUsing area: geom_count()\n\n\nTangential issue: impact of discretising on relationship between variables\nQuestions and feedback\nTechnical footnote and thanks\nLicensing\n\nOverprinting\nOverprinting is where one point on a graph overlies another. It’s mainly a problem with scattergrams and if you have large numbers of points and few discrete values it can make a plot completely misleading. OK, let’s make up some data.\n\nI am showing the raw R within the Rmarkdown code blocks. I have tried to comment things liberally. Click on “Show code” to see the code.\n\n\nShow code\n\nn <- 5000 # a lot of points means that overprinting is inevitable \nnVals <- 5 # discretising continuous variables to this number of values (below) makes it even more certain\nset.seed(12345) # ensures we get the same result every time \n\n### now generate x and y variables as a tibble\nas_tibble(list(x = rnorm(n),\n               y = rnorm(n))) -> tibDat\n\n### create strong correlation between them by adding x to y (!)\ntibDat %>%\n  mutate(y = x + y) -> tibDat\n\n### now we want to discretise into equiprobable scores so find the empirical quantiles\nvecXcuts <- quantile(tibDat$x, probs = seq(0, 1, 1/nVals))\nvecYcuts <- quantile(tibDat$y, probs = seq(0, 1, 1/nVals))\n\n### now use those to transform the raw variables to equiprobable scores in range 1:5\ntibDat %>%\n  mutate(x5 = cut(x, breaks = vecXcuts, include.lowest = TRUE, labels = FALSE, right = TRUE),\n         y5 = cut(y, breaks = vecYcuts, include.lowest = TRUE, labels = FALSE, right = TRUE)) -> tibDat\n\n\nNow let’s have a simple scatterplot.\n\n\nShow code\n\n### use ggplot to generate the simple scattergram for the raw variables\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  theme_bw()\n\n\n\nThe relationship between the two variables is clear but we don’t know about any overprinting. We can add a loess smoothed regression which clarifies the relationship between the scores but doesn’t resolve the overprinting issue.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth() + # adding the loess smoother\n  theme_bw()\n\n\n\nHowever, to really drive home the point about overprinting, if those points are transformed and discretised to five equiprobable scores then things look like this.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) + # use discretised variables instead of raw variables\n  geom_point() +\n  theme_bw()\n\n\n\nWhoops: much overprinting as 5000 points have collapsed to 25 visible points on the scattergram but we can’t see how much and no apparent relationship between the variables at all.\nAgain we can add a regression to that plot for amusement and to show that the transform hasn’t removed the relationship. (Has to be a linear regression as the number of distinct points doesn’t allow for loess smoothing.)\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") + # linear regression fit\n  theme_bw()\n\n\n\nJittering\nOne way around overprinting it is to jitter the points. Here I have used geom_jitter(width = .2, height = .2) which adds random “jittering” to both x and y values spread across .2 of the “implied bins”. I’ve left the raw data in in blue.\nThere are situations in which you just want jittering on one axis and not the other so you can use geom_jitter(width = .2). Sometimes playing around with width helps get the what seems the best visual fit to the counts.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_jitter(width = .2, height = .2) + # jittered data\n  geom_point(data = tibDat,\n             aes(x = x5, y = y5),\n             colour = \"blue\") +\n  theme_bw()\n\n\n\nUsing transparency\nAnother approach is to use transparency. Here you just have the one parameter, alpha and again, sometimes you need to play with different values.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_point(alpha = .01) +\n  theme_bw()\n\n\n\nThat’s not working terribly well as we have so many points (n = 5000).\nUsing area to show counts: geom_count()\nAnd another approach, good when values are widely spaced as here, is geom_count().\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_count() +\n  scale_size_area(n.breaks = 10) +\n  theme_bw()\n\n\n\nI used a rather excessive number of breaks there but it makes the point.\nHow do those methods work with the original data?\nIn this next set of blocks I’ve applied the same three tricks but to both the raw data and the discretised data.\nJittering\n\n\nShow code\n\n### reshape data to make it easy to get plots side by side using facetting\ntibDat %>%\n  ### first pivot longer \n  pivot_longer(cols = everything()) %>%\n  ### which gets something like this\n  #   # A tibble: 20,000 x 2\n  #    name    value\n  #    <chr>   <dbl>\n  #  1 x      0.586 \n  #  2 y     -0.107 \n  #  3 x5     4     \n  #  4 y5     3     \n  #  5 x      0.709 \n  #  6 y      1.83  \n  #  7 x5     4     \n  #  8 y5     5     \n  #  9 x     -0.109 \n  # 10 y      0.0652\n  # # … with 19,990 more rows\n  ### now get new variables one for x and y\n  mutate(variable = str_sub(name, 1, 1),\n       ### and one for the transform\n       transform = str_sub(name, 2, 2),\n       transform = if_else(transform == \"5\", \"discretised\", \"raw\"),\n       transform = factor(transform,\n                          levels = c(\"raw\", \"discretised\")),\n       ### create an id variable clumping each set of four scores together\n       id = (3 + row_number()) %/% 4) %>%\n  ### so now we can pivot back \n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = variable) -> tibDat2\n### to get this\n# A tibble: 10,000 x 4\n#       id transform        x       y\n#    <dbl> <chr>        <dbl>   <dbl>\n#  1     1 raw          0.586 -0.107 \n#  2     1 discretised  4      3     \n#  3     2 raw          0.709  1.83  \n#  4     2 discretised  4      5     \n#  5     3 raw         -0.109  0.0652\n#  6     3 discretised  3      3     \n#  7     4 raw         -0.453 -2.42  \n#  8     4 discretised  2      1     \n#  9     5 raw          0.606 -1.04  \n# 10     5 discretised  4      2     \n# # … with 9,990 more rows\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = .2)  +\n  facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\nAmusing! I’ve put the linear regression fit line on both.\nTransparency\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n  facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_point(alpha = .01) +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\nTransparency that works on the right for the discretised data, well works up to a point, is a bit too thin for the raw data. I can’t see that as of now (26.i.21 and ggplot version 3.3.3) that you can map transparency, i.e. alpha to a variable as you can, say, for colour. So to get a side-by-side plot I’m using a different approach. There are various ways of doing this, a useful page seems to be: http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/81-ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page/\n\n\nShow code\n\nggplot(data = filter(tibDat2, transform == \"raw\"), # select just the raw data\n       aes(x = x, y = y)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() -> tmpPlot1\n\n\nggplot(data = filter(tibDat2, transform == \"discretised\"), # select just the raw data\n       aes(x = x, y = y)) +\n  geom_point(alpha = .01) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() -> tmpPlot2\n\n### use ggarrange from the ggpubr package, see the URL for other options\nggpubr::ggarrange(tmpPlot1, tmpPlot2)\n\n\n\nUsing area: geom_count()\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n    facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_count() +\n  geom_smooth(method = \"lm\") +\n  scale_size_area(n.breaks = 10) +\n  theme_bw()\n\n\n\nI used a rather excessive number of breaks there but it makes the point.\nTangential issue: impact of discretising on relationship between variables\n\n\nShow code\n\nvalRawCorr <- cor(tibDat$x, tibDat$y)\nvalDisc5Corr <- cor(tibDat$x5, tibDat$y5)\n\nvecRawCorrCI <- cor.test(tibDat$x, tibDat$y)$conf.int\nvecDisc5CorrCI <- cor.test(tibDat$x5, tibDat$y5)$conf.int\n\n### or here's another, tidyverse way to do this\n### seems like unnecessary faff except that it makes \n### it so easy to do a micro forest plot (see below)\ngetParmPearsonCI <- function(x, y){\n  ### little function to get parametric 95% CI from two vectors\n  obsCorr <- cor(x, y)\n  tmpCI <- cor.test(x, y)$conf.int\n  return(list(LCL = tmpCI[1],\n              obsCorr = obsCorr,\n              UCL = tmpCI[2]))\n}\ntibDat2 %>%\n  group_by(transform) %>%\n  summarise(pearson = list(getParmPearsonCI(x, y))) %>%\n  unnest_wider(pearson) -> tibCorrs\n### which gives us this\n# tibCorrs\n# # A tibble: 2 x 4\n#   transform     LCL obsCorr   UCL\n#   <fct>       <dbl>   <dbl> <dbl>\n# 1 raw         0.676   0.691 0.705\n# 2 discretised 0.609   0.626 0.643\n\n\nThe correlation between the original variables is\n0.691 with parametric 95% confidence interval (CI) from\n0.676 to\n0.705 whereas that between the discretised variables is\n0.626 with 95% CI from\n0.609 to\n0.643 so some clear attenuation there. Micro forest plot of that:\n\n\nShow code\n\nggplot(data = tibCorrs,\n       aes(x = transform, y = obsCorr)) +\n  geom_point() +\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) +\n  ylim(.5, 1) +\n  theme_bw()\n\n\n\nYup, that’s a fairly large and clear difference on a y scale from .5 to 1.0. What about the linear regression?\n\n\nShow code\n\n### raw variables\nlm(scale(y) ~ scale(x), data = tibDat)\n\n\nCall:\nlm(formula = scale(y) ~ scale(x), data = tibDat)\n\nCoefficients:\n(Intercept)     scale(x)  \n  1.066e-17    6.906e-01  \n\nShow code\n\n### discretised variables\nlm(scale(y5) ~ scale(x5), data = tibDat)\n\n\nCall:\nlm(formula = scale(y5) ~ scale(x5), data = tibDat)\n\nCoefficients:\n(Intercept)    scale(x5)  \n -1.726e-17    6.265e-01  \n\nShow code\n\n### or tidyverse way\n### I confess I haven't really got my head aound the broomverse but this is powerful\ntibDat2 %>% \n  group_by(transform) %>%\n  do(broom::tidy(lm(scale(y) ~ scale(x), data = .))) %>%\n  pander(justify = \"llrrrr\", split.tables = Inf)\n\ntransform\nterm\nestimate\nstd.error\nstatistic\np.value\nraw\n(Intercept)\n1.066e-17\n0.01023\n1.042e-15\n1\nraw\nscale(x)\n0.6906\n0.01023\n67.51\n0\ndiscretised\n(Intercept)\n-1.726e-17\n0.01102\n-1.566e-15\n1\ndiscretised\nscale(x)\n0.6265\n0.01102\n56.83\n0\n\nOh dear, oh dear! I think I should have known that the standardised regression (slope) coefficients of a simple, two variable linear regression are the Pearson correlations!\nQuestions and feedback\nhttps://www.psyctc.org/psyctc/ web site. In most browsers I think that will open in a new page and if you close it when you have sent your message I think that and most browsers will bring you back here.\nTechnical footnote and thanks\nThis has been created using the distill package in R (and in Rstudio). Distill is a publication format for scientific and technical writing, native to the web. There is a bit more information about distill at https://rstudio.github.io/distill but I found the youtube (ugh) presentation by Maëlle Salmon at https://www.youtube.com/watch?v=Xyc4-bJjdys much more useful than the very minimal notes at that github page. After watching (the first half of) that presentation the github documentation becomes useful.\nLicensing\nAs with most things I put on the web, I am putting this under the Creative Commons Attribution Share-Alike licence.\n\n\n\n",
    "preview": "posts/2021-01-27-handling-overprinting/handling-overprinting_files/figure-html5/scatter1-1.png",
    "last_modified": "2023-08-25T13:37:10+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-27-nudgingonaxes/",
    "title": "Nudging groupings on plot axes",
    "description": "How to nudge categories on an axis of a ggplot plot.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [
      "Graphics",
      "Nudging",
      "Jittering",
      "R tricks"
    ],
    "contents": "\n\nContents\nHow to “nudge” plots\n\nHow to “nudge” plots\nI can never remember how to do this and keep looking it up. Emily asked me about it so I thought I should crack it and make a file about it to remind myself.\nI’m going to use a little function to get bootstrap confidence intervals around observed means so here’s the code for that.\n\nI am showing the raw R within the Rmarkdown code blocks. I have tried to comment things liberally. Click on “Show code” to see the code.\n\n\nShow code\n\n### function using boot() and boot.ci() from the the boot package to get bootstrap CIs around observed means\ngetCIbootMean <- function(data, ciInt = .95, bootReps = 1000){\n  getMeanForBoot <- function(dat, ind) {mean(dat[ind])} # ind indexes the particular bootstrap sample of vector dat\n  tmpRes <- boot::boot(data, getMeanForBoot, R = bootReps)  # gets the boostrap results\n  tmpCI <- boot::boot.ci(tmpRes, type =  \"perc\")$percent[1,4:5] # gets the percentile method CI\n  return(list(LCL = tmpCI[1],\n              obsMean = tmpRes$t0,\n              UCL = tmpCI[2]))\n}\n# getCIbootMean(1:30) # testing!\n\n\nNow let’s get some demonstation data.\n\n\nShow code\n\nn <- 500 # sample size\nset.seed(1245) # get same result every run\ntibble(genderNum = sample(0:1, n, replace = TRUE), # generate gender\n       ageNum = sample(13:17, n, replace = TRUE), # generate age\n       gender = if_else(genderNum == 1, \"F\", \"M\"),\n       score = rnorm(n) + # get randomness unsystematically related to gender or age\n         genderNum*.1*rnorm(n) + # add a simple gender effect\n         ageNum*.1*rnorm(n) + # add a simple age effect\n         (genderNum*(ageNum - 15)*.5*rnorm(n))^2 + # and an interaction\n         20, # make sure values are positive\n       age = as.factor(ageNum)) %>%\n  group_by(age, gender) %>%\n  summarise(mean = list(getCIbootMean(score))) %>%\n  unnest_wider(mean) -> tibDat\n\n\nHere’s a crude way to separate things by nudging them on the x axis.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = interaction(age, gender), y = obsMean, colour = gender)) +\n       geom_point() +\n       geom_linerange(aes(ymin = LCL, ymax = UCL))\n\n\n\nBut that’s aesthetically and informatively rubbish as it’s not reflecting the grouping. I think what we want is something like this.\n\n\nShow code\n\nvalXdodge = .25 # setting it here makes it easier to try different values when you have multiple geoms you want to dodge\nggplot(data = tibDat,\n       aes(x = age, y = obsMean, colour = gender, group = gender)) + # key thing is that dodging is by the grouping\n  geom_point(position = position_dodge2(width = valXdodge)) +\n  geom_linerange(aes(ymin = LCL, ymax = UCL),\n                 position = position_dodge(width = valXdodge)) \n\n\n\nI think “nudge” would have been a much better term than “dodge” but that may be because dodging has a particular meaning in manual printing of photos (where it’s all about changing the darkness of particular areas of the image) which was something I learned about long, long ago.\nI also think the help for dodge is truly awful and is compounded by the fact that dodging works differently depending on the geom you are using (I’ve been lazy and not gotten to the bottom of that but the basic issue is that it works differently for geom_bar() and geom_histogram() where I think it assumes that the x aesthetic is a grouping whereas with geom_point(), geom_linerange() and geom_errorbar() (and probably geom_line()) it needs to be told the grouping on which you are dodging.\nNotwithstanding my grousing, it’s incredibly useful for depicting things. I guess it has something in common with my previous post here https://www.psyctc.org/Rblog/posts/2021-01-27-handling-overprinting/ as both tricks have in common that they actually distort the literal mappings to create mappings that are far more informative and less misleading than the simply “accurate” mapping.\n\n\n\n",
    "preview": "posts/2021-01-27-nudgingonaxes/nudgingonaxes_files/figure-html5/plot1-1.png",
    "last_modified": "2023-08-25T13:37:24+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
