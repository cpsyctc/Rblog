[
  {
    "path": "posts/2021-02-06-how-ive-done-this/",
    "title": "How I've done this",
    "description": "Just documenting how I have created these pages",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-02-06",
    "categories": [],
    "contents": "\n\nContents\nDistill\nAutomate transfer to my web server\nHow to get images into the preview\n\n\n\n\nDistill\nThese pages have been created using the distill package in R. To quote its authors: “Distill is a publication format for scientific and technical writing, native to the web. Learn more about using Distill at https://rstudio.github.io/distill.”\nThat’s a pretty good summary and the documentation there covers most of the powers of Distill. However, as with much software documentation, I also felt there were things missing that I needed or that would have speeded things up for me. It’s the usual problem that the people who write the code, and many of the people who use it, are very clever and know what they are doing but don’t always remember that we’re not all that clever or that some things had become so familiar to them that they don’t notice they haven’t put those things in the documentation.\nSo Distill is an R package and I suppose it could be run without Rstudio but it’s clearly designed to dovetail with Rstudio. So I installed the package and followed the instructions to create a site at https://rstudio.github.io/distill/#creating-an-article. The system is as they say “a publication format” and they frame it as a tool with which to make a blog. It actually has what I would call “base pages” as well as pages that it creates as “blog posts”. It took me a while to realise that I had to create pages and posts at the command line withdistill::create_article()\nanddistill::create_post() (with some arguments, pretty much all you need to do withdistill::create_post() is to give the post a title: distill::create_post(\"My latest post\")).\nThe package code creates a template page which is basically Rmarkdown, just as happens when you create a new Rmarkdown page in Rstudio. You have all the usual Rmarkdown capacities: “knitting” together blocks of code and blocks of text, embedded figures, inline code in text blocks, TeX/LaTeX equation formatting inline and in equation blocks, tables, citations and reference insertion, tables of contents etc. The help at https://rstudio.github.io/distill goes through the various very nice things the templates can do for you that go a bit beyond what Rmarkdown does:\ncode folding (which I have used throughout) which “folds” code away but allows the reader of the page to open it by just clicking\nnice syntax highlighting in the code blocks pretty much mimicking the syntax highlighting in Rstudio\nyou can change theme with css (so I have a Rblog.css file where I’ve reset the background colour)\nfootnotes\nThere’s a lot that has been done to make some of the things you want for open scientific/academic/research publishing easy that is set in “yaml” (a recursive acronym for “YAML Ain’t Markup Language”) … it’s a header block above the markdown/markup in many markdown/up files. My _site.yml file (as of 6.ii.21) is this:\nname: \"test2\"\ntitle: \"Chris (Evans) R SAFAQ\"\nbase_url: https://www.psyctc.org/R_blog\ndescription: |\n  CE's pages \"blog posts\" about using R\noutput_dir: \"_site\"\nnavbar:\n  logo:\n    image: images/g2_128.gif\n    href: https://www.psyctc.org/Rblog/\n    icon: images/g2_128.gif\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Welcome\"\n      href: \"Welcome.html\"\n    - text: \"About\"\n      href: about.html\n    - text: \"Copyright/permissions\"\n      href: Copyright_and_permissions.html\noutput: \n  distill::distill_article:\n    theme: Rblog.css\ncitations: true\ncookie_consent:\n  style: simple\n  type: express\n  palette: light\n  lang: en\n  cookies_policy: url\nLet’s break that up and comment it some things that are perhaps not totally obvious. (Hm, not sure if you can comment yaml, hm, yes I think you can.)\nThis first block is defining the whole site.\nname: \"test2\" # this is the directory\ntitle: \"Chris (Evans) R SAFAQ\"\nbase_url: https://www.psyctc.org/R_blog # this makes sure the pages index to that URL\ndescription: |\n  CE's pages \"blog posts\" about using R\noutput_dir: \"_site\" # and this is the directory in which the site is compiled by Distill\nDistill automatically creates a simple site structure with a navigation bar at the top. The next bits define that. This first bit just allows you to put an image and icon in. (I could do with a bigger one!)\nnavbar:\n  logo:\n    image: images/g2_128.gif\n    href: https://www.psyctc.org/Rblog/\n    icon: images/g2_128.gif\nAnd this bit puts in links to pages you may have created with distill::create_article() … you have to put these into the navigation bar manually by putting lines like these next ones.\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Welcome\"\n      href: \"Welcome.html\"\n    - text: \"About\"\n      href: about.html\n    - text: \"Copyright/permissions\"\n      href: Copyright_and_permissions.html\noutput: \n  distill::distill_article: # not sure what this does!\n    theme: Rblog.css # this is where I invoke my theme/style css\nThen some very nice convenience powers of the package.\ncitations: true # automatically inserts a nice footnote about citing things on the site\ncookie_consent: # and a nice cookie consent for you\n  style: simple\n  type: express\n  palette: light\n  lang: en\n  cookies_policy: url\nPages created with distill::create_article(), like all Rmarkdown, start with their own yaml blocks and again these allow some nice things.\nAutomate transfer to my web server\nThis took me some hours today to sort out but will save me many hours over the years ahead. I suspect that anyone who is more familiar with git than I was will manage to do this much more quickly than I did. What I’ve done is:\ninstall git on the machine on which I’m running Rstudio and storing the site pages\ntell Rstudio that git is there and is to be used for “version control”, i.e. automatic backing up of all changes that you “commit” keeping a full historical archive of the changes\ncreated a free personal account on gitHub (https://github.com/cpsyctc/) and create a respository in it (https://github.com/cpsyctc/Rblog)\ncreated a personal token which works instead of a password to log into my repository there and makes sure that I’m the only one who can write things to that repository (but anyone can download, “pull” in git terminology, from it) (I have now discovered from https://usethis.r-lib.org/reference/use_github.html that these bits might have bee expedited with a )\nuse that to “push” each new committed update to the pages to that github repository\ninstall git on my web server (pretty sure my ISP had already done this actually)\n[this bit, and the next, are linux specific but could be done, though the terminology is different, in Windoze] create a little shell script on the server that “pulls” a copy of the repository content down to the server from github (git handles the tracking of changes and makes sure that only the minimum necessary material is stored and transferred) and uses rsync to copy things to the web pages (rsync, a bit like git, will only copy changed files)\nput a call into crontab to run that little script every ten minutes\nSo I’ve now got a site/blog developing here as an Rstudio project that I can commit and push to github (where anyone can pull it if they want it) and which then automatically updates my server, at slowest, ten minutes later.\nNow I need to spend a bit more time creating more content but perhaps I’ll browse some other people’s examples first: see https://pkgs.rstudio.com/distill/articles/examples.html.\nHow to get images into the preview\n[Added 7.ii.21] I couldn’t work out how to get an ordinary image into listing of “posts” in the base of the “blog” but, courtesy of Shannon Pileggi of the excellent https://www.pipinghotdata.com/ site she created with Distill, I now have the trick: put the graphic in the directory holding the post and put a line in the yaml pointing to it. So here’s the YAML header of the Rmarkdown file that creates this page:\n---\ntitle: \"How I've done this\"\ndescription: |\n  Just documenting how I have created these pages\nbase_url: https://www.psyctc.org/psyctc/Rblog/\npreview: distill.png\nauthor:\n  - name: Chris Evans\n    url: https://www.psyctc.org/R_blog/\n    affiliation: PSYCTC.org\n    affiliation_url: https://www.psyctc.org/psyctc/\n    orcid_id: 0000-0002-4197-4202\n\ndate: 2021-02-06\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3\n    self_contained: false\n---\nYou see the crucial preview: distill.png (I downloaded the graphic from https://blog.rstudio.com/2020/12/07/distill/distill.png). That’s it: thanks Shannon! Shannon also pointed me to her public github repository at https://github.com/shannonpileggi which has all the code for her blog at https://github.com/shannonpileggi/pipinghotdata_distill … I should have been able to find that without Emailing her.\n\n\n\n",
    "preview": "posts/2021-02-06-how-ive-done-this/distill.png",
    "last_modified": "2021-02-07T09:25:39+00:00",
    "input_file": {},
    "preview_width": 2521,
    "preview_height": 2911
  },
  {
    "path": "posts/2021-01-27-bootstrapspearman/",
    "title": "Bootstrap_Spearman",
    "description": "A quick exploration of bootstrapping a Spearman and why you might, or might not, want it.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [],
    "contents": "\n\nContents\nBootstrapping Spearman correlation coefficient\n\nBootstrapping Spearman correlation coefficient\nTraditionally people used the Spearman correlation coefficient where the sample observed distributions of the variables being correlated were clearly not Gaussian. The logic is that as the Spearman correlation is a measure of correlation between the ranks of the values, the distribution of the scores, population or sample, was irrelevant to any inferential interpretation of the Spearman correlation. By contrast, inference about the statistical (im)probability of a Pearson correlation, or a confidence interval (CI) around an observed correlation, was based on maths which assumed that population values were Gaussian. This is simply and irrevocably true: so if the distributions of your sample scores are way off Gaussian then the p values and CIs for a Pearson can be very misleading.\nThe logic of doing a test of fit to Gaussian on your sample data (univariate and/or bivariate test of fit) is dodgy as if your sample is small then even a large deviation from Gaussian that may give very poor p values and CIs has a fair risk of not being flagged as statistically significantly different from Gaussian and with a large sample, even trivial deviations from Gaussian that would have no effect on the p values and CIs will show as statistically significant. How severe that problem is should really have simulation exploration and I haven’t searched for that but the theoretical/logical problem is crystal clear.\nKeen supporters of non-parametrical statistical methods sometimes argued, reasonably to my mind, that the simple answer was to use non-parametric tests regardless of sample distributions, their opponents argued that this threw away some statistical power: true but the loss wasn’t huge.\nAll this has been fairly much swept away, again, to my mind, by the arrival of bootstrapping which allows you, for anything above a very small sample and for pretty much all but very, very weird distributions, to get pretty robust CIs around observed Pearson correlations regardless of the distributions, population or sample distributions, of the variables.\nBecause of this I now report Pearson correlations with bootstrap CIs around them for any correlations unless there is something very weird about the data. This has all the advantages of CIs over p values and is robust to most distributions.\nHowever, I often want to compare with historical findings (including my own!) that were reported using Spearman’s correlation so I still often report Spearman correlations. However, there is no simple parametric CI for the Spearman correlation and I’m not sure there should be outside the edge case where you have perfect ranking (i.e. no ties on either variable). Then the Spearman correlation is the Pearson correlation of the ranks and I think the approximation of using the parametric Pearson CI computation for the Spearman is probably sensible. I am not at all sure that once you have ties that you can apply the same logic though probably putting in n as the lower of the number of distinct values of the two variables probably gives a safe but often madly wide CI. (“Safe” in the sense that it will include the true population correlation 95% of the time (assuming that you are computing the usual 95% CI)).\nFortunately, I can see reason why the bootstrap cannot be used to find a CI around an observed Spearman correlation and this is what I do now when I am reporting a Spearman correlation.\n\n\nShow code\n\ngetCISpearmanTxt <- function(x, bootReps = 999, conf = .95, digits = 2) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  in format \"rho (LCL to UCL)\"\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  ### digits sets the rounding\n  require(boot) # need boot package!\n  ### now we need a function that \n  spearmanForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i, 1], \n        x[i, 2],\n        method = \"spearman\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = spearmanForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### get observed Spearman correlation and confidence limits as vector\n  retVal <- (c(tmpBootRes$t0,\n           tmpCI$percent[4],\n           tmpCI$percent[5]))\n  ### round that\n  retVal <- round(retVal, digits)\n  ### return it as a single character variable\n  retVal <- paste0(retVal[1],\n                   \" (\",\n                   retVal[2],\n                   \" to \",\n                   retVal[3],\n                   \")\")\n  retVal\n}\n\ngetCISpearmanList <- function(x, bootReps = 999, conf = .95) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  returns a list with items obsCorr, LCL and UCL\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  require(boot) # need boot package!\n  ### now we need a function that \n  spearmanForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"spearman\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = spearmanForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### return observed Spearman correlation and confidence limits as a list\n  retVal <- list(obsCorrSpear = as.numeric(tmpBootRes$t0),\n                 LCLSpear = tmpCI$percent[4],\n                 UCLSpear = tmpCI$percent[5])\n  retVal\n}\n\ngetCIPearsonTxt <- function(x, bootReps = 999, conf = .95, digits = 2) {\n  ### function to give bootstrap CI around bivariate PearsonR\n  ###  in format \"R (LCL to UCL)\"\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  ### digits sets the rounding\n  require(boot) # need boot package!\n  ### now we need a function that \n  pearsonForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"pearson\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = pearsonForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### get observed Spearman correlation and confidence limits as vector\n  retVal <- (c(tmpBootRes$t0,\n           tmpCI$percent[4],\n           tmpCI$percent[5]))\n  ### round that\n  retVal <- round(retVal, digits)\n  ### return it as a single character variable\n  retVal <- paste0(retVal[1],\n                   \" (\",\n                   retVal[2],\n                   \" to \",\n                   retVal[3],\n                   \")\")\n  retVal\n}\n\ngetCIPearsonList <- function(x, bootReps = 999, conf = .95) {\n  ### function to give bootstrap CI around bivariate Spearman rho\n  ###  returns a list with items obsCorr, LCL and UCL\n  ### expects input data in a two column matrix, data frame or tibble: x\n  ### bootReps, surprise, surprise, sets the number of bootstrap replications\n  ### conf sets the width of the confidence interval (.95 = 95%)\n  require(boot) # need boot package!\n  ### now we need a function that \n  pearsonForBoot <- function(x,i) {\n    ### function for use bootstrapping Spearman correlations\n    cor(x[i,1], \n        x[i,2],\n        method = \"pearson\",\n        use = \"pairwise.complete.obs\")\n  }\n  ### now use that to do the bootstrapping\n  tmpBootRes <- boot(x, statistic = pearsonForBoot, R = bootReps)\n  ### and now get the CI from that, I've used the percentile method\n  tmpCI <- boot.ci(tmpBootRes, type = \"perc\", conf = conf)\n  ### return observed Spearman correlation and confidence limits as a list\n  retVal <- list(obsCorrPears = as.numeric(tmpBootRes$t0),\n                 LCLPears = tmpCI$percent[4],\n                 UCLPears = tmpCI$percent[5])\n  retVal\n}\n\n\n\n\n\nShow code\n\n### generate some Gaussian and some non-Gaussian data\nn <- 5000 # sample size\nset.seed(12345) # get replicable results\n\nas_tibble(list(x = rnorm(n),\n               y = rnorm(n))) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) %>%\n  summarise(absMinVal = abs(min(value))) %>%\n  pull() -> varMinVal\n\ntibDat %>%\n  mutate(xSquared = x^2,\n         ySquared = y^2,\n         xLn = log(x + varMinVal + 0.2),\n         yLn = log(y + varMinVal + 0.2),\n         xInv = 1/(x + varMinVal + 0.1),\n         yInv = 1/(y + varMinVal + 0.1)) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) -> tibDatLong\n\nggplot(data = tibDatLong,\n       aes(x = value)) +\n  facet_wrap(facets = vars(name),\n             ncol = 2,\n             scales = \"free\",\n             dir = \"v\") +\n  geom_histogram(bins = 100) +\n  theme_bw()\n\n\n\nShow code\n\nggpairs(tibDat)\n\n\n\n\nGood! Got some weird variables there: x and y are Gaussian random variables and uncorrelated then we have their squares, a natural log (after adding enough to the values to avoid trying to get ln(0)) and their inverses (with the same tweak to avoid getting 1/0).\n\n\nShow code\n\noptions(dplyr.summarise.inform = FALSE)\ntibDatLong %>%\n  mutate(id = (1 + row_number()) %/% 2 ,\n         var = str_sub(name, 1, 1),\n         transform = str_sub(name, 2, 20),\n         transform = if_else(transform == \"\", \"none\", transform),\n         transform = ordered(transform,\n                             levels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"),\n                             labels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"))) %>%\n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = var) -> tibDatLong2\n\ntibDatLong2 %>%\n  group_by(transform) %>%\n  select(x, y) %>%\n  summarise(corrS = list(getCISpearmanList(cur_data())),\n            corrP = list(getCIPearsonList(cur_data()))) %>%\n  unnest_wider(corrS) %>%\n  unnest_wider(corrP) %>% \n  pander(justify = \"lrrrrrr\", split.tables = Inf)\n\n\nAdding missing grouping variables: transform\ntransform\nobsCorrSpear\nLCLSpear\nUCLSpear\nobsCorrPears\nLCLPears\nUCLPears\nnone\n-0.03547\n-0.06164\n-0.005442\n-0.02368\n-0.0495\n0.003609\nLn\n-0.03547\n-0.06259\n-0.008897\n-0.01938\n-0.04774\n0.01032\nInv\n-0.03547\n-0.06106\n-0.006073\n0.0001815\n-0.0324\n0.0382\nSquared\n-0.00467\n-0.03253\n0.02274\n-0.009833\n-0.03985\n0.02017\n\nThat’s what we would expect to see: the observed Spearman correlations are the same for the raw data, the ln and inv transformed values (as these are transforms that preserve monotonic, i.e. ranked, ordered, relationships between values while changing the values a lot) but the value is different for the squared transform as that’s not monotonic. The values for the Pearson correlations change with ln and inv transforming as they should as the correlations between the transformed values are not the same as between the raw values. The CIs for the Spearman raw and ln and inv transformed values are not quite identical because the bootstrapping will have produced different bootstrapped samples for each. (I think there’s a way I could have got all three in the same call to boot() but that would have needed a different function to bootstrap.)\nReassuring that all the CIs include zero: you’d hope so really with n = 5000 and uncorrelated raw values.\nNow let’s get a moderately correlated pair of variables.\n\n\nShow code\n\n### generate some Gaussian and some non-Gaussian data\nn <- 5000 # sample size\nset.seed(12345) # get replicable results\n\ntibDat %>% \n  mutate(y = x + y) %>%\n  select(x, y) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) %>%\n  summarise(absMinVal = abs(min(value))) %>%\n  pull() -> varMinVal\n\ntibDat %>%\n  mutate(xSquared = x^2,\n         ySquared = y^2,\n         xLn = log(x + varMinVal + 0.2),\n         yLn = log(y + varMinVal + 0.2),\n         xInv = 1/(x + varMinVal + 0.1),\n         yInv = 1/(y + varMinVal + 0.1)) -> tibDat\n\ntibDat %>%\n  pivot_longer(cols = everything()) -> tibDatLong\n\nggplot(data = tibDatLong,\n       aes(x = value)) +\n  facet_wrap(facets = vars(name),\n             ncol = 2,\n             scales = \"free\",\n             dir = \"v\") +\n  geom_histogram(bins = 100) +\n  theme_bw()\n\n\n\nShow code\n\nggpairs(tibDat)\n\n\n\n\n\n\nShow code\n\noptions(dplyr.summarise.inform = FALSE)\ntibDatLong %>%\n  mutate(id = (1 + row_number()) %/% 2 ,\n         var = str_sub(name, 1, 1),\n         transform = str_sub(name, 2, 20),\n         transform = if_else(transform == \"\", \"none\", transform),\n         transform = ordered(transform,\n                             levels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"),\n                             labels = c(\"none\", \"Ln\", \"Inv\", \"Squared\"))) %>%\n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = var) -> tibDatLong2\n\ntibDatLong2 %>%\n  group_by(transform) %>%\n  select(x, y) %>%\n  summarise(corrS = list(getCISpearmanList(cur_data())),\n            corrP = list(getCIPearsonList(cur_data()))) %>%\n  unnest_wider(corrS) %>%\n  unnest_wider(corrP) %>% \n  pander(justify = \"lrrrrrr\", split.tables = Inf)\n\n\nAdding missing grouping variables: transform\ntransform\nobsCorrSpear\nLCLSpear\nUCLSpear\nobsCorrPears\nLCLPears\nUCLPears\nnone\n0.666\n0.6485\n0.6833\n0.6906\n0.6755\n0.7049\nLn\n0.666\n0.6496\n0.683\n0.679\n0.6625\n0.6956\nInv\n0.666\n0.6488\n0.6823\n0.279\n0.2516\n0.6728\nSquared\n0.3559\n0.3311\n0.3815\n0.4992\n0.4617\n0.5347\n\nGreat: exactly what we’d expect again.\n\n\n\n",
    "preview": "posts/2021-01-27-bootstrapspearman/bootstrapspearman_files/figure-html5/simulate1-1.png",
    "last_modified": "2021-01-30T18:12:00+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-27-handling-overprinting/",
    "title": "Handling overprinting",
    "description": "This is the first of my blog posts here, about the issue of overprinting and some ways to handle it \nusing R and ggplot().  There's a small spin off topic on the impact on bivariate correlations and on\nlinear regression of discretising continuous variables.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [],
    "contents": "\n\nContents\nOverprinting\nJittering\nUsing transparency\nUsing area to show counts: geom_count()\nHow do those methods work with the original data?\nJittering\nTransparency\nUsing area: geom_count()\n\n\nTangential issue: impact of discretising on relationship between variables\nQuestions and feedback\nTechnical footnote and thanks\nLicensing\n\nOverprinting\nOverprinting is where one point on a graph overlies another. It’s mainly a problem with scattergrams and if you have large numbers of points and few discrete values it can make a plot completely misleading. OK, let’s make up some data.\n\nI am showing the raw R within the Rmarkdown code blocks. I have tried to comment things liberally. Click on “Show code” to see the code.\n\n\nShow code\n\nn <- 5000 # a lot of points means that overprinting is inevitable \nnVals <- 5 # discretising continuous variables to this number of values (below) makes it even more certain\nset.seed(12345) # ensures we get the same result every time \n\n### now generate x and y variables as a tibble\nas_tibble(list(x = rnorm(n),\n               y = rnorm(n))) -> tibDat\n\n### create strong correlation between them by adding x to y (!)\ntibDat %>%\n  mutate(y = x + y) -> tibDat\n\n### now we want to discretise into equiprobable scores so find the empirical quantiles\nvecXcuts <- quantile(tibDat$x, probs = seq(0, 1, 1/nVals))\nvecYcuts <- quantile(tibDat$y, probs = seq(0, 1, 1/nVals))\n\n### now use those to transform the raw variables to equiprobable scores in range 1:5\ntibDat %>%\n  mutate(x5 = cut(x, breaks = vecXcuts, include.lowest = TRUE, labels = FALSE, right = TRUE),\n         y5 = cut(y, breaks = vecYcuts, include.lowest = TRUE, labels = FALSE, right = TRUE)) -> tibDat\n\n\n\nNow let’s have a simple scatterplot.\n\n\nShow code\n\n### use ggplot to generate the simple scattergram for the raw variables\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nThe relationship between the two variables is clear but we don’t know about any overprinting. We can add a loess smoothed regression which clarifies the relationship between the scores but doesn’t resolve the overprinting issue.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth() + # adding the loess smoother\n  theme_bw()\n\n\n\n\nHowever, to really drive home the point about overprinting, if those points are transformed and discretised to five equiprobable scores then things look like this.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) + # use discretised variables instead of raw variables\n  geom_point() +\n  theme_bw()\n\n\n\n\nWhoops: much overprinting as 5000 points have collapsed to 25 visible points on the scattergram but we can’t see how much and no apparent relationship between the variables at all.\nAgain we can add a regression to that plot for amusement and to show that the transform hasn’t removed the relationship. (Has to be a linear regression as the number of distinct points doesn’t allow for loess smoothing.)\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") + # linear regression fit\n  theme_bw()\n\n\n\n\nJittering\nOne way around overprinting it is to jitter the points. Here I have used geom_jitter(width = .2, height = .2) which adds random “jittering” to both x and y values spread across .2 of the “implied bins”. I’ve left the raw data in in blue.\nThere are situations in which you just want jittering on one axis and not the other so you can use geom_jitter(width = .2). Sometimes playing around with width helps get the what seems the best visual fit to the counts.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_jitter(width = .2, height = .2) + # jittered data\n  geom_point(data = tibDat,\n             aes(x = x5, y = y5),\n             colour = \"blue\") +\n  theme_bw()\n\n\n\n\nUsing transparency\nAnother approach is to use transparency. Here you just have the one parameter, alpha and again, sometimes you need to play with different values.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_point(alpha = .01) +\n  theme_bw()\n\n\n\n\nThat’s not working terribly well as we have so many points (n = 5000).\nUsing area to show counts: geom_count()\nAnd another approach, good when values are widely spaced as here, is geom_count().\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = x5, y = y5)) +\n  geom_count() +\n  scale_size_area(n.breaks = 10) +\n  theme_bw()\n\n\n\n\nI used a rather excessive number of breaks there but it makes the point.\nHow do those methods work with the original data?\nIn this next set of blocks I’ve applied the same three tricks but to both the raw data and the discretised data.\nJittering\n\n\nShow code\n\n### reshape data to make it easy to get plots side by side using facetting\ntibDat %>%\n  ### first pivot longer \n  pivot_longer(cols = everything()) %>%\n  ### which gets something like this\n  #   # A tibble: 20,000 x 2\n  #    name    value\n  #    <chr>   <dbl>\n  #  1 x      0.586 \n  #  2 y     -0.107 \n  #  3 x5     4     \n  #  4 y5     3     \n  #  5 x      0.709 \n  #  6 y      1.83  \n  #  7 x5     4     \n  #  8 y5     5     \n  #  9 x     -0.109 \n  # 10 y      0.0652\n  # # … with 19,990 more rows\n  ### now get new variables one for x and y\n  mutate(variable = str_sub(name, 1, 1),\n       ### and one for the transform\n       transform = str_sub(name, 2, 2),\n       transform = if_else(transform == \"5\", \"discretised\", \"raw\"),\n       transform = factor(transform,\n                          levels = c(\"raw\", \"discretised\")),\n       ### create an id variable clumping each set of four scores together\n       id = (3 + row_number()) %/% 4) %>%\n  ### so now we can pivot back \n  pivot_wider(id_cols = c(id, transform), values_from = value, names_from = variable) -> tibDat2\n### to get this\n# A tibble: 10,000 x 4\n#       id transform        x       y\n#    <dbl> <chr>        <dbl>   <dbl>\n#  1     1 raw          0.586 -0.107 \n#  2     1 discretised  4      3     \n#  3     2 raw          0.709  1.83  \n#  4     2 discretised  4      5     \n#  5     3 raw         -0.109  0.0652\n#  6     3 discretised  3      3     \n#  7     4 raw         -0.453 -2.42  \n#  8     4 discretised  2      1     \n#  9     5 raw          0.606 -1.04  \n# 10     5 discretised  4      2     \n# # … with 9,990 more rows\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n  geom_jitter(width = .2, height = .2)  +\n  facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\n\nAmusing! I’ve put the linear regression fit line on both.\nTransparency\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n  facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_point(alpha = .01) +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\n\nTransparency that works on the right for the discretised data, well works up to a point, is a bit too thin for the raw data. I can’t see that as of now (26.i.21 and ggplot version 3.3.3) that you can map transparency, i.e. alpha to a variable as you can, say, for colour. So to get a side-by-side plot I’m using a different approach. There are various ways of doing this, a useful page seems to be: http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/81-ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page/\n\n\nShow code\n\nggplot(data = filter(tibDat2, transform == \"raw\"), # select just the raw data\n       aes(x = x, y = y)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() -> tmpPlot1\n\n\nggplot(data = filter(tibDat2, transform == \"discretised\"), # select just the raw data\n       aes(x = x, y = y)) +\n  geom_point(alpha = .01) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() -> tmpPlot2\n\n### use ggarrange from the ggpubr package, see the URL for other options\nggpubr::ggarrange(tmpPlot1, tmpPlot2)\n\n\n\n\nUsing area: geom_count()\n\n\nShow code\n\nggplot(data = tibDat2,\n       aes(x = x, y = y)) +\n    facet_wrap(facets = vars(transform),\n             ncol = 2,\n             scales = \"free\") +\n  geom_count() +\n  geom_smooth(method = \"lm\") +\n  scale_size_area(n.breaks = 10) +\n  theme_bw()\n\n\n\n\nI used a rather excessive number of breaks there but it makes the point.\nTangential issue: impact of discretising on relationship between variables\n\n\nShow code\n\nvalRawCorr <- cor(tibDat$x, tibDat$y)\nvalDisc5Corr <- cor(tibDat$x5, tibDat$y5)\n\nvecRawCorrCI <- cor.test(tibDat$x, tibDat$y)$conf.int\nvecDisc5CorrCI <- cor.test(tibDat$x5, tibDat$y5)$conf.int\n\n### or here's another, tidyverse way to do this\n### seems like unnecessary faff except that it makes \n### it so easy to do a micro forest plot (see below)\ngetParmPearsonCI <- function(x, y){\n  ### little function to get parametric 95% CI from two vectors\n  obsCorr <- cor(x, y)\n  tmpCI <- cor.test(x, y)$conf.int\n  return(list(LCL = tmpCI[1],\n              obsCorr = obsCorr,\n              UCL = tmpCI[2]))\n}\ntibDat2 %>%\n  group_by(transform) %>%\n  summarise(pearson = list(getParmPearsonCI(x, y))) %>%\n  unnest_wider(pearson) -> tibCorrs\n### which gives us this\n# tibCorrs\n# # A tibble: 2 x 4\n#   transform     LCL obsCorr   UCL\n#   <fct>       <dbl>   <dbl> <dbl>\n# 1 raw         0.676   0.691 0.705\n# 2 discretised 0.609   0.626 0.643\n\n\n\nThe correlation between the original variables is 0.691 with parametric 95% confidence interval (CI) from 0.676 to 0.705 whereas that between the discretised variables is 0.626 with 95% CI from 0.609 to 0.643 so some clear attenuation there. Micro forest plot of that:\n\n\nShow code\n\nggplot(data = tibCorrs,\n       aes(x = transform, y = obsCorr)) +\n  geom_point() +\n  geom_linerange(aes(ymin = LCL, ymax = UCL)) +\n  ylim(.5, 1) +\n  theme_bw()\n\n\n\n\nYup, that’s a fairly large and clear difference on a y scale from .5 to 1.0. What about the linear regression?\n\n\nShow code\n\n### raw variables\nlm(scale(y) ~ scale(x), data = tibDat)\n\n\n\nCall:\nlm(formula = scale(y) ~ scale(x), data = tibDat)\n\nCoefficients:\n(Intercept)     scale(x)  \n  1.066e-17    6.906e-01  \n\nShow code\n\n### discretised variables\nlm(scale(y5) ~ scale(x5), data = tibDat)\n\n\n\nCall:\nlm(formula = scale(y5) ~ scale(x5), data = tibDat)\n\nCoefficients:\n(Intercept)    scale(x5)  \n -1.726e-17    6.265e-01  \n\nShow code\n\n### or tidyverse way\n### I confess I haven't really got my head aound the broomverse but this is powerful\ntibDat2 %>% \n  group_by(transform) %>%\n  do(broom::tidy(lm(scale(y) ~ scale(x), data = .))) %>%\n  pander(justify = \"llrrrr\", split.tables = Inf)\n\n\ntransform\nterm\nestimate\nstd.error\nstatistic\np.value\nraw\n(Intercept)\n1.066e-17\n0.01023\n1.042e-15\n1\nraw\nscale(x)\n0.6906\n0.01023\n67.51\n0\ndiscretised\n(Intercept)\n-1.726e-17\n0.01102\n-1.566e-15\n1\ndiscretised\nscale(x)\n0.6265\n0.01102\n56.83\n0\n\nOh dear, oh dear! I think I should have known that the standardised regression (slope) coefficients of a simple, two variable linear regression are the Pearson correlations!\nQuestions and feedback\nhttps://www.psyctc.org/psyctc/ web site. In most browsers I think that will open in a new page and if you close it when you have sent your message I think that and most browsers will bring you back here.\nTechnical footnote and thanks\nThis has been created using the distill package in R (and in Rstudio). Distill is a publication format for scientific and technical writing, native to the web. There is a bit more information about distill at https://rstudio.github.io/distill but I found the youtube (ugh) presentation by Maëlle Salmon at https://www.youtube.com/watch?v=Xyc4-bJjdys much more useful than the very minimal notes at that github page. After watching (the first half of) that presentation the github documentation becomes useful.\nLicensing\nAs with most things I put on the web, I am putting this under the Creative Commons Attribution Share-Alike licence.\n\n\n\n",
    "preview": "posts/2021-01-27-handling-overprinting/handling-overprinting_files/figure-html5/scatter1-1.png",
    "last_modified": "2021-01-27T16:31:46+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-27-nudgingonaxes/",
    "title": "Nudging groupings on plot axes",
    "description": "How to nudge categories on an axis of a ggplot plot.",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [],
    "contents": "\n\nContents\nHow to “nudge” plots\n\nHow to “nudge” plots\nI can never remember how to do this and keep looking it up. Emily asked me about it so I thought I should crack it and make a file about it to remind myself.\nI’m going to use a little function to get bootstrap confidence intervals around observed means so here’s the code for that.\n\nI am showing the raw R within the Rmarkdown code blocks. I have tried to comment things liberally. Click on “Show code” to see the code.\n\n\nShow code\n\n### function using boot() and boot.ci() from the the boot package to get bootstrap CIs around observed means\ngetCIbootMean <- function(data, ciInt = .95, bootReps = 1000){\n  getMeanForBoot <- function(dat, ind) {mean(dat[ind])} # ind indexes the particular bootstrap sample of vector dat\n  tmpRes <- boot::boot(data, getMeanForBoot, R = bootReps)  # gets the boostrap results\n  tmpCI <- boot::boot.ci(tmpRes, type =  \"perc\")$percent[1,4:5] # gets the percentile method CI\n  return(list(LCL = tmpCI[1],\n              obsMean = tmpRes$t0,\n              UCL = tmpCI[2]))\n}\n# getCIbootMean(1:30) # testing!\n\n\n\nNow let’s get some demonstation data.\n\n\nShow code\n\nn <- 500 # sample size\nset.seed(1245) # get same result every run\ntibble(genderNum = sample(0:1, n, replace = TRUE), # generate gender\n       ageNum = sample(13:17, n, replace = TRUE), # generate age\n       gender = if_else(genderNum == 1, \"F\", \"M\"),\n       score = rnorm(n) + # get randomness unsystematically related to gender or age\n         genderNum*.1*rnorm(n) + # add a simple gender effect\n         ageNum*.1*rnorm(n) + # add a simple age effect\n         (genderNum*(ageNum - 15)*.5*rnorm(n))^2 + # and an interaction\n         20, # make sure values are positive\n       age = as.factor(ageNum)) %>%\n  group_by(age, gender) %>%\n  summarise(mean = list(getCIbootMean(score))) %>%\n  unnest_wider(mean) -> tibDat\n\n\n\nHere’s a crude way to separate things by nudging them on the x axis.\n\n\nShow code\n\nggplot(data = tibDat,\n       aes(x = interaction(age, gender), y = obsMean, colour = gender)) +\n       geom_point() +\n       geom_linerange(aes(ymin = LCL, ymax = UCL))\n\n\n\n\nBut that’s aesthetically and informatively rubbish as it’s not reflecting the grouping. I think what we want is something like this.\n\n\nShow code\n\nvalXdodge = .25 # setting it here makes it easier to try different values when you have multiple geoms you want to dodge\nggplot(data = tibDat,\n       aes(x = age, y = obsMean, colour = gender, group = gender)) + # key thing is that dodging is by the grouping\n  geom_point(position = position_dodge2(width = valXdodge)) +\n  geom_linerange(aes(ymin = LCL, ymax = UCL),\n                 position = position_dodge(width = valXdodge)) \n\n\n\n\nI think “nudge” would have been a much better term than “dodge” but that may be because dodging has a particular meaning in manual printing of photos (where it’s all about changing the darkness of particular areas of the image) which was something I learned about long, long ago.\nI also think the help for dodge is truly awful and is compounded by the fact that dodging works differently depending on the geom you are using (I’ve been lazy and not gotten to the bottom of that but the basic issue is that it works differently for geom_bar() and geom_histogram() where I think it assumes that the x aesthetic is a grouping whereas with geom_point(), geom_linerange() and geom_errorbar() (and probably geom_line()) it needs to be told the grouping on which you are dodging.\nNotwithstanding my grousing, it’s incredibly useful for depicting things. I guess it has something in common with my previous post here https://www.psyctc.org/Rblog/posts/2021-01-27-handling-overprinting/ as both tricks have in common that they actually distort the literal mappings to create mappings that are far more informative and less misleading than the simply “accurate” mapping.\n\n\n\n",
    "preview": "posts/2021-01-27-nudgingonaxes/nudgingonaxes_files/figure-html5/plot1-1.png",
    "last_modified": "2021-01-27T18:34:40+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to CE blog test",
    "description": "Welcome to my blog which I hope will be useful to people using R to analyse data",
    "author": [
      {
        "name": "Chris Evans",
        "url": "https://www.psyctc.org/R_blog/"
      }
    ],
    "date": "2021-01-27",
    "categories": [],
    "contents": "\n\n\n\nWelcome to this first attempt to create a blog that should provide easy access to some Rmarkdown files I’m creating that show R tricks I’ve found useful.\nDo contact me if you find issues with them or have questions or suggestions. To do that, use the contact me form on my https://www.psyctc.org/psyctc/ web site. In most browsers I think that will open in a new page and if you close it when you have sent your message I think that and most browsers will bring you back here.\n\n\n\n",
    "preview": "posts/welcome/welcome_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-02-07T09:14:40+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
